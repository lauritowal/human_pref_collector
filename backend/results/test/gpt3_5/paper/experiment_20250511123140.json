{
    "timestamp": "20250511123140",
    "username": "test",
    "email": "test@fsdfds.de",
    "model": "gpt3_5",
    "category": "paper",
    "totalLLMChoices": 3,
    "totalHumanChoices": 2,
    "totalNoPreference": 0,
    "userChoices": [
        {
            "index": 0,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to textprocessing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-grain modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.",
                    "filename": "Statistical_Decision-Tree_Models_For_Parsing",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P95-1037/Documents_xml/P95-1037.xml",
                    "title": "Statistical Decision-Tree Models For Parsing"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces statistical decision-tree models for parsing natural language sentences. The paper focuses on automatically discovering disambiguation criteria for parsing decisions using features like words, word relationships, and constituent relationships. The decision tree models are used to estimate probabilities for each parsing decision, allowing for non-deterministic decision-making based on probabilities. The SPATTER parser is developed based on decision-tree models, showing promising results when compared to a grammar-based parser on common tasks. Experiments on different domains, such as IBM Computer Manuals and the Wall Street Journal, demonstrate SPATTER's ability to parse accurately without complex grammar rules. The paper concludes that statistical models for parsing, utilizing decision-tree technology, can effectively handle the complexities of natural language parsing without the need for extensive linguistic knowledge."
                    ],
                    "filename": "statistical_decisiontree_models_-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 184 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Statistical Decision-Tree Models For Parsing</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents.</S>\n    <S sid=\"7\" ssid=\"2\">Traditionally, disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process.</S>\n    <S sid=\"8\" ssid=\"3\">However, these approaches have proved too brittle for most interesting natural language problems.</S>\n    <S sid=\"9\" ssid=\"4\">This work addresses the problem of automatically discovering the disambiguation criteria for all of the decisions made during the parsing process, given the set of possible features which can act as disambiguators.</S>\n    <S sid=\"10\" ssid=\"5\">The candidate disambiguators are the words in the sentence, relationships among the words, and relationships among constituents already constructed in the parsing process.</S>\n    <S sid=\"11\" ssid=\"6\">Since most natural language rules are not absolute, the disambiguation criteria discovered in this work are never applied deterministically.</S>\n    <S sid=\"12\" ssid=\"7\">Instead, all decisions are pursued non-deterministically according to the probability of each choice.</S>\n    <S sid=\"13\" ssid=\"8\">These probabilities are estimated using statistical decision tree models.</S>\n    <S sid=\"14\" ssid=\"9\">The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (c11) conditioned on all previous decisions: Each decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability.</S>\n    <S sid=\"15\" ssid=\"10\">By combining a stack decoder search with a breadthfirst algorithm with probabilistic pruning, it is possible to identify the highest-probability parse for any sentence using a reasonable amount of memory and time.</S>\n    <S sid=\"16\" ssid=\"11\">The claim of this work is that statistics from a large corpus of parsed sentences combined with information-theoretic classification and training algorithms can produce an accurate natural language parser without the aid of a complicated knowledge base or grammar.</S>\n    <S sid=\"17\" ssid=\"12\">This claim is justified by constructing a parser, called SPATTER (Statistical PATTErn Recognizer), based on very limited linguistic information, and comparing its performance to a state-of-the-art grammar-based parser on a common task.</S>\n    <S sid=\"18\" ssid=\"13\">It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application.</S>\n    <S sid=\"19\" ssid=\"14\">This will be the subject of future experiments.</S>\n    <S sid=\"20\" ssid=\"15\">One of the important points of this work is that statistical models of natural language should not be restricted to simple, context-insensitive models.</S>\n    <S sid=\"21\" ssid=\"16\">In a problem like parsing, where long-distance lexical information is crucial to disambiguate interpretations accurately, local models like probabilistic context-free grammars are inadequate.</S>\n    <S sid=\"22\" ssid=\"17\">This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions, and which have few enough parameters to be trained using existing resources.</S>\n    <S sid=\"23\" ssid=\"18\">I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models.</S>\n    <S sid=\"24\" ssid=\"19\">Then I briefly describe the training and parsing procedures used in SPATTER.</S>\n    <S sid=\"25\" ssid=\"20\">Finally, I present some results of experiments comparing SPATTER with a grammarian's rulebased statistical parser, along with more recent results showing SPATTER applied to the Wall Street Journal domain.</S>\n  </SECTION>\n  <SECTION title=\"2 Decision-Tree Modeling\" number=\"2\">\n    <S sid=\"26\" ssid=\"1\">Much of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms.</S>\n    <S sid=\"27\" ssid=\"2\">The decisions under consideration involve identifying constituents and constituent labels in natural language sentences.</S>\n    <S sid=\"28\" ssid=\"3\">Grammarians, the human decision-makers in parsing, solve this problem by enumerating the features of a sentence which affect the disambiguation decisions and indicating which parse to select based on the feature values.</S>\n    <S sid=\"29\" ssid=\"4\">The grammarian is accomplishing two critical tasks: identifying the features which are relevant to each decision, and deciding which choice to select based on the values of the relevant features.</S>\n    <S sid=\"30\" ssid=\"5\">Decision-tree classification algorithms account for both of these tasks, and they also accomplish a third task which grammarians classically find difficult.</S>\n    <S sid=\"31\" ssid=\"6\">By assigning a probability distribution to the possible choices, decision trees provide a ranking system which not only specifies the order of preference for the possible choices, but also gives a measure of the relative likelihood that each choice is the one which should be selected.</S>\n    <S sid=\"32\" ssid=\"7\">A decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision: P(f Ih), where f is an element of the future vocabulary (the set of choices) and h is a history (the context of the decision).</S>\n    <S sid=\"33\" ssid=\"8\">This probability P(flh) is determined by asking a sequence of questions qi q2...q„ about the context, where the ith question asked is uniquely determined by the answers to the i —1 previous questions.</S>\n    <S sid=\"34\" ssid=\"9\">For instance, consider the part-of-speech tagging problem.</S>\n    <S sid=\"35\" ssid=\"10\">The first question a decision tree might ask is: If the answer is the, then the decision tree needs to ask no more questions; it is clear that the decision tree should assign the tag f = determiner with probability 1.</S>\n    <S sid=\"36\" ssid=\"11\">If, instead, the answer to question 1 is bear, the decision tree might next ask the question: If the answer to question 2 is determiner, the decision tree might stop asking questions and assign the tag f = noun with very high probability, and the tag f = verb with much lower probability.</S>\n    <S sid=\"37\" ssid=\"12\">However, if the answer to question 2 is noun, the decision tree would need to ask still more questions to get a good estimate of the probability of the tagging decision.</S>\n    <S sid=\"38\" ssid=\"13\">The decision tree described in this paragraph is shown in Figure 1.</S>\n    <S sid=\"39\" ssid=\"14\">Each question asked by the decision tree is represented by a tree node (an oval in the figure) and the possible answers to this question are associated with branches emanating from the node.</S>\n    <S sid=\"40\" ssid=\"15\">Each node defines a probability distribution on the space of possible decisions.</S>\n    <S sid=\"41\" ssid=\"16\">A node at which the decision tree stops asking questions is a leaf node.</S>\n    <S sid=\"42\" ssid=\"17\">The leaf nodes represent the unique states in the decision-making problem, i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision.</S>\n    <S sid=\"43\" ssid=\"18\">A decision-tree model is not really very different from an interpolated n-gram model.</S>\n    <S sid=\"44\" ssid=\"19\">In fact, they are equivalent in representational power.</S>\n    <S sid=\"45\" ssid=\"20\">The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated.</S>\n    <S sid=\"46\" ssid=\"21\">First, let's be very clear on what we mean by an n-grain model.</S>\n    <S sid=\"47\" ssid=\"22\">Usually, an n-gram model refers to a Markov process where the probability of a particular token being generating is dependent on the values of the previous n — 1 tokens generated by the same process.</S>\n    <S sid=\"48\" ssid=\"23\">By this definition, an n-gram model has I WI&amp;quot; parameters, where IWI is the number of unique tokens generated by the process.</S>\n    <S sid=\"49\" ssid=\"24\">However, here let's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n-1 random variables, P(.flhi h2 .</S>\n    <S sid=\"50\" ssid=\"25\">• . hn-1).</S>\n    <S sid=\"51\" ssid=\"26\">There is no assumption in the definition that any of the random variables F or Hi range over the same vocabulary.</S>\n    <S sid=\"52\" ssid=\"27\">The number of parameters in this n-gram model is IFI H IHil.</S>\n    <S sid=\"53\" ssid=\"28\">Using this definition, an n-gram model can be represented by a decision-tree model with n — 1 questions.</S>\n    <S sid=\"54\" ssid=\"29\">For instance, the part-of-speech tagging model P(t11w2t1—iti-2) can be interpreted as a 4gram model, where HI is the variable denoting the word being tagged, 112 is the variable denoting the tag of the previous word, and H3 is the variable denoting the tag of the word two words back.</S>\n    <S sid=\"55\" ssid=\"30\">Hence, this 4-gram tagging model is the same as a decisiontree model which always asks the sequence of 3 questions: But can a decision-tree model be represented by an n-gram model?</S>\n    <S sid=\"56\" ssid=\"31\">No, but it can be represented by an interpolated n-gram model.</S>\n    <S sid=\"57\" ssid=\"32\">The proof of this assertion is given in the next section.</S>\n    <S sid=\"58\" ssid=\"33\">The standard approach to estimating an n-gram model is a two step process.</S>\n    <S sid=\"59\" ssid=\"34\">The first step is to count the number of occurrences of each n-gram from a training corpus.</S>\n    <S sid=\"60\" ssid=\"35\">This process determines the empirical distribution, The second step is smoothing the empirical distribution using a separate, held-out corpus.</S>\n    <S sid=\"61\" ssid=\"36\">This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information.</S>\n    <S sid=\"62\" ssid=\"37\">A commonly-used technique for smoothing is deleted interpolation._ Deleted interpolation estimates a model P(flhi 112 .</S>\n    <S sid=\"63\" ssid=\"38\">• • hn-1) by using a linear combination of empirical models P(fihki hk, . h,.</S>\n    <S sid=\"64\" ssid=\"39\">), where m &lt; n and k1_1 &lt; ki &lt; n for all i &lt; m. For example, a model P(f Ihi h2h3) might be interpolated as follows: where E Ai (hi h2h3) = 1 for all histories hi h2h3.</S>\n    <S sid=\"65\" ssid=\"40\">The optimal values for the Ai functions can be estimated using the forward-backward algorithm (Baum, 1972).</S>\n    <S sid=\"66\" ssid=\"41\">A decision-tree model can be represented by an interpolated n-gram model as follows.</S>\n    <S sid=\"67\" ssid=\"42\">A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf.</S>\n    <S sid=\"68\" ssid=\"43\">Thus, a leaf node defines a probability distribution based on values of those questions: P(f Ihkihh .</S>\n    <S sid=\"69\" ssid=\"44\">.</S>\n    <S sid=\"70\" ssid=\"45\">. hk..), where m &lt; n and ki_I &lt; k &lt; n, and where hk, is the answer to one of the questions asked on the path from the root to the leaf.'</S>\n    <S sid=\"71\" ssid=\"46\">But this is the same as one of the terms in the interpolated n-gram model.</S>\n    <S sid=\"72\" ssid=\"47\">So, a decision 'Note that in a decision tree, the leaf distribution is not affected by the order in which questions are asked.</S>\n    <S sid=\"73\" ssid=\"48\">Asking about Pi, followed by h2 yields the same future distribution as asking about h2 followed by h1. tree can be defined as an interpolated n-grain model where the Ai function is defined as: 1 if hki hk, hk_ is a leaf, Ai (hki hk, • • • hk,.)</S>\n    <S sid=\"74\" ssid=\"49\">= o otherwise.</S>\n    <S sid=\"75\" ssid=\"50\">The point of showing the equivalence between ngram models and decision-tree models is to make clear that the power of decision-tree models is not in their expressiveness, but instead in how they can be automatically acquired for very large modeling problems.</S>\n    <S sid=\"76\" ssid=\"51\">As n grows, the parameter space for an n-gram model grows exponentially, and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation.</S>\n    <S sid=\"77\" ssid=\"52\">Also, as n grows large, the likelihood that the deleted interpolation process will converge to an optimal or even near-optimal parameter setting becomes vanishingly small.</S>\n    <S sid=\"78\" ssid=\"53\">On the other hand, the decision-tree learning algorithm increases the size of a model only as the training data allows.</S>\n    <S sid=\"79\" ssid=\"54\">Thus, it can consider very large history spaces, i.e. n-gram models with very large n. Regardless of the value of n, the number of parameters in the resulting model will remain relatively constant, depending mostly on the number of training examples.</S>\n    <S sid=\"80\" ssid=\"55\">The leaf distributions in decision trees are empirical estimates, i.e. relative-frequency counts from the training data.</S>\n    <S sid=\"81\" ssid=\"56\">Unfortunately, they assign probability zero to events which can possibly occur.</S>\n    <S sid=\"82\" ssid=\"57\">Therefore, just as it is necessary to smooth empirical ngram models, it is also necessary to smooth empirical decision-tree models.</S>\n    <S sid=\"83\" ssid=\"58\">The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989).</S>\n    <S sid=\"84\" ssid=\"59\">The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984).</S>\n    <S sid=\"85\" ssid=\"60\">For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994).</S>\n    <S sid=\"86\" ssid=\"61\">An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees.</S>\n    <S sid=\"87\" ssid=\"62\">A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values.</S>\n    <S sid=\"88\" ssid=\"63\">For example, a question about a word is represented as 30 binary questions.</S>\n    <S sid=\"89\" ssid=\"64\">These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992).</S>\n    <S sid=\"90\" ssid=\"65\">The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions.</S>\n    <S sid=\"91\" ssid=\"66\">For more discussion of the use of binary decision-tree questions, see (Magerman, 1994).</S>\n  </SECTION>\n  <SECTION title=\"3 SPATTER Parsing\" number=\"3\">\n    <S sid=\"92\" ssid=\"1\">The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process.</S>\n    <S sid=\"93\" ssid=\"2\">A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed.</S>\n    <S sid=\"94\" ssid=\"3\">This pattern recognition process is driven by the decision-tree models described in the previous section.</S>\n    <S sid=\"95\" ssid=\"4\">A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label.</S>\n    <S sid=\"96\" ssid=\"5\">If a parse tree is interpreted as a geometric pattern, a constituent is no more than a set of edges which meet at the same tree node.</S>\n    <S sid=\"97\" ssid=\"6\">For instance, the noun phrase, &amp;quot;a brown cow,&amp;quot; consists of an edge extending to the right from &amp;quot;a,&amp;quot; an edge extending to the left from &amp;quot;cow,&amp;quot; and an edge extending straight up from &amp;quot;brown&amp;quot;.</S>\n    <S sid=\"98\" ssid=\"7\">In SPATTER, a parse tree is encoded in terms of four elementary components, or features: words, tags, labels, and extensions.</S>\n    <S sid=\"99\" ssid=\"8\">Each feature has a fixed vocabulary, with each element of a given feature vocabulary having a unique representation.</S>\n    <S sid=\"100\" ssid=\"9\">The word feature can take on any value of any word.</S>\n    <S sid=\"101\" ssid=\"10\">The tag feature can take on any value in the part-of-speech tag set.</S>\n    <S sid=\"102\" ssid=\"11\">The label feature can take on any value in the non-terminal set.</S>\n    <S sid=\"103\" ssid=\"12\">The extension can take on any of the following five values: right - the node is the first child of a constituent; left - the node is the last child of a constituent; up - the node is neither the first nor the last child of a constituent; unary - the node is a child of a unary constituent; root - the node is the root of the tree.</S>\n    <S sid=\"104\" ssid=\"13\">For an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence.</S>\n    <S sid=\"105\" ssid=\"14\">The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S>\n    <S sid=\"106\" ssid=\"15\">A deterministic lookup table based on the label of the internal node and the labels of the children is used to approximate this linguistic notion.</S>\n    <S sid=\"107\" ssid=\"16\">The SPATTER representation of the sentence is shown in Figure 3.</S>\n    <S sid=\"108\" ssid=\"17\">The nodes are constructed bottom-up from left-to-right, with the constraint that no constituent node is constructed until all of its children have been constructed.</S>\n    <S sid=\"109\" ssid=\"18\">The order in which the nodes of the example sentence are constructed is indicated in the figure.</S>\n    <S sid=\"110\" ssid=\"19\">Each code used by the PC is listed SPATTER consists of three main decision-tree models: a part-of-speech tagging model, a nodeextension model, and a node-labeling model.</S>\n    <S sid=\"111\" ssid=\"20\">Each of these decision-tree models are grown using the following questions, where X is one of word, tag, label, or extension, and Y is either left and right: For each of the nodes listed above, the decision tree could also ask about the number of children and span of the node.</S>\n    <S sid=\"112\" ssid=\"21\">For the tagging model, the values of the previous two words and their tags are also asked, since they might differ from the head words of the previous two constituents.</S>\n    <S sid=\"113\" ssid=\"22\">The training algorithm proceeds as follows.</S>\n    <S sid=\"114\" ssid=\"23\">The training corpus is divided into two sets, approximately 90% for tree growing and 10% for tree smoothing.</S>\n    <S sid=\"115\" ssid=\"24\">For each parsed sentence in the tree growing corpus, the correct state sequence is traversed.</S>\n    <S sid=\"116\" ssid=\"25\">Each state transition from si to si+i is an event; the history is made up of the answers to all of the questions at state si and the future is the value of the action taken from state si to state s.1.</S>\n    <S sid=\"117\" ssid=\"26\">Each event is used as a training example for the decisiontree growing process for the appropriate feature's tree (e.g. each tagging event is used for growing the tagging tree, etc.).</S>\n    <S sid=\"118\" ssid=\"27\">After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in (Magerman, 1994).</S>\n    <S sid=\"119\" ssid=\"28\">The parsing procedure is a search for the highest probability parse tree.</S>\n    <S sid=\"120\" ssid=\"29\">The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models.</S>\n    <S sid=\"121\" ssid=\"30\">Because of the size of the search space, (roughly 0(1711N1), where IT1 is the number of part-ofspeech tags, n is the number of words in the sentence, and 'NI is the number of non-terminal labels), it is not possible to compute the probability of every parse.</S>\n    <S sid=\"122\" ssid=\"31\">However, the specific search algorithm used is not very important, so long as there are no search errors.</S>\n    <S sid=\"123\" ssid=\"32\">A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses.</S>\n    <S sid=\"124\" ssid=\"33\">SPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence.</S>\n    <S sid=\"125\" ssid=\"34\">First, the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence.</S>\n    <S sid=\"126\" ssid=\"35\">Once the stack decoder has found a complete parse of reasonable probability (&gt; 10—s), it switches to a breadth-first mode to pursue all of the partial parses which have not been explored by the stack decoder.</S>\n    <S sid=\"127\" ssid=\"36\">In this second mode, it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse.</S>\n    <S sid=\"128\" ssid=\"37\">Using these two search modes, SPATTER guarantees that it will find the highest probability parse.</S>\n    <S sid=\"129\" ssid=\"38\">The only limitation of this search technique is that, for sentences which are modeled poorly, the search might exhaust the available memory before completing both phases.</S>\n    <S sid=\"130\" ssid=\"39\">However, these search errors conveniently occur on sentences which SPATTER is likely to get wrong anyway, so there isn't much performance lossed due to the search errors.</S>\n    <S sid=\"131\" ssid=\"40\">Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiment Results\" number=\"4\">\n    <S sid=\"132\" ssid=\"1\">In the absence of an NL system, SPATTER can be evaluated by comparing its top-ranking parse with the treebank analysis for each test sentence.</S>\n    <S sid=\"133\" ssid=\"2\">The parser was applied to two different domains, IBM Computer Manuals and the Wall Street Journal.</S>\n    <S sid=\"134\" ssid=\"3\">The first experiment uses the IBM Computer Manuals domain, which consists of sentences extracted from IBM computer manuals.</S>\n    <S sid=\"135\" ssid=\"4\">The training and test sentences were annotated by the University of Lancaster.</S>\n    <S sid=\"136\" ssid=\"5\">The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels.</S>\n    <S sid=\"137\" ssid=\"6\">This treebank is described in great detail in (Black et al., 1993).</S>\n    <S sid=\"138\" ssid=\"7\">The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based, unification-style probabilistic context-free grammar for parsing this domain.</S>\n    <S sid=\"139\" ssid=\"8\">The purpose of the experiment was to estimate SPATTER's ability to learn the syntax for this domain directly from a treebank, instead of depending on the interpretive expertise of a grammarian.</S>\n    <S sid=\"140\" ssid=\"9\">The parser was trained on the first 30,800 sentences from the Lancaster treebank.</S>\n    <S sid=\"141\" ssid=\"10\">The test set included 1,473 new sentences, whose lengths range from 3 to 30 words, with a mean length of 13.7 words.</S>\n    <S sid=\"142\" ssid=\"11\">These sentences are the same test sentences used in the experiments reported for IBM's parser in (Black et al., 1993).</S>\n    <S sid=\"143\" ssid=\"12\">In (Black et al., 1993), IBM's parser was evaluated using the 0-crossingbrackets measure, which represents the percentage of sentences for which none of the constituents in the parser's parse violates the constituent boundaries of any constituent in the correct parse.</S>\n    <S sid=\"144\" ssid=\"13\">After over ten years of grammar development, the IBM parser achieved a 0-crossing-brackets score of 69%.</S>\n    <S sid=\"145\" ssid=\"14\">On this same test set, SPATTER scored 76%.</S>\n    <S sid=\"146\" ssid=\"15\">The experiment is intended to illustrate SPATTER's ability to accurately parse a highly-ambiguous, large-vocabulary domain.</S>\n    <S sid=\"147\" ssid=\"16\">These experiments use the Wall Street Journal domain, as annotated in the Penn Treebank, version 2.</S>\n    <S sid=\"148\" ssid=\"17\">The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels.2 The WSJ portion of the Penn Treebank is divided into 25 sections, numbered 00 - 24.</S>\n    <S sid=\"149\" ssid=\"18\">In these experiments, SPATTER was trained on sections 02 - 21, which contains approximately 40,000 sentences.</S>\n    <S sid=\"150\" ssid=\"19\">The test results reported here are from section 00, which contains 1920 sentences.3 Sections 01, 22, 23, and 24 will be used as test data in future experiments.</S>\n    <S sid=\"151\" ssid=\"20\">The Penn Treebank is already tokenized and sentence detected by human annotators, and thus the test results reported here reflect this.</S>\n    <S sid=\"152\" ssid=\"21\">SPATTER parses word sequences, not tag sequences.</S>\n    <S sid=\"153\" ssid=\"22\">Furthermore, SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing.</S>\n    <S sid=\"154\" ssid=\"23\">Instead, it uses a probabilistic model to assign tags to the words, and considers all possible tag sequences according to the probability they are assigned by the model.</S>\n    <S sid=\"155\" ssid=\"24\">No information about the legal tags for a word are extracted from the test corpus.</S>\n    <S sid=\"156\" ssid=\"25\">In fact, no information other than the words is used from the test corpus.</S>\n    <S sid=\"157\" ssid=\"26\">For the sake of efficiency, only the sentences of 40 words or fewer are included in these experiments.4 For this test set, SPATTER takes on average 12 seconds per sentence on an SGI R4400 with 160 megabytes of RAM.</S>\n    <S sid=\"158\" ssid=\"27\">To evaluate SPATTER's performance on this domain, I am using the PARSEVAL measures, as defined in (Black et al., 1991): Precision no. of correct constituents in SPATTER parse no. of constituents in SPATTER parse Recall no. of correct constituents in SPATTER parse no. of constituents in treebank parse Crossing Brackets no. of constituents which violate constituent boundaries with a constituent in the treebank parse.</S>\n    <S sid=\"159\" ssid=\"28\">The precision and recall measures do not consider constituent labels in their evaluation of a parse, since the treebank label set will not necessarily coincide with the labels used by a given grammar.</S>\n    <S sid=\"160\" ssid=\"29\">Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall.</S>\n    <S sid=\"161\" ssid=\"30\">These measures are computed by considering a constituent to be correct if and only if it's label matches the label in the treebank.</S>\n    <S sid=\"162\" ssid=\"31\">Table 1 shows the results of SPATTER evaluated against the Penn Treebank on the Wall Street Journal section 00.</S>\n    <S sid=\"163\" ssid=\"32\">Figures 5, 6, and 7 illustrate the performance of SPATTER as a function of sentence length.</S>\n    <S sid=\"164\" ssid=\"33\">SPATTER's performance degrades slowly for sentences up to around 28 words, and performs more poorly and more erratically as sentences get longer.</S>\n    <S sid=\"165\" ssid=\"34\">Figure 4 indicates the frequency of each sentence length in the test corpus. function of sentence length for Wall Street Journal experiments.</S>\n  </SECTION>\n  <SECTION title=\"5 Conclusion\" number=\"5\">\n    <S sid=\"166\" ssid=\"1\">Regardless of what techniques are used for parsing disambiguation, one thing is clear: if a particular piece of information is necessary for solving a disambiguation problem, it must be made available to the disambiguation mechanism.</S>\n    <S sid=\"167\" ssid=\"2\">The words in the sentence are clearly necessary to make parsing decisions, and in some cases long-distance structural information is also needed.</S>\n    <S sid=\"168\" ssid=\"3\">Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of.</S>\n    <S sid=\"169\" ssid=\"4\">The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "f978dc482a9beff41540d6eb4b943fc3",
                    "include_year": false,
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo",
                    "origin": "LLM",
                    "title": "Statistical Decision-Tree Models For Parsing"
                }
            }
        },
        {
            "index": 1,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.",
                    "filename": "Improving_Machine_Translation_Performance_By_Exploiting_Non-Parallel_Corpora",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J05-4003/Documents_xml/J05-4003.xml",
                    "title": "Improving Machine Translation Performance By Exploiting Non-Parallel Corpora"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces a novel method for extracting parallel sentences from comparable, non-parallel corpora. By training a maximum entropy classifier to determine if sentences are translations of each other, the system is able to extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. The extracted data is shown to improve the performance of statistical machine translation systems significantly. The method is particularly beneficial for language pairs with limited parallel resources, enabling the creation of a quality MT system from scratch with minimal initial parallel data. The study demonstrates the robustness and efficiency of the approach, highlighting its potential for increasing the amount of training data for machine translation systems."
                    ],
                    "filename": "improving_machine_translation_pe-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 123 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Improving Machine Translation Performance By Exploiting Non-Parallel Corpora</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.</S>\n    <S sid=\"8\" ssid=\"2\">We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.</S>\n    <S sid=\"9\" ssid=\"3\">Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.</S>\n    <S sid=\"10\" ssid=\"4\">We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.</S>\n    <S sid=\"11\" ssid=\"5\">We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.</S>\n    <S sid=\"12\" ssid=\"6\">Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"13\" ssid=\"1\">Parallel texts—texts that are translations of each other—are an important resource in many NLP applications.</S>\n    <S sid=\"14\" ssid=\"2\">They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001).</S>\n    <S sid=\"15\" ssid=\"3\">Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register.</S>\n    <S sid=\"16\" ssid=\"4\">There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations).</S>\n    <S sid=\"17\" ssid=\"5\">This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles).</S>\n    <S sid=\"18\" ssid=\"6\">One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora.</S>\n    <S sid=\"19\" ssid=\"7\">Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information.</S>\n    <S sid=\"20\" ssid=\"8\">Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc.</S>\n    <S sid=\"21\" ssid=\"9\">Such texts are widely available on the Web for many language pairs and domains.</S>\n    <S sid=\"22\" ssid=\"10\">They often contain many sentence pairs that are fairly good translations of each other.</S>\n    <S sid=\"23\" ssid=\"11\">The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora.</S>\n    <S sid=\"24\" ssid=\"12\">However, identifying good translations in comparable corpora is hard.</S>\n    <S sid=\"25\" ssid=\"13\">Even texts that convey the same information will exhibit great differences at the sentence level.</S>\n    <S sid=\"26\" ssid=\"14\">Consider the two newspaper articles in Figure 1.</S>\n    <S sid=\"27\" ssid=\"15\">They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang.</S>\n    <S sid=\"28\" ssid=\"16\">The lines in the figure connect sentence pairs that are approximate translations of each other.</S>\n    <S sid=\"29\" ssid=\"17\">Discovering these links automatically is clearly non-trivial.</S>\n    <S sid=\"30\" ssid=\"18\">Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document.</S>\n    <S sid=\"31\" ssid=\"19\">Thus, they perform poorly on comparable, non-parallel texts.</S>\n    <S sid=\"32\" ssid=\"20\">What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context.</S>\n    <S sid=\"33\" ssid=\"21\">This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).</S>\n    <S sid=\"34\" ssid=\"22\">We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.</S>\n    <S sid=\"35\" ssid=\"23\">Using this classifier, we extract parallel sentences from very large comparable corpora of newspaper articles.</S>\n    <S sid=\"36\" ssid=\"24\">We demonstrate the quality of our A pair of comparable texts. extracted sentences by showing that adding them to the training data of an SMT system improves the system’s performance.</S>\n    <S sid=\"37\" ssid=\"25\">We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data.</S>\n    <S sid=\"38\" ssid=\"26\">Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.</S>\n    <S sid=\"39\" ssid=\"27\">In such a situation, the test data is in-domain, and the training data is out-of-domain.</S>\n    <S sid=\"40\" ssid=\"28\">The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn’t really help the system to produce good translations.</S>\n    <S sid=\"41\" ssid=\"29\">What is needed is additional in-domain training data.</S>\n    <S sid=\"42\" ssid=\"30\">Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system.</S>\n    <S sid=\"43\" ssid=\"31\">We work in the context of Arabic-English and Chinese-English statistical machine translation systems.</S>\n    <S sid=\"44\" ssid=\"32\">Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles.</S>\n    <S sid=\"45\" ssid=\"33\">In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium:1 In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles.</S>\n    <S sid=\"46\" ssid=\"34\">Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings.</S>\n    <S sid=\"47\" ssid=\"35\">We are interested in the situation that exists when we need to translate news data but only have UN data available for training.</S>\n    <S sid=\"48\" ssid=\"36\">The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set.</S>\n    <S sid=\"49\" ssid=\"37\">The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution.</S>\n    <S sid=\"50\" ssid=\"38\">We can train baseline systems on UN parallel data (using the data from the first bullet in the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet).</S>\n    <S sid=\"51\" ssid=\"39\">In the next section, we give a high-level overview of our parallel sentence extraction system.</S>\n    <S sid=\"52\" ssid=\"40\">In Section 3, we describe in detail the core of the system, the parallel sentence classifier.</S>\n    <S sid=\"53\" ssid=\"41\">In Section 4, we discuss several data extraction experiments.</S>\n    <S sid=\"54\" ssid=\"42\">In Section 5, we evaluate the extracted data by showing that adding it to out-of-domain parallel data improves the in-domain performance of an out-of-domain MT system, and in Section 6, we show that in certain cases, even larger improvements can be obtained by using bootstrapping.</S>\n    <S sid=\"55\" ssid=\"43\">In Section 7, we present examples of sentence pairs extracted by our method and discuss some of its weaknesses.</S>\n    <S sid=\"56\" ssid=\"44\">Before concluding, we discuss related work.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"57\" ssid=\"1\">The general architecture of our extraction system is presented in Figure 2.</S>\n    <S sid=\"58\" ssid=\"2\">Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1).</S>\n    <S sid=\"59\" ssid=\"3\">From each such pair, we generate all possible sentence pairs and pass them through a simple word-overlapbased filter (Section 2.2), thus obtaining candidate sentence pairs.</S>\n    <S sid=\"60\" ssid=\"4\">The candidates are presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the sentences in each pair are mutual translations of each other.</S>\n    <S sid=\"61\" ssid=\"5\">The resources required by the system are minimal: a bilingual dictionary and a small amount of parallel data (used for training the ME classifier).</S>\n    <S sid=\"62\" ssid=\"6\">The dictionaries used in our experiments are learned automatically from (out-of-domain) parallel corpora;2 thus, the only resource used by our system consists of parallel sentences.</S>\n    <S sid=\"63\" ssid=\"7\">2 If such a resource is unavailable, other dictionaries can be used.</S>\n    <S sid=\"64\" ssid=\"8\">Our comparable corpus consists of two large, non-parallel, news corpora, one in English and the other in the foreign language of interest (in our case, Chinese or Arabic).</S>\n    <S sid=\"65\" ssid=\"9\">The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one.</S>\n    <S sid=\"66\" ssid=\"10\">This step of the process emphasizes recall rather than precision.</S>\n    <S sid=\"67\" ssid=\"11\">For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents.</S>\n    <S sid=\"68\" ssid=\"12\">The subsequent components of the system are robust enough to filter out the extra noise introduced by the selection of additional (possibly bad) English documents.</S>\n    <S sid=\"69\" ssid=\"13\">We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan 2001).</S>\n    <S sid=\"70\" ssid=\"14\">We first index all the English documents into a database.</S>\n    <S sid=\"71\" ssid=\"15\">For each foreign document, we take the top five translations of each of its words (according to our probabilistic dictionary) and create an English language query.</S>\n    <S sid=\"72\" ssid=\"16\">The translation probabilities are only used to choose the word translations; they do not appear in the query.</S>\n    <S sid=\"73\" ssid=\"17\">We use the query to run TF-IDF retrieval against the database, take the top 20 English documents returned by Lemur, and pair each of them with the foreign query document.</S>\n    <S sid=\"74\" ssid=\"18\">This document matching procedure is both slow (it looks at all possible document pairs, so it is quadratic in the number of documents) and imprecise (due to noise in the dictionary, the query will contain many wrong words).</S>\n    <S sid=\"75\" ssid=\"19\">We attempt to fix these problems by using the following heuristic: we consider it likely that articles with similar content have publication dates that are close to each other.</S>\n    <S sid=\"76\" ssid=\"20\">Thus, each query is actually run only against English documents published within a window of five days around the publication date of the foreign query document; we retrieve the best 20 of these documents.</S>\n    <S sid=\"77\" ssid=\"21\">Each query is thus run against fewer documents, so it becomes faster and has a better chance of getting the right documents at the top.</S>\n    <S sid=\"78\" ssid=\"22\">Our experiments have shown that the final performance of the system does not depend too much on the size of the window (for example, doubling the size to 10 days made no difference).</S>\n    <S sid=\"79\" ssid=\"23\">However, having no window at all leads to a decrease in the overall performance of the system.</S>\n    <S sid=\"80\" ssid=\"24\">From each foreign document and set of associated English documents, we take all possible sentence pairs and pass them through a word-overlap filter.</S>\n    <S sid=\"81\" ssid=\"25\">The filter verifies that the ratio of the lengths of the two sentences is no greater than two.</S>\n    <S sid=\"82\" ssid=\"26\">It then checks that at least half the words in each sentence have a translation in the other sentence, according to the dictionary.</S>\n    <S sid=\"83\" ssid=\"27\">Pairs that do not fulfill these two conditions are discarded.</S>\n    <S sid=\"84\" ssid=\"28\">The others are passed on to the parallel sentence selection stage.</S>\n    <S sid=\"85\" ssid=\"29\">This step removes most of the noise (i.e., pairs of non-parallel sentences) introduced by our recall-oriented document selection procedure.</S>\n    <S sid=\"86\" ssid=\"30\">It also removes good pairs that fail to pass the filter because the dictionary does not contain the necessary entries; but those pairs could not have been handled reliably anyway, so the overall effect of the filter is to improve the precision and robustness of the system.</S>\n    <S sid=\"87\" ssid=\"31\">However, the filter also accepts many wrong pairs, because the word-overlap condition is weak; for instance, stopwords almost always have a translation on the other side, so if a few of the content For each candidate sentence pair, we need a reliable way of deciding whether the two sentences in the pair are mutual translations.</S>\n    <S sid=\"88\" ssid=\"32\">This is achieved by a Maximum Entropy (ME) classifier (described at length in Section 3), which is the core component of our system.</S>\n    <S sid=\"89\" ssid=\"33\">Those pairs that are classified as being translations of each other constitute the output of the system.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"90\" ssid=\"1\">In the Maximum Entropy (ME) statistical modeling framework, we impose constraints on the model of our data by defining a set of feature functions.</S>\n    <S sid=\"91\" ssid=\"2\">These feature functions emphasize properties of the data that we believe to be useful for the modeling task.</S>\n    <S sid=\"92\" ssid=\"3\">For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel.</S>\n    <S sid=\"93\" ssid=\"4\">We therefore define a feature function f (sp), whose value is the word overlap of the sentences in sp.</S>\n    <S sid=\"94\" ssid=\"5\">According to the ME principle, the optimal parametric form of the model of our data, taking into account the constraints imposed by the feature functions, is a log linear combination of these functions.</S>\n    <S sid=\"95\" ssid=\"6\">Thus, for our classification problem, we have: where ci is the class (c0=”parallel”, c1=”not parallel”), Z(sp) is a normalization factor, and fij are the feature functions (indexed both by class and by feature).</S>\n    <S sid=\"96\" ssid=\"7\">The resulting model has free parameters λj, the feature weights.</S>\n    <S sid=\"97\" ssid=\"8\">The parameter values that maximize the likelihood of a given training corpus can be computed using various optimization algorithms (see [Malouf 2002] for a comparison of such algorithms).</S>\n    <S sid=\"98\" ssid=\"9\">For our particular classification problem, we need to find feature functions that distinguish between parallel and non-parallel sentence pairs.</S>\n    <S sid=\"99\" ssid=\"10\">For this purpose, we compute and exploit word-level alignments between the sentences in each pair.</S>\n    <S sid=\"100\" ssid=\"11\">A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other.</S>\n    <S sid=\"101\" ssid=\"12\">Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990).</S>\n    <S sid=\"102\" ssid=\"13\">Since then, they were found useful in many other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question answering [Echihabi and Marcu 2003]).</S>\n    <S sid=\"103\" ssid=\"14\">Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus.</S>\n    <S sid=\"104\" ssid=\"15\">Each figure contains two alignments.</S>\n    <S sid=\"105\" ssid=\"16\">The one on the left is a correct alignment, produced by a human, while the one on the right Alignments between two parallel sentences. was computed automatically.</S>\n    <S sid=\"106\" ssid=\"17\">As can be seen from the gloss next to the Arabic words, the sentences in Figure 3 are parallel while the sentences in Figure 4 are not.</S>\n    <S sid=\"107\" ssid=\"18\">In a correct alignment between two non-parallel sentences, most words would have no translation equivalents; in contrast, in an alignment between parallel sentences, most words would be aligned.</S>\n    <S sid=\"108\" ssid=\"19\">Automatically computed alignments, however, may have incorrect connections; for example, on the right side of Figure 3, the Arabic word issue is connected to the comma; and in Figure 4, the Arabic word at is connected to the English phrase its case to the.</S>\n    <S sid=\"109\" ssid=\"20\">Such errors are due to noisy dictionary entries and to Alignments between two non-parallel sentences. shortcomings of the model used to generate the alignments.</S>\n    <S sid=\"110\" ssid=\"21\">Thus, merely looking at the number of unconnected words, while helpful, is not discriminative enough.</S>\n    <S sid=\"111\" ssid=\"22\">Still, automatically produced alignments have certain additional characteristics that can be exploited.</S>\n    <S sid=\"112\" ssid=\"23\">We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.</S>\n    <S sid=\"113\" ssid=\"24\">The presence, in an automatically computed alignment between a pair of sentences, of words of high fertility (such as the Arabic word at in Figure 4) is indicative of non-parallelism.</S>\n    <S sid=\"114\" ssid=\"25\">Most likely, these connections were produced because of a lack of better alternatives.</S>\n    <S sid=\"115\" ssid=\"26\">Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring.</S>\n    <S sid=\"116\" ssid=\"27\">Such a span may contain a few words without any connection (a small percentage of the length of the span), but no word with a connection outside the span.</S>\n    <S sid=\"117\" ssid=\"28\">Examples of such spans can be seen in Figure 3: the English strings after saudi mediation failed or to the international court ofjustice together with their Arabic counterparts.</S>\n    <S sid=\"118\" ssid=\"29\">Long contiguous connected spans are indicative of parallelism, since they suggest that the two sentences have long phrases in common.</S>\n    <S sid=\"119\" ssid=\"30\">And, in contrast, long substrings whose words are all unconnected are indicative of non-parallelism.</S>\n    <S sid=\"120\" ssid=\"31\">To summarize, our classifier uses the following features, defined over two sentences and an automatically computed alignment between them.</S>\n    <S sid=\"121\" ssid=\"32\">General features (independent of the word alignment): In order to compute word alignments we need a simple and efficient model.</S>\n    <S sid=\"122\" ssid=\"33\">We want to align a large number of sentences, with many out-of-vocabulary words, in reasonable time.</S>\n    <S sid=\"123\" ssid=\"34\">We also want a model with as few parameters as possible—preferably only wordfor-word translation probabilities.</S>\n    <S sid=\"124\" ssid=\"35\">One such model is the IBM Model 1 (Brown et al. 1993).</S>\n    <S sid=\"125\" ssid=\"36\">According to this model, given foreign sentence (fj1&lt;=j&lt;=m), English sentence (ei1&lt;=i&lt;=l), and translation probabilities t(fj|ei), the best alignment f → e is obtained by linking each foreign word fj to its most likely English translation argmaxeit(fj|ei).</S>\n    <S sid=\"126\" ssid=\"37\">Thus, each foreign word is aligned to exactly one English word (or to a special NULL token).</S>\n    <S sid=\"127\" ssid=\"38\">Due to its simplicity, this model has several shortcomings, some more structural than others (see Moore [2004] for a discussion).</S>\n    <S sid=\"128\" ssid=\"39\">Thus, we use a version that is augmented with two simple heuristics that attempt to alleviate some of these shortcomings.</S>\n    <S sid=\"129\" ssid=\"40\">One possible improvement concerns English words that appear more than once in a sentence.</S>\n    <S sid=\"130\" ssid=\"41\">According to the model, a foreign word that prefers to be aligned with such an English word could be equally well aligned with any instance of that word.</S>\n    <S sid=\"131\" ssid=\"42\">In such situations, instead of arbitrarily choosing the first instance or a random instance, we attempt to make a ”smarter” decision.</S>\n    <S sid=\"132\" ssid=\"43\">First, we create links only for those English words that appear exactly once; next, for words that appear more than once, we choose which instance to link with so that we minimize the number of crossings with already existing links.</S>\n    <S sid=\"133\" ssid=\"44\">The second heuristic attempts to improve the choice of the most likely English translation of a foreign word.</S>\n    <S sid=\"134\" ssid=\"45\">Our translation probabilities are automatically learned from parallel data, and we learn values for both t(fj|ei) and t(ei|fj).</S>\n    <S sid=\"135\" ssid=\"46\">We can therefore decide that the most likely English translation of fj is argmaxei{t(fj|ei),t(ei|fj)}.</S>\n    <S sid=\"136\" ssid=\"47\">Using both sets of probabilities is likely to help us make a better-informed decision.</S>\n    <S sid=\"137\" ssid=\"48\">Using this alignment strategy, we follow (Och and Ney 2003) and compute one alignment for each translation direction (f - 4e and e -4 f), and then combine them.</S>\n    <S sid=\"138\" ssid=\"49\">Och and Ney present three combination methods: intersection, union, and refined (a form of intersection expanded with certain additional neighboring links).</S>\n    <S sid=\"139\" ssid=\"50\">Thus, for each sentence pair, we compute five alignments (two modified-IBMModel-1 plus three combinations) and then extract one set of general features and five sets of alignment features (as described in the previous section).</S>\n    <S sid=\"140\" ssid=\"51\">We create training instances for our classifier from a small parallel corpus.</S>\n    <S sid=\"141\" ssid=\"52\">The simplest way to obtain classifier training data from a parallel corpus is to generate all possible sentence pairs from the corpus (the Cartesian product).</S>\n    <S sid=\"142\" ssid=\"53\">This generates 5,0002 training instances, out of which 5,000 are positive (i.e., belong to class ”parallel”) and the rest are negative.</S>\n    <S sid=\"143\" ssid=\"54\">One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones.</S>\n    <S sid=\"144\" ssid=\"55\">Classifiers trained on such data do not achieve good performance; they generally tend to predict the majority class, i.e., classify most sentences as non-parallel (which has indeed been the case in our experiments).</S>\n    <S sid=\"145\" ssid=\"56\">Our solution to this is to downsample, i.e., eliminate a number of (randomly selected) negative instances.</S>\n    <S sid=\"146\" ssid=\"57\">Another problem is that the large majority of sentence pairs in the Cartesian product have low word overlap (i.e., few words that are translations of each other).</S>\n    <S sid=\"147\" ssid=\"58\">As explained in Section 2 (and shown in Figure 2), when extracting data from a comparable corpus, we only apply the classifier on the output of the word-overlap filter.</S>\n    <S sid=\"148\" ssid=\"59\">Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.</S>\n    <S sid=\"149\" ssid=\"60\">We therefore use for training only those pairs from the Cartesian product that are accepted by the word-overlap filter.</S>\n    <S sid=\"150\" ssid=\"61\">This has the additional advantage that, since all these pairs have many words in common, the classifier learns to make distinctions that cannot be made based on word overlap alone.</S>\n    <S sid=\"151\" ssid=\"62\">To summarize, we prepare our classifier training set in the following manner: starting from a parallel corpus of about 5,000 sentence pairs, we generate all the sentence pairs in the Cartesian product; we discard the pairs that do not fulfill the conditions of the word-overlap filter; if the resulting set is imbalanced, i.e., the ratio of non-parallel to parallel pairs is greater than five, we balance it by removing randomly chosen nonparallel pairs.</S>\n    <S sid=\"152\" ssid=\"63\">We then compute word alignments and extract feature values.</S>\n    <S sid=\"153\" ssid=\"64\">Using the training set, we compute values for the classifier feature weights using the YASMET4 implementation of the GIS algorithm (Darroch and Ratcliff 1974).</S>\n    <S sid=\"154\" ssid=\"65\">Since we are dealing with few parameters and have sufficiently many training instances, using more advanced training algorithms is unlikely to bring significant improvements.</S>\n    <S sid=\"155\" ssid=\"66\">We test the performance of the classifier by generating test instances from a different parallel corpus (also around 5,000 sentence pairs) and checking how many of these instances are correctly classified.</S>\n    <S sid=\"156\" ssid=\"67\">We prepare the test set by creating the Cartesian product of the sentences in the test parallel corpus and applying the word-overlap filter (we do not perform any balancing).</S>\n    <S sid=\"157\" ssid=\"68\">Although we apply the filter, we still conceptually classify all pairs from the Cartesian product in a two-stage classification process: all pairs discarded by the filter are classified as ”non-parallel,” and for the rest, we obtain predictions from the classifier.</S>\n    <S sid=\"158\" ssid=\"69\">Since this is how we apply the system on truly unseen data, this is the process in whose performance we are interested.</S>\n    <S sid=\"159\" ssid=\"70\">We measure the performance of the classification process by computing precision and recall.</S>\n    <S sid=\"160\" ssid=\"71\">Precision is the ratio of sentence pairs correctly judged as parallel to the total number of pairs judged as parallel by the classifier.</S>\n    <S sid=\"161\" ssid=\"72\">Recall is the ratio of sentence pairs correctly identified as parallel by the classifier to the total number of truly parallel pairs—i.e., the number of pairs in the parallel corpus used to generate the test instances.</S>\n    <S sid=\"162\" ssid=\"73\">Both numbers are expressed as percentages.</S>\n    <S sid=\"163\" ssid=\"74\">More formally: let classified parallel be the total number of sentence pairs from our test set that the classifier judged as parallel, classified well be the number of pairs that the classifier correctly judged as parallel, and true parallel be the total number of parallel pairs in the test set.</S>\n    <S sid=\"164\" ssid=\"75\">Then: classified parallel true parallel There are two factors that influence a classifier’s performance: dictionary coverage and similarity between the domains of the training and test instances.</S>\n    <S sid=\"165\" ssid=\"76\">We performed evaluation experiments to account for both these factors.</S>\n    <S sid=\"166\" ssid=\"77\">All our dictionaries are automatically learned from parallel data; thus, we can create dictionaries of various coverage by learning them from parallel corpora of different sizes.</S>\n    <S sid=\"167\" ssid=\"78\">We use five dictionaries, learned from five initial out-of-domain parallel corpora, whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English side.</S>\n    <S sid=\"168\" ssid=\"79\">Since we want to use the classifier to extract sentence pairs from our in-domain comparable corpus, we test it on instances generated from an in-domain parallel corpus.</S>\n    <S sid=\"169\" ssid=\"80\">In order to measure the effect of the domain difference, we use two training sets: one generated from an in-domain parallel corpus and another one from an out-ofdomain parallel corpus.</S>\n    <S sid=\"170\" ssid=\"81\">In summary, for each language pair, we use the following corpora: Precision and recall of the Arabic-English classifiers.</S>\n    <S sid=\"171\" ssid=\"82\">From each initial, out-of-domain corpus, we learn a dictionary.</S>\n    <S sid=\"172\" ssid=\"83\">We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances.</S>\n    <S sid=\"173\" ssid=\"84\">We train two classifiers (one on each training set) and evaluate both of them on the test set.</S>\n    <S sid=\"174\" ssid=\"85\">The parallel corpora used for generating training and test instances have around 5k sentence pairs each (approximately 150k English tokens), and generate around 10k training instances (for each training set) and 8k test instances.</S>\n    <S sid=\"175\" ssid=\"86\">Precision and recall of the Chinese-English classifiers.</S>\n    <S sid=\"176\" ssid=\"87\">Figures 5 and 6 show the recall and precision of our classifiers, for both ArabicEnglish and Chinese-English.</S>\n    <S sid=\"177\" ssid=\"88\">The results show that the precision of our classification process is robust with respect to dictionary coverage and training domain.</S>\n    <S sid=\"178\" ssid=\"89\">Even when starting from a very small initial parallel corpus, we can build a high-precision classifier.</S>\n    <S sid=\"179\" ssid=\"90\">Having a good dictionary and training data from the right domain does help though, mainly with respect to recall.</S>\n    <S sid=\"180\" ssid=\"91\">The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments.</S>\n    <S sid=\"181\" ssid=\"92\">The low recall results are partly due to the word-overlap filter (the first stage of the classification process), which discards many parallel pairs.</S>\n    <S sid=\"182\" ssid=\"93\">If we don’t apply the filter before the classifier, the recall results increase by about 20% (with no loss in precision).</S>\n    <S sid=\"183\" ssid=\"94\">However, the filter plays a very important role in keeping the extraction pipeline robust and efficient (as shown in Figure 7, the filter discards 99% of the candidate pairs), so this loss of recall is a price worth paying.</S>\n    <S sid=\"184\" ssid=\"95\">Classifier evaluations using different subsets of features show that most of the classifier performance comes from the general features together with the alignment features concerning the percentage and number of words that have no connection.</S>\n    <S sid=\"185\" ssid=\"96\">However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no The amounts of data processed by our system during extraction from the Chinese-English comparable corpus. longer be accounted for only by counting the linked words; thus, the other features should become more important.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"186\" ssid=\"1\">The comparable corpora that we use for parallel sentence extraction are collections of news stories published by the Agence France Presse and Xinhua News agencies.</S>\n    <S sid=\"187\" ssid=\"2\">They are parts of the Arabic, English, and Chinese Gigaword corpora which are available from the Linguistic Data Consortium.</S>\n    <S sid=\"188\" ssid=\"3\">From these collections, for each language pair, we create an in-domain comparable corpus by putting together articles coming from the same agency and the same time period.</S>\n    <S sid=\"189\" ssid=\"4\">Table 1 presents in detail the sources and sizes of the resulting comparable corpora.</S>\n    <S sid=\"190\" ssid=\"5\">The remainder of the section presents the various data sets that we extracted automatically from these corpora, under various experimental conditions.</S>\n    <S sid=\"191\" ssid=\"6\">In the experiments described in Section 3.4, we started out with five out-of-domain initial parallel corpora of various sizes and obtained five dictionaries and five out-ofdomain trained classifiers (per language pair).</S>\n    <S sid=\"192\" ssid=\"7\">We now plug in each of these classifiers (and their associated dictionaries) in our extraction system (Section 2) and apply it to our comparable corpora.</S>\n    <S sid=\"193\" ssid=\"8\">We thus obtain five Arabic-English and five Chinese-English extracted corpora.</S>\n    <S sid=\"194\" ssid=\"9\">Note that in each of these experiments the only resource used by our system is the initial, out-of-domain parallel corpus.</S>\n    <S sid=\"195\" ssid=\"10\">Thus, the experiments fit in the framework of interest described in Section 1, which assumes the availability of (limited amounts of) out-of-domain training data and (large amounts of) in-domain comparable data.</S>\n    <S sid=\"196\" ssid=\"11\">Table 2 shows the sizes of the extracted corpora for each initial corpus size, for both Chinese-English and Arabic-English.</S>\n    <S sid=\"197\" ssid=\"12\">As can be seen, when the initial parallel corpus is very small, the amount of extracted data is also quite small.</S>\n    <S sid=\"198\" ssid=\"13\">This is due to the low coverage of the dictionary learned from that corpus.</S>\n    <S sid=\"199\" ssid=\"14\">Our candidate pair selection step (Section 2.2) discards pairs with too many unknown (or unrelated) words, according to the dictionary; thus, only few sentences fulfill the word-overlap condition of our filter.</S>\n    <S sid=\"200\" ssid=\"15\">As mentioned in Section 1, our goal is to use the extracted data as additional MT training data and obtain better translation performance on a given in-domain MT test set.</S>\n    <S sid=\"201\" ssid=\"16\">A simple way of estimating the usefulness of the data for this purpose is to measure its coverage of the test set, i.e., the percentage of running n-grams from the test corpus that are also in our corpus.</S>\n    <S sid=\"202\" ssid=\"17\">Tables 3 and 4 present the coverage of our extracted corpora.</S>\n    <S sid=\"203\" ssid=\"18\">For each initial corpus size, the first column shows the coverage of that initial corpus, and the second column shows the coverage of the initial corpus plus the extracted corpus.</S>\n    <S sid=\"204\" ssid=\"19\">Each cell contains four numbers that represent the coverage with respect to unigrams, bigrams, trigrams, and 4-grams.</S>\n    <S sid=\"205\" ssid=\"20\">The numbers show that unigram coverage depends only on the size of the corpus (and not on the domain), but for longer n-grams, our in-domain extracted data brings significant improvements in coverage.</S>\n    <S sid=\"206\" ssid=\"21\">The extraction experiments from the previous section are controlled experiments in which we only use limited amounts of parallel data for our extraction system.</S>\n    <S sid=\"207\" ssid=\"22\">In this section, we describe experiments in which the goal is to assess the applicability of our method to data that we mined from the Web.</S>\n    <S sid=\"208\" ssid=\"23\">We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently.</S>\n    <S sid=\"209\" ssid=\"24\">In order to get as many articles as possible, we used the web site’s search engine to get lists of articles and their URLs, and then crawled those lists.</S>\n    <S sid=\"210\" ssid=\"25\">We used the AgentBuilder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling.</S>\n    <S sid=\"211\" ssid=\"26\">The tool can be programmed to automatically initiate searches with different parameters and to identify and extract the desired article URLs (as well as other information such as dates and titles) from the result pages.</S>\n    <S sid=\"212\" ssid=\"27\">Table 5 shows the sources, time periods, and size of the datasets that we downloaded.</S>\n    <S sid=\"213\" ssid=\"28\">For the extraction experiments, we used dictionaries of high coverage, learned from all our available parallel training data.</S>\n    <S sid=\"214\" ssid=\"29\">The sizes of these training corpora, measured in number of English tokens, are as follows: We applied our extraction method on both the LDC-released Gigaword corpora and the Web-downloaded comparable corpora.</S>\n    <S sid=\"215\" ssid=\"30\">For each language pair, we used the highest precision classifier from those presented in Section 3.4.</S>\n    <S sid=\"216\" ssid=\"31\">In order to obtain data of higher quality, we didn’t use all the sentences classified as parallel, but only those for which the probability computed by our classifier was higher than 0.70.</S>\n    <S sid=\"217\" ssid=\"32\">Table 6 shows the amounts of extracted data, measured in number of English tokens.</S>\n    <S sid=\"218\" ssid=\"33\">For ArabicEnglish, we were able to extract from the Gigaword corpora much more data than in our previous experiments (see Table 2), clearly due to the better dictionary.</S>\n    <S sid=\"219\" ssid=\"34\">For ChineseEnglish, there was no increase in the size of extracted data (although the amount from Table 6 is smaller than that from Table 2, it counts only sentence pairs extracted with confidence higher than 0.70).</S>\n    <S sid=\"220\" ssid=\"35\">In the previous section, we measured, for our training corpora, their coverage of the test set (Tables 3 and 4).</S>\n    <S sid=\"221\" ssid=\"36\">We repeated the measurements for the training data from Table 6 and obtained very similar results: using the additional extracted data improves coverage, especially for longer n-grams.</S>\n    <S sid=\"222\" ssid=\"37\">To give the reader an idea of the amount of data that is funneled through our system, we show in Figure 7 the sizes of the data processed by each of the system’s components during extraction from the Gigaword and Web-based Chinese-English comparable corpora.</S>\n    <S sid=\"223\" ssid=\"38\">We use a dictionary learned from a parallel corpus on 190M English tokens and a classifier trained on instances generated from a parallel corpus of 220k English tokens.</S>\n    <S sid=\"224\" ssid=\"39\">We start with a comparable corpus consisting of 500k Chinese articles and 600k English articles.</S>\n    <S sid=\"225\" ssid=\"40\">The article selection step (Section 2.1) outputs 7.5M similar article pairs; from each article pair we generate all possible sentence pairs and obtain 2,400M pairs.</S>\n    <S sid=\"226\" ssid=\"41\">Of these, less than 1% (17M) pass the candidate selection stage (Section 2.2) and are presented to the ME classifier.</S>\n    <S sid=\"227\" ssid=\"42\">The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7).</S>\n    <S sid=\"228\" ssid=\"43\">The figure also presents, in the lower part, the parameters that control the filtering at each stage. the particular sentence pair to be parallel; the higher the value, the higher the classifier’s confidence.</S>\n    <S sid=\"229\" ssid=\"44\">Thus, in order to obtain higher precision, we can choose to define as parallel only those pairs for which the classifier probability is above a certain threshold.</S>\n    <S sid=\"230\" ssid=\"45\">In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7.</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"231\" ssid=\"1\">Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system.</S>\n    <S sid=\"232\" ssid=\"2\">Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance.</S>\n    <S sid=\"233\" ssid=\"3\">We first evaluate the extracted corpora presented in Section 4.1.</S>\n    <S sid=\"234\" ssid=\"4\">The extraction system used to obtain each of those corpora made use of a certain initial out-of-domain parallel corpus.</S>\n    <S sid=\"235\" ssid=\"5\">We train a Baseline MT system on that initial corpus.</S>\n    <S sid=\"236\" ssid=\"6\">We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus.</S>\n    <S sid=\"237\" ssid=\"7\">In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data.</S>\n    <S sid=\"238\" ssid=\"8\">For each initial corpus, we use the same amount of human-translated data as there is extracted data (see Table 2).</S>\n    <S sid=\"239\" ssid=\"9\">Thus, for each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline, PlusExtracted, and UpperBound.</S>\n    <S sid=\"240\" ssid=\"10\">All our MT systems were trained using a variant of the alignment template model described in (Och 2003).</S>\n    <S sid=\"241\" ssid=\"11\">Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system.</S>\n    <S sid=\"242\" ssid=\"12\">This ensured that any differences in performance are caused only by differences in the training data.</S>\n    <S sid=\"243\" ssid=\"13\">The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations.</S>\n    <S sid=\"244\" ssid=\"14\">Figures 8 and 9 show the BLEU scores obtained by our MT systems.</S>\n    <S sid=\"245\" ssid=\"15\">The 95% confidence intervals of the scores computed by bootstrap resampling (Koehn 2004) are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for Chinese-English.</S>\n    <S sid=\"246\" ssid=\"16\">As the results show, the automatically extracted additional training data yields significant improvements in performance over most initial training corpora for both language pairs.</S>\n    <S sid=\"247\" ssid=\"17\">At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data.</S>\n    <S sid=\"248\" ssid=\"18\">And, as can be expected, the impact of the extracted data decreases as the size of the initial corpus increases.</S>\n    <S sid=\"249\" ssid=\"19\">In order to check that the classifier really does something important, we performed a few experiments without it.</S>\n    <S sid=\"250\" ssid=\"20\">After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus.</S>\n    <S sid=\"251\" ssid=\"21\">The resulting BLEU scores were practically the same as the baseline; thus, our classifier does indeed help to discover higher-quality parallel data.</S>\n    <S sid=\"252\" ssid=\"22\">We also measured the MT performance impact of the extracted corpora described in Section 4.2.</S>\n    <S sid=\"253\" ssid=\"23\">We trained a Baseline MT system on all our available (in-domain and MT performance improvements for Arabic-English. out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the extracted in-domain data.</S>\n    <S sid=\"254\" ssid=\"24\">Clearly, we have access to no UpperBound system in this case.</S>\n    <S sid=\"255\" ssid=\"25\">The results are presented in the first two rows of Table 7.</S>\n    <S sid=\"256\" ssid=\"26\">Adding the extracted corpus lowers the score for the Arabic-English system and improves the score for the Chinese-English one; however, none of the differences are statistically significant.</S>\n    <S sid=\"257\" ssid=\"27\">Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact.</S>\n    <S sid=\"258\" ssid=\"28\">In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data.</S>\n    <S sid=\"259\" ssid=\"29\">The BLEU scores obtained by the systems we trained on them are presented in the third row of Table 7.</S>\n    <S sid=\"260\" ssid=\"30\">For comparison purposes, the last line of the table shows the scores of systems trained on 10M English tokens of outof-domain data.</S>\n    <S sid=\"261\" ssid=\"31\">As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size.</S>\n    <S sid=\"262\" ssid=\"32\">It’s true that this is not a fair comparison, since the extracted corpora were obtained using all our available parallel data.</S>\n    <S sid=\"263\" ssid=\"33\">The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"264\" ssid=\"1\">As can be seen from Table 2, the amount of data we can extract from our comparable corpora is adversely affected by poor dictionary coverage.</S>\n    <S sid=\"265\" ssid=\"2\">Thus, if we start with very little parallel data, we do not make good use of the comparable corpora.</S>\n    <S sid=\"266\" ssid=\"3\">One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again.</S>\n    <S sid=\"267\" ssid=\"4\">Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004).</S>\n    <S sid=\"268\" ssid=\"5\">We performed bootstrapping iterations starting from two very small corpora: 100k English tokens and 1M English tokens, respectively.</S>\n    <S sid=\"269\" ssid=\"6\">After each iteration, we trained MT performance improvements for Chinese-English.</S>\n    <S sid=\"270\" ssid=\"7\">(and evaluated) an MT system on the initial data plus the data extracted in that iteration.</S>\n    <S sid=\"271\" ssid=\"8\">We did not use any of the data extracted in previous iterations since it is mostly a subset of that extracted in the current iteration.</S>\n    <S sid=\"272\" ssid=\"9\">We iterated until there were no further improvements in MT performance on our development data.</S>\n    <S sid=\"273\" ssid=\"10\">Figures 10 and 11 show the sizes of the data extracted at each iteration, for both initial corpus sizes.</S>\n    <S sid=\"274\" ssid=\"11\">Iteration 0 is the one that uses the dictionary learned from the initial corpus.</S>\n    <S sid=\"275\" ssid=\"12\">Starting with 100k words of parallel data, we eventually collect 20M words of in-domain Arabic-English data and 90M words of in-domain Chinese-English data.</S>\n    <S sid=\"276\" ssid=\"13\">Figures 12 and 13 show the BLEU scores of these MT systems.</S>\n    <S sid=\"277\" ssid=\"14\">For comparison purposes, we also plotted on each graph the performance of our best MT system for that language pair, trained on all our available parallel data (Table 7).</S>\n    <S sid=\"278\" ssid=\"15\">As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores.</S>\n    <S sid=\"279\" ssid=\"16\">Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7–10 BLEU points of systems trained on parallel corpora of more than 100M English tokens.</S>\n    <S sid=\"280\" ssid=\"17\">This shows that using our method, a good-quality MT system can be built from very little parallel data and a large amount of comparable, non-parallel data.</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"281\" ssid=\"1\">We conclude the description of our method by presenting a few sentence pairs extracted by our system.</S>\n    <S sid=\"282\" ssid=\"2\">We chose the examples by looking for cases when a given foreign sentence was judged parallel to several different English sentences.</S>\n    <S sid=\"283\" ssid=\"3\">Figures 14 and 15 show the foreign sentence in Arabic and Chinese, respectively, followed by a human-produced translation in bold italic font, followed by the automatically extracted matching English sentences in normal font.</S>\n    <S sid=\"284\" ssid=\"4\">The sentences are picked from the data sets presented in Section 4.2.</S>\n    <S sid=\"285\" ssid=\"5\">The examples reveal the two main types of errors that our system makes.</S>\n    <S sid=\"286\" ssid=\"6\">The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7.</S>\n    <S sid=\"287\" ssid=\"7\">The second concerns pairs in which the two sentences convey different amounts of information.</S>\n    <S sid=\"288\" ssid=\"8\">In such pairs, one of the sentences contains a transSizes of the Chinese-English corpora extracted using bootstrapping, in millions of English tokens.</S>\n    <S sid=\"289\" ssid=\"9\">BLEU scores of the Arabic-English MT systems using bootstrapping. lation of the other, plus additional (often quite long) phrases (Figure 15, examples 1 and 5).</S>\n    <S sid=\"290\" ssid=\"10\">These errors are caused by the noise present in the automatically learned dictionaries and by the use of a weak word alignment model for extracting the classifier BLEU scores of the Chinese-English MT systems using bootstrapping. features.</S>\n    <S sid=\"291\" ssid=\"11\">In an automatically learned dictionary, many words (especially the frequent, non-content ones) will have a lot of spurious translations.</S>\n    <S sid=\"292\" ssid=\"12\">The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words.</S>\n    <S sid=\"293\" ssid=\"13\">Alignments computed using this model and a noisy, automatically learned, dictionary will contain many incorrect links.</S>\n    <S sid=\"294\" ssid=\"14\">Thus, if two sentences share several content words, these incorrect links together with the correct links between the common content words will yield an alignment good enough to make the classifier judge the sentence pair as parallel.</S>\n    <S sid=\"295\" ssid=\"15\">The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6.</S>\n    <S sid=\"296\" ssid=\"16\">The sentences in that example are tables of soccer team statistics.</S>\n    <S sid=\"297\" ssid=\"17\">They are judged parallel because corresponding digits align to each other, and according to our dictionary, the Arabic word for “Mexico” can be translated as any of the country names listed in the example.</S>\n    <S sid=\"298\" ssid=\"18\">These examples also show that the problem of finding only true translation pairs is hard.</S>\n    <S sid=\"299\" ssid=\"19\">Two sentences may share many content words and yet express different meanings (see Figure 14, example 1).</S>\n    <S sid=\"300\" ssid=\"20\">However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance.</S>\n  </SECTION>\n  <SECTION title=\"8.\" number=\"9\">\n    <S sid=\"301\" ssid=\"1\">While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004).</S>\n    <S sid=\"302\" ssid=\"2\">We are aware of only three previous efforts aimed at discovering parallel sentences.</S>\n    <S sid=\"303\" ssid=\"3\">Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus.</S>\n    <S sid=\"304\" ssid=\"4\">Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus.</S>\n    <S sid=\"305\" ssid=\"5\">Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stories from different sources and time periods.</S>\n    <S sid=\"306\" ssid=\"6\">The first two systems extend algorithms designed to perform sentence alignment of parallel texts.</S>\n    <S sid=\"307\" ssid=\"7\">They start by attempting to identify similar article pairs from the two corpora.</S>\n    <S sid=\"308\" ssid=\"8\">Then they treat each of those pairs as parallel texts and align their sentences by defining a sentence pair similarity score and use dynamic programming to find the least-cost alignment over the whole document pair.</S>\n    <S sid=\"309\" ssid=\"9\">In the article pair selection stage, the researchers try to identify, for an article in one language, the best matching article in the other language.</S>\n    <S sid=\"310\" ssid=\"10\">Zhao and Vogel (2002) measure article similarity by defining a generative model in which an English story generates a Chinese story with a given probability.</S>\n    <S sid=\"311\" ssid=\"11\">Utiyama et al. (2003) use the BM25 (Robertson and Walker 1994) similarity measure.</S>\n    <S sid=\"312\" ssid=\"12\">The two works also differ in the way they define the sentence similarity score.</S>\n    <S sid=\"313\" ssid=\"13\">Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model.</S>\n    <S sid=\"314\" ssid=\"14\">Utiyama et al. (2003) define a score based on word overlap (i.e., number of word pairs from the two sentences that are translations of each other), which also includes the similarity score of the article pair from which the sentence pair originates.</S>\n    <S sid=\"315\" ssid=\"15\">The performance of these approaches depends heavily on the ability to reliably find similar document pairs.</S>\n    <S sid=\"316\" ssid=\"16\">Moreover, comparable article pairs, even those similar in content, may exhibit great differences at the sentence level (reorderings, additions, etc).</S>\n    <S sid=\"317\" ssid=\"17\">Therefore, they pose hard problems for the dynamic programming alignment approach.</S>\n    <S sid=\"318\" ssid=\"18\">In contrast, our method is more robust.</S>\n    <S sid=\"319\" ssid=\"19\">The document pair selection part plays a minor role; it only acts as a filter.</S>\n    <S sid=\"320\" ssid=\"20\">We do not attempt to find the best-matching English document for each foreign one, but rather a set of similar documents.</S>\n    <S sid=\"321\" ssid=\"21\">And, most importantly, we are able to reliably judge each sentence pair in isolation, without need for context.</S>\n    <S sid=\"322\" ssid=\"22\">On the other hand, the dynamic programming approach enables discovery of many-to-one sentence alignments, whereas our method is limited to finding one-toone alignments.</S>\n    <S sid=\"323\" ssid=\"23\">The approach of Fung and Cheung (2004) is a simpler version of ours.</S>\n    <S sid=\"324\" ssid=\"24\">They match each foreign document with a set of English documents, using a threshold on their cosine similarity.</S>\n    <S sid=\"325\" ssid=\"25\">Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel.</S>\n    <S sid=\"326\" ssid=\"26\">Using the set of extracted sentences, they learn a new dictionary, try to extend their set of matching document pairs (by looking for other documents that contain these sentences), and iterate.</S>\n    <S sid=\"327\" ssid=\"27\">The evaluation methodologies of these previous approaches are less direct than ours.</S>\n    <S sid=\"328\" ssid=\"28\">Utiyama et al. (2003) evaluate their sentence pairs manually; they estimate that about 90% of the sentence pairs in their final corpus are parallel.</S>\n    <S sid=\"329\" ssid=\"29\">Fung and Cheung (2004) also perform a manual evaluation of the extracted sentences and estimate their precision to be 65.7% after bootstrapping.</S>\n    <S sid=\"330\" ssid=\"30\">In addition, they also estimate the quality of a lexicon automatically learned from those sentences.</S>\n    <S sid=\"331\" ssid=\"31\">Zhao and Vogel (2002) go one step further and show that the sentences extracted with their method improve the accuracy of automatically computed word alignments, to an F-score of 52.56% over a baseline of 46.46%.</S>\n    <S sid=\"332\" ssid=\"32\">In a subsequent publication, Vogel (2003) evaluates these sentences in the context of an MT system and shows that they bring improvement under special circumstances (i.e., a language model constructed from reference translations) designed to reduce the noise introduced by the automatically extracted corpus.</S>\n    <S sid=\"333\" ssid=\"33\">We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing.</S>\n    <S sid=\"334\" ssid=\"34\">Moreover, we show that our approach works even when only a limited amount of initial parallel data (i.e., a low-coverage dictionary) is available.</S>\n    <S sid=\"335\" ssid=\"35\">The problem of aligning sentences in comparable corpora was also addressed for monolingual texts.</S>\n    <S sid=\"336\" ssid=\"36\">Barzilay and Elhadad (2003) present a method of aligning sentences in two comparable English corpora for the purpose of building a training set of text-totext rewriting examples.</S>\n    <S sid=\"337\" ssid=\"37\">Monolingual parallel sentence detection presents a particular challenge: there are many sentence pairs that have low lexical overlap but are nevertheless parallel.</S>\n    <S sid=\"338\" ssid=\"38\">Therefore pairs cannot be judged in isolation, and context becomes an important factor.</S>\n    <S sid=\"339\" ssid=\"39\">Barzilay and Elhadad (2003) make use of contextual information by detecting the topical structure of the articles in the two corpora and aligning them at paragraph level based on the topic assigned to each paragraph.</S>\n    <S sid=\"340\" ssid=\"40\">Afterwards, they proceed and align sentences within paragraph pairs using dynamic programming.</S>\n    <S sid=\"341\" ssid=\"41\">Their results show that both the induced topical structure and the paragraph alignment improve the precision of their extraction method.</S>\n    <S sid=\"342\" ssid=\"42\">A line of research that is both complementary and related to ours is that of Resnik and Smith (2003).</S>\n    <S sid=\"343\" ssid=\"43\">Their STRAND Web-mining system has a purpose that is similar to ours: to identify translational pairs.</S>\n    <S sid=\"344\" ssid=\"44\">However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.</S>\n    <S sid=\"345\" ssid=\"45\">Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs.</S>\n    <S sid=\"346\" ssid=\"46\">Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method.</S>\n  </SECTION>\n  <SECTION title=\"9.\" number=\"10\">\n    <S sid=\"347\" ssid=\"1\">The most important feature of our parallel sentence selection approach is its robustness.</S>\n    <S sid=\"348\" ssid=\"2\">Comparable corpora are inherently noisy environments, where even similar content may be expressed in very different ways.</S>\n    <S sid=\"349\" ssid=\"3\">Moreover, out-of-domain corpora introduce additional difficulties related to limited dictionary coverage.</S>\n    <S sid=\"350\" ssid=\"4\">Therefore, the ability to reliably judge sentence pairs in isolation is crucial.</S>\n    <S sid=\"351\" ssid=\"5\">Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms.</S>\n    <S sid=\"352\" ssid=\"6\">The computational processes involved in our system are quite modest.</S>\n    <S sid=\"353\" ssid=\"7\">All the operations necessary for the classification of a sentence pair (filter, word alignment computation, and feature extraction) can be implemented efficiently and scaled up to very large amounts of data.</S>\n    <S sid=\"354\" ssid=\"8\">The task can be easily parallelized for increased speed.</S>\n    <S sid=\"355\" ssid=\"9\">For example, extracting data from 600k English documents and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors.</S>\n    <S sid=\"356\" ssid=\"10\">The data that we extract is useful.</S>\n    <S sid=\"357\" ssid=\"11\">Its impact on MT performance is comparable to that of human-translated data of similar size and domain.</S>\n    <S sid=\"358\" ssid=\"12\">Thus, although we have focused our experiments on the particular scenario where there is little in-domain training data available, we believe that our method can be useful for increasing the amount of training data, regardless of the domain of interest.</S>\n    <S sid=\"359\" ssid=\"13\">As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available.</S>\n    <S sid=\"360\" ssid=\"14\">By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system.</S>\n    <S sid=\"361\" ssid=\"15\">We suspect our approach can be used on comparable corpora coming from any domain.</S>\n    <S sid=\"362\" ssid=\"16\">The only domain-dependent element of the system is the date window parameter of the article selection stage (Figure 7); for other domains, this can be replaced with a more appropriate indication of where the parallel sentences are likely to be found.</S>\n    <S sid=\"363\" ssid=\"17\">For example, if the domain were that of technical manuals, one would cluster printer manuals and aircraft manuals separately.</S>\n    <S sid=\"364\" ssid=\"18\">It is important to note that our work assumes that the comparable corpus does contain parallel sentences (which is the case for our data).</S>\n    <S sid=\"365\" ssid=\"19\">Whether this is true for comparable corpora from other domains is an empirical question outside the scope of this article; however, both our results and those of Resnik and Smith (2003) strongly indicate that good data is available on the Web.</S>\n    <S sid=\"366\" ssid=\"20\">Lack of parallel corpora is a major bottleneck in the development of SMT systems for most language pairs.</S>\n    <S sid=\"367\" ssid=\"21\">The method presented in this paper is a step towards the important goal of automatic acquisition of such corpora.</S>\n    <S sid=\"368\" ssid=\"22\">Comparable texts are available on the Web in large quantities for many language pairs and domains.</S>\n    <S sid=\"369\" ssid=\"23\">In this article, we have shown how they can be efficiently mined for parallel sentences.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"11\">\n    <S sid=\"370\" ssid=\"1\">This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276.</S>\n    <S sid=\"371\" ssid=\"2\">The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc).</S>\n    <S sid=\"372\" ssid=\"3\">We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments.</S>\n    <S sid=\"373\" ssid=\"4\">Any remaining errors are of course our own.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "bd5d2ad5547cf25c8a707f177e6597cc",
                    "include_year": false,
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo",
                    "origin": "LLM",
                    "title": "Improving Machine Translation Performance By Exploiting Non-Parallel Corpora"
                }
            }
        },
        {
            "index": 2,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.",
                    "filename": "Unsupervised_Multilingual_Learning_for_Morphological_Segmentation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P08-1084/Documents_xml/P08-1084.xml",
                    "title": "Unsupervised Multilingual Learning for Morphological Segmentation"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores the potential benefits of unsupervised multilingual learning for morphological segmentation across different languages. The model presented in the paper aims to capitalize on structural commonalities across languages to improve morphological analysis. By incorporating probabilistic segmentation approaches and hierarchical Bayesian models, the study demonstrates successful exploitation of cross-lingual patterns in the task of unsupervised morphological segmentation. Results indicate that related languages can provide significant improvements in segmentation accuracy, especially when character-to-character phonetic correspondences are considered as an abstract morpheme prior. The study opens avenues for applying multilingual models to other unsupervised analysis tasks and further investigating the impact of language relatedness on model performance."
                    ],
                    "filename": "unsupervised_multilingual_learni-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 124 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Unsupervised Multilingual Learning for Morphological Segmentation</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995).</S>\n    <S sid=\"8\" ssid=\"2\">The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals.</S>\n    <S sid=\"9\" ssid=\"3\">The connection between languages should be a powerful source of information for automatic linguistic analysis as well.</S>\n    <S sid=\"10\" ssid=\"4\">In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning?</S>\n    <S sid=\"11\" ssid=\"5\">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>\n    <S sid=\"12\" ssid=\"6\">We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning).</S>\n    <S sid=\"13\" ssid=\"7\">For example, the English word misunderstanding would be segmented into mis understand - ing.</S>\n    <S sid=\"14\" ssid=\"8\">This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004).</S>\n    <S sid=\"15\" ssid=\"9\">The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows:</S>\n  </SECTION>\n  <SECTION title=\"Arabic: Hebrew: Aramaic:\" number=\"2\">\n    <S sid=\"16\" ssid=\"1\">This example illustrates the potential benefits of unsupervised multilingual learning.</S>\n    <S sid=\"17\" ssid=\"2\">The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land.</S>\n    <S sid=\"18\" ssid=\"3\">They also use an identical suffix (-y) to represent the first person possessive pronoun (my).</S>\n    <S sid=\"19\" ssid=\"4\">These similarities in form should guide the model by constraining the space of joint segmentations.</S>\n    <S sid=\"20\" ssid=\"5\">The corresponding English phrase lacks this resemblance to its Semitic counterparts.</S>\n    <S sid=\"21\" ssid=\"6\">However, in this as in many cases, no segmentation is required for English as all the morphemes are expressed as individual words.</S>\n    <S sid=\"22\" ssid=\"7\">For this reason, English should provide a strong source of disambiguation for highly inflected languages, such as Arabic and Hebrew.</S>\n    <S sid=\"23\" ssid=\"8\">In general, we pose the following question.</S>\n    <S sid=\"24\" ssid=\"9\">In which scenario will multilingual learning be most effective?</S>\n    <S sid=\"25\" ssid=\"10\">Will it be for related languages, which share a common core of linguistic features, or for distant languages, whose linguistic divergence can provide strong sources of disambiguation?</S>\n    <S sid=\"26\" ssid=\"11\">As a first step towards answering this question, we propose a model which can take advantage of both similarities and differences across languages.</S>\n    <S sid=\"27\" ssid=\"12\">This joint bilingual model identifies optimal morphemes for two languages and at the same time finds compact multilingual representations.</S>\n    <S sid=\"28\" ssid=\"13\">For each language in the pair, the model favors segmentations which yield high frequency morphemes.</S>\n    <S sid=\"29\" ssid=\"14\">Moreover, bilingual morpheme pairs which consistently share a common semantic or syntactic function are treated as abstract morphemes, generated by a single language-independent process.</S>\n    <S sid=\"30\" ssid=\"15\">These abstract morphemes are induced automatically by the model from recurring bilingual patterns.</S>\n    <S sid=\"31\" ssid=\"16\">For example, in the case above, the tuple (in, fy, b-, b-) would constitute one of three abstract morphemes in the phrase.</S>\n    <S sid=\"32\" ssid=\"17\">When a morpheme occurs in one language without a direct counterpart in the other language, our model can explain away the stray morpheme as arising through a language-specific process.</S>\n    <S sid=\"33\" ssid=\"18\">To achieve this effect in a probabilistic framework, we formulate a hierarchical Bayesian model with Dirichlet Process priors.</S>\n    <S sid=\"34\" ssid=\"19\">This framework allows us to define priors over the infinite set of possible morphemes in each language.</S>\n    <S sid=\"35\" ssid=\"20\">In addition, we define a prior over abstract morphemes.</S>\n    <S sid=\"36\" ssid=\"21\">This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood.</S>\n    <S sid=\"37\" ssid=\"22\">The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages.</S>\n    <S sid=\"38\" ssid=\"23\">We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations.</S>\n    <S sid=\"39\" ssid=\"24\">The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977).</S>\n    <S sid=\"40\" ssid=\"25\">Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation.</S>\n    <S sid=\"41\" ssid=\"26\">When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%.</S>\n    <S sid=\"42\" ssid=\"27\">Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning.</S>\n    <S sid=\"43\" ssid=\"28\">However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"3\">\n    <S sid=\"44\" ssid=\"1\">Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006).</S>\n    <S sid=\"45\" ssid=\"2\">Most of this research assumes that one language has annotations for the task of interest.</S>\n    <S sid=\"46\" ssid=\"3\">Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language.</S>\n    <S sid=\"47\" ssid=\"4\">In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer.</S>\n    <S sid=\"48\" ssid=\"5\">An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006).</S>\n    <S sid=\"49\" ssid=\"6\">While their approach does not require a parallel corpus it does assume the availability of annotations in one language.</S>\n    <S sid=\"50\" ssid=\"7\">Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language.</S>\n    <S sid=\"51\" ssid=\"8\">The key assumption here is that certain distributional properties are invariant across languages from the same language families.</S>\n    <S sid=\"52\" ssid=\"9\">An example of such a property is the distribution of part-of-speech bigrams.</S>\n    <S sid=\"53\" ssid=\"10\">Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version.</S>\n    <S sid=\"54\" ssid=\"11\">The approach presented here differs from previous work in two significant ways.</S>\n    <S sid=\"55\" ssid=\"12\">First, we do not assume supervised data in any of the languages.</S>\n    <S sid=\"56\" ssid=\"13\">Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time.</S>\n    <S sid=\"57\" ssid=\"14\">This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language.</S>\n    <S sid=\"58\" ssid=\"15\">Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007).</S>\n    <S sid=\"59\" ssid=\"16\">Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution.</S>\n    <S sid=\"60\" ssid=\"17\">The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings.</S>\n    <S sid=\"61\" ssid=\"18\">Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007).</S>\n    <S sid=\"62\" ssid=\"19\">In these approaches, models with short description length are preferred.</S>\n    <S sid=\"63\" ssid=\"20\">Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon.</S>\n    <S sid=\"64\" ssid=\"21\">A locally optimal segmentation is identified using a task-specific greedy search.</S>\n    <S sid=\"65\" ssid=\"22\">In contrast to previous approaches, our model induces morphological segmentation for multiple related languages simultaneously.</S>\n    <S sid=\"66\" ssid=\"23\">By representing morphemes abstractly through the simultaneous alignment and segmentation of data in two languages, our algorithm capitalizes on deep connections between morpheme usage across different languages.</S>\n  </SECTION>\n  <SECTION title=\"3 Multilingual Morphological Segmentation\" number=\"4\">\n    <S sid=\"67\" ssid=\"1\">The underlying assumption of our work is that structural commonality across different languages is a powerful source of information for morphological analysis.</S>\n    <S sid=\"68\" ssid=\"2\">In this section, we provide several examples that motivate this assumption.</S>\n    <S sid=\"69\" ssid=\"3\">The main benefit of joint multilingual analysis is that morphological structure ambiguous in one language is sometimes explicitly marked in another language.</S>\n    <S sid=\"70\" ssid=\"4\">For example, in Hebrew, the preposition meaning “in”, b-, is always prefixed to its nominal argument.</S>\n    <S sid=\"71\" ssid=\"5\">On the other hand, in Arabic, the most common corresponding particle is fy, which appears as a separate word.</S>\n    <S sid=\"72\" ssid=\"6\">By modeling crosslingual morpheme alignments while simultaneously segmenting, the model effectively propagates information between languages and in this case would be encouraged to segment the Hebrew prefix b-.</S>\n    <S sid=\"73\" ssid=\"7\">Cognates are another important means of disambiguation in the multilingual setting.</S>\n    <S sid=\"74\" ssid=\"8\">Consider translations of the phrase “...and they wrote it...”: In both languages, the triliteral root ktb is used to express the act of writing.</S>\n    <S sid=\"75\" ssid=\"9\">By considering the two phrases simultaneously, the model can be encouraged to split off the respective Hebrew and Arabic prefixes w- and f- in order to properly align the cognate root ktb.</S>\n    <S sid=\"76\" ssid=\"10\">In the following section, we describe a model that can model both generic cross-lingual patterns (fy and b-), as well as cognates between related languages (ktb for Hebrew and Arabic).</S>\n  </SECTION>\n  <SECTION title=\"4 Model\" number=\"5\">\n    <S sid=\"77\" ssid=\"1\">Overview In order to simultaneously model probabilistic dependencies across languages as well as morpheme distributions within each language, we employ a hierarchical Bayesian model.2 Our segmentation model is based on the notion that stable recurring string patterns within words are indicative of morphemes.</S>\n    <S sid=\"78\" ssid=\"2\">In addition to learning independent morpheme patterns for each language, the model will prefer, when possible, to join together frequently occurring bilingual morpheme pairs into single abstract morphemes.</S>\n    <S sid=\"79\" ssid=\"3\">The model is fully unsupervised and is driven by a preference for stable and high frequency cross-lingual morpheme patterns.</S>\n    <S sid=\"80\" ssid=\"4\">In addition the model can incorporate character-to-character phonetic correspondences between alphabets as prior information, thus allowing the implicit modeling of cognates.</S>\n    <S sid=\"81\" ssid=\"5\">Our aim is to induce a model which concentrates probability on highly frequent patterns while still allowing for the possibility of those previously unseen.</S>\n    <S sid=\"82\" ssid=\"6\">Dirichlet processes are particularly suitable for such conditions.</S>\n    <S sid=\"83\" ssid=\"7\">In this framework, we can encode prior knowledge over the infinite sets of possible morpheme strings as well as abstract morphemes.</S>\n    <S sid=\"84\" ssid=\"8\">Distributions drawn from a Dirichlet process nevertheless produce sparse representations with most probability mass concentrated on a small number of observed and predicted patterns.</S>\n    <S sid=\"85\" ssid=\"9\">Our model utilizes a Dirichlet process prior for each language, as well as for the cross-lingual links (abstract morphemes).</S>\n    <S sid=\"86\" ssid=\"10\">Thus, a distribution over morphemes and morpheme alignments is first drawn from the set of Dirichlet processes and then produces the observed data.</S>\n    <S sid=\"87\" ssid=\"11\">In practice, we never deal with such distributions directly, but rather integrate over them during Gibbs sampling.</S>\n    <S sid=\"88\" ssid=\"12\">In the next section we describe our model’s “generative story” for producing the data we observe.</S>\n    <S sid=\"89\" ssid=\"13\">We formalize our model in the context of two languages £ and F. However, the formulation can be extended to accommodate evidence from multiple languages as well.</S>\n    <S sid=\"90\" ssid=\"14\">We provide an example of parallel phrase generation in Figure 1.</S>\n    <S sid=\"91\" ssid=\"15\">High-level Generative Story We have a parallel corpus of several thousand short phrases in the two languages £ and F. Our model provides a generative story explaining how these parallel phrases were probabilistically created.</S>\n    <S sid=\"92\" ssid=\"16\">The core of the model consists of three components: a distribution A over bilingual morpheme pairs (abstract morphemes), a distribution E over stray morphemes in language £ occurring without a counterpart in language F, and a similar distribution F for stray morphemes in language F. As usual for hierarchical Bayesian models, the generative story begins by drawing the model parameters themselves – in our case the three distributions A, E, and F. These three distributions are drawn from three separate Dirichlet processes, each with appropriately defined base distributions.</S>\n    <S sid=\"93\" ssid=\"17\">The Dirichlet processes ensure that the resulting distributions concentrate their probability mass on a small number of morphemes while holding out reasonable probability for unseen possibilities.</S>\n    <S sid=\"94\" ssid=\"18\">Once A, E, and F have been drawn, we model our parallel corpus of short phrases as a series of independent draws from a phrase-pair generation model.</S>\n    <S sid=\"95\" ssid=\"19\">For each new phrase-pair, the model first chooses the number and type of morphemes to be generated.</S>\n    <S sid=\"96\" ssid=\"20\">In particular, it must choose how many unaligned stray morphemes from language £, unaligned stray morphemes from language F, and abstract morphemes are to compose the parallel phrases.</S>\n    <S sid=\"97\" ssid=\"21\">These three numbers, respectively denoted as m, n, and k, are drawn from a Poisson distribution.</S>\n    <S sid=\"98\" ssid=\"22\">This step is illustrated in Figure 1 part (a).</S>\n    <S sid=\"99\" ssid=\"23\">The model then proceeds to independently draw m language £ morphemes from distribution E, n language-F morphemes from distribution F, and k abstract morphemes from distribution A.</S>\n    <S sid=\"100\" ssid=\"24\">This step is illustrated in part (b) of Figure 1.</S>\n    <S sid=\"101\" ssid=\"25\">The m + k resulting language-£ morphemes are then ordered and fused to form a phrase in language £, and likewise for the n + k resulting languageF morphemes.</S>\n    <S sid=\"102\" ssid=\"26\">The ordering and fusing decisions are modeled as draws from a uniform distribution over the set of all possible orderings and fusings for sizes m, n, and k. These final steps are illustrated in parts (c)-(d) of Figure 1.</S>\n    <S sid=\"103\" ssid=\"27\">Now we describe the model more formally.</S>\n    <S sid=\"104\" ssid=\"28\">Stray Morpheme Distributions Sometimes a morpheme occurs in a phrase in one language without a corresponding foreign language morpheme in the parallel phrase.</S>\n    <S sid=\"105\" ssid=\"29\">We call these “stray morphemes,” and we employ language-specific morpheme distributions to model their generation.</S>\n    <S sid=\"106\" ssid=\"30\">For each language, we draw a distribution over all possible morphemes (finite-length strings composed of characters in the appropriate alphabet) from a Dirichlet process with concentration parameter α and base distribution Pe or Pf respectively: The base distributions Pe and Pf can encode prior knowledge about the properties of morphemes in each of the two languages, such as length and character n-grams.</S>\n    <S sid=\"107\" ssid=\"31\">For simplicity, we use a geometric distribution over the length of the string with a final end-morpheme character.</S>\n    <S sid=\"108\" ssid=\"32\">The distributions E and F which result from the respective Dirichlet processes place most of their probability mass on a small number of morphemes with the degree of concentration As before, the resulting distribution A will give non-zero probability to all abstract morphemes (e, f).</S>\n    <S sid=\"109\" ssid=\"33\">The base distribution acts as a prior on such pairs.</S>\n    <S sid=\"110\" ssid=\"34\">To define we can simply use a mixture of geometric distributions in the lengths of the component morphemes.</S>\n    <S sid=\"111\" ssid=\"35\">However, if the languages £ and are related and the regular phonetic correspondences between the letter in the two alphabets are known, then we can use to assign higher likelihood to potential cognates.</S>\n    <S sid=\"112\" ssid=\"36\">In particular we define the prior f) to be the probabilistic string-edit distance (Ristad and Yianilos, 1998) between and the known phonetic correspondences to parameterize the string-edit model.</S>\n    <S sid=\"113\" ssid=\"37\">In particular, insertion and deletion probabilities are held constant for all characters, and substitution probabilities are determined based on the known sound correspondences.</S>\n    <S sid=\"114\" ssid=\"38\">We report results for both the simple geometric prior as well as the string-edit prior.</S>\n    <S sid=\"115\" ssid=\"39\">Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution.</S>\n    <S sid=\"116\" ssid=\"40\">These three integers represent the number and type of the morphemes d the number of coupled bilingual morpheme pairs, respectively. controlled by the prior Nevertheless, some nonzero probability is reserved for every possible string.</S>\n    <S sid=\"117\" ssid=\"41\">We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006).</S>\n    <S sid=\"118\" ssid=\"42\">Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes.</S>\n    <S sid=\"119\" ssid=\"43\">This model assigns probabilities to all pairs of morphemes is, all pairs of finite strings from the respective alphabets (e, f).</S>\n    <S sid=\"120\" ssid=\"44\">Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g.</S>\n    <S sid=\"121\" ssid=\"45\">(fy, b-) for in Arabic and Hebrew).</S>\n    <S sid=\"122\" ssid=\"46\">These morpheme pairs can thus be viewed as representing abstract morphemes.</S>\n    <S sid=\"123\" ssid=\"47\">As with the stray morpheme models, we wish to define a distribution which concentrates probability mass on a small number of highly co-occurring morpheme pairs while still holding out some probability for all other pairs.</S>\n    <S sid=\"124\" ssid=\"48\">We define this abstract morpheme model A as a draw from an that compose the parallel phrase, giving the number of stray morphemes in each language £ and an m, n, k ti Given these values, we now draw the appropriate number of stray an d abstract morphemes from the corresponding distributions: e1, ..., em ∼ E f1, ..., fn ∼ F (ei, fi), ..., (ek, f�k) ∼ A The sets of morphemes drawn for each language are then ordered: ˜e1, ..., ˜em+k ∼ ORDER|e1, ..., em, ei, ..., ek ˜f1, ..., ˜fn+k ∼ ORDER|f1, ..., fn, f1, ..., fk Finally the ordered morphemes are fused into the words that form the parallel phrases: To keep the model as simple as possible, we employ uniform distributions over the sets of orderings and fusings.</S>\n    <S sid=\"125\" ssid=\"49\">In other words, given a set of r morphemes (for each language), we define the distribution over permutations of the morphemes to simply be ORDER(·|r) = 1��.</S>\n    <S sid=\"126\" ssid=\"50\">Then, given a fixed morpheme order, we consider fusing each adjacent morpheme into a single word.</S>\n    <S sid=\"127\" ssid=\"51\">Again, we simply model the distribution over the r − 1 fusing decisions uniformly as FUSE(·|r) = 1 2''−1 .</S>\n    <S sid=\"128\" ssid=\"52\">Implicit Alignments Note that nowhere do we explicitly assign probabilities to morpheme alignments between parallel phrases.</S>\n    <S sid=\"129\" ssid=\"53\">However, our model allows morphemes to be generated in precisely one of two ways: as a lone stray morpheme or as part of a bilingual abstract morpheme pair.</S>\n    <S sid=\"130\" ssid=\"54\">Thus, our model implicitly assumes that each morpheme is either unaligned, or aligned to exactly one morpheme in the opposing language.</S>\n    <S sid=\"131\" ssid=\"55\">If we are given a parallel phrase with already segmented morphemes we can easily induce the distribution over alignments implied by our model.</S>\n    <S sid=\"132\" ssid=\"56\">As we will describe in the next section, drawing from these induced alignment distributions plays a crucial role in our inference procedure.</S>\n    <S sid=\"133\" ssid=\"57\">Inference Given our corpus of short parallel bilingual phrases, we wish to make segmentation decisions which yield a set of morphemes with high joint probability.</S>\n    <S sid=\"134\" ssid=\"58\">To assess the probability of a potential morpheme set, we need to marginalize over all possible alignments (i.e. possible abstract morpheme pairings and stray morpheme assignments).</S>\n    <S sid=\"135\" ssid=\"59\">We also need to marginalize over all possible draws of the distributions A, E, and F from their respective Dirichlet process priors.</S>\n    <S sid=\"136\" ssid=\"60\">We achieve these aims by performing Gibbs sampling.</S>\n    <S sid=\"137\" ssid=\"61\">Sampling We follow (Neal, 1998) in the derivation of our blocked and collapsed Gibbs sampler.</S>\n    <S sid=\"138\" ssid=\"62\">Gibbs sampling starts by initializing all random variables to arbitrary starting values.</S>\n    <S sid=\"139\" ssid=\"63\">At each iteration, the sampler selects a random variable Xi, and draws a new value for Xi from the conditional distribution of Xi given the current value of the other variables: P(Xi|X_i).</S>\n    <S sid=\"140\" ssid=\"64\">The stationary distribution of variables derived through this procedure is guaranteed to converge to the true joint distribution of the random variables.</S>\n    <S sid=\"141\" ssid=\"65\">However, if some variables can be jointly sampled, then it may be beneficial to perform block sampling of these variables to speed convergence.</S>\n    <S sid=\"142\" ssid=\"66\">In addition, if a random variable is not of direct interest, we can avoid sampling it directly by marginalizing it out, yielding a collapsed sampler.</S>\n    <S sid=\"143\" ssid=\"67\">We utilize variable blocking by jointly sampling multiple segmentation and alignment decisions.</S>\n    <S sid=\"144\" ssid=\"68\">We also collapse our Gibbs sampler in the standard way, by using predictive posteriors marginalized over all possible draws from the Dirichlet processes (resulting in Chinese Restaurant Processes).</S>\n    <S sid=\"145\" ssid=\"69\">Resampling For each bilingual phrase, we resample each word in the phrase in turn.</S>\n    <S sid=\"146\" ssid=\"70\">For word w in language E, we consider at once all possible segmentations, and for each segmentation all possible alignments.</S>\n    <S sid=\"147\" ssid=\"71\">We keep fixed the previously sampled segmentation decisions for all other words in the phrase as well as sampled alignments involving morphemes in other words.</S>\n    <S sid=\"148\" ssid=\"72\">We are thus considering at once: all possible segmentations of w along with all possible alignments involving morphemes in w with some subset of previously sampled languageF morphemes.3 The sampling formulas are easily derived as products of the relevant Chinese Restaurant Processes (with a minor adjustment to take into account the number of stray and abstract morphemes resulting from each decision).</S>\n    <S sid=\"149\" ssid=\"73\">See (Neal, 1998) for general formulas for Gibbs sampling from distributions with Dirichlet process priors.</S>\n    <S sid=\"150\" ssid=\"74\">All results reported are averaged over five runs using simulated annealing.</S>\n  </SECTION>\n  <SECTION title=\"5 Experimental Set-Up\" number=\"6\">\n    <S sid=\"151\" ssid=\"1\">Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots.</S>\n    <S sid=\"152\" ssid=\"2\">We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation.</S>\n    <S sid=\"153\" ssid=\"3\">Dataset As a source of parallel data, we use the Hebrew Bible and translations.</S>\n    <S sid=\"154\" ssid=\"4\">For the Hebrew version, we use an edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006).</S>\n    <S sid=\"155\" ssid=\"5\">This Bible edition is augmented by gold standard morphological analysis (including segmentation) performed by biblical scholars.</S>\n    <S sid=\"156\" ssid=\"6\">For the Arabic, Aramaic, and English versions, fied by augmenting the model with a pair of “morphemeidentity” variables deterministically drawn from each abstract morpheme.</S>\n    <S sid=\"157\" ssid=\"7\">Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism. we use the Van Dyke Arabic translation,4 Targum Onkelos,5 and the Revised Standard Version (Nelson, 1952), respectively.</S>\n    <S sid=\"158\" ssid=\"8\">We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data (Habash and Rambow, 2005).</S>\n    <S sid=\"159\" ssid=\"9\">The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included.</S>\n    <S sid=\"160\" ssid=\"10\">We don’t have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic.</S>\n    <S sid=\"161\" ssid=\"11\">To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph.</S>\n    <S sid=\"162\" ssid=\"12\">This process allows us to group together phrases like fy s.bah. in Arabic and bbqr in Hebrew while being reasonably certain that all the relevant morphemes are contained in the short extracted phrases.</S>\n    <S sid=\"163\" ssid=\"13\">The number of words in such phrases ranges from one to four words in the Semitic languages and up to six words in English.</S>\n    <S sid=\"164\" ssid=\"14\">Before performing any experiments, a manual inspection of the generated parallel phrases revealed that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment.</S>\n    <S sid=\"165\" ssid=\"15\">Therefore, we eliminated all parallel phrases that occur fewer than five times.</S>\n    <S sid=\"166\" ssid=\"16\">As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English.</S>\n    <S sid=\"167\" ssid=\"17\">The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic.</S>\n    <S sid=\"168\" ssid=\"18\">For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of (Thackston, 1999).</S>\n    <S sid=\"169\" ssid=\"19\">All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero.</S>\n    <S sid=\"170\" ssid=\"20\">Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score.</S>\n    <S sid=\"171\" ssid=\"21\">This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points.</S>\n    <S sid=\"172\" ssid=\"22\">The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase.</S>\n    <S sid=\"173\" ssid=\"23\">During testing, we do not allow the models to consider any multilingual evidence.</S>\n    <S sid=\"174\" ssid=\"24\">This restriction allows us to simulate future performance on purely monolingual data.</S>\n    <S sid=\"175\" ssid=\"25\">Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart.</S>\n    <S sid=\"176\" ssid=\"26\">However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR (Creutz and Lagus, 2007), a state-of-the-art unsupervised system for morphological segmentation.</S>\n    <S sid=\"177\" ssid=\"27\">While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English.</S>\n    <S sid=\"178\" ssid=\"28\">The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task.</S>\n    <S sid=\"179\" ssid=\"29\">We use the publicly available implementation of this system.</S>\n    <S sid=\"180\" ssid=\"30\">To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decisions based on a coin weighted with the true segmentation frequency.</S>\n  </SECTION>\n  <SECTION title=\"6 Results\" number=\"7\">\n    <S sid=\"181\" ssid=\"1\">Table 1 shows the performance of the various automatic segmentation methods.</S>\n    <S sid=\"182\" ssid=\"2\">The first three rows provide baselines, as mentioned in the previous section.</S>\n    <S sid=\"183\" ssid=\"3\">Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don’t use character-tocharacter phonetic correspondences to capture cognate information.</S>\n    <S sid=\"184\" ssid=\"4\">We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL.</S>\n    <S sid=\"185\" ssid=\"5\">We notice that in general, adding English – which has comparatively little morphological ambiguity – is about as useful as adding a more closely related Semitic language.</S>\n    <S sid=\"186\" ssid=\"6\">However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusions and Future Work\" number=\"8\">\n    <S sid=\"187\" ssid=\"1\">We started out by posing two questions: (i) Can we exploit cross-lingual patterns to improve unsupervised analysis?</S>\n    <S sid=\"188\" ssid=\"2\">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>\n    <S sid=\"189\" ssid=\"3\">The model and results presented in this paper answer the first question in the affirmative, at least for the task of morphological segmentation.</S>\n    <S sid=\"190\" ssid=\"4\">We also provided some evidence that considering closely related languages may be more beneficial than distant pairs if the model is able to explicitly represent shared language structure (the characterto-character phonetic correspondences in our case).</S>\n    <S sid=\"191\" ssid=\"5\">In the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models.</S>\n    <S sid=\"192\" ssid=\"6\">7</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "7903b8f167b56c622a27fa8561ca1d4c",
                    "include_year": false,
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo",
                    "origin": "LLM",
                    "title": "Unsupervised Multilingual Learning for Morphological Segmentation"
                }
            }
        },
        {
            "index": 3,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "author: Online conversational text, typified by microblogs, and text is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at:",
                    "filename": "Improved_Part-of-Speech_Tagging_for_Online_Conversational_Text_with_Word_Clusters",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/N13-1039/Documents_xml/N13-1039.xml",
                    "title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters"
                },
                "llm": {
                    "descriptions": [
                        "This academic paper presents an improved part-of-speech tagging system for online conversational text, focusing on Twitter and Internet Relay Chat (IRC) messages. The system utilizes a first-order maximum entropy Markov model (MEMM) for tagging, incorporating new large-scale distributional features. Additionally, the paper introduces unsupervised word clusters derived from a large set of unlabeled tweets, which significantly enhance tagging performance for online conversational text. The experiments conducted on annotated datasets demonstrate the system's state-of-the-art accuracy compared to previous works. The study highlights the importance of word clusters in handling non-traditional words and showcases the effectiveness of the tagging model in various linguistic contexts. Overall, the research provides valuable resources to the research community, including the datasets, annotation guidelines, and the open-source tagger, aiming to advance the understanding and analysis of online conversational text genres."
                    ],
                    "filename": "improved_partofspeech_tagging_fo-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 201 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"13\" ssid=\"1\">asked for your last name so he can add you on Facebook.</S>\n    <S sid=\"14\" ssid=\"2\">The tagset is defined in Appendix A.</S>\n    <S sid=\"15\" ssid=\"3\">Refer to Fig.</S>\n    <S sid=\"16\" ssid=\"4\">2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.</S>\n    <S sid=\"17\" ssid=\"5\">Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010).</S>\n    <S sid=\"18\" ssid=\"6\">To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features.</S>\n    <S sid=\"19\" ssid=\"7\">This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text.</S>\n    <S sid=\"20\" ssid=\"8\">We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets.</S>\n    <S sid=\"21\" ssid=\"9\">We release all of these resources to the research community:</S>\n  </SECTION>\n  <SECTION title=\"2 MEMM Tagger\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1, and is parameterized by a multiclass logistic regression: We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A).</S>\n    <S sid=\"23\" ssid=\"2\">Our feature sets will be discussed below in detail.</S>\n    <S sid=\"24\" ssid=\"3\">Decoding.</S>\n    <S sid=\"25\" ssid=\"4\">For experiments reported in this paper, we use the O(|x|K2) Viterbi algorithm for prediction; K is the number of tags.</S>\n    <S sid=\"26\" ssid=\"5\">This exactly maximizes p(y  |x), but the MEMM also naturally allows a faster O(|x|K) left-to-right greedy decoding: fort = 1... |x|: yt arg maxk p(yt = k  |�yt−1, x, t; 3) which we find is 3 times faster and yields similar accuracy as Viterbi (an insignificant accuracy decrease of less than 0.1% absolute on the DAILY547 test set discussed below).</S>\n    <S sid=\"27\" ssid=\"6\">Speed is paramount for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software.</S>\n    <S sid=\"28\" ssid=\"7\">2Although when compared to CRFs, MEMMs theoretically suffer from the “label bias” problem (Lafferty et al., 2001), our system substantially outperforms the CRF-based taggers of previous work; and when comparing to Gimpel et al. system with similar feature sets, we observed little difference in accuracy.</S>\n    <S sid=\"29\" ssid=\"8\">This is consistent with conventional wisdom that the quality of lexical features is much more important than the parametric form of the sequence model, at least in our setting: part-ofspeech tagging with a small labeled training set.</S>\n    <S sid=\"30\" ssid=\"9\">This greedy tagger runs at 800 tweets/sec.</S>\n    <S sid=\"31\" ssid=\"10\">(10,000 tokens/sec.) on a single CPU core, about 40 times faster than Gimpel et al.’s system.</S>\n    <S sid=\"32\" ssid=\"11\">The tokenizer by itself (§4) runs at 3,500 tweets/sec.3 Training and regularization.</S>\n    <S sid=\"33\" ssid=\"12\">During training, the MEMM log-likelihood for a tagged tweet (x, y) is the sum over the observed token tags yt, each conditional on the tweet being tagged and the observed previous tag (with a start symbol before the first token in x), We optimize the parameters 3 with OWL-QN, an L1-capable variant of L-BFGS (Andrew and Gao, 2007; Liu and Nocedal, 1989) to minimize the regularized objective where N is the number of tokens in the corpus and the sum ranges over all tagged tweets (x, y) in the training data.</S>\n    <S sid=\"34\" ssid=\"13\">We use elastic net regularization (Zou and Hastie, 2005), which is a linear combination of L1 and L2 penalties; here j indexes over all features: Using even a very small L1 penalty eliminates many irrelevant or noisy features.</S>\n  </SECTION>\n  <SECTION title=\"3 Unsupervised Word Clusters\" number=\"3\">\n    <S sid=\"35\" ssid=\"1\">Our POS tagger can make use of any number of possibly overlapping features.</S>\n    <S sid=\"36\" ssid=\"2\">While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web.</S>\n    <S sid=\"37\" ssid=\"3\">Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia).</S>\n    <S sid=\"38\" ssid=\"4\">We use a similar approach here to improve tagging performance for online conversational text.</S>\n    <S sid=\"39\" ssid=\"5\">We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre.</S>\n    <S sid=\"40\" ssid=\"6\">We obtained hierarchical word clusters via Brown clustering (Brown et al., 1992) on a large set of unlabeled tweets.4 The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint.</S>\n    <S sid=\"41\" ssid=\"7\">Not only does Brown clustering produce effective features for discriminative models, but its variants are better unsupervised POS taggers than some models developed nearly 20 years later; see comparisons in Blunsom and Cohn (2011).</S>\n    <S sid=\"42\" ssid=\"8\">The algorithm is attractive for our purposes since it scales to large amounts of data.</S>\n    <S sid=\"43\" ssid=\"9\">When training on tweets drawn from a single day, we observed time-specific biases (e.g., numerical dates appearing in the same cluster as the word tonight), so we assembled our unlabeled data from a random sample of 100,000 tweets per day from September 10, 2008 to August 14, 2012, and filtered out non-English tweets (about 60% of the sample) using langid.py (Lui and Baldwin, 2012).5 Each tweet was processed with our tokenizer and lowercased.</S>\n    <S sid=\"44\" ssid=\"10\">We normalized all atmentions to (@MENTION) and URLs/email addresses to their domains (e.g. http://bit.ly/ dP8rR8 ==&gt;- (URL-bit.ly)).</S>\n    <S sid=\"45\" ssid=\"11\">In an effort to reduce spam, we removed duplicated tweet texts (this also removes retweets) before word clustering.</S>\n    <S sid=\"46\" ssid=\"12\">This normalization and cleaning resulted in 56 million unique tweets (847 million tokens).</S>\n    <S sid=\"47\" ssid=\"13\">We set the clustering software’s count threshold to only cluster words appearing 40 or more times, yielding 216,856 word types, which took 42 hours to cluster on a single CPU.</S>\n    <S sid=\"48\" ssid=\"14\">Fig.</S>\n    <S sid=\"49\" ssid=\"15\">2 shows example clusters.</S>\n    <S sid=\"50\" ssid=\"16\">Some of the challenging words in the example tweet (Fig.</S>\n    <S sid=\"51\" ssid=\"17\">1) are highlighted.</S>\n    <S sid=\"52\" ssid=\"18\">The term lololol (an extension of lol for “laughing out loud”) is grouped with a large number of laughter acronyms (A1: “laughing my (fucking) ass off,” “cracking the fuck up”).</S>\n    <S sid=\"53\" ssid=\"19\">Since expressions of laughter are so prevalent on Twitter, the algorithm creates another laughter cluster (A1’s sibling A2), that tends to have onomatopoeic, non-acronym variants (e.g., haha).</S>\n    <S sid=\"54\" ssid=\"20\">The acronym ikr (“I know, right?”) is grouped with expressive variations of “yes” and “no” (A4).</S>\n    <S sid=\"55\" ssid=\"21\">Note that A1–A4 are grouped in a fairly specific subtree; and indeed, in this message ikr and lololol are both tagged as interjections. smh (“shaking my head,” indicating disapproval) seems related, though is always tagged in the annotated data as a miscellaneous abbreviation (G); the difference between acronyms that are interjections versus other acronyms may be complicated.</S>\n    <S sid=\"56\" ssid=\"22\">Here, smh is in a related but distinct subtree from the above expressions (A5); its usage in this example is slightly different from its more common usage, which it shares with the other words in its cluster: message-ending expressions of commentary or emotional reaction, sometimes as a metacomment on the author’s message; e.g., Maybe you could get a guy to date you if you actually respected yourself #smh or There is really NO reason why other girls should send my boyfriend a goodmorning text #justsaying.</S>\n    <S sid=\"57\" ssid=\"23\">We observe many variants of categories traditionally considered closed-class, including pronouns (B: u = “you”) and prepositions (C: fir = “for”).</S>\n    <S sid=\"58\" ssid=\"24\">There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012).</S>\n    <S sid=\"59\" ssid=\"25\">This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011).</S>\n    <S sid=\"60\" ssid=\"26\">We have built an HTML viewer to browse these and numerous other interesting examples.7 We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English).</S>\n    <S sid=\"61\" ssid=\"27\">To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” tion symbols, and emoji to mean symbols rendered in software as small pictures, in line with the text.</S>\n    <S sid=\"62\" ssid=\"28\">Since our tokenizer is careful to preserve emoticons and other symbols (see §4), they are clustered just like other words.</S>\n    <S sid=\"63\" ssid=\"29\">Similar emoticons are clustered together (G1–G4), including separate clusters of happy [[ :) =) ^_^ ]], sad/disappointed [[ :/ :( -_- &lt;/3 ]], love [[ ❑xoxo ❑.❑ ]] and winking [[ ;) (^_-) ]] emoticons.</S>\n    <S sid=\"64\" ssid=\"30\">The clusters are not perfectly aligned with our POS annotation guidelines; for example, the “sad” emoticon cluster included emotion-bearing terms that our guidelines define as non-emoticons, such as #ugh, #tear, and #fml (“fuck my life”), though these seem potentially useful for sentiment analysis.</S>\n    <S sid=\"65\" ssid=\"31\">One difficult task is classifying different types of symbols in tweets: our annotation guidelines differentiate between emoticons, punctuation, and garbage (apparently non-meaningful symbols or tokenization errors).</S>\n    <S sid=\"66\" ssid=\"32\">Several Unicode character ranges are reserved for emoji-style symbols (including the three Unicode hearts in G4); however, depending on the user’s software, characters in these ranges might be rendered differently or not at all.</S>\n    <S sid=\"67\" ssid=\"33\">We have found instances where the clustering algorithm groups proprietary iOS emoji symbols along with normal emoticons; for example, the character U+E056, which is interpreted on iOS as a smiling face, is in the same G2 cluster as smiley face emoticons.</S>\n    <S sid=\"68\" ssid=\"34\">The symbol U+E12F, which represents a picture of a bag of money, is grouped with the words cash and money.</S>\n    <S sid=\"69\" ssid=\"35\">Since Brown clusters are hierarchical in a binary tree, each word is associated with a tree path represented as a bitstring with length &lt; 16; we use prefixes of the bitstring as features (for all prefix lengths E 12, 4, 6,... ,16}).</S>\n    <S sid=\"70\" ssid=\"36\">This allows sharing of statistical strength between similar clusters.</S>\n    <S sid=\"71\" ssid=\"37\">Using prefix features of hierarchical clusters in this way was similarly found to be effective for named-entity recognition (Turian et al., 2010) and Twitter POS tagging (Ritter et al., 2011).</S>\n    <S sid=\"72\" ssid=\"38\">When checking to see if a word is associated with a cluster, the tagger first normalizes the word using the same techniques as described in §3.1, then creates a priority list of fuzzy match transformations of the word by removing repeated punctuation and repeated characters.</S>\n    <S sid=\"73\" ssid=\"39\">If the normalized word is not in a cluster, the tagger considers the fuzzy matches.</S>\n    <S sid=\"74\" ssid=\"40\">Although only about 3% of the tokens in the development set (§6) did not appear in a clustering, this method resulted in a relative error decrease of 18% among such word tokens.</S>\n    <S sid=\"75\" ssid=\"41\">Besides unsupervised word clusters, there are two other sets of features that contain generalized lexical class information.</S>\n    <S sid=\"76\" ssid=\"42\">We use the tag dictionary feature from Gimpel et al., which adds a feature for a word’s most frequent part-of-speech tag.8 This can be viewed as a feature-based domain adaptation method, since it gives lexical type-level information for standard English words, which the model learns to map between PTB tags to the desired output tags.</S>\n    <S sid=\"77\" ssid=\"43\">Second, since the lack of consistent capitalization conventions on Twitter makes it especially difficult to recognize names—Gimpel et al. and Foster et al.</S>\n    <S sid=\"78\" ssid=\"44\">(2011) found relatively low accuracy on proper nouns—we added a token-level name list feature, which fires on (non-function) words from names from several sources: Freebase lists of celebrities and video games (Google, 2012), the Moby Words list of US Locations,9 and lists of male, female, family, and proper names from Mark Kantrowitz’s name corpus.10</S>\n  </SECTION>\n  <SECTION title=\"4 Tokenization and Emoticon Detection\" number=\"4\">\n    <S sid=\"79\" ssid=\"1\">Word segmentation on Twitter is challenging due to the lack of orthographic conventions; in particular, punctuation, emoticons, URLs, and other symbols may have no whitespace separation from textual 8Frequencies came from the Wall Street Journal and Brown corpus sections of the Penn Treebank.</S>\n    <S sid=\"80\" ssid=\"2\">If a word has multiple PTB tags, each tag is a feature with value for the frequency rank; e.g. for three different tags in the PTB, this feature gives a value of 1 for the most frequent tag, 2/3 for the second, etc.</S>\n    <S sid=\"81\" ssid=\"3\">Coarse versions of the PTB tags are used (Petrov et al., 2011).</S>\n    <S sid=\"82\" ssid=\"4\">While 88% of words in the dictionary have only one tag, using rank information seemed to give a small but consistent gain over only using the most common tag, or using binary features conjoined with rank as in Gimpel et al. words (e.g. no:-d,yes should parse as four tokens), and internally may contain alphanumeric symbols that could be mistaken for words: a naive split(/[^azA-Z0-9]+/) tokenizer thinks the words “p” and “d” are among the top 100 most common words on Twitter, due to misanalysis of :p and :d. Traditional Penn Treebank–style tokenizers are hardly better, often breaking a string of punctuation characters into a single token per character.</S>\n    <S sid=\"83\" ssid=\"5\">We rewrote twokenize (O’Connor et al., 2010), a rule-based tokenizer, emoticon, and URL detector, for use in the tagger.</S>\n    <S sid=\"84\" ssid=\"6\">Emoticons are especially challenging, since they are open-class and productive.</S>\n    <S sid=\"85\" ssid=\"7\">We revise O’Connor et al.’s regular expression grammar that describes possible emoticons, adding a grammar of horizontal emoticons (e.g.</S>\n    <S sid=\"86\" ssid=\"8\">-_-), known as “Eastern-style,”11 though we observe high usage in English-speaking Twitter (Fig.</S>\n    <S sid=\"87\" ssid=\"9\">2, G2–G3).</S>\n    <S sid=\"88\" ssid=\"10\">We also add a number of other improvements to the patterns.</S>\n    <S sid=\"89\" ssid=\"11\">Because this system was used as preprocessing for the word clustering experiment in §3, we were able to infer the emoticon clusters in Fig.</S>\n    <S sid=\"90\" ssid=\"12\">2.</S>\n    <S sid=\"91\" ssid=\"13\">Furthermore, whether a token matches the emoticon pattern is also used as a feature in the tagger (§2).</S>\n    <S sid=\"92\" ssid=\"14\">URL recognition is also difficult, since the http:// is often dropped, resulting in protocol-less URLs like about.me.</S>\n    <S sid=\"93\" ssid=\"15\">We add recognition patterns for these by using a list of top-level and country domains.</S>\n  </SECTION>\n  <SECTION title=\"5 Annotated Dataset\" number=\"5\">\n    <S sid=\"94\" ssid=\"1\">Gimpel et al. (2011) provided a dataset of POStagged tweets consisting almost entirely of tweets sampled from one particular day (October 27, 2010).</S>\n    <S sid=\"95\" ssid=\"2\">We were concerned about overfitting to timespecific phenomena; for example, a substantial fraction of the messages are about a basketball game happening that day.</S>\n    <S sid=\"96\" ssid=\"3\">We created a new test set of 547 tweets for evaluation.</S>\n    <S sid=\"97\" ssid=\"4\">The test set consists of one random English tweet from every day between January 1, 2011 and June 30, 2012.</S>\n    <S sid=\"98\" ssid=\"5\">In order for a tweet to be considered English, it had to contain at least one English word other than a URL, emoticon, or at-mention.</S>\n    <S sid=\"99\" ssid=\"6\">We noticed biases in the outputs of langid.py, so we instead selected these messages completely manually (going through a random sample of one day’s messages until an English message was found).</S>\n    <S sid=\"100\" ssid=\"7\">Gimpel et al. provided a tagset for Twitter (shown in Appendix A), which we used unmodified.</S>\n    <S sid=\"101\" ssid=\"8\">The original annotation guidelines were not published, but in this work we recorded the rules governing tagging decisions and made further revisions while annotating the new data.12 Some of our guidelines reiterate or modify rules made by Penn Treebank annotators, while others treat specific phenomena found on Twitter (refer to the next section).</S>\n    <S sid=\"102\" ssid=\"9\">Our tweets were annotated by two annotators who attempted to match the choices made in Gimpel et al.’s dataset.</S>\n    <S sid=\"103\" ssid=\"10\">The annotators also consulted the POS annotations in the Penn Treebank (Marcus et al., 1993) as an additional reference.</S>\n    <S sid=\"104\" ssid=\"11\">Differences were reconciled by a third annotator in discussion with all annotators.13 During this process, an inconsistency was found in Gimpel et al.’s data, which we corrected (concerning the tagging of this/that, a change to 100 labels, 0.4%).</S>\n    <S sid=\"105\" ssid=\"12\">The new version of Gimpel et al.’s data (called OCT27), as well as the newer messages (called DAILY547), are both included in our data release.</S>\n    <S sid=\"106\" ssid=\"13\">Ritter et al. (2011) annotated tweets using an augmented version of the PTB tagset and presumably followed the PTB annotation guidelines.</S>\n    <S sid=\"107\" ssid=\"14\">We wrote new guidelines because the PTB conventions are inappropriate for Twitter in several ways, as shown in the design of Gimpel et al.’s tagset.</S>\n    <S sid=\"108\" ssid=\"15\">Importantly, “compound” tags (e.g., nominal+verbal and nominal+possessive) are used because tokenization is difficult or seemingly impossible for the nonstandard word forms that are commonplace in conversational text.</S>\n    <S sid=\"109\" ssid=\"16\">For example, the PTB tokenization splits contractions containing apostrophes: I’m ==&gt;.</S>\n    <S sid=\"110\" ssid=\"17\">I/PRP ’m/VBP.</S>\n    <S sid=\"111\" ssid=\"18\">But conversational text often contains variants that resist a single PTB tag (like im), or even challenge traditional English grammatical categories (like imma or umma, which both mean “I am going to”).</S>\n    <S sid=\"112\" ssid=\"19\">One strategy would be to analyze these forms into a PTB-style tokenization, as discussed in Forsyth (2007), who proposes to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult.</S>\n    <S sid=\"113\" ssid=\"20\">We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc.</S>\n    <S sid=\"114\" ssid=\"21\">These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig.</S>\n    <S sid=\"115\" ssid=\"22\">2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near.</S>\n    <S sid=\"116\" ssid=\"23\">We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task.</S>\n    <S sid=\"117\" ssid=\"24\">In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011).</S>\n    <S sid=\"118\" ssid=\"25\">For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal?</S>\n    <S sid=\"119\" ssid=\"26\">Instead, we would like to directly analyze the syntax of online conversational text on its own terms.</S>\n    <S sid=\"120\" ssid=\"27\">Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS patterns that seem quite compatible with our approach.</S>\n    <S sid=\"121\" ssid=\"28\">More complex downstream processing like parsing is an interesting challenge, since contraction parsing on traditional text is probably a benefit to current parsers.</S>\n    <S sid=\"122\" ssid=\"29\">We believe that any PTB-trained tool requires substantial retraining and adaptation for Twitter due to the huge genre and stylistic differences (Foster et al., 2011); thus tokenization conventions are a relatively minor concern.</S>\n    <S sid=\"123\" ssid=\"30\">Our simple-toannotate conventions make it easier to produce new training data.</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments\" number=\"6\">\n    <S sid=\"124\" ssid=\"1\">We are primarily concerned with performance on our annotated datasets described in §5 (OCT27, DAILY547), though for comparison to previous work we also test on other corpora (RITTERTW in §6.2, NPSCHAT in §6.3).</S>\n    <S sid=\"125\" ssid=\"2\">The annotated datasets are listed in Table 1.</S>\n    <S sid=\"126\" ssid=\"3\">We use OCT27 to refer to the entire dataset described in Gimpel et al. ; it is split into training, development, and test portions (OCT27TRAIN, OCT27DEV, OCT27TEST).</S>\n    <S sid=\"127\" ssid=\"4\">We use DAILY547 as an additional test set.</S>\n    <S sid=\"128\" ssid=\"5\">Neither OCT27TEST nor DAILY547 were extensively evaluated against until final ablation testing when writing this paper.</S>\n    <S sid=\"129\" ssid=\"6\">The total number of features is 3.7 million, all of which are used under pure L2 regularization; but only 60,000 are selected by elastic net regularization with (A1, A2) = (0.25, 2), which achieves nearly the same (but no better) accuracy as pure L2,16 and we use it for all experiments.</S>\n    <S sid=\"130\" ssid=\"7\">We observed that it was 16We conducted a grid search for the regularizer values on part of DAILY547, and many regularizer values give the best or nearly the best results.</S>\n    <S sid=\"131\" ssid=\"8\">We suspect a different setup would have yielded similar results. possible to get radically smaller models with only a slight degradation in performance: (4, 0.06) has 0.5% worse accuracy but uses only 1,632 features, a small enough number to browse through manually.</S>\n    <S sid=\"132\" ssid=\"9\">First, we evaluate on the new test set, training on all of OCT27.</S>\n    <S sid=\"133\" ssid=\"10\">Due to DAILY547’s statistical representativeness, we believe this gives the best view of the tagger’s accuracy on English Twitter text.</S>\n    <S sid=\"134\" ssid=\"11\">The full tagger attains 93.2% accuracy (final row of Table 2).</S>\n    <S sid=\"135\" ssid=\"12\">To facilitate comparisons with previous work, we ran a series of experiments training only on OCT27’s training and development sets, then report test results on both OCT27TEST and all of DAILY547, shown in Table 2.</S>\n    <S sid=\"136\" ssid=\"13\">Our tagger achieves substantially higher accuracy than Gimpel et al. (2011).17 Feature ablation.</S>\n    <S sid=\"137\" ssid=\"14\">A number of ablation tests indicate the word clusters are a very strong source of lexical knowledge.</S>\n    <S sid=\"138\" ssid=\"15\">When dropping the tag dictionaries and name lists, the word clusters maintain most of the accuracy (row 2).</S>\n    <S sid=\"139\" ssid=\"16\">If we drop the clusters and rely only on tag dictionaries and namelists, accuracy decreases significantly (row 3).</S>\n    <S sid=\"140\" ssid=\"17\">In fact, we can remove all observation features except for word clusters—no word features, orthographic fea17These numbers differ slightly from those reported by Gimpel et al., due to the corrections we made to the OCT27 data, noted in Section 5.1.</S>\n    <S sid=\"141\" ssid=\"18\">We retrained and evaluated their tagger (version 0.2) on our corrected dataset. tures, affix n-grams, capitalization, emoticon patterns, etc.—and the accuracy is in fact still better than the previous work (row 4).18 We also wanted to know whether to keep the tag dictionary and name list features, but the splits reported in Fig.</S>\n    <S sid=\"142\" ssid=\"19\">2 did not show statistically significant differences; so to better discriminate between ablations, we created a lopsided train/test split of all data with a much larger test portion (26,974 tokens), having greater statistical power (tighter confidence intervals of f 0.3%).19 The full system got 90.8% while the no–tag dictionary, no-namelists ablation had 90.0%, a statistically significant difference.</S>\n    <S sid=\"143\" ssid=\"20\">Therefore we retain these features.</S>\n    <S sid=\"144\" ssid=\"21\">Compared to the tagger in Gimpel et al., most of our feature changes are in the new lexical features described in §3.5.20 We do not reuse the other lexical features from the previous work, including a phonetic normalizer (Metaphone), a name list consisting of words that are frequently capitalized, and distributional features trained on a much smaller unlabeled corpus; they are all worse than our new lexical features described here.</S>\n    <S sid=\"145\" ssid=\"22\">(We did include, however, a variant of the tag dictionary feature that uses phonetic normalization for lookup; it seemed to yield a small improvement.)</S>\n    <S sid=\"146\" ssid=\"23\">18Furthermore, when evaluating the clusters as unsupervised (hard) POS tags, we obtain a many-to-one accuracy of 89.2% on DAILY547.</S>\n    <S sid=\"147\" ssid=\"24\">Before computing this, we lowercased the text to match the clusters and removed tokens tagged as URLs and at-mentions.</S>\n    <S sid=\"148\" ssid=\"25\">19Reported confidence intervals in this paper are 95% binomial normal approximation intervals for the proportion of correctly tagged tokens: f1.96.,/p(1 − p)/ntokm &lt; 1/\\/n.</S>\n    <S sid=\"149\" ssid=\"26\">20Details on the exact feature set are available in a technical report (Owoputi et al., 2012), also available on the website.</S>\n    <S sid=\"150\" ssid=\"27\">Non-traditional words.</S>\n    <S sid=\"151\" ssid=\"28\">The word clusters are especially helpful with words that do not appear in traditional dictionaries.</S>\n    <S sid=\"152\" ssid=\"29\">We constructed a dictionary by lowercasing the union of the ispell ‘American’, ‘British’, and ‘English’ dictionaries, plus the standard Unix words file from Webster’s Second International dictionary, totalling 260,985 word types.</S>\n    <S sid=\"153\" ssid=\"30\">After excluding tokens defined by the gold standard as punctuation, URLs, at-mentions, or emoticons,21 22% of DAILY547’s tokens do not appear in this dictionary.</S>\n    <S sid=\"154\" ssid=\"31\">Without clusters, they are very difficult to classify (only 79.2% accuracy), but adding clusters generates a 5.7 point improvement—much larger than the effect on in-dictionary tokens (Table 3).</S>\n    <S sid=\"155\" ssid=\"32\">Varying the amount of unlabeled data.</S>\n    <S sid=\"156\" ssid=\"33\">A tagger that only uses word clusters achieves an accuracy of 88.6% on the OCT27 development set.22 We created several clusterings with different numbers of unlabeled tweets, keeping the number of clusters constant at 800.</S>\n    <S sid=\"157\" ssid=\"34\">As shown in Fig.</S>\n    <S sid=\"158\" ssid=\"35\">3, there was initially a logarithmic relationship between number of tweets and accuracy, but accuracy (and lexical coverage) levels out after 750,000 tweets.</S>\n    <S sid=\"159\" ssid=\"36\">We use the largest clustering (56 million tweets and 1,000 clusters) as the default for the released tagger. tagset plus several Twitter-specific tags, referred to in Table 1 as RITTERTW.</S>\n    <S sid=\"160\" ssid=\"37\">Linguistic concerns notwithstanding (§5.2), for a controlled comparison, we train and test our system on this data with the same 4-fold cross-validation setup they used, attaining 90.0% (f0.5%) accuracy.</S>\n    <S sid=\"161\" ssid=\"38\">Ritter et al.’s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3% (Table 4).</S>\n    <S sid=\"162\" ssid=\"39\">First, we compare to a tagger in the same setup as experiments on this data in Forsyth (2007), training on 90% of the data and testing on 10%; we average results across 10-fold cross-validation.25 The full tagger model achieved 93.4% (f0.3%) accuracy, significantly improving over the best result they report, 90.8% accuracy with a tagger trained on a mix of several POS-annotated corpora.</S>\n    <S sid=\"163\" ssid=\"40\">We also perform the ablation experiments on this corpus, with a slightly different experimental setup: we first filter out system messages then split data 24Release 1.0: http://faculty.nps.edu/ cmartell/NPSChat.htm 25Forsyth actually used 30 different 90/10 random splits; we prefer cross-validation because the same test data is never repeated, thus allowing straightforward confidence estimation of accuracy from the number of tokens (via binomial sample variance, footnote 19).</S>\n    <S sid=\"164\" ssid=\"41\">In all cases, the models are trained on the same amount of data (90%). into 5,067 training and 2,868 test messages.</S>\n    <S sid=\"165\" ssid=\"42\">Results show a similar pattern as the Twitter data (see final column of Table 2).</S>\n    <S sid=\"166\" ssid=\"43\">Thus the Twitter word clusters are also useful for language in the medium of text chat rooms; we suspect these clusters will be applicable for deeper syntactic and semantic analysis in other online conversational text mediums, such as text messages and instant messages.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"7\">\n    <S sid=\"167\" ssid=\"1\">We have constructed a state-of-the-art part-ofspeech tagger for the online conversational text genres of Twitter and IRC, and have publicly released our new evaluation data, annotation guidelines, open-source tagger, and word clusters at http://www.ark.cs.cmu.edu/TweetNLP.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "80daef7807e8a24ff04898fd1209e936",
                    "include_year": false,
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo",
                    "origin": "LLM",
                    "title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters"
                }
            }
        },
        {
            "index": 4,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms. In the past, the four tasks have been treated independently, using a widevariety of algorithms. These four seman tic classes, however, are a tiny sample of the full range of semantic phenomena, andwe cannot afford to create ad hoc algo rithms for each semantic phenomenon; weneed to seek a unified approach. We propose to subsume a broad range of phenom ena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.",
                    "filename": "A_Uniform_Approach_to_Analogies_Synonyms_Antonyms_and_Associations",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/C08-1114/Documents_xml/C08-1114.xml",
                    "title": "A Uniform Approach to Analogies Synonyms Antonyms and Associations"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a uniform approach to handling analogies, synonyms, antonyms, and associations by subsuming them under analogies. The authors propose a standard supervised machine learning algorithm, PairClass, for classifying word pairs based on semantic relations. The algorithm uses a large corpus to generate feature vectors and applies a support vector machine for classification. Experimental results show competitive performance in recognizing analogies and significant improvements in identifying synonyms, antonyms, and associations. The paper discusses the limitations of PairClass, the need for a large corpus, and suggests future applications in word sense disambiguation, information retrieval, and metaphor interpretation. The study aims to unify the field of semantics under the framework of analogies."
                    ],
                    "filename": "a_uniform_approach_to_analogies_-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 135 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n\t<S sid=\"0\">A Uniform Approach to Analogies Synonyms Antonyms and Associations</S><SECTION title=\"Introduction\" number=\"1\">\n\t\t\t<S sid=\"7\" ssid=\"7\">A pair of words (petrify:stone) is analogous to another pair (vaporize:gas) when the semantic re lations between the words in the first pair are highly similar to the relations in the second pair.</S>\n\t\t\t<S sid=\"8\" ssid=\"8\">Two words (levied and imposed) are synonymousin a context (levied a tax) when they can be interchanged (imposed a tax), they are are antony mous when they have opposite meanings (black c ? 2008, National Research Council of Canada (NRC).Licensed to the Coling 2008 Organizing Committee for pub lication in Coling 2008 and for re-publishing in any form or medium.</S>\n\t\t\t<S sid=\"9\" ssid=\"9\">and white), and they are associated when they tend to co-occur (doctor and hospital).On the surface, it appears that these are four distinct semantic classes, requiring distinct NLP al gorithms, but we propose a uniform approach to all four.</S>\n\t\t\t<S sid=\"10\" ssid=\"10\">We subsume synonyms, antonyms, and associations under analogies.</S>\n\t\t\t<S sid=\"11\" ssid=\"11\">In essence, we say that X and Y are antonyms when the pair X:Y is analogous to the pair black:white, X and Y are synonyms when they are analogous to the pair levied:imposed, and X and Y are associated when they are analogous to the pair doctor:hospital.</S>\n\t\t\t<S sid=\"12\" ssid=\"12\">There is past work on recognizing analogies(Reitman, 1965), synonyms (Landauer and Dumais, 1997), antonyms (Lin et al, 2003), and asso ciations (Lesk, 1969), but each of these four tasks has been examined separately, in isolation from the others.</S>\n\t\t\t<S sid=\"13\" ssid=\"13\">As far as we know, the algorithm proposed here is the first attempt to deal with all four tasks using a uniform approach.</S>\n\t\t\t<S sid=\"14\" ssid=\"14\">We believe that it isimportant to seek NLP algorithms that can han dle a broad range of semantic phenomena, becausedeveloping a specialized algorithm for each phe nomenon is a very inefficient research strategy.It might seem that a lexicon, such as Word Net (Fellbaum, 1998), contains all the information we need to handle these four tasks.</S>\n\t\t\t<S sid=\"15\" ssid=\"15\">However, weprefer to take a corpus-based approach to seman tics.</S>\n\t\t\t<S sid=\"16\" ssid=\"16\">Veale (2004) used WordNet to answer 374 multiple-choice SAT analogy questions, achievingan accuracy of 43%, but the best corpus-based ap proach attains an accuracy of 56% (Turney, 2006).</S>\n\t\t\t<S sid=\"17\" ssid=\"17\">Another reason to prefer a corpus-based approachto a lexicon-based approach is that the former re quires less human labour, and thus it is easier to extend to other languages.In Section 2, we describe our algorithm for rec ognizing analogies.</S>\n\t\t\t<S sid=\"18\" ssid=\"18\">We use a standard supervised 905 machine learning approach, with feature vectorsbased on the frequencies of patterns in a large cor pus.</S>\n\t\t\t<S sid=\"19\" ssid=\"19\">We use a support vector machine (SVM) to learn how to classify the feature vectors (Platt, 1998; Witten and Frank, 1999).</S>\n\t\t\t<S sid=\"20\" ssid=\"20\">Section 3 presents four sets of experiments.</S>\n\t\t\t<S sid=\"21\" ssid=\"21\">We apply our algorithm for recognizing analogies to multiple-choice analogy questions from the SAT college entrance test, multiple-choice synonym questions from the TOEFL (test of English as aforeign language), ESL (English as a second language) practice questions for distinguishing syn onyms and antonyms, and a set of word pairs thatare labeled similar, associated, and both, devel oped for experiments in cognitive psychology.We discuss the results of the experiments in Section 4.</S>\n\t\t\t<S sid=\"22\" ssid=\"22\">The accuracy of the algorithm is competitive with other systems, but the strength of the al gorithm is that it is able to handle all four tasks, with no tuning of the learning parameters to the particular task.</S>\n\t\t\t<S sid=\"23\" ssid=\"23\">It performs well, although it iscompeting against specialized algorithms, devel oped for single tasks.Related work is examined in Section 5 and limitations and future work are considered in Sec tion 6.</S>\n\t\t\t<S sid=\"24\" ssid=\"24\">We conclude in Section 7.</S>\n\t</SECTION>\n\t<SECTION title=\"Classifying Analogous Word Pairs. \" number=\"2\">\n\t\t\t<S sid=\"25\" ssid=\"1\">An analogy, A:B::C:D, asserts that A is to B as C is to D; for example, traffic:street::water:riverbed asserts that traffic is to street as water is to riverbed; that is, the semantic relations between traffic and street are highly similar to the semantic relations between water and riverbed.</S>\n\t\t\t<S sid=\"26\" ssid=\"2\">We may view the task of recognizing word analogies as a problem of classifying word pairs (see Table 1).</S>\n\t\t\t<S sid=\"27\" ssid=\"3\">Word pair Class label carpenter:wood artisan:material mason:stone artisan:material potter:clay artisan:material glassblower:glass artisan:material traffic:street entity:carrier water:riverbed entity:carrier packets:network entity:carrier gossip:grapevine entity:carrierTable 1: Examples of how the task of recogniz ing word analogies may be viewed as a problem of classifying word pairs.</S>\n\t\t\t<S sid=\"28\" ssid=\"4\">We approach this as a standard classificationproblem for supervised machine learning.</S>\n\t\t\t<S sid=\"29\" ssid=\"5\">The al gorithm takes as input a training set of word pairs with class labels and a testing set of word pairs without labels.</S>\n\t\t\t<S sid=\"30\" ssid=\"6\">Each word pair is represented as a vector in a feature space and a supervised learning algorithm is used to classify the feature vectors.</S>\n\t\t\t<S sid=\"31\" ssid=\"7\">The elements in the feature vectors are based on the frequencies of automatically defined patterns in a large corpus.</S>\n\t\t\t<S sid=\"32\" ssid=\"8\">The output of the algorithm is anassignment of labels to the word pairs in the test ing set.</S>\n\t\t\t<S sid=\"33\" ssid=\"9\">For some of the experiments, we selecta unique label for each word pair; for other ex periments, we assign probabilities to each possible label for each word pair.</S>\n\t\t\t<S sid=\"34\" ssid=\"10\">For a given word pair, such as mason:stone, the first step is to generate morphological variations,such as masons:stones.</S>\n\t\t\t<S sid=\"35\" ssid=\"11\">In the following experi ments, we use morpha (morphological analyzer)and morphg (morphological generator) for mor phological processing (Minnen et al, 2001).1 The second step is to search in a large corpus for all phrases of the following form: ?[0 to 1 words] X [0 to 3 words] Y [0 to 1 words]?</S>\n\t\t\t<S sid=\"36\" ssid=\"12\">In this template, X:Y consists of morphologicalvariations of the given word pair, in either order; for example, mason:stone, stone:mason, masons:stones, and so on.</S>\n\t\t\t<S sid=\"37\" ssid=\"13\">A typical phrase for ma son:stone would be ?the mason cut the stone with?.</S>\n\t\t\t<S sid=\"38\" ssid=\"14\">We then normalize all of the phrases that are found, by using morpha to remove suffixes.</S>\n\t\t\t<S sid=\"39\" ssid=\"15\">The template we use here is similar to Turney (2006), but we have added extra context wordsbefore the X and after the Y . Our morpholog ical processing also differs from Turney (2006).In the following experiments, we search in a cor pus of 5 ? 1010 words (about 280 GB of plain text), consisting of web pages gathered by a web crawler.2 To retrieve phrases from the corpus, weuse Wumpus (Bu?ttcher and Clarke, 2005), an effi cient search engine for passage retrieval from large corpora.3 The next step is to generate patterns from allof the phrases that were found for all of the in put word pairs (from both the training and testingsets).</S>\n\t\t\t<S sid=\"40\" ssid=\"16\">To generate patterns from a phrase, we re place the given word pairs with variables, X and Y , and we replace the remaining words with a wild card symbol (an asterisk) or leave them as they are.</S>\n\t\t\t<S sid=\"41\" ssid=\"17\">1http://www.informatics.susx.ac.uk/research/groups/nlp/ carroll/morph.html.</S>\n\t\t\t<S sid=\"42\" ssid=\"18\">2The corpus was collected by Charles Clarke, University of Waterloo.</S>\n\t\t\t<S sid=\"43\" ssid=\"19\">We can provide copies on request.</S>\n\t\t\t<S sid=\"44\" ssid=\"20\">3http://www.wumpus-search.org/.</S>\n\t\t\t<S sid=\"45\" ssid=\"21\">906 For example, the phrase ?the mason cut the stone with?</S>\n\t\t\t<S sid=\"46\" ssid=\"22\">yields the patterns ?the X cut * Y with?, ?* X * the Y *?, and so on.</S>\n\t\t\t<S sid=\"47\" ssid=\"23\">If a phrase contains n words, then it yields 2(n?2) patterns.Each pattern corresponds to a feature in the feature vectors that we will generate.</S>\n\t\t\t<S sid=\"48\" ssid=\"24\">Since a typical input set of word pairs yields millions of pat terns, we need to use feature selection, to reduce the number of patterns to a manageable quantity.</S>\n\t\t\t<S sid=\"49\" ssid=\"25\">For each pattern, we count the number of input word pairs that generated the pattern.</S>\n\t\t\t<S sid=\"50\" ssid=\"26\">For example, ?* X cut * Y *?</S>\n\t\t\t<S sid=\"51\" ssid=\"27\">is generated by both mason:stone and carpenter:wood.</S>\n\t\t\t<S sid=\"52\" ssid=\"28\">We then sort the patterns in descending order of the number of word pairs that generated them.</S>\n\t\t\t<S sid=\"53\" ssid=\"29\">If there are N input word pairs (and thus N feature vectors, including both the training and testing sets), then we select the topkN patterns and drop the remainder.</S>\n\t\t\t<S sid=\"54\" ssid=\"30\">In the fol lowing experiments, k is set to 20.</S>\n\t\t\t<S sid=\"55\" ssid=\"31\">The algorithm is not sensitive to the precise value of k.The reasoning behind the feature selection al gorithm is that shared patterns make more useful features than rare patterns.</S>\n\t\t\t<S sid=\"56\" ssid=\"32\">The number of features (kN ) depends on the number of word pairs (N ), because, if we have more feature vectors, then we need more features to distinguish them.</S>\n\t\t\t<S sid=\"57\" ssid=\"33\">Turney (2006) also selects patterns based on the numberof pairs that generate them, but the number of se lected patterns is a constant (8000), independent of the number of input word pairs.</S>\n\t\t\t<S sid=\"58\" ssid=\"34\">The next step is to generate feature vectors, one vector for each input word pair.</S>\n\t\t\t<S sid=\"59\" ssid=\"35\">Each of the N feature vectors has kN elements, one element for each selected pattern.</S>\n\t\t\t<S sid=\"60\" ssid=\"36\">The value of an element in a vector is given by the logarithm of the frequency in the corpus of the corresponding pattern for the given word pair.</S>\n\t\t\t<S sid=\"61\" ssid=\"37\">For example, suppose the given pair is mason:stone and the pattern is ?* X cut * Y *?.</S>\n\t\t\t<S sid=\"62\" ssid=\"38\">We look at the normalized phrases that we collected for mason:stone and we count how many match this pattern.</S>\n\t\t\t<S sid=\"63\" ssid=\"39\">If f phrases match thepattern, then the value of this element in the fea ture vector is log(f +1) (we add 1 because log(0)is undefined).</S>\n\t\t\t<S sid=\"64\" ssid=\"40\">Each feature vector is then normal ized to unit length.</S>\n\t\t\t<S sid=\"65\" ssid=\"41\">The normalization ensures that features in vectors for high-frequency word pairs (traffic:street) are comparable to features in vectors for low-frequency word pairs (water:riverbed).Now that we have a feature vector for each in put word pair, we can apply a standard supervised learning algorithm.</S>\n\t\t\t<S sid=\"66\" ssid=\"42\">In the following experiments, we use a sequential minimal optimization (SMO)support vector machine (SVM) with a radial basis function (RBF) kernel (Platt, 1998), as implemented in Weka (Waikato Environment for Knowl edge Analysis) (Witten and Frank, 1999).4 The algorithm generates probability estimates for each class by fitting logistic regression models to the outputs of the SVM.</S>\n\t\t\t<S sid=\"67\" ssid=\"43\">We disable the normalizationoption in Weka, since the vectors are already nor malized to unit length.</S>\n\t\t\t<S sid=\"68\" ssid=\"44\">We chose the SMO RBF algorithm because it is fast, robust, and it easily handles large numbers of features.For convenience, we will refer to the above algo rithm as PairClass.</S>\n\t\t\t<S sid=\"69\" ssid=\"45\">In the following experiments, PairClass is applied to each of the four problemswith no adjustments or tuning to the specific prob lems.</S>\n\t\t\t<S sid=\"70\" ssid=\"46\">Some work is required to fit each probleminto the general framework of PairClass (supervised classification of word pairs) but the core al gorithm is the same in each case.</S>\n\t</SECTION>\n\t<SECTION title=\"Experiments. \" number=\"3\">\n\t\t\t<S sid=\"71\" ssid=\"1\">This section presents four sets of experiments, with analogies, synonyms, antonyms, and associations.</S>\n\t\t\t<S sid=\"72\" ssid=\"2\">We explain how each task is treated as a problem of classifying analogous word pairs, we give the experimental results, and we discuss past work on each of the four tasks.</S>\n\t\t\t<S sid=\"73\" ssid=\"3\">3.1 SAT Analogies.</S>\n\t\t\t<S sid=\"74\" ssid=\"4\">In this section, we apply PairClass to the taskof recognizing analogies.</S>\n\t\t\t<S sid=\"75\" ssid=\"5\">To evaluate the performance, we use a set of 374 multiple-choice ques tions from the SAT college entrance exam.</S>\n\t\t\t<S sid=\"76\" ssid=\"6\">Table 2 shows a typical question.</S>\n\t\t\t<S sid=\"77\" ssid=\"7\">The target pair is called the stem.</S>\n\t\t\t<S sid=\"78\" ssid=\"8\">The task is to select the choice pair that is most analogous to the stem pair.</S>\n\t\t\t<S sid=\"79\" ssid=\"9\">Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood Table 2: An example of a question from the 374 SAT analogy questions.</S>\n\t\t\t<S sid=\"80\" ssid=\"10\">The problem of recognizing word analogies wasfirst attempted with a system called Argus (Reit 4http://www.cs.waikato.ac.nz/ml/weka/.</S>\n\t\t\t<S sid=\"81\" ssid=\"11\">907man, 1965), using a small hand-built semantic network with a spreading activation algorithm.</S>\n\t\t\t<S sid=\"82\" ssid=\"12\">Turney et al (2003) used a combination of 13 independent modules.</S>\n\t\t\t<S sid=\"83\" ssid=\"13\">Veale (2004) used a spread ing activation algorithm with WordNet (in effect, treating WordNet as a semantic network).</S>\n\t\t\t<S sid=\"84\" ssid=\"14\">Turney (2006) used a corpus-based algorithm.We may view Table 2 as a binary classification problem, in which mason:stone and carpen ter:wood are positive examples and the remaining word pairs are negative examples.</S>\n\t\t\t<S sid=\"85\" ssid=\"15\">The difficulty is that the labels of the choice pairs must be hidden from the learning algorithm.</S>\n\t\t\t<S sid=\"86\" ssid=\"16\">That is, the training set consists of one positive example (the stem pair)and the testing set consists of five unlabeled exam ples (the five choice pairs).</S>\n\t\t\t<S sid=\"87\" ssid=\"17\">To make this task more tractable, we randomly choose a stem pair from one of the 373 other SAT analogy questions, andwe assume that this new stem pair is a negative ex ample, as shown in Table 3.</S>\n\t\t\t<S sid=\"88\" ssid=\"18\">Word pair Train or test Class label mason:stone train positive tutor:pupil train negative teacher:chalk test hidden carpenter:wood test hidden soldier:gun test hidden photograph:camera test hidden book:word test hidden Table 3: How to fit a SAT analogy question into the framework of supervised pair classification.</S>\n\t\t\t<S sid=\"89\" ssid=\"19\">To answer the SAT question, we use PairClass to estimate the probability that each testing example is positive, and we guess the testing example with the highest probability.</S>\n\t\t\t<S sid=\"90\" ssid=\"20\">Learning from a trainingset with only one positive example and one nega tive example is difficult, since the learned model can be highly unstable.</S>\n\t\t\t<S sid=\"91\" ssid=\"21\">To increase the stability, we repeat the learning process 10 times, using adifferent randomly chosen negative training exam ple each time.</S>\n\t\t\t<S sid=\"92\" ssid=\"22\">For each testing word pair, the 10 probability estimates are averaged together.</S>\n\t\t\t<S sid=\"93\" ssid=\"23\">This is a form of bagging (Breiman, 1996).</S>\n\t\t\t<S sid=\"94\" ssid=\"24\">PairClass attains an accuracy of 52.1%.</S>\n\t\t\t<S sid=\"95\" ssid=\"25\">Forcomparison, the ACL Wiki lists 12 previously published results with the 374 SAT analogy ques tions.5 Only 2 of the 12 algorithms have higher accuracy.</S>\n\t\t\t<S sid=\"96\" ssid=\"26\">The best previous result is an accuracy of 56.1% (Turney, 2006).</S>\n\t\t\t<S sid=\"97\" ssid=\"27\">Random guessing would 5For more information, see SAT Analogy Questions (State of the art) at http://aclweb.org/aclwiki/.</S>\n\t\t\t<S sid=\"98\" ssid=\"28\">yield an accuracy of 20%.</S>\n\t\t\t<S sid=\"99\" ssid=\"29\">The average senior high school student achieves 57% correct (Turney, 2006).</S>\n\t\t\t<S sid=\"100\" ssid=\"30\">3.2 TOEFL Synonyms.</S>\n\t\t\t<S sid=\"101\" ssid=\"31\">Now we apply PairClass to the task of recogniz ing synonyms, using a set of 80 multiple-choicesynonym questions from the TOEFL (test of En glish as a foreign language).</S>\n\t\t\t<S sid=\"102\" ssid=\"32\">A sample question is shown in Table 4.</S>\n\t\t\t<S sid=\"103\" ssid=\"33\">The task is to select the choice word that is most similar in meaning to the stem word.</S>\n\t\t\t<S sid=\"104\" ssid=\"34\">Stem: levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed Table 4: An example of a question from the 80 TOEFL questions.</S>\n\t\t\t<S sid=\"105\" ssid=\"35\">Synonymy can be viewed as a high degree of semantic similarity.</S>\n\t\t\t<S sid=\"106\" ssid=\"36\">The most common way tomeasure semantic similarity is to measure the dis tance between words in WordNet (Resnik, 1995; Jiang and Conrath, 1997; Hirst and St-Onge, 1998).</S>\n\t\t\t<S sid=\"107\" ssid=\"37\">Corpus-based measures of word similarityare also common (Lesk, 1969; Landauer and Du mais, 1997; Turney, 2001).We may view Table 4 as a binary classifica tion problem, in which the pair levied:imposed is a positive example of the class synonymous and the other possible pairings are negative examples, as shown in Table 5.</S>\n\t\t\t<S sid=\"108\" ssid=\"38\">Word pair Class label levied:imposed positive levied:believed negative levied:requested negative levied:correlated negative Table 5: How to fit a TOEFL question into the framework of supervised pair classification.</S>\n\t\t\t<S sid=\"109\" ssid=\"39\">The 80 TOEFL questions yield 320 (80 ? 4) word pairs, 80 labeled positive and 240 labelednegative.</S>\n\t\t\t<S sid=\"110\" ssid=\"40\">We apply PairClass to the word pairs us ing ten-fold cross-validation.</S>\n\t\t\t<S sid=\"111\" ssid=\"41\">In each random fold, 90% of the pairs are used for training and 10% are used for testing.</S>\n\t\t\t<S sid=\"112\" ssid=\"42\">For each fold, the model that is learned from the training set is used to assign probabilities to the pairs in the testing set.</S>\n\t\t\t<S sid=\"113\" ssid=\"43\">With 908ten separate folds, the ten non-overlapping test ing sets cover the whole dataset.</S>\n\t\t\t<S sid=\"114\" ssid=\"44\">Our guess foreach TOEFL question is the choice with the high est probability of being positive, when paired with the corresponding stem.</S>\n\t\t\t<S sid=\"115\" ssid=\"45\">PairClass attains an accuracy of 76.2%.</S>\n\t\t\t<S sid=\"116\" ssid=\"46\">Forcomparison, the ACL Wiki lists 15 previously published results with the 80 TOEFL synonym questions.6 Of the 15 algorithms, 8 have higher accu racy and 7 have lower.</S>\n\t\t\t<S sid=\"117\" ssid=\"47\">The best previous resultis an accuracy of 97.5% (Turney et al, 2003), ob tained using a hybrid of four different algorithms.</S>\n\t\t\t<S sid=\"118\" ssid=\"48\">Random guessing would yield an accuracy of 25%.</S>\n\t\t\t<S sid=\"119\" ssid=\"49\">The average foreign applicant to a US university achieves 64.5% correct (Landauer and Dumais, 1997).</S>\n\t\t\t<S sid=\"120\" ssid=\"50\">3.3 Synonyms and Antonyms.</S>\n\t\t\t<S sid=\"121\" ssid=\"51\">The task of classifying word pairs as either syn onyms or antonyms readily fits into the framework of supervised classification of word pairs.</S>\n\t\t\t<S sid=\"122\" ssid=\"52\">Table 6shows some examples from a set of 136 ESL (En glish as a second language) practice questions that we collected from various ESL websites.</S>\n\t\t\t<S sid=\"123\" ssid=\"53\">Word pair Class label galling:irksome synonyms yield:bend synonyms naive:callow synonyms advise:suggest synonyms dissimilarity:resemblance antonyms commend:denounce antonyms expose:camouflage antonyms unveil:veil antonyms Table 6: Examples of synonyms and antonyms from 136 ESL practice questions.</S>\n\t\t\t<S sid=\"124\" ssid=\"54\">Lin et al (2003) distinguish synonyms from antonyms using two patterns, ?from X to Y ? and ?either X or Y ?.</S>\n\t\t\t<S sid=\"125\" ssid=\"55\">When X and Y are antonyms, they occasionally appear in a large corpus in oneof these two patterns, but it is very rare for syn onyms to appear in these patterns.</S>\n\t\t\t<S sid=\"126\" ssid=\"56\">Our approach is similar to Lin et al (2003), but we do not rely on hand-coded patterns; instead, PairClass patterns are generated automatically.Using ten-fold cross-validation, PairClass at tains an accuracy of 75.0%.</S>\n\t\t\t<S sid=\"127\" ssid=\"57\">Always guessing the majority class would result in an accuracy of 65.4%.</S>\n\t\t\t<S sid=\"128\" ssid=\"58\">The average human score is unknown and 6For more information, see TOEFL Synonym Questions (State of the art) at http://aclweb.org/aclwiki/.</S>\n\t\t\t<S sid=\"129\" ssid=\"59\">there are no previous results for comparison.</S>\n\t\t\t<S sid=\"130\" ssid=\"60\">3.4 Similar, Associated, and Both.</S>\n\t\t\t<S sid=\"131\" ssid=\"61\">A common criticism of corpus-based measures ofword similarity (as opposed to lexicon-based mea sures) is that they are merely detecting associations(co-occurrences), rather than actual semantic similarity (Lund et al, 1995).</S>\n\t\t\t<S sid=\"132\" ssid=\"62\">To address this criti cism, Lund et al (1995) evaluated their algorithm for measuring word similarity with word pairs that were labeled similar, associated, or both.</S>\n\t\t\t<S sid=\"133\" ssid=\"63\">Theselabeled pairs were originally created for cogni tive psychology experiments with human subjects(Chiarello et al, 1990).</S>\n\t\t\t<S sid=\"134\" ssid=\"64\">Table 7 shows some ex amples from this collection of 144 word pairs (48 pairs in each of the three classes).</S>\n\t\t\t<S sid=\"135\" ssid=\"65\">Word pair Class label table:bed similar music:art similar hair:fur similar house:cabin similar cradle:baby associated mug:beer associated camel:hump associated cheese:mouse associated ale:beer both uncle:aunt both pepper:salt both frown:smile both Table 7: Examples of word pairs labeled similar, associated, or both.</S>\n\t\t\t<S sid=\"136\" ssid=\"66\">Lund et al (1995) did not measure the accuracy of their algorithm on this three-class classification problem.</S>\n\t\t\t<S sid=\"137\" ssid=\"67\">Instead, following standard practice incognitive psychology, they showed that their al gorithm?s similarity scores for the 144 word pairs were correlated with the response times of human subjects in priming tests.</S>\n\t\t\t<S sid=\"138\" ssid=\"68\">In a typical priming test, a human subject reads a priming word (cradle) and is then asked to complete a partial word (complete bab as baby).</S>\n\t\t\t<S sid=\"139\" ssid=\"69\">The time required to perform thetask is taken to indicate the strength of the cogni tive link between the two words (cradle and baby).Using ten-fold cross-validation, PairClass at tains an accuracy of 77.1% on the 144 word pairs.</S>\n\t\t\t<S sid=\"140\" ssid=\"70\">Since the three classes are of equal size, guessing the majority class and random guessing both yield an accuracy of 33.3%.</S>\n\t\t\t<S sid=\"141\" ssid=\"71\">The average human score is unknown and there are no previous results for comparison.</S>\n\t\t\t<S sid=\"142\" ssid=\"72\">909</S>\n\t</SECTION>\n\t<SECTION title=\"Discussion. \" number=\"4\">\n\t\t\t<S sid=\"143\" ssid=\"1\">The four experiments are summarized in Tables 8 and 9.</S>\n\t\t\t<S sid=\"144\" ssid=\"2\">For the first two experiments, where there are previous results, PairClass is not the best, butit performs competitively.</S>\n\t\t\t<S sid=\"145\" ssid=\"3\">For the second two ex periments, PairClass performs significantly abovethe baselines.</S>\n\t\t\t<S sid=\"146\" ssid=\"4\">However, the strength of this ap proach is not its performance on any one task, but the range of tasks it can handle.As far as we know, this is the first time a standard supervised learning algorithm has been ap plied to any of these four problems.</S>\n\t\t\t<S sid=\"147\" ssid=\"5\">The advantageof being able to cast these problems in the frame work of standard supervised learning problems isthat we can now exploit the huge literature on su pervised learning.</S>\n\t\t\t<S sid=\"148\" ssid=\"6\">Past work on these problems has required implicitly coding our knowledge ofthe nature of the task into the structure of the algo rithm.</S>\n\t\t\t<S sid=\"149\" ssid=\"7\">For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con tains a theory of synonymy (Landauer and Dumais, 1997).</S>\n\t\t\t<S sid=\"150\" ssid=\"8\">The problem with this approach is that it can be very difficult to work out how to modify the algorithm if it does not behave the way we want.On the other hand, with a supervised learning algo rithm, we can put our knowledge into the labeling of the feature vectors, instead of putting it directly into the algorithm.</S>\n\t\t\t<S sid=\"151\" ssid=\"9\">This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual?</S>\n\t\t\t<S sid=\"152\" ssid=\"10\">or ?simulated?</S>\n\t\t\t<S sid=\"153\" ssid=\"11\">negative example).</S>\n\t\t\t<S sid=\"154\" ssid=\"12\">In effect, a single example (mason:stone) becomes a sui generis; it con stitutes a class of its own.</S>\n\t\t\t<S sid=\"155\" ssid=\"13\">It may be possible to apply the machinery of supervised learning toother problems that apparently call for unsupervised learning (for example, clustering or measur ing similarity), by using this sui generis device.</S>\n\t</SECTION>\n\t<SECTION title=\"Related Work. \" number=\"5\">\n\t\t\t<S sid=\"156\" ssid=\"1\">One of the first papers using supervised ma chine learning to classify word pairs was Rosarioand Hearst?s (2001) paper on classifying nounmodifier pairs in the medical domain.</S>\n\t\t\t<S sid=\"157\" ssid=\"2\">For ex ample, the noun-modifier expression brain biopsy was classified as Procedure.</S>\n\t\t\t<S sid=\"158\" ssid=\"3\">Rosario and Hearst(2001) constructed feature vectors for each nounmodifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources.</S>\n\t\t\t<S sid=\"159\" ssid=\"4\">They then trained a neu ral network to distinguish 13 classes of semantic relations, such as Cause, Location, Measure, andInstrument.</S>\n\t\t\t<S sid=\"160\" ssid=\"5\">Nastase and Szpakowicz (2003) explored a similar approach to classifying general domain noun-modifier pairs, using WordNet and Roget?s Thesaurus as lexical resources.</S>\n\t\t\t<S sid=\"161\" ssid=\"6\">Turney and Littman (2005) used corpus-based features for classifying noun-modifier pairs.</S>\n\t\t\t<S sid=\"162\" ssid=\"7\">Their features were based on 128 hand-coded patterns.</S>\n\t\t\t<S sid=\"163\" ssid=\"8\">They used a nearest-neighbour learning algorithm to classify general-domain noun-modifier pairsinto 30 different classes of semantic relations.</S>\n\t\t\t<S sid=\"164\" ssid=\"9\">Tur ney (2006) later addressed the same problem using 8000 automatically generated patterns.One of the tasks in SemEval 2007 was the clas sification of semantic relations between nominals (Girju et al, 2007).</S>\n\t\t\t<S sid=\"165\" ssid=\"10\">The problem is to classifysemantic relations between nouns and noun com pounds in the context of a sentence.</S>\n\t\t\t<S sid=\"166\" ssid=\"11\">The task attracted 14 teams who created 15 systems, all of which used supervised machine learning with features that were lexicon-based, corpus-based, or both.PairClass is most similar to the algorithm of Tur ney (2006), but it differs in the following ways:?</S>\n\t\t\t<S sid=\"167\" ssid=\"12\">PairClass does not use a lexicon to find syn onyms for the input word pairs.</S>\n\t\t\t<S sid=\"168\" ssid=\"13\">One of our goals in this paper is to show that a pure corpus-based algorithm can handle synonymswithout a lexicon.</S>\n\t\t\t<S sid=\"169\" ssid=\"14\">This considerably simpli fies the algorithm.</S>\n\t\t\t<S sid=\"170\" ssid=\"15\">PairClass uses a support vector machine (SVM) instead of a nearest neighbour (NN) learning algorithm.</S>\n\t\t\t<S sid=\"171\" ssid=\"16\">PairClass does not use the singular value decomposition (SVD) to smooth the feature vectors.</S>\n\t\t\t<S sid=\"172\" ssid=\"17\">It has been our experience that SVD is not necessary with SVMs.</S>\n\t\t\t<S sid=\"173\" ssid=\"18\">PairClass generates probability estimates,whereas Turney (2006) uses a cosine mea sure of similarity.</S>\n\t\t\t<S sid=\"174\" ssid=\"19\">Probability estimates canbe readily used in further downstream pro cessing, but cosines are less useful.?</S>\n\t\t\t<S sid=\"175\" ssid=\"20\">The automatically generated patterns in PairClass are slightly more general than the pat terns of Turney (2006).</S>\n\t\t\t<S sid=\"176\" ssid=\"21\">The morphological processing in PairClass (Minnen et al, 2001) is more sophisticated than in Turney (2006).</S>\n\t\t\t<S sid=\"177\" ssid=\"22\">910 Experiment Number of vectors Number of features Number of classes SAT Analogies 2,244 (374 ? 6) 44,880 (2, 244 ? 20) 374 TOEFL Synonyms 320 (80 ? 4) 6,400 (320 ? 20) 2 Synonyms and Antonyms 136 2,720 (136 ? 20) 2 Similar, Associated, and Both 144 2,880 (144 ? 20) 3 Table 8: Summary of the four tasks.</S>\n\t\t\t<S sid=\"178\" ssid=\"23\">See Section 3 for explanations.</S>\n\t\t\t<S sid=\"179\" ssid=\"24\">Experiment Accuracy Best previous Human Baseline Rank SAT Analogies 52.1% 56.1% 57.0% 20.0% 2 higher out of 12 TOEFL Synonyms 76.2% 97.5% 64.5% 25.0% 8 higher out of 15 Synonyms and Antonyms 75.0% none unknown 65.4% none Similar, Associated, and Both 77.1% none unknown 33.3% none Table 9: Summary of experimental results.</S>\n\t\t\t<S sid=\"180\" ssid=\"25\">See Section 3 for explanations.</S>\n\t\t\t<S sid=\"181\" ssid=\"26\">However, we believe that the main contribution of this paper is not PairClass itself, but the extension of supervised word pair classification beyond theclassification of noun-modifier pairs and semantic relations between nominals, to analogies, syn onyms, antonyms, and associations.</S>\n\t\t\t<S sid=\"182\" ssid=\"27\">As far as we know, this has not been done before.</S>\n\t</SECTION>\n\t<SECTION title=\"Limitations and Future Work. \" number=\"6\">\n\t\t\t<S sid=\"183\" ssid=\"1\">The main limitation of PairClass is the need for a large corpus.</S>\n\t\t\t<S sid=\"184\" ssid=\"2\">Phrases that contain a pair of wordstend to be more rare than phrases that contain either of the members of the pair, thus a large cor pus is needed to ensure that sufficient numbers of phrases are found for each input word pair.</S>\n\t\t\t<S sid=\"185\" ssid=\"3\">The size of the corpus has a cost in terms of disk spaceand processing time.</S>\n\t\t\t<S sid=\"186\" ssid=\"4\">In the future, as hardware im proves, this will become less of an issue, but there may be ways to improve the algorithm, so that a smaller corpus is sufficient.Another area for future work is to apply Pair Class to more tasks.</S>\n\t\t\t<S sid=\"187\" ssid=\"5\">WordNet includes more thana dozen semantic relations (e.g., synonyms, hy ponyms, hypernyms, meronyms, holonyms, and antonyms).</S>\n\t\t\t<S sid=\"188\" ssid=\"6\">PairClass should be applicable to allof these relations.</S>\n\t\t\t<S sid=\"189\" ssid=\"7\">Other potential applications in clude any task that involves semantic relations, such as word sense disambiguation, informationretrieval, information extraction, and metaphor in terpretation.</S>\n\t</SECTION>\n\t<SECTION title=\"Conclusion. \" number=\"7\">\n\t\t\t<S sid=\"190\" ssid=\"1\">In this paper, we have described a uniform approach to analogies, synonyms, antonyms, and as sociations, in which all of these phenomena are subsumed by analogies.</S>\n\t\t\t<S sid=\"191\" ssid=\"2\">We view the problem ofrecognizing analogies as the classification of se mantic relations between words.</S>\n\t\t\t<S sid=\"192\" ssid=\"3\">We believe that most of our lexical knowledge is relational, not attributional.</S>\n\t\t\t<S sid=\"193\" ssid=\"4\">That is, meaning is largely about relations among words, rather thanproperties of individual words, considered in isolation.</S>\n\t\t\t<S sid=\"194\" ssid=\"5\">For example, consider the knowledge en coded in WordNet: much of the knowledge in WordNet is embedded in the graph structure that connects words.</S>\n\t\t\t<S sid=\"195\" ssid=\"6\">Analogies of the form A:B::C:D are calledproportional analogies.</S>\n\t\t\t<S sid=\"196\" ssid=\"7\">These types of lowerlevel analogies may be contrasted with higher level analogies, such as the analogy between the solar system and Rutherford?s model of the atom (Falkenhainer et al, 1989), which are sometimes called conceptual analogies.</S>\n\t\t\t<S sid=\"197\" ssid=\"8\">We believe that the difference between these two types is largely a matter of complexity.</S>\n\t\t\t<S sid=\"198\" ssid=\"9\">A higher-level analogy is composed of many lower-level analogies.</S>\n\t\t\t<S sid=\"199\" ssid=\"10\">Progresswith algorithms for processing lower-level analo gies will eventually contribute to algorithms for higher-level analogies.The idea of subsuming a broad range of semantic phenomena under analogies has been sug gested by many researchers.</S>\n\t\t\t<S sid=\"200\" ssid=\"11\">Minsky (1986) wrote, ?How do we ever understand anything?</S>\n\t\t\t<S sid=\"201\" ssid=\"12\">Almost always, I think, by using one or another kind of analogy.?</S>\n\t\t\t<S sid=\"202\" ssid=\"13\">Hofstadter (2007) claimed, ?all meaningcomes from analogies.?</S>\n\t\t\t<S sid=\"203\" ssid=\"14\">In NLP, analogical algo rithms have been applied to machine translation(Lepage and Denoual, 2005), morphology (Lep age, 1998), and semantic relations (Turney and Littman, 2005).</S>\n\t\t\t<S sid=\"204\" ssid=\"15\">Analogy provides a frameworkthat has the potential to unify the field of seman tics.</S>\n\t\t\t<S sid=\"205\" ssid=\"16\">This paper is a small step towards that goal.</S>\n\t\t\t<S sid=\"206\" ssid=\"17\">AcknowledgementsThanks to Joel Martin and the anonymous review ers of Coling 2008 for their helpful comments.</S>\n\t\t\t<S sid=\"207\" ssid=\"18\">911</S>\n\t</SECTION>\n</PAPER>",
                    "generation_prompt_uid": "fb9a76059611c2a9feea759ae955c349",
                    "include_year": false,
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo",
                    "origin": "LLM",
                    "title": "A Uniform Approach to Analogies Synonyms Antonyms and Associations"
                }
            }
        }
    ]
}