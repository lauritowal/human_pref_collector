{
    "timestamp": "20250511123626",
    "username": "test",
    "email": "test@fsdfds.de",
    "model": "gpt3_5",
    "category": "paper",
    "totalLLMChoices": 14,
    "totalHumanChoices": 11,
    "totalNoPreference": 0,
    "userChoices": [
        {
            "index": 0,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Syntactic natural language parsers have shown themselves to be inadequate for processing highly-ambiguous large-vocabulary text, as is evidenced by their poor performance on domains like the Wall Street Journal, and by the movement away from parsing-based approaches to textprocessing in general. In this paper, I describe SPATTER, a statistical parser based on decision-tree learning techniques which constructs a complete parse for every sentence and achieves accuracy rates far better than any published result. This work is based on the following premises: (1) grammars are too complex and detailed to develop manually for most interesting domains; (2) parsing models must rely heavily on lexical and contextual information to analyze sentences accurately; and (3) existing n-grain modeling techniques are inadequate for parsing models. In experiments comparing SPATTER with IBM's computer manuals parser, SPATTER significantly outperforms the grammar-based parser. Evaluating SPATTER against the Penn Treebank Wall Street Journal corpus using the PARSEVAL measures, SPATachieves 86% precision, and 1.3 crossing brackets per sentence for sentences of 40 words or less, and 91% precision, 90% recall, and 0.5 crossing brackets for sentences between 10 and 20 words in length.",
                    "filename": "Statistical_Decision-Tree_Models_For_Parsing",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P95-1037/Documents_xml/P95-1037.xml",
                    "title": "Statistical Decision-Tree Models For Parsing"
                },
                "llm": {
                    "descriptions": [
                        "This academic paper presents a statistical decision-tree model for parsing natural language sentences. The paper introduces the concept of disambiguation decisions in the parsing process and proposes the use of decision-tree models to automatically discover disambiguation criteria. The paper discusses the decision-tree modeling process, training and parsing procedures, and the SPATTER parsing algorithm. The effectiveness of the SPATTER parser is evaluated through experiments on different domains, including IBM Computer Manuals and the Wall Street Journal. The results show promising performance, with SPATTER outperforming a state-of-the-art grammar-based parser in the IBM Computer Manuals domain. The paper concludes by emphasizing the importance of incorporating large amounts of contextual information into statistical models for parsing. Overall, the paper highlights the potential of decision-tree models in effectively parsing natural language sentences without the need for a complicated knowledge base or grammar.",
                        "This paper presents statistical decision-tree models for natural language parsing. The authors address the problem of automatically discovering disambiguation criteria for parsing decisions, based on a set of possible features. The models use probabilities estimated through decision tree models to make parsing decisions, allowing for non-deterministic application of disambiguation criteria. The authors demonstrate the construction and estimation of decision-tree models and compare the performance of their parser, SPATTER, with a state-of-the-art grammar-based parser. They also experiment with parsing in different domains, achieving promising results. The paper highlights the advantages of decision-tree models in acquiring large contextual information for parsing and emphasizes the importance of incorporating extensive contextual features into statistical models for more accurate parsing. The results indicate that SPATTER's efficient parsing algorithm can find the highest probability parse for a wide range of sentences, making it a promising approach for natural language parsing.",
                        "This academic paper presents statistical decision-tree models for parsing natural language sentences. It introduces the idea of making disambiguation decisions during parsing by automatically discovering disambiguation criteria using a set of possible features. The paper highlights the use of statistical decision tree models for estimating probabilities and parsing sentences, and it discusses the SPATTER parsing algorithm, which is based on statistical pattern recognition. The paper includes experiment results from parsing different domains, such as IBM Computer Manuals and the Wall Street Journal, showing the performance and accuracy of the SPATTER parser. The conclusion emphasizes the need for statistical models to consider a wide range of features and contextual information for effective parsing, and it illustrates how decision-tree learning algorithms can be applied to a large annotated corpus to achieve this. Overall, the paper demonstrates the effectiveness of statistical decision-tree models for parsing natural language sentences.",
                        "This paper presents the SPATTER (Statistical PATTErn Recognizer) parser, which uses statistical decision-tree models for parsing natural language sentences. The decision-tree models are used to automatically discover disambiguation criteria for parsing decisions, incorporating features such as words, relationships among words, and relationships among constituents. The paper demonstrates the equivalence of decision-tree models and interpolated n-gram models, and discusses the training and parsing procedures used in SPATTER. Experimental results show that SPATTER outperforms a state-of-the-art grammar-based parser in parsing accuracy for both IBM Computer Manuals and the Wall Street Journal domain. The paper emphasizes the need for statistical models to consider a wide range of features and a large amount of contextual information, and illustrates how decision-tree learning algorithms can be applied to achieve this. Overall, the paper highlights the effectiveness of statistical decision-tree models in natural language parsing, demonstrating their ability to outperform traditional grammar-based parsers."
                    ],
                    "filename": "statistical_decisiontree_models_-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 184 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Statistical Decision-Tree Models For Parsing</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Parsing a natural language sentence can be viewed as making a sequence of disambiguation decisions: determining the part-of-speech of the words, choosing between possible constituent structures, and selecting labels for the constituents.</S>\n    <S sid=\"7\" ssid=\"2\">Traditionally, disambiguation problems in parsing have been addressed by enumerating possibilities and explicitly declaring knowledge which might aid the disambiguation process.</S>\n    <S sid=\"8\" ssid=\"3\">However, these approaches have proved too brittle for most interesting natural language problems.</S>\n    <S sid=\"9\" ssid=\"4\">This work addresses the problem of automatically discovering the disambiguation criteria for all of the decisions made during the parsing process, given the set of possible features which can act as disambiguators.</S>\n    <S sid=\"10\" ssid=\"5\">The candidate disambiguators are the words in the sentence, relationships among the words, and relationships among constituents already constructed in the parsing process.</S>\n    <S sid=\"11\" ssid=\"6\">Since most natural language rules are not absolute, the disambiguation criteria discovered in this work are never applied deterministically.</S>\n    <S sid=\"12\" ssid=\"7\">Instead, all decisions are pursued non-deterministically according to the probability of each choice.</S>\n    <S sid=\"13\" ssid=\"8\">These probabilities are estimated using statistical decision tree models.</S>\n    <S sid=\"14\" ssid=\"9\">The probability of a complete parse tree (T) of a sentence (S) is the product of each decision (c11) conditioned on all previous decisions: Each decision sequence constructs a unique parse, and the parser selects the parse whose decision sequence yields the highest cumulative probability.</S>\n    <S sid=\"15\" ssid=\"10\">By combining a stack decoder search with a breadthfirst algorithm with probabilistic pruning, it is possible to identify the highest-probability parse for any sentence using a reasonable amount of memory and time.</S>\n    <S sid=\"16\" ssid=\"11\">The claim of this work is that statistics from a large corpus of parsed sentences combined with information-theoretic classification and training algorithms can produce an accurate natural language parser without the aid of a complicated knowledge base or grammar.</S>\n    <S sid=\"17\" ssid=\"12\">This claim is justified by constructing a parser, called SPATTER (Statistical PATTErn Recognizer), based on very limited linguistic information, and comparing its performance to a state-of-the-art grammar-based parser on a common task.</S>\n    <S sid=\"18\" ssid=\"13\">It remains to be shown that an accurate broad-coverage parser can improve the performance of a text processing application.</S>\n    <S sid=\"19\" ssid=\"14\">This will be the subject of future experiments.</S>\n    <S sid=\"20\" ssid=\"15\">One of the important points of this work is that statistical models of natural language should not be restricted to simple, context-insensitive models.</S>\n    <S sid=\"21\" ssid=\"16\">In a problem like parsing, where long-distance lexical information is crucial to disambiguate interpretations accurately, local models like probabilistic context-free grammars are inadequate.</S>\n    <S sid=\"22\" ssid=\"17\">This work illustrates that existing decision-tree technology can be used to construct and estimate models which selectively choose elements of the context which contribute to disambiguation decisions, and which have few enough parameters to be trained using existing resources.</S>\n    <S sid=\"23\" ssid=\"18\">I begin by describing decision-tree modeling, showing that decision-tree models are equivalent to interpolated n-gram models.</S>\n    <S sid=\"24\" ssid=\"19\">Then I briefly describe the training and parsing procedures used in SPATTER.</S>\n    <S sid=\"25\" ssid=\"20\">Finally, I present some results of experiments comparing SPATTER with a grammarian's rulebased statistical parser, along with more recent results showing SPATTER applied to the Wall Street Journal domain.</S>\n  </SECTION>\n  <SECTION title=\"2 Decision-Tree Modeling\" number=\"2\">\n    <S sid=\"26\" ssid=\"1\">Much of the work in this paper depends on replacing human decision-making skills with automatic decision-making algorithms.</S>\n    <S sid=\"27\" ssid=\"2\">The decisions under consideration involve identifying constituents and constituent labels in natural language sentences.</S>\n    <S sid=\"28\" ssid=\"3\">Grammarians, the human decision-makers in parsing, solve this problem by enumerating the features of a sentence which affect the disambiguation decisions and indicating which parse to select based on the feature values.</S>\n    <S sid=\"29\" ssid=\"4\">The grammarian is accomplishing two critical tasks: identifying the features which are relevant to each decision, and deciding which choice to select based on the values of the relevant features.</S>\n    <S sid=\"30\" ssid=\"5\">Decision-tree classification algorithms account for both of these tasks, and they also accomplish a third task which grammarians classically find difficult.</S>\n    <S sid=\"31\" ssid=\"6\">By assigning a probability distribution to the possible choices, decision trees provide a ranking system which not only specifies the order of preference for the possible choices, but also gives a measure of the relative likelihood that each choice is the one which should be selected.</S>\n    <S sid=\"32\" ssid=\"7\">A decision tree is a decision-making device which assigns a probability to each of the possible choices based on the context of the decision: P(f Ih), where f is an element of the future vocabulary (the set of choices) and h is a history (the context of the decision).</S>\n    <S sid=\"33\" ssid=\"8\">This probability P(flh) is determined by asking a sequence of questions qi q2...q„ about the context, where the ith question asked is uniquely determined by the answers to the i —1 previous questions.</S>\n    <S sid=\"34\" ssid=\"9\">For instance, consider the part-of-speech tagging problem.</S>\n    <S sid=\"35\" ssid=\"10\">The first question a decision tree might ask is: If the answer is the, then the decision tree needs to ask no more questions; it is clear that the decision tree should assign the tag f = determiner with probability 1.</S>\n    <S sid=\"36\" ssid=\"11\">If, instead, the answer to question 1 is bear, the decision tree might next ask the question: If the answer to question 2 is determiner, the decision tree might stop asking questions and assign the tag f = noun with very high probability, and the tag f = verb with much lower probability.</S>\n    <S sid=\"37\" ssid=\"12\">However, if the answer to question 2 is noun, the decision tree would need to ask still more questions to get a good estimate of the probability of the tagging decision.</S>\n    <S sid=\"38\" ssid=\"13\">The decision tree described in this paragraph is shown in Figure 1.</S>\n    <S sid=\"39\" ssid=\"14\">Each question asked by the decision tree is represented by a tree node (an oval in the figure) and the possible answers to this question are associated with branches emanating from the node.</S>\n    <S sid=\"40\" ssid=\"15\">Each node defines a probability distribution on the space of possible decisions.</S>\n    <S sid=\"41\" ssid=\"16\">A node at which the decision tree stops asking questions is a leaf node.</S>\n    <S sid=\"42\" ssid=\"17\">The leaf nodes represent the unique states in the decision-making problem, i.e. all contexts which lead to the same leaf node have the same probability distribution for the decision.</S>\n    <S sid=\"43\" ssid=\"18\">A decision-tree model is not really very different from an interpolated n-gram model.</S>\n    <S sid=\"44\" ssid=\"19\">In fact, they are equivalent in representational power.</S>\n    <S sid=\"45\" ssid=\"20\">The main differences between the two modeling techniques are how the models are parameterized and how the parameters are estimated.</S>\n    <S sid=\"46\" ssid=\"21\">First, let's be very clear on what we mean by an n-grain model.</S>\n    <S sid=\"47\" ssid=\"22\">Usually, an n-gram model refers to a Markov process where the probability of a particular token being generating is dependent on the values of the previous n — 1 tokens generated by the same process.</S>\n    <S sid=\"48\" ssid=\"23\">By this definition, an n-gram model has I WI&amp;quot; parameters, where IWI is the number of unique tokens generated by the process.</S>\n    <S sid=\"49\" ssid=\"24\">However, here let's define an n-gram model more loosely as a model which defines a probability distribution on a random variable given the values of n-1 random variables, P(.flhi h2 .</S>\n    <S sid=\"50\" ssid=\"25\">• . hn-1).</S>\n    <S sid=\"51\" ssid=\"26\">There is no assumption in the definition that any of the random variables F or Hi range over the same vocabulary.</S>\n    <S sid=\"52\" ssid=\"27\">The number of parameters in this n-gram model is IFI H IHil.</S>\n    <S sid=\"53\" ssid=\"28\">Using this definition, an n-gram model can be represented by a decision-tree model with n — 1 questions.</S>\n    <S sid=\"54\" ssid=\"29\">For instance, the part-of-speech tagging model P(t11w2t1—iti-2) can be interpreted as a 4gram model, where HI is the variable denoting the word being tagged, 112 is the variable denoting the tag of the previous word, and H3 is the variable denoting the tag of the word two words back.</S>\n    <S sid=\"55\" ssid=\"30\">Hence, this 4-gram tagging model is the same as a decisiontree model which always asks the sequence of 3 questions: But can a decision-tree model be represented by an n-gram model?</S>\n    <S sid=\"56\" ssid=\"31\">No, but it can be represented by an interpolated n-gram model.</S>\n    <S sid=\"57\" ssid=\"32\">The proof of this assertion is given in the next section.</S>\n    <S sid=\"58\" ssid=\"33\">The standard approach to estimating an n-gram model is a two step process.</S>\n    <S sid=\"59\" ssid=\"34\">The first step is to count the number of occurrences of each n-gram from a training corpus.</S>\n    <S sid=\"60\" ssid=\"35\">This process determines the empirical distribution, The second step is smoothing the empirical distribution using a separate, held-out corpus.</S>\n    <S sid=\"61\" ssid=\"36\">This step improves the empirical distribution by finding statistically unreliable parameter estimates and adjusting them based on more reliable information.</S>\n    <S sid=\"62\" ssid=\"37\">A commonly-used technique for smoothing is deleted interpolation._ Deleted interpolation estimates a model P(flhi 112 .</S>\n    <S sid=\"63\" ssid=\"38\">• • hn-1) by using a linear combination of empirical models P(fihki hk, . h,.</S>\n    <S sid=\"64\" ssid=\"39\">), where m &lt; n and k1_1 &lt; ki &lt; n for all i &lt; m. For example, a model P(f Ihi h2h3) might be interpolated as follows: where E Ai (hi h2h3) = 1 for all histories hi h2h3.</S>\n    <S sid=\"65\" ssid=\"40\">The optimal values for the Ai functions can be estimated using the forward-backward algorithm (Baum, 1972).</S>\n    <S sid=\"66\" ssid=\"41\">A decision-tree model can be represented by an interpolated n-gram model as follows.</S>\n    <S sid=\"67\" ssid=\"42\">A leaf node in a decision tree can be represented by the sequence of question answers, or history values, which leads the decision tree to that leaf.</S>\n    <S sid=\"68\" ssid=\"43\">Thus, a leaf node defines a probability distribution based on values of those questions: P(f Ihkihh .</S>\n    <S sid=\"69\" ssid=\"44\">.</S>\n    <S sid=\"70\" ssid=\"45\">. hk..), where m &lt; n and ki_I &lt; k &lt; n, and where hk, is the answer to one of the questions asked on the path from the root to the leaf.'</S>\n    <S sid=\"71\" ssid=\"46\">But this is the same as one of the terms in the interpolated n-gram model.</S>\n    <S sid=\"72\" ssid=\"47\">So, a decision 'Note that in a decision tree, the leaf distribution is not affected by the order in which questions are asked.</S>\n    <S sid=\"73\" ssid=\"48\">Asking about Pi, followed by h2 yields the same future distribution as asking about h2 followed by h1. tree can be defined as an interpolated n-grain model where the Ai function is defined as: 1 if hki hk, hk_ is a leaf, Ai (hki hk, • • • hk,.)</S>\n    <S sid=\"74\" ssid=\"49\">= o otherwise.</S>\n    <S sid=\"75\" ssid=\"50\">The point of showing the equivalence between ngram models and decision-tree models is to make clear that the power of decision-tree models is not in their expressiveness, but instead in how they can be automatically acquired for very large modeling problems.</S>\n    <S sid=\"76\" ssid=\"51\">As n grows, the parameter space for an n-gram model grows exponentially, and it quickly becomes computationally infeasible to estimate the smoothed model using deleted interpolation.</S>\n    <S sid=\"77\" ssid=\"52\">Also, as n grows large, the likelihood that the deleted interpolation process will converge to an optimal or even near-optimal parameter setting becomes vanishingly small.</S>\n    <S sid=\"78\" ssid=\"53\">On the other hand, the decision-tree learning algorithm increases the size of a model only as the training data allows.</S>\n    <S sid=\"79\" ssid=\"54\">Thus, it can consider very large history spaces, i.e. n-gram models with very large n. Regardless of the value of n, the number of parameters in the resulting model will remain relatively constant, depending mostly on the number of training examples.</S>\n    <S sid=\"80\" ssid=\"55\">The leaf distributions in decision trees are empirical estimates, i.e. relative-frequency counts from the training data.</S>\n    <S sid=\"81\" ssid=\"56\">Unfortunately, they assign probability zero to events which can possibly occur.</S>\n    <S sid=\"82\" ssid=\"57\">Therefore, just as it is necessary to smooth empirical ngram models, it is also necessary to smooth empirical decision-tree models.</S>\n    <S sid=\"83\" ssid=\"58\">The decision-tree learning algorithms used in this work were developed over the past 15 years by the IBM Speech Recognition group (Bahl et al., 1989).</S>\n    <S sid=\"84\" ssid=\"59\">The growing algorithm is an adaptation of the CART algorithm in (Breiman et al., 1984).</S>\n    <S sid=\"85\" ssid=\"60\">For detailed descriptions and discussions of the decisiontree algorithms used in this work, see (Magerman, 1994).</S>\n    <S sid=\"86\" ssid=\"61\">An important point which has been omitted from this discussion of decision trees is the fact that only binary questions are used in these decision trees.</S>\n    <S sid=\"87\" ssid=\"62\">A question which has k values is decomposed into a sequence of binary questions using a classification tree on those k values.</S>\n    <S sid=\"88\" ssid=\"63\">For example, a question about a word is represented as 30 binary questions.</S>\n    <S sid=\"89\" ssid=\"64\">These 30 questions are determined by growing a classification tree on the word vocabulary as described in (Brown et al., 1992).</S>\n    <S sid=\"90\" ssid=\"65\">The 30 questions represent 30 different binary partitions of the word vocabulary, and these questions are defined such that it is possible to identify each word by asking all 30 questions.</S>\n    <S sid=\"91\" ssid=\"66\">For more discussion of the use of binary decision-tree questions, see (Magerman, 1994).</S>\n  </SECTION>\n  <SECTION title=\"3 SPATTER Parsing\" number=\"3\">\n    <S sid=\"92\" ssid=\"1\">The SPATTER parsing algorithm is based on interpreting parsing as a statistical pattern recognition process.</S>\n    <S sid=\"93\" ssid=\"2\">A parse tree for a sentence is constructed by starting with the sentence's words as leaves of a tree structure, and labeling and extending nodes these nodes until a single-rooted, labeled tree is constructed.</S>\n    <S sid=\"94\" ssid=\"3\">This pattern recognition process is driven by the decision-tree models described in the previous section.</S>\n    <S sid=\"95\" ssid=\"4\">A parse tree can be viewed as an n-ary branching tree, with each node in a tree labeled by either a non-terminal label or a part-of-speech label.</S>\n    <S sid=\"96\" ssid=\"5\">If a parse tree is interpreted as a geometric pattern, a constituent is no more than a set of edges which meet at the same tree node.</S>\n    <S sid=\"97\" ssid=\"6\">For instance, the noun phrase, &amp;quot;a brown cow,&amp;quot; consists of an edge extending to the right from &amp;quot;a,&amp;quot; an edge extending to the left from &amp;quot;cow,&amp;quot; and an edge extending straight up from &amp;quot;brown&amp;quot;.</S>\n    <S sid=\"98\" ssid=\"7\">In SPATTER, a parse tree is encoded in terms of four elementary components, or features: words, tags, labels, and extensions.</S>\n    <S sid=\"99\" ssid=\"8\">Each feature has a fixed vocabulary, with each element of a given feature vocabulary having a unique representation.</S>\n    <S sid=\"100\" ssid=\"9\">The word feature can take on any value of any word.</S>\n    <S sid=\"101\" ssid=\"10\">The tag feature can take on any value in the part-of-speech tag set.</S>\n    <S sid=\"102\" ssid=\"11\">The label feature can take on any value in the non-terminal set.</S>\n    <S sid=\"103\" ssid=\"12\">The extension can take on any of the following five values: right - the node is the first child of a constituent; left - the node is the last child of a constituent; up - the node is neither the first nor the last child of a constituent; unary - the node is a child of a unary constituent; root - the node is the root of the tree.</S>\n    <S sid=\"104\" ssid=\"13\">For an n word sentence, a parse tree has n leaf nodes, where the word feature value of the ith leaf node is the ith word in the sentence.</S>\n    <S sid=\"105\" ssid=\"14\">The word feature value of the internal nodes is intended to contain the lexical head of the node's constituent.</S>\n    <S sid=\"106\" ssid=\"15\">A deterministic lookup table based on the label of the internal node and the labels of the children is used to approximate this linguistic notion.</S>\n    <S sid=\"107\" ssid=\"16\">The SPATTER representation of the sentence is shown in Figure 3.</S>\n    <S sid=\"108\" ssid=\"17\">The nodes are constructed bottom-up from left-to-right, with the constraint that no constituent node is constructed until all of its children have been constructed.</S>\n    <S sid=\"109\" ssid=\"18\">The order in which the nodes of the example sentence are constructed is indicated in the figure.</S>\n    <S sid=\"110\" ssid=\"19\">Each code used by the PC is listed SPATTER consists of three main decision-tree models: a part-of-speech tagging model, a nodeextension model, and a node-labeling model.</S>\n    <S sid=\"111\" ssid=\"20\">Each of these decision-tree models are grown using the following questions, where X is one of word, tag, label, or extension, and Y is either left and right: For each of the nodes listed above, the decision tree could also ask about the number of children and span of the node.</S>\n    <S sid=\"112\" ssid=\"21\">For the tagging model, the values of the previous two words and their tags are also asked, since they might differ from the head words of the previous two constituents.</S>\n    <S sid=\"113\" ssid=\"22\">The training algorithm proceeds as follows.</S>\n    <S sid=\"114\" ssid=\"23\">The training corpus is divided into two sets, approximately 90% for tree growing and 10% for tree smoothing.</S>\n    <S sid=\"115\" ssid=\"24\">For each parsed sentence in the tree growing corpus, the correct state sequence is traversed.</S>\n    <S sid=\"116\" ssid=\"25\">Each state transition from si to si+i is an event; the history is made up of the answers to all of the questions at state si and the future is the value of the action taken from state si to state s.1.</S>\n    <S sid=\"117\" ssid=\"26\">Each event is used as a training example for the decisiontree growing process for the appropriate feature's tree (e.g. each tagging event is used for growing the tagging tree, etc.).</S>\n    <S sid=\"118\" ssid=\"27\">After the decision trees are grown, they are smoothed using the tree smoothing corpus using a variation of the deleted interpolation algorithm described in (Magerman, 1994).</S>\n    <S sid=\"119\" ssid=\"28\">The parsing procedure is a search for the highest probability parse tree.</S>\n    <S sid=\"120\" ssid=\"29\">The probability of a parse is just the product of the probability of each of the actions made in constructing the parse, according to the decision-tree models.</S>\n    <S sid=\"121\" ssid=\"30\">Because of the size of the search space, (roughly 0(1711N1), where IT1 is the number of part-ofspeech tags, n is the number of words in the sentence, and 'NI is the number of non-terminal labels), it is not possible to compute the probability of every parse.</S>\n    <S sid=\"122\" ssid=\"31\">However, the specific search algorithm used is not very important, so long as there are no search errors.</S>\n    <S sid=\"123\" ssid=\"32\">A search error occurs when the the highest probability parse found by the parser is not the highest probability parse in the space of all parses.</S>\n    <S sid=\"124\" ssid=\"33\">SPATTER's search procedure uses a two phase approach to identify the highest probability parse of a sentence.</S>\n    <S sid=\"125\" ssid=\"34\">First, the parser uses a stack decoding algorithm to quickly find a complete parse for the sentence.</S>\n    <S sid=\"126\" ssid=\"35\">Once the stack decoder has found a complete parse of reasonable probability (&gt; 10—s), it switches to a breadth-first mode to pursue all of the partial parses which have not been explored by the stack decoder.</S>\n    <S sid=\"127\" ssid=\"36\">In this second mode, it can safely discard any partial parse which has a probability lower than the probability of the highest probability completed parse.</S>\n    <S sid=\"128\" ssid=\"37\">Using these two search modes, SPATTER guarantees that it will find the highest probability parse.</S>\n    <S sid=\"129\" ssid=\"38\">The only limitation of this search technique is that, for sentences which are modeled poorly, the search might exhaust the available memory before completing both phases.</S>\n    <S sid=\"130\" ssid=\"39\">However, these search errors conveniently occur on sentences which SPATTER is likely to get wrong anyway, so there isn't much performance lossed due to the search errors.</S>\n    <S sid=\"131\" ssid=\"40\">Experimentally, the search algorithm guarantees the highest probability parse is found for over 96% of the sentences parsed.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiment Results\" number=\"4\">\n    <S sid=\"132\" ssid=\"1\">In the absence of an NL system, SPATTER can be evaluated by comparing its top-ranking parse with the treebank analysis for each test sentence.</S>\n    <S sid=\"133\" ssid=\"2\">The parser was applied to two different domains, IBM Computer Manuals and the Wall Street Journal.</S>\n    <S sid=\"134\" ssid=\"3\">The first experiment uses the IBM Computer Manuals domain, which consists of sentences extracted from IBM computer manuals.</S>\n    <S sid=\"135\" ssid=\"4\">The training and test sentences were annotated by the University of Lancaster.</S>\n    <S sid=\"136\" ssid=\"5\">The Lancaster treebank uses 195 part-ofspeech tags and 19 non-terminal labels.</S>\n    <S sid=\"137\" ssid=\"6\">This treebank is described in great detail in (Black et al., 1993).</S>\n    <S sid=\"138\" ssid=\"7\">The main reason for applying SPATTER to this domain is that IBM had spent the previous ten years developing a rule-based, unification-style probabilistic context-free grammar for parsing this domain.</S>\n    <S sid=\"139\" ssid=\"8\">The purpose of the experiment was to estimate SPATTER's ability to learn the syntax for this domain directly from a treebank, instead of depending on the interpretive expertise of a grammarian.</S>\n    <S sid=\"140\" ssid=\"9\">The parser was trained on the first 30,800 sentences from the Lancaster treebank.</S>\n    <S sid=\"141\" ssid=\"10\">The test set included 1,473 new sentences, whose lengths range from 3 to 30 words, with a mean length of 13.7 words.</S>\n    <S sid=\"142\" ssid=\"11\">These sentences are the same test sentences used in the experiments reported for IBM's parser in (Black et al., 1993).</S>\n    <S sid=\"143\" ssid=\"12\">In (Black et al., 1993), IBM's parser was evaluated using the 0-crossingbrackets measure, which represents the percentage of sentences for which none of the constituents in the parser's parse violates the constituent boundaries of any constituent in the correct parse.</S>\n    <S sid=\"144\" ssid=\"13\">After over ten years of grammar development, the IBM parser achieved a 0-crossing-brackets score of 69%.</S>\n    <S sid=\"145\" ssid=\"14\">On this same test set, SPATTER scored 76%.</S>\n    <S sid=\"146\" ssid=\"15\">The experiment is intended to illustrate SPATTER's ability to accurately parse a highly-ambiguous, large-vocabulary domain.</S>\n    <S sid=\"147\" ssid=\"16\">These experiments use the Wall Street Journal domain, as annotated in the Penn Treebank, version 2.</S>\n    <S sid=\"148\" ssid=\"17\">The Penn Treebank uses 46 part-of-speech tags and 27 non-terminal labels.2 The WSJ portion of the Penn Treebank is divided into 25 sections, numbered 00 - 24.</S>\n    <S sid=\"149\" ssid=\"18\">In these experiments, SPATTER was trained on sections 02 - 21, which contains approximately 40,000 sentences.</S>\n    <S sid=\"150\" ssid=\"19\">The test results reported here are from section 00, which contains 1920 sentences.3 Sections 01, 22, 23, and 24 will be used as test data in future experiments.</S>\n    <S sid=\"151\" ssid=\"20\">The Penn Treebank is already tokenized and sentence detected by human annotators, and thus the test results reported here reflect this.</S>\n    <S sid=\"152\" ssid=\"21\">SPATTER parses word sequences, not tag sequences.</S>\n    <S sid=\"153\" ssid=\"22\">Furthermore, SPATTER does not simply pre-tag the sentences and use only the best tag sequence in parsing.</S>\n    <S sid=\"154\" ssid=\"23\">Instead, it uses a probabilistic model to assign tags to the words, and considers all possible tag sequences according to the probability they are assigned by the model.</S>\n    <S sid=\"155\" ssid=\"24\">No information about the legal tags for a word are extracted from the test corpus.</S>\n    <S sid=\"156\" ssid=\"25\">In fact, no information other than the words is used from the test corpus.</S>\n    <S sid=\"157\" ssid=\"26\">For the sake of efficiency, only the sentences of 40 words or fewer are included in these experiments.4 For this test set, SPATTER takes on average 12 seconds per sentence on an SGI R4400 with 160 megabytes of RAM.</S>\n    <S sid=\"158\" ssid=\"27\">To evaluate SPATTER's performance on this domain, I am using the PARSEVAL measures, as defined in (Black et al., 1991): Precision no. of correct constituents in SPATTER parse no. of constituents in SPATTER parse Recall no. of correct constituents in SPATTER parse no. of constituents in treebank parse Crossing Brackets no. of constituents which violate constituent boundaries with a constituent in the treebank parse.</S>\n    <S sid=\"159\" ssid=\"28\">The precision and recall measures do not consider constituent labels in their evaluation of a parse, since the treebank label set will not necessarily coincide with the labels used by a given grammar.</S>\n    <S sid=\"160\" ssid=\"29\">Since SPATTER uses the same syntactic label set as the Penn Treebank, it makes sense to report labelled precision and labelled recall.</S>\n    <S sid=\"161\" ssid=\"30\">These measures are computed by considering a constituent to be correct if and only if it's label matches the label in the treebank.</S>\n    <S sid=\"162\" ssid=\"31\">Table 1 shows the results of SPATTER evaluated against the Penn Treebank on the Wall Street Journal section 00.</S>\n    <S sid=\"163\" ssid=\"32\">Figures 5, 6, and 7 illustrate the performance of SPATTER as a function of sentence length.</S>\n    <S sid=\"164\" ssid=\"33\">SPATTER's performance degrades slowly for sentences up to around 28 words, and performs more poorly and more erratically as sentences get longer.</S>\n    <S sid=\"165\" ssid=\"34\">Figure 4 indicates the frequency of each sentence length in the test corpus. function of sentence length for Wall Street Journal experiments.</S>\n  </SECTION>\n  <SECTION title=\"5 Conclusion\" number=\"5\">\n    <S sid=\"166\" ssid=\"1\">Regardless of what techniques are used for parsing disambiguation, one thing is clear: if a particular piece of information is necessary for solving a disambiguation problem, it must be made available to the disambiguation mechanism.</S>\n    <S sid=\"167\" ssid=\"2\">The words in the sentence are clearly necessary to make parsing decisions, and in some cases long-distance structural information is also needed.</S>\n    <S sid=\"168\" ssid=\"3\">Statistical models for parsing need to consider many more features of a sentence than can be managed by n-gram modeling techniques and many more examples than a human can keep track of.</S>\n    <S sid=\"169\" ssid=\"4\">The SPATTER parser illustrates how large amounts of contextual information can be incorporated into a statistical model for parsing by applying decision-tree learning algorithms to a large annotated corpus.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "f978dc482a9beff41540d6eb4b943fc3",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Statistical Decision-Tree Models For Parsing"
                }
            }
        },
        {
            "index": 1,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we explore the use of BFs for language modelling in statistical machine translation. show how a BF containing can enable us to use much larger corpora and higher-order models complementing a con- LM within an SMT system. We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams. Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.",
                    "filename": "Randomised_Language_Modelling_for_Statistical_Machine_Translation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P07-1065/Documents_xml/P07-1065.xml",
                    "title": "Randomised Language Modelling for Statistical Machine Translation"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores the use of Bloom filters for language modeling in statistical machine translation. The authors propose using a randomised representation of n-gram statistics to complement a conventional smoothed trigram model within an SMT decoder. They introduce the log-frequency Bloom filter, which efficiently associates frequency information with items stored in the filter. The paper also presents a sub-sequence filtering mechanism to reduce error rates. Experiments using publicly available resources show that Bloom filter language models can improve translation performance with little additional memory. The results highlight the trade-off between memory allocation, false positive rate, and translation quality. The paper concludes with a discussion of the potential of the Bloom filter as a practical alternative to conventional associative data structures in computational linguistics.",
                        "This academic paper explores the use of Bloom filters for language modelling in statistical machine translation (SMT). The paper discusses the challenges of using large language models (LMs) in SMT and proposes a novel approach to language modelling using Bloom filters. The authors present the log-frequency Bloom filter, an extension of the standard Bloom filter that efficiently associates frequency information with items stored in the filter. The paper also introduces a sub-sequence filtering mechanism to reduce error rates in these models. The experiments conducted demonstrate the effectiveness of Bloom filters for language modelling, especially when used in conjunction with conventional smoothed trigram models. The results show that Bloom filters can enable the use of higher-order n-grams and larger monolingual corpora for language modelling in SMT. The paper concludes by highlighting the potential of Bloom filters as a practical alternative to conventional associative data structures used in computational linguistics.",
                        "This academic paper explores the use of Bloom filters as a space-efficient method for language modeling in statistical machine translation (SMT). The challenges of deploying large language models (LMs) in SMT, such as the exponential increase in the number of parameters and the need for local memory storage, are addressed through the use of Bloom filters. The paper presents a novel log-frequency Bloom filter scheme that efficiently associates frequency information with items stored in the filter, along with a sub-sequence filtering strategy to reduce error rates. Experimental results demonstrate the effectiveness of Bloom filters for language modeling, particularly in handling higher-order n-grams and larger monolingual corpora. The study also compares the performance of different Bloom filter models and provides insights into the error-space trade-off and the impact of sub-sequence filtering. Overall, the findings suggest that Bloom filters can serve as a practical alternative to conventional associative data structures in computational linguistics.",
                        "This paper presents a new approach to language modeling for statistical machine translation (SMT) using Bloom filters. The authors propose a method for storing n-gram statistics within a Bloom filter, including a novel log-frequency Bloom filter scheme that efficiently associates frequency information with items stored in the filter. They also introduce a sub-sequence filtering technique to reduce error rates. The paper includes a range of experiments to explore the effectiveness and trade-offs of using Bloom filters for language modeling in SMT, with results showing improved translation performance and reduced memory requirements. The authors conclude that Bloom filters can form the basis for space-efficient language modeling in SMT, offering a practical alternative to conventional associative data structures used in computational linguistics.",
                        "This paper explores the use of Bloom filters as a space-efficient representation for language modeling in statistical machine translation (SMT). The authors propose a log-frequency Bloom filter that efficiently encodes frequency information for n-grams, as well as a sub-sequence filtering strategy to reduce error rates. The experiments demonstrate the effectiveness of Bloom filters for SMT language modeling, particularly in incorporating higher-order n-grams and larger monolingual corpora. Results show a clear correlation between memory allocation, false positive rate, and translation performance. The study also provides an analysis of the error rates and memory requirements of the log-frequency Bloom filter compared to a non-redundant scheme. Overall, the paper presents a compelling case for the practical application of Bloom filters in computational linguistics and SMT.",
                        "This academic paper explores the use of Bloom filters for language modeling in statistical machine translation (SMT). The authors propose a novel log-frequency Bloom filter scheme for efficiently representing n-gram statistics, allowing for the use of higher-order n-grams and larger monolingual corpora in SMT. They also present a sub-sequence filtering strategy to reduce error rates, providing empirical evidence of the effectiveness of these models. The experiments demonstrate the trade-off between memory allocation to the Bloom filter models and translation performance, highlighting the potential for improved translation quality with minimal additional memory. The authors compare the log-frequency Bloom filter with the standard Boolean Bloom filter, showing that the former can provide useful information with fine-grained quantization. Overall, the paper aims to establish the Bloom filter as a practical alternative to conventional data structures in computational linguistics.",
                        "This paper explores the use of Bloom filters for language modeling in statistical machine translation (SMT). The authors propose a novel log-frequency Bloom filter scheme that efficiently encodes frequency information for n-gram types drawn from a training corpus. Additionally, they develop a strategy for reducing error rates by sub-sequence filtering. The experiments demonstrate the effectiveness and space-efficiency of the Bloom filter models, showcasing their impact on translation quality and false positive rates. Results indicate that the Bloom filter models can complement conventional n-gram models and allow for the use of higher-order n-grams and larger monolingual corpora in language modeling for SMT. The paper also presents empirical analysis and comparisons with other models, establishing the Bloom filter as a practical alternative for language modeling.",
                        "This academic paper discusses the use of Bloom filters for language modeling in statistical machine translation (SMT). It addresses the challenges of using large language models by proposing a randomised representation using Bloom filters. The paper presents a log-frequency Bloom filter scheme for efficiently storing n-gram statistics and a sub-sequence filtering mechanism for reducing error rates. The experiments conducted explore the effectiveness and trade-offs of Bloom filters for language modeling in SMT, including the impact of different order n-grams and larger corpora on translation quality. The results show a clear correlation between memory allocation, false positive rate, and translation performance. Overall, the paper demonstrates that Bloom filters can provide a space-efficient and effective alternative for language modeling in SMT.",
                        "This paper introduces a novel method of language modeling for statistical machine translation (SMT) using the Bloom filter (BF). Traditional n-gram language models can be memory intensive, especially for large corpora, but the use of BF allows for efficient representation of statistics from larger corpora and higher-order n-grams. The paper discusses the implementation of the log-frequency Bloom filter, which associates frequency information with items stored in the filter, and presents a sub-sequence filtering mechanism for reducing error rates. The authors conducted experiments to evaluate the effectiveness of the BF models in improving translation quality and reducing memory requirements. The results show a clear correlation between memory allocation, false positive rate, and translation performance. The paper concludes with the proposal of the BF as a practical alternative to conventional associative data structures in computational linguistics.",
                        "This paper explores the use of Bloom filters for language modeling in statistical machine translation (SMT) by extending the standard Bloom filter structure to encode corpus frequency information and developing a strategy for reducing error rates through sub-sequence filtering. The authors conducted experiments to investigate the effectiveness and error-space trade-off of Bloom filters for language modeling in SMT. The results show a clear correlation between memory/false positive rate and translation performance, with Bloom filters enabling higher-order n-grams and larger monolingual corpora to be used more easily for language modeling in SMT. The paper concludes that the Bloom filter can be a practical alternative to conventional associative data structures used in computational linguistics, with the potential for entirely replacing standard n-gram models in SMT systems."
                    ],
                    "filename": "randomised_language_modelling_fo-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 131 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Randomised Language Modelling for Statistical Machine Translation</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Language modelling (LM) is a crucial component in statistical machine translation (SMT).</S>\n    <S sid=\"7\" ssid=\"2\">Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models, e.g.</S>\n    <S sid=\"8\" ssid=\"3\">(Chiang, 2005).</S>\n    <S sid=\"9\" ssid=\"4\">Although it is well-known that higher-order LMs and models trained on additional monolingual corpora can yield better translation performance, the challenges in deploying large LMs are not trivial.</S>\n    <S sid=\"10\" ssid=\"5\">Increasing the order of an n-gram model can result in an exponential increase in the number of parameters; for corpora such as the English Gigaword corpus, for instance, there are 300 million distinct trigrams and over 1.2 billion 5-grams.</S>\n    <S sid=\"11\" ssid=\"6\">Since a LM may be queried millions of times per sentence, it should ideally reside locally in memory to avoid time-consuming remote or disk-based look-ups.</S>\n    <S sid=\"12\" ssid=\"7\">Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation.</S>\n    <S sid=\"13\" ssid=\"8\">In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.</S>\n    <S sid=\"14\" ssid=\"9\">1 The space requirements of a Bloom filter are quite spectacular, falling significantly below informationtheoretic error-free lower bounds while query times are constant.</S>\n    <S sid=\"15\" ssid=\"10\">This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.</S>\n    <S sid=\"16\" ssid=\"11\">False negatives, on the other hand, will never occur: the error is said to be one-sided.</S>\n    <S sid=\"17\" ssid=\"12\">In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.</S>\n    <S sid=\"18\" ssid=\"13\">We then propose a mechanism, sub-sequence filtering, for reducing the error rates of these models by using the fact that an n-gram’s frequency is bound from above by the frequency of its least frequent sub-sequence.</S>\n    <S sid=\"19\" ssid=\"14\">We present machine translation experiments using these models to represent information regarding higher-order n-grams and additional larger monolingual corpora in combination with conventional smoothed trigram models.</S>\n    <S sid=\"20\" ssid=\"15\">We also run experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.</S>\n    <S sid=\"21\" ssid=\"16\">Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.</S>\n  </SECTION>\n  <SECTION title=\"2 The Bloom filter\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">In this section, we give a brief overview of the Bloom filter (BF); refer to Broder and Mitzenmacher (2005) for a more in detailed presentation.</S>\n    <S sid=\"23\" ssid=\"2\">A BF represents a set S = {x1, x2, ..., xn} with n elements drawn from a universe U of size N. The structure is attractive when N » n. The only significant storage used by a BF consists of a bit array of size m. This is initially set to hold zeroes.</S>\n    <S sid=\"24\" ssid=\"3\">To train the filter we hash each item in the set k times using distinct hash functions h1, h2, ..., hk.</S>\n    <S sid=\"25\" ssid=\"4\">Each function is assumed to be independent from each other and to map items in the universe to the range 1 to m uniformly at random.</S>\n    <S sid=\"26\" ssid=\"5\">The k bits indexed by the hash values for each item are set to 1; the item is then discarded.</S>\n    <S sid=\"27\" ssid=\"6\">Once a bit has been set to 1 it remains set for the lifetime of the filter.</S>\n    <S sid=\"28\" ssid=\"7\">Distinct items may not be hashed to k distinct locations in the filter; we ignore collisons.</S>\n    <S sid=\"29\" ssid=\"8\">Bits in the filter can, therefore, be shared by distinct items allowing significant space savings but introducing a non-zero probability of false positives at test time.</S>\n    <S sid=\"30\" ssid=\"9\">There is no way of directly retrieving or ennumerating the items stored in a BF.</S>\n    <S sid=\"31\" ssid=\"10\">At test time we wish to discover whether a given item was a member of the original set.</S>\n    <S sid=\"32\" ssid=\"11\">The filter is queried by hashing the test item using the same k hash functions.</S>\n    <S sid=\"33\" ssid=\"12\">If all bits referenced by the k hash values are 1 then we assume that the item was a member; if any of them are 0 then we know it was not.</S>\n    <S sid=\"34\" ssid=\"13\">True members are always correctly identified, but a false positive will occur if all k corresponding bits were set by other items during training and the item was not a member of the training set.</S>\n    <S sid=\"35\" ssid=\"14\">This is known as a one-sided error.</S>\n    <S sid=\"36\" ssid=\"15\">The probability of a false postive, f, is clearly the probability that none of k randomly selected bits in the filter are still 0 after training.</S>\n    <S sid=\"37\" ssid=\"16\">Letting p be the proportion of bits that are still zero after these n elements have been inserted, this gives, As n items have been entered in the filter by hashing each k times, the probability that a bit is still zero is, which is the expected value of p. Hence the false positive rate can be approximated as, f = (1 − p)k : (1 − p�)k : (1 − e−kn m)k .</S>\n    <S sid=\"38\" ssid=\"17\">By taking the derivative we find that the number of functions k* that minimizes f is, m k* = ln 2 · . n which leads to the intuitive result that exactly half the bits in the filter will be set to 1 when the optimal number of hash functions is chosen.</S>\n    <S sid=\"39\" ssid=\"18\">The fundmental difference between a Bloom filter’s space requirements and that of any lossless representation of a set is that the former does not depend on the size of the (exponential) universe N from which the set is drawn.</S>\n    <S sid=\"40\" ssid=\"19\">A lossless representation scheme (for example, a hash map, trie etc.) must depend on N since it assigns a distinct representation to each possible set drawn from the universe.</S>\n  </SECTION>\n  <SECTION title=\"3 Language modelling with Bloom filters\" number=\"3\">\n    <S sid=\"41\" ssid=\"1\">In our experiments we make use of both standard (i.e.</S>\n    <S sid=\"42\" ssid=\"2\">Boolean) BFs containing n-gram types drawn from a training corpus and a novel BF scheme, the log-frequency Bloom filter, that allows frequency information to be associated efficiently with items stored in the filter. return BF The efficiency of our scheme for storing n-gram statistics within a BF relies on the Zipf-like distribution of n-gram frequencies in natural language corpora: most events occur an extremely small number of times, while a small number are very frequent.</S>\n    <S sid=\"43\" ssid=\"3\">We quantise raw frequencies, c(x), using a logarithmic codebook as follows, The precision of this codebook decays exponentially with the raw counts and the scale is determined by the base of the logarithm b; we examine the effect of this parameter in experiments below.</S>\n    <S sid=\"44\" ssid=\"4\">Given the quantised count qc(x) for an n-gram x, the filter is trained by entering composite events consisting of the n-gram appended by an integer counter j that is incremented from 1 to qc(x) into the filter.</S>\n    <S sid=\"45\" ssid=\"5\">To retrieve the quantised count for an n-gram, it is first appended with a count of 1 and hashed under the k functions; if this tests positive, the count is incremented and the process repeated.</S>\n    <S sid=\"46\" ssid=\"6\">The procedure terminates as soon as any of the k hash functions hits a 0 and the previous count is reported.</S>\n    <S sid=\"47\" ssid=\"7\">The one-sided error of the BF and the training scheme ensure that the actual quantised count cannot be larger than this value.</S>\n    <S sid=\"48\" ssid=\"8\">As the counts are quantised logarithmically, the counter will be incremented only a small number of times.</S>\n    <S sid=\"49\" ssid=\"9\">The training and testing routines are given here as Algorithms 1 and 2 respectively.</S>\n    <S sid=\"50\" ssid=\"10\">Errors for the log-frequency BF scheme are onesided: frequencies will never be underestimated.</S>\n    <S sid=\"51\" ssid=\"11\">Algorithm 2 Test frequency BF Input: x, MAXQCOUNT, {h1, ...hk} and BF Output: Upper bound on qc(x) ∈ Strain for j = 1 to MAXQCOUNT do for i = 1 to k do The probability of overestimating an item’s frequency decays exponentially with the size of the overestimation error d (i.e. as fd for d &gt; 0) since each erroneous increment corresponds to a single false positive and d such independent events must occur together.</S>\n    <S sid=\"52\" ssid=\"12\">The error analysis in Section 2 focused on the false positive rate of a BF; if we deploy a BF within an SMT decoder, however, the actual error rate will also depend on the a priori membership probability of items presented to it.</S>\n    <S sid=\"53\" ssid=\"13\">The error rate Err is, This implies that, unlike a conventional lossless data structure, the model’s accuracy depends on other components in system and how it is queried.</S>\n    <S sid=\"54\" ssid=\"14\">We take advantage of the monotonicity of the ngram event space to place upper bounds on the frequency of an n-gram prior to testing for it in the filter and potentially truncate the outer loop in Algorithm 2 when we know that the test could only return postive in error.</S>\n    <S sid=\"55\" ssid=\"15\">Specifically, if we have stored lower-order ngrams in the filter, we can infer that an n-gram cannot present, if any of its sub-sequences test negative.</S>\n    <S sid=\"56\" ssid=\"16\">Since our scheme for storing frequencies can never underestimate an item’s frequency, this relation will generalise to frequencies: an n-gram’s frequency cannot be greater than the frequency of its least frequent sub-sequence as reported by the filter, We use this to reduce the effective error rate of BFLMs that we use in the experiments below.</S>\n    <S sid=\"57\" ssid=\"17\">A standard BF can implement a Boolean ‘language model’ test: have we seen some fragment of language before?</S>\n    <S sid=\"58\" ssid=\"18\">This does not use any frequency information.</S>\n    <S sid=\"59\" ssid=\"19\">The Boolean BF-LM is a standard BF containing all n-grams of a certain length in the training corpus, Strain.</S>\n    <S sid=\"60\" ssid=\"20\">It implements the following binary feature function in a log-linear decoder, Separate Boolean BF-LMs can be included for different order n and assigned distinct log-linear weights that are learned as part of a minimum error rate training procedure (see Section 4).</S>\n    <S sid=\"61\" ssid=\"21\">The log-frequency BF-LM implements a multinomial feature function in the decoder that returns the value associated with an n-gram by Algorithm 2.</S>\n    <S sid=\"62\" ssid=\"22\">Sub-sequence filtering can be performed by using the minimum value returned by lower-order models as an upper-bound on the higher-order models.</S>\n    <S sid=\"63\" ssid=\"23\">By boosting the score of hypotheses containing ngrams observed in the training corpus while remaining agnostic for unseen n-grams (with the exception of errors), these feature functions have more in common with maximum entropy models than conventionally smoothed n-gram models.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiments\" number=\"4\">\n    <S sid=\"64\" ssid=\"1\">We conducted a range of experiments to explore the effectiveness and the error-space trade-off of Bloom filters for language modelling in SMT.</S>\n    <S sid=\"65\" ssid=\"2\">The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.</S>\n    <S sid=\"66\" ssid=\"3\">While our main experiments use the Bloom filter models in conjunction with a conventional smoothed trigram model, we also present experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.</S>\n    <S sid=\"67\" ssid=\"4\">Finally, we present some empirical analysis of both the logfrequency Bloom filter and the sub-sequence filtering technique which may be of independent interest.</S>\n    <S sid=\"68\" ssid=\"5\">All of our experiments use publically available resources.</S>\n    <S sid=\"69\" ssid=\"6\">We use the French-English section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003) and the English Gigaword Corpus (LDC2003T05; GW) for additional language modelling.</S>\n    <S sid=\"70\" ssid=\"7\">Decoding is carried-out using the Moses decoder (Koehn and Hoang, 2007).</S>\n    <S sid=\"71\" ssid=\"8\">We hold out 500 test sentences and 250 development sentences from the parallel text for evaluation purposes.</S>\n    <S sid=\"72\" ssid=\"9\">The feature functions in our models are optimised using minimum error rate training and evaluation is performed using the BLEU score.</S>\n    <S sid=\"73\" ssid=\"10\">Our baseline LM and other comparison models are conventional n-gram models smoothed using modified Kneser-Ney and built using the SRILM Toolkit (Stolcke, 2002); as is standard practice these models drop entries for n-grams of size 3 and above when the corresponding discounted count is less than 1.</S>\n    <S sid=\"74\" ssid=\"11\">The baseline language model, EP-KN-3, is a trigram model trained on the English portion of the parallel corpus.</S>\n    <S sid=\"75\" ssid=\"12\">For additional comparisons we also trained a smoothed 4-gram model on this Europarl data (EPKN-4) and a trigram model on the Agence France Press section of the Gigaword Corpus (AFP-KN-3).</S>\n    <S sid=\"76\" ssid=\"13\">Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.</S>\n    <S sid=\"77\" ssid=\"14\">We give the gzip compressed size as an optimistic lower bound on the size of any lossless representation of each model.2 2Note, in particular, that gzip compressed files do not support direct random access as required by our application.</S>\n    <S sid=\"78\" ssid=\"15\">To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW).</S>\n    <S sid=\"79\" ssid=\"16\">Table 2 shows the numbers of distinct n-grams in these corpora.</S>\n    <S sid=\"80\" ssid=\"17\">Note that we use no pruning for these models and that the numbers of distinct n-grams is of the same order as that of the recently released Google Ngrams dataset (LDC2006T13).</S>\n    <S sid=\"81\" ssid=\"18\">In our experiments we create a range of models referred to by the corpus used (EP or GW), the order of the n-gram(s) entered into the filter (1 to 10), whether the model is Boolean (Bool-BF) or provides frequency information (FreqBF), whether or not sub-sequence filtering was used (FTR) and whether it was used in conjunction with the baseline trigram (+EP-KN-3).</S>\n    <S sid=\"82\" ssid=\"19\">Our first set of experiments examines the relationship between memory allocated to the BF and BLEU score.</S>\n    <S sid=\"83\" ssid=\"20\">We present results using the Boolean BFLM in isolation and then both the Boolean and logfrequency BF-LMS to add 4-grams to our baseline 3-gram model.Our second set of experiments adds 3-grams and 5-grams from the Gigaword Corpus to our baseline.</S>\n    <S sid=\"84\" ssid=\"21\">Here we constrast the Boolean BFLM with the log-frequency BF-LM with different quantisation bases (2 = fine-grained and 5 = coarsegrained).</S>\n    <S sid=\"85\" ssid=\"22\">We then evaluate the sub-sequence filtering approach to reducing the actual error rate of these models by adding both 3 and 4-grams from the Gigaword Corpus to the baseline.</S>\n    <S sid=\"86\" ssid=\"23\">Since the BF-LMs easily allow us to deploy very high-order n-gram models, we use them to evaluate the impact of different order n-grams on the translation process presenting results using the Boolean and log-frequency BF-LM in isolation for n-grams of order 1 to 10.</S>\n    <S sid=\"87\" ssid=\"24\">We analyse our log-frequency BF scheme in terms of the additional memory it requires and the error rate compared to a non-redundant scheme.</S>\n    <S sid=\"88\" ssid=\"25\">The nonredundant scheme involves entering just the exact quantised count for each n-gram and then searching over the range of possible counts at test time starting with the count with maximum a priori probability (i.e.</S>\n    <S sid=\"89\" ssid=\"26\">1) and incrementing until a count is found or the whole codebook has been searched (here the size is 16).</S>\n    <S sid=\"90\" ssid=\"27\">We also analyse the sub-sequence filtering scheme directly by creating a BF with only 3-grams and a BF containing both 2-grams and 3-grams and comparing their actual error rates when presented with 3-grams that are all known to be negatives.</S>\n  </SECTION>\n  <SECTION title=\"5 Results\" number=\"5\">\n    <S sid=\"91\" ssid=\"1\">Table 3 shows the results of the baseline (EP-KN3) and other conventional n-gram models trained on larger corpora (AFP-KN-3) and using higher-order dependencies (EP-KN-4).</S>\n    <S sid=\"92\" ssid=\"2\">The larger models improve somewhat on the baseline performance.</S>\n    <S sid=\"93\" ssid=\"3\">Figure 1 shows the relationship between space allocated to the BF models and BLEU score (left) and false positive rate (right) respectively.</S>\n    <S sid=\"94\" ssid=\"4\">These experiments do not include the baseline model.</S>\n    <S sid=\"95\" ssid=\"5\">We can see a clear correlation between memory / false positive rate and translation performance.</S>\n    <S sid=\"96\" ssid=\"6\">Adding 4-grams in the form of a Boolean BF or a log-frequency BF (see Figure 2) improves on the 3gram baseline with little additional memory (around 4MBs) while performing on a par with or above the Europarl 4-gram model with around 10MBs; this suggests that a lossy representation of the unpruned set of 4-grams contains more useful information than a lossless representation of the pruned set.3 As the false positive rate exceeds 0.20 the performance is severly degraded.</S>\n    <S sid=\"97\" ssid=\"7\">Adding 3-grams drawn from the whole of the Gigaword corpus rather than simply the Agence France Press section results in slightly improved performance with signficantly less memory than the AFP-KN-3 model (see Figure 3).</S>\n    <S sid=\"98\" ssid=\"8\">Figure 4 shows the results of adding 5-grams drawn from the Gigaword corpus to the baseline.</S>\n    <S sid=\"99\" ssid=\"9\">It also contrasts the Boolean BF and the log-frequency BF suggesting in this case that the log-frequency BF can provide useful information when the quantisation base is relatively fine-grained (base 2).</S>\n    <S sid=\"100\" ssid=\"10\">The Boolean BF and the base 5 (coarse-grained quantisation) log-frequency BF perform approximately the same.</S>\n    <S sid=\"101\" ssid=\"11\">The base 2 quantisation performs worse for smaller amounts of memory, possibly due to the larger set of events it is required to store.</S>\n    <S sid=\"102\" ssid=\"12\">Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).</S>\n    <S sid=\"103\" ssid=\"13\">We believe this to be the result of an increased a priori membership probability for n-grams presented to the filter under the sub-sequence filtering scheme.</S>\n    <S sid=\"104\" ssid=\"14\">Figure 6 shows that for this task the most useful n-gram sizes are between 3 and 6.</S>\n    <S sid=\"105\" ssid=\"15\">Figure 8 compares the memory requirements of the log-frequencey BF (base 2) and the Boolean BF for various order n-gram sets from the Gigaword Corpus with the same underlying false positive rate (0.125).</S>\n    <S sid=\"106\" ssid=\"16\">The additional space required by our scheme for storing frequency information is less than a factor of 2 compared to the standard BF.</S>\n    <S sid=\"107\" ssid=\"17\">Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.</S>\n    <S sid=\"108\" ssid=\"18\">We presented 500K negatives to the filter and recorded the frequency of overestimation errors of each size.</S>\n    <S sid=\"109\" ssid=\"19\">As shown in Section 3.1, the probability of overestimating an item’s frequency under the log-frequency BF scheme decays exponentially in the size of this overestimation error.</S>\n    <S sid=\"110\" ssid=\"20\">Although the non-redundant scheme requires fewer items be stored in the filter and, therefore, has a lower underlying false positive rate (0.076 versus 0.</S>\n    <S sid=\"111\" ssid=\"21\">159), in practice it incurs a much higher error rate (0.717) with many large errors.</S>\n    <S sid=\"112\" ssid=\"22\">Figure 9 shows the impact of sub-sequence filtering on the actual error rate.</S>\n    <S sid=\"113\" ssid=\"23\">Although, the false positive rate for the BF containing 2-grams, in addition, to 3-grams (filtered) is higher than the false positive rate of the unfiltered BF containing only 3-grams, the actual error rate of the former is lower for models with less memory.</S>\n    <S sid=\"114\" ssid=\"24\">By testing for 2-grams prior to querying for the 3-grams, we can avoid performing some queries that may otherwise have incurred errors using the fact that a 3-gram cannot be present if one of its constituent 2-grams is absent.</S>\n  </SECTION>\n  <SECTION title=\"6 Related Work\" number=\"6\">\n    <S sid=\"115\" ssid=\"1\">We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.</S>\n    <S sid=\"116\" ssid=\"2\">Deploying such LMs requires either a cluster of machines (and the overheads of remote procedure calls), per-sentence filtering (which again, is slow) and/or the use of some other lossy compression (Goodman and Gao, 2000).</S>\n    <S sid=\"117\" ssid=\"3\">Our approach can complement all these techniques.</S>\n    <S sid=\"118\" ssid=\"4\">Bloom filters have been widely used in database applications for reducing communications overheads and were recently applied to encode word frequencies in information retrieval (Linari and Weikum, 2006) using a method that resembles the non-redundant scheme described above.</S>\n    <S sid=\"119\" ssid=\"5\">Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusions\" number=\"7\">\n    <S sid=\"120\" ssid=\"1\">We have shown that Bloom Filters can form the basis for space-efficient language modelling in SMT.</S>\n    <S sid=\"121\" ssid=\"2\">Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.</S>\n    <S sid=\"122\" ssid=\"3\">In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.</S>\n    <S sid=\"123\" ssid=\"4\">We hope the present work will help establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.</S>\n    <S sid=\"124\" ssid=\"5\">The framework presented here shows that with some consideration for its workings, the randomised nature of the Bloom filter need not be a significant impediment to is use in applications.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "911dc0c46b59e07081546f0fe3bcdde1",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Randomised Language Modelling for Statistical Machine Translation"
                }
            }
        },
        {
            "index": 2,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Most work in statistical parsing has focused on a single corpus: the Wall Street Journal portion of the Penn Treebank. While this has allowed for quantitative comparison of parsing techniques, it has left open the question of how other types of text might affect parser performance, and how portable parsing models are across corpora. We examine these questions by comparing results for the Brown and WSJ corpora, and also consider which parts of the parser's probability model are particularly tuned to the corpus on which it was trained. This leads us to a technique for pruning parameters to reduce the size of the parsing model.",
                    "filename": "Corpus_Variation_And_Parser_Performance",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W01-0521/Documents_xml/W01-0521.xml",
                    "title": "Corpus Variation And Parser Performance"
                },
                "llm": {
                    "descriptions": [
                        "This paper investigates the impact of corpus variation on parser performance. The study examines the differences in parsing results on the Wall Street Journal and the Brown corpus, as well as the effect of lexical dependencies. The findings suggest that a small amount of matched training data is more useful than a large amount of unmatched data. Furthermore, lexical bigram statistics appear to be corpus-specific and of little use when attempting to generalize to new training data. The paper also proposes a selective pruning technique for reducing the size of parsing models, which could be particularly useful for cases where memory usage is a consideration.",
                        "This academic paper examines the impact of corpus variation on the performance of statistical parsers. The study compares parser performance on the Wall Street Journal corpus and the Brown corpus. The results show that a small amount of matched training data is more useful than a large amount of unmatched data, and that lexical bigram statistics appear to be corpus-specific. Surprisingly, removing lexical bigrams from the model had little impact on performance. The study also explores a selective pruning technique to reduce the size of the parsing model without sacrificing performance. The findings have implications for parser memory usage and suggest that more complex models may need further tuning to match the peculiarities of specific corpora.",
                        "This paper examines the impact of corpus variation on parser performance in natural language processing. The study investigates the differences between corpora, as well as the effectiveness of pruning parser parameters and eliminating lexical dependencies. Results show that training data matched to the test material is more useful than unmatched data, with little benefit from lexical bigram statistics. Additionally, selective pruning techniques offer a way to reduce model size without sacrificing performance, making it applicable to scenarios with limited computing resources. Overall, the findings highlight the importance of corpus-specific considerations in statistical parsers and potential future work in exploring the application of lexical co-occurrences in parser models.",
                        "This paper investigates the impact of corpus variation on the performance of statistical natural language parsers. The authors conducted experiments using the Wall Street Journal (WSJ) and Brown corpora to analyze parser performance across different data sets. They found that lexical bigram statistics are corpus-specific and have little benefit when attempting to generalize to new training data. Additionally, the study explores a selective pruning technique to reduce the size of the parsing model without sacrificing performance. The results indicate that matched training data is more useful than unmatched data, and that a lightweight parser can be achieved without impacting performance. This research has implications for applications with limited computing resources."
                    ],
                    "filename": "corpus_variation_and_parser_perf-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 105 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Corpus Variation And Parser Performance</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"5\" ssid=\"1\">The past several years have seen great progress in the field of natural language parsing, through the use of statistical methods trained using large corpora of hand-parsed training data.</S>\n    <S sid=\"6\" ssid=\"2\">The techniques of Charniak (1997), Collins (1997), and Ratnaparkhi (1997) achieved roughly comparable results using the same sets of training and test data.</S>\n    <S sid=\"7\" ssid=\"3\">In each case, the corpus used was the Penn Treebank's hand-annotated parses of Wall Street Journal articles.</S>\n    <S sid=\"8\" ssid=\"4\">Relatively few quantitative parsing results have been reported on other corpora (though see Stolcke et al. (1996) for results on Switchboard, as well as Collins et al.</S>\n    <S sid=\"9\" ssid=\"5\">(1999) for results on Czech and Hwa (1999) for bootstrapping from WSJ to ATIS).</S>\n    <S sid=\"10\" ssid=\"6\">The inclusion of parses for the Brown corpus in the Penn Treebank allows us to compare parser performance across corpora.</S>\n    <S sid=\"11\" ssid=\"7\">In this paper we examine the following questions: Our investigation of these questions leads us to a surprising result about parsing the WSJ corpus: over a third of the model's parameters can be eliminated with little impact on performance.</S>\n    <S sid=\"12\" ssid=\"8\">Aside from cross-corpus considerations, this is an important finding if a lightweight parser is desired or memory usage is a consideration.</S>\n  </SECTION>\n  <SECTION title=\"2 Previous Comparisons of Corpora\" number=\"2\">\n    <S sid=\"13\" ssid=\"1\">A great deal of work has been done outside of the parsing community analyzing the variations between corpora and different genres of text.</S>\n    <S sid=\"14\" ssid=\"2\">Biber (1993) investigated variation in a number syntactic features over genres, or registers, of language.</S>\n    <S sid=\"15\" ssid=\"3\">Of particular importance to statistical parsers is the investigation of frequencies for verb subcategorizations such as Roland and Jurafsky (1998).</S>\n    <S sid=\"16\" ssid=\"4\">Roland et al. (2000) find that subcategorization frequencies for certain verbs vary significantly between the Wall Street Journal corpus and the mixed-genre Brown corpus, but that they vary less so between genre-balanced British and American corpora.</S>\n    <S sid=\"17\" ssid=\"5\">Argument structure is essentially the task that automatic parsers attempt to solve, and the frequencies of various structures in training data are reflected in a statistical parser's probability model.</S>\n    <S sid=\"18\" ssid=\"6\">The variation in verb argument structure found by previous research caused us to wonder to what extent a model trained on one corpus would be useful in parsing another.</S>\n    <S sid=\"19\" ssid=\"7\">The probability models of modern parsers include not only the number and syntactic type of a word's arguments, but lexical information about their fillers.</S>\n    <S sid=\"20\" ssid=\"8\">Although we are not aware of previous comparisons of the frequencies of argument fillers, we can only assume that they vary at least as much as the syntactic subcategorization frames.</S>\n  </SECTION>\n  <SECTION title=\"3 The Parsing Model\" number=\"3\">\n    <S sid=\"21\" ssid=\"1\">We take as our baseline parser the statistical model of Model 1 of Collins (1997).</S>\n    <S sid=\"22\" ssid=\"2\">The model is a historybased, generative model, in which the probability for a parse tree is found by expanding each node in the tree in turn into its child nodes, and multiplying the probabilities for each action in the derivation.</S>\n    <S sid=\"23\" ssid=\"3\">It can be thought of as a variety of lexicalized probabilistic context-free grammar, with the rule probabilities factored into three distributions.</S>\n    <S sid=\"24\" ssid=\"4\">The first distribution gives probability of the syntactic category H of the head child of a parent node with category P, head word Hhw with the head tag (the part of speech tag of the head word) Hht: The head word and head tag of the new node H are defined to be the same as those of its parent.</S>\n    <S sid=\"25\" ssid=\"5\">The remaining two distributions generate the non-head children one after the other.</S>\n    <S sid=\"26\" ssid=\"6\">A special #STOP# symbol is generated to terminate the sequence of children for a given parent.</S>\n    <S sid=\"27\" ssid=\"7\">Each child is generated in two steps: first its syntactic category C and head tag Cht are chosen given the parent's and head child's features and a function A representing the distance from the head child: Then the new child's head word Chw is chosen: For each of the three distributions, the empirical distribution of the training data is interpolated with less specific backoff distributions, as we will see in Section 5.</S>\n    <S sid=\"28\" ssid=\"8\">Further details of the model, including the distance features used and special handling of punctuation, conjunctions, and base noun phrases, are described in Collins (1999).</S>\n    <S sid=\"29\" ssid=\"9\">The fundamental features of used in the probability distributions are the lexical heads and head tags of each constituent, the co-occurrences of parent nodes and their head children, and the cooccurrences of child nodes with their head siblings and parents.</S>\n    <S sid=\"30\" ssid=\"10\">The probability models of Charniak (1997), Magerman (1995) and Ratnaparkhi (1997) differ in their details but are based on similar features.</S>\n    <S sid=\"31\" ssid=\"11\">Models 2 and 3 of Collins (1997) add some slightly more elaborate features to the probability model, as do the additions of Charniak (2000) to the model of Charniak (1997).</S>\n    <S sid=\"32\" ssid=\"12\">Our implementation of Collins' Model 1 performs at 86% precision and recall of labeled parse constituents on the standard Wall Street Journal training and test sets.</S>\n    <S sid=\"33\" ssid=\"13\">While this does not reflect the state-of-the-art performance on the WSJ task achieved by the more the complex models of Charniak (2000) and Collins (2000), we regard it as a reasonable baseline for the investigation of corpus effects on statistical parsing.</S>\n  </SECTION>\n  <SECTION title=\"4 Parsing Results on the Brown Corpus\" number=\"4\">\n    <S sid=\"34\" ssid=\"1\">We conducted separate experiments using WSJ data, Brown data, and a combination of the two as training material.</S>\n    <S sid=\"35\" ssid=\"2\">For the WSJ data, we observed the standard division into training (sections 2 through 21 of the treebank) and test (section 23) sets.</S>\n    <S sid=\"36\" ssid=\"3\">For the Brown data, we reserved every tenth sentence in the corpus as test data, using the other nine for training.</S>\n    <S sid=\"37\" ssid=\"4\">This may underestimate the difficulty of the Brown corpus by including sentences from the same documents in training and test sets.</S>\n    <S sid=\"38\" ssid=\"5\">However, because of the variation within the Brown corpus, we felt that a single contiguous test section might not be representative.</S>\n    <S sid=\"39\" ssid=\"6\">Only the subset of the Brown corpus available in the Treebank II bracketing format was used.</S>\n    <S sid=\"40\" ssid=\"7\">This subset consists primarily of various fiction genres.</S>\n    <S sid=\"41\" ssid=\"8\">Corpus sizes are shown in Results for the Brown corpus, along with WSJ results for comparison, are shown in Table 2.</S>\n    <S sid=\"42\" ssid=\"9\">The basic mismatch between the two corpora is shown in the significantly lower performance of the WSJtrained model on Brown data than on WSJ data (rows 1 and 2).</S>\n    <S sid=\"43\" ssid=\"10\">A model trained on Brown data only does significantly better, despite the smaller size of the training set.</S>\n    <S sid=\"44\" ssid=\"11\">Combining the WSJ and Brown training data in one model improves performance further, but by less than 0.5% absolute.</S>\n    <S sid=\"45\" ssid=\"12\">Similarly, adding the Brown data to the WSJ model increased performance on WSJ by less than 0.5%.</S>\n    <S sid=\"46\" ssid=\"13\">Thus, even a large amount of additional data seems to have relatively little impact if it is not matched to the test material.</S>\n    <S sid=\"47\" ssid=\"14\">The more varied nature of the Brown corpus also seems to impact results, as all the results on Brown are lower than the WSJ result.</S>\n  </SECTION>\n  <SECTION title=\"5 The Effect of Lexical Dependencies\" number=\"5\">\n    <S sid=\"48\" ssid=\"1\">The parsers cited above all use some variety of lexical dependency feature to capture statistics on the cooccurrence of pairs of words being found in parentchild relations within the parse tree.</S>\n    <S sid=\"49\" ssid=\"2\">These word pair relations, also called lexical bigrams (Collins, 1996), are reminiscent of dependency grammars such as Melcuk (1988) and the link grammar of Sleator and Temperley (1993).</S>\n    <S sid=\"50\" ssid=\"3\">In Collins' Model 1, the word pair statistics occur in the distribution where Hhw represent the head word of a parent node in the tree and Chw the head word of its (non-head) child.</S>\n    <S sid=\"51\" ssid=\"4\">(The head word of a parent is the same as the head word of its head child.)</S>\n    <S sid=\"52\" ssid=\"5\">Because this is the only part of the model that involves pairs of words, it is also where the bulk of the parameters are found.</S>\n    <S sid=\"53\" ssid=\"6\">The large number of possible pairs of words in the vocabulary make the training data necessarily sparse.</S>\n    <S sid=\"54\" ssid=\"7\">In order to avoid assigning zero probability to unseen events, it is necessary to smooth the training data.</S>\n    <S sid=\"55\" ssid=\"8\">The Collins model uses linear interpolation to estimate probabilities from empirical distributions of varying specificities: where P� represents the empirical distribution derived directly from the counts in the training data.</S>\n    <S sid=\"56\" ssid=\"9\">The interpolation weights A1, A2 are chosen as a function of the number of examples seen for the conditioning events and the number of unique values seen for the predicted variable.</S>\n    <S sid=\"57\" ssid=\"10\">Only the first distribution in this interpolation scheme involves pairs of words, and the third component is simply the probability of a word given its part of speech.</S>\n    <S sid=\"58\" ssid=\"11\">Because the word pair feature is the most specific in the model, it is likely to be the most corpusspecific.</S>\n    <S sid=\"59\" ssid=\"12\">The vocabularies used in corpora vary, as do the word frequencies.</S>\n    <S sid=\"60\" ssid=\"13\">It is reasonable to expect word co-occurrences to vary as well.</S>\n    <S sid=\"61\" ssid=\"14\">In order to test this hypothesis, we removed the distribution �P(ChwlP, H, Hht, Hhw, C, Cht) from the parsing model entirely, relying on the interpolation of the two less specific distributions in the parser: We performed cross-corpus experiments as before to determine whether the simpler parsing model might be more robust to corpus effects.</S>\n    <S sid=\"62\" ssid=\"15\">Results are shown in Table 3.</S>\n    <S sid=\"63\" ssid=\"16\">Perhaps the most striking result is just how little the elimination of lexical bigrams affects the baseline system: performance on the WSJ corpus decreases by less than 0.5% absolute.</S>\n    <S sid=\"64\" ssid=\"17\">Moreover, the performance of a WSJ-trained system without lexical bigrams on Brown test data is identical to the WSJtrained system with lexical bigrams.</S>\n    <S sid=\"65\" ssid=\"18\">Lexical cooccurrence statistics seem to be of no benefit when attempting to generalize to a new corpus.</S>\n  </SECTION>\n  <SECTION title=\"6 Pruning Parser Parameters\" number=\"6\">\n    <S sid=\"66\" ssid=\"1\">The relatively high performance of a parsing model with no lexical bigram statistics on the WSJ task led us to explore whether it might be possible to significantly reduce the size of the parsing model by selectively removing parameters without sacrificing performance.</S>\n    <S sid=\"67\" ssid=\"2\">Such a technique reduces the parser's memory requirements as well as the overhead of loading and storing the model, which could be desirable for an application where limited computing resources are available.</S>\n    <S sid=\"68\" ssid=\"3\">Significant effort has gone into developing techniques for pruning statistical language models for speech recognition, and we borrow from this work, using the weighted difference technique of Seymore and Rosenfeld (1996).</S>\n    <S sid=\"69\" ssid=\"4\">This technique applies to any statistical model which estimates probabilities by backing off, that is, using probabilities from a less specific distribution when no data are available are available for the full distribution, as the following equations show for the general case: Here e is the event to be predicted, h is the set of conditioning events or history, a is a backoff weight, and h' is the subset of conditioning events used for the less specific backoff distribution.</S>\n    <S sid=\"70\" ssid=\"5\">BO is the backoff set of events for which no data are present in the specific distribution P1.</S>\n    <S sid=\"71\" ssid=\"6\">In the case of n-gram language modeling, e is the next word to be predicted, and the conditioning events are the n — 1 preceding words.</S>\n    <S sid=\"72\" ssid=\"7\">In our case the specific distribution P1 of the backoff model is Pcw of equation 1, itself a linear interpolation of three empirical distributions from the training data.</S>\n    <S sid=\"73\" ssid=\"8\">The less specific distribution P2 of the backoff model is Pcw2 of equation 2, an interpolation of two empirical distributions.</S>\n    <S sid=\"74\" ssid=\"9\">The backoff weight a is simply 1 — A1 in our linear interpolation model.</S>\n    <S sid=\"75\" ssid=\"10\">The Seymore/Rosenfeld pruning technique can be used to prune backoff probability models regardless of whether the backoff weights are derived from linear interpolation weights or discounting techniques such as Good-Turing.</S>\n    <S sid=\"76\" ssid=\"11\">In order to ensure that the model's probabilities still sum to one, the backoff weight a must be adjusted whenever a parameter is removed from the model.</S>\n    <S sid=\"77\" ssid=\"12\">In the Seymore/Rosenfeld approach, parameters are pruned according to the following criterion: where p'(elh') represents the new backed off probability estimate after removing p(eIh) from the model and adjusting the backoff weight, and N(e, h) is the count in the training data.</S>\n    <S sid=\"78\" ssid=\"13\">This criterion aims to prune probabilities that are similar to their backoff estimates, and that are not frequently used.</S>\n    <S sid=\"79\" ssid=\"14\">As shown by Stolcke (1998), this criterion is an approximation of the relative entropy between the original and pruned distributions, but does not take into account the effect of changing the backoff weight on other events' probabilities.</S>\n    <S sid=\"80\" ssid=\"15\">Adjusting the threshold 0 below which parameters are pruned allows us to successively remove more and more parameters.</S>\n    <S sid=\"81\" ssid=\"16\">Results for different values of 0 are shown in Table 4.</S>\n    <S sid=\"82\" ssid=\"17\">The complete parsing model derived from the WSJ training set has 735,850 parameters in a total of nine distributions: three levels of backoff for each of the three distributions Ph, P, and P,,,,.</S>\n    <S sid=\"83\" ssid=\"18\">The lexical bigrams are contained in the most specific distribution for P,,,,.</S>\n    <S sid=\"84\" ssid=\"19\">Removing all these parameters reduces the total model size by 43%.</S>\n    <S sid=\"85\" ssid=\"20\">The results show a gradual degradation as more parameters are pruned.</S>\n    <S sid=\"86\" ssid=\"21\">The ten lexical bigrams with the highest scores for the pruning metric are shown in Table 5 for WSJ and Table 6.</S>\n    <S sid=\"87\" ssid=\"22\">The pruning metric of equation 3 has been normalized by corpus size to allow comparison between WSJ and Brown.</S>\n    <S sid=\"88\" ssid=\"23\">The only overlap between the two sets is for pairs of unknown word tokens.</S>\n    <S sid=\"89\" ssid=\"24\">The WSJ bigrams are almost all specific to finance, are all word pairs that are likely to appear immediately adjacent to one another, and are all children of the base NP syntactic category.</S>\n    <S sid=\"90\" ssid=\"25\">The Brown bigrams, which have lower correlation values by our metric, include verb/subject and preposition/object relations and seem more broadly applicable as a model of English.</S>\n    <S sid=\"91\" ssid=\"26\">However, the pairs are not strongly related semantically, no doubt because the first term of the pruning criterion favors the most frequent words, such as forms of the verbs &amp;quot;be&amp;quot; and &amp;quot;have&amp;quot;.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"7\">\n    <S sid=\"92\" ssid=\"1\">Our results show strong corpus effects for statistical parsing models: a small amount of matched training data appears to be more useful than a large amount of unmatched data.</S>\n    <S sid=\"93\" ssid=\"2\">The standard WSJ task seems to be simplified by its homogenous style.</S>\n    <S sid=\"94\" ssid=\"3\">Adding training data from from an unmatched corpus doesn't hurt, but doesn't help a great deal either.</S>\n    <S sid=\"95\" ssid=\"4\">In particular, lexical bigram statistics appear to be corpus-specific, and our results show that they are of no use when attempting to generalize to new training data.</S>\n    <S sid=\"96\" ssid=\"5\">In fact, they are of surprisingly little benefit even for matched training and test data removing them from the model entirely reduces performance by less than 0.5% on the standard WSJ parsing task.</S>\n    <S sid=\"97\" ssid=\"6\">Our selective pruning technique allows for a more fine grained tuning of parser model size, and would be particularly applicable to cases where large amounts of training data are available but memory usage is a consideration.</S>\n    <S sid=\"98\" ssid=\"7\">In our implementation, pruning allowed models to run within 256MB that, unpruned, required larger machines.</S>\n    <S sid=\"99\" ssid=\"8\">The parsing models of Charniak (2000) and Collins (2000) add more complex features to the parsing model that we use as our baseline.</S>\n    <S sid=\"100\" ssid=\"9\">An area for future work is investigation of the degree to which such features apply across corpora, or, on the other hand, further tune the parser to the peculiarities of the Wall Street Journal.</S>\n    <S sid=\"101\" ssid=\"10\">Of particular interest are the automatic clusterings of lexical co-occurrences used in Charniak (1997) and Magerman (1995).</S>\n    <S sid=\"102\" ssid=\"11\">Cross-corpus experiments could reveal whether these clusters uncover generally applicable semantic categories for the parser's use.</S>\n    <S sid=\"103\" ssid=\"12\">Acknowledgments This work was undertaken as part of the FrameNet project at ICSI, with funding from National Science Foundation grant ITR/HCI #0086132.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "6d11708c7ba7fd4d6fd7ef377386961b",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Corpus Variation And Parser Performance"
                }
            }
        },
        {
            "index": 3,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged &quot;best&quot; by some probabilistic figure of merit (FOM). Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM. This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finergrained control over parsing effort. We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG. The results obtained are about a facof twenty improvement over the best results — that is, our parser achieves equivalent results using one twentieth the number of edges. Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing.",
                    "filename": "Edge-Based_Best-First_Chart_Parsing",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W98-1115/Documents_xml/W98-1115.xml",
                    "title": "Edge-Based Best-First Chart Parsing"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a method for edge-based best-first chart parsing to find parses for sentences according to a context-free grammar. The paper compares their approach to previous work and demonstrates that their method significantly reduces the number of edges required to find a parse. They use a tree-bank grammar induced from the Penn Wall Street Journal text and apply a binarization technique to the grammar. The experiment shows that the number of popped edges decreases as the normalization constant increases, but the precision and recall decrease as well. The results suggest that their parsing system is efficient and effective for PCFG grammars induced from large tree-banks. The paper concludes by recommending their technique to others interested in PCFG parsing.",
                        "This paper presents an edge-based best-first chart parsing method for context-free grammar. The traditional 0(n3) parsing algorithms are not efficient for large grammars and long sentences. The paper explores the use of a figure of merit (FOM) to rank edges in a best-first parsing approach. The approach is compared to previous work and experimental results show a significant reduction in the number of edges required for parsing. The study also investigates the impact of normalization constants on parsing accuracy. The results suggest that the proposed method can achieve high accuracy with a relatively small number of edges. The paper concludes by recommending the edge-based best-first chart parsing technique as the most efficient method for PCFG parsing from large tree-banks. Further research is suggested to explore the applicability of this technique to other probability distributions and grammars.",
                        "This paper presents a method for efficient chart parsing using the best-first approach. The authors introduce the use of a figure of merit (FOM) to rank edges for processing, allowing for the reduction of the work required for parsing. The paper details the method's application to a PCFG induced from the Penn II WSJ corpus and presents experimental results demonstrating the effectiveness of the approach. It is shown that the proposed technique significantly reduces the number of edges required for parsing, leading to more efficient processing. The authors compare their results with previous work and discuss the implications of their findings. Overall, the paper provides a comprehensive overview of edge-based best-first chart parsing and recommends this technique for PCFG parsing.",
                        "This paper presents the use of edge-based best-first chart parsing for context-free grammar parsing, addressing the limitations of traditional parsing algorithms. The approach involves ranking all edges, complete and incomplete, based on a figure of merit (FOM) derived from PCFG probabilities. The authors compare their results with previous work, demonstrating a significant reduction in the number of edges required for parsing. By applying a normalization factor, the authors achieve improved accuracy with a minimal increase in the number of edges. The paper concludes by recommending this technique as the most efficient parsing method for PCFG grammars induced from large tree-banks. Overall, the results suggest that edge-based best-first chart parsing offers a promising approach for enhancing the efficiency and accuracy of PCFG parsing."
                    ],
                    "filename": "edgebased_bestfirst_chart_parsin-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 141 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Edge-Based Best-First Chart Parsing</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Finding one (or all) parses for a sentence according to a context-free grammar requires search.</S>\n    <S sid=\"8\" ssid=\"2\">Fortunately, there are well known 0(n3) algorithms for parsing, where n is the length of the sentence.</S>\n    <S sid=\"9\" ssid=\"3\">Unfortunately, for large grammars (such as the PCFG induced from the Penn II WSJ corpus, which contains around 1.6 • 104 rules) and longish sentences (say, 40 words and punctuation), even 0(713) looks pretty bleak.</S>\n    <S sid=\"10\" ssid=\"4\">One well-known 0(n3) parsing method (Kay, 1980) is chart parsing.</S>\n    <S sid=\"11\" ssid=\"5\">In this approach one maintains an agenda of items remaining to be processed, one of which is processed during each iteration.</S>\n    <S sid=\"12\" ssid=\"6\">As each item is pulled off the agenda, it is added to the chart (unless it is already there, in which case it can be discarded) and used to extend and create additional items.</S>\n    <S sid=\"13\" ssid=\"7\">In &amp;quot;exhaustive&amp;quot; chart parsing one removes items from the agenda in some relatively simple way (last-in, first-out is common), and continues to do so until nothing remains.</S>\n    <S sid=\"14\" ssid=\"8\">A commonly discussed alternative is to remove the constituents from the agenda according to a figure of merit (FOM).</S>\n    <S sid=\"15\" ssid=\"9\">The idea is that the FOM selects &amp;quot;good&amp;quot; items to be processed, leaving the &amp;quot;bad&amp;quot; ones— the ones that are not, in fact, part of the correct parse— sitting on the agenda.</S>\n    <S sid=\"16\" ssid=\"10\">When one has a completed parse, or perhaps several possible parses, one simply stops parsing, leaving items remaining on the agenda.</S>\n    <S sid=\"17\" ssid=\"11\">The time that would have been spent processing these remaining items is time saved, and thus time earned.</S>\n    <S sid=\"18\" ssid=\"12\">In our work we have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence.</S>\n    <S sid=\"19\" ssid=\"13\">Numbers like this suggest that any approach that offers the possibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers.</S>\n    <S sid=\"20\" ssid=\"14\">Early on, Kay (1980) suggested the use of the chart agenda for this purpose.</S>\n    <S sid=\"21\" ssid=\"15\">More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM.</S>\n    <S sid=\"22\" ssid=\"16\">Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here.</S>\n    <S sid=\"23\" ssid=\"17\">Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox.</S>\n    <S sid=\"24\" ssid=\"18\">1994: Kochman and Kupin.</S>\n    <S sid=\"25\" ssid=\"19\">1991: N1agerman and Marcus, 1991).</S>\n    <S sid=\"26\" ssid=\"20\">Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&amp;C) (Forthcoming).</S>\n    <S sid=\"27\" ssid=\"21\">They consider a large number of FOMs, and view them as approximations of some &amp;quot;ideal&amp;quot; (but only computable after the fact) FOM.</S>\n    <S sid=\"28\" ssid=\"22\">Of these they recommend one as the best of the lot.</S>\n    <S sid=\"29\" ssid=\"23\">In this paper we basically adopt both their framework and their recommended FOM.</S>\n    <S sid=\"30\" ssid=\"24\">The next section describes their work in more detail, Besides C&amp;C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997).</S>\n    <S sid=\"31\" ssid=\"25\">Goodman uses an FOM that is similar to that of C&amp;C but one that should, in general, be somewhat more accurate.</S>\n    <S sid=\"32\" ssid=\"26\">However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here.</S>\n    <S sid=\"33\" ssid=\"27\">We briefly compare our results to theirs in Section 5.</S>\n    <S sid=\"34\" ssid=\"28\">As noted, our paper takes off from that of C&amp;C and uses the same FOM.</S>\n    <S sid=\"35\" ssid=\"29\">The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather than simply completed constituents, as was done by C&amp;C.</S>\n    <S sid=\"36\" ssid=\"30\">What is interesting about our approach is that such a seemingly simple change can produce rather dramatic results.</S>\n    <S sid=\"37\" ssid=\"31\">Rather than the thousands of edges required by C&amp;C, the parser presented here requires hundreds, or even, if one is willing to pay a small price in accuracy, tens.</S>\n  </SECTION>\n  <SECTION title=\"2 Constituent-Based Best-First\" number=\"2\">\n    <S sid=\"38\" ssid=\"1\">In the approach taken in C&amp;C, only completed edges, i.e., constituents, are entered into the agenda; incomplete edges are always processed as soon as they are constructed.</S>\n    <S sid=\"39\" ssid=\"2\">At each iteration the constituent with the highest figure of merit is removed from the agenda, added to the chart, and used to extend current partially completed constituents.</S>\n    <S sid=\"40\" ssid=\"3\">Thus we characterize their work as constituent-based best-first chart parsing.</S>\n    <S sid=\"41\" ssid=\"4\">C&amp;C take as an &amp;quot;ideal&amp;quot; FOM the quantity to,,).</S>\n    <S sid=\"42\" ssid=\"5\">Here NJ, is a constituent of type i (e.g., NP, VP, etc.) that spans the constituents from j up to but not including k, and tom are the n parts-of-speech (tags) of the sentence.</S>\n    <S sid=\"43\" ssid=\"6\">Note that C&amp;C simplify parsing by assuming that the input is a sequence of tags, not words.</S>\n    <S sid=\"44\" ssid=\"7\">We make the same assumption in this paper.</S>\n    <S sid=\"45\" ssid=\"8\">Thus taking p(N;rk I to,,) as an FOM says that one should work on the constituent that is most likely to be correct .given the tags of the sentence.</S>\n    <S sid=\"46\" ssid=\"9\">As p(N;,k to,n) can only be computed precisely after a full parse of the sentence, C&amp;C derive several approximations, in each case starting from the well known equation for p(Nlk I tom) in terms of the inside and outside probabilities, /3(1V.1,k) and where fi(Mjkj ) and a(N'k ) are defined as follows: (4) Informally, this can be obtained by approximating the outside probability a(N.;,k) in Equation 1 with a bitag estimate.</S>\n    <S sid=\"47\" ssid=\"10\">Of the five terms in Equation 4, two can be directly estimated from training data: the &amp;quot;boundary statistics&amp;quot; p(N.:,k I tj) (the probability of a constituent of type NIAstarting just after the tag tj) and p(tk I NIA) (the probability of tk appearing just after the end of a constituent of type N.4).</S>\n    <S sid=\"48\" ssid=\"11\">The tag sequence probabilitiy in the denominator is approximated using a bi-tag approximation: The basic algorithm then is quite simple.</S>\n    <S sid=\"49\" ssid=\"12\">One uses the standard chart-parsing algorithm, except at each iteration one takes from the agenda the constituent that maximizes the FOM described in Equation 4.</S>\n    <S sid=\"50\" ssid=\"13\">There are, however, two minor complexities that need to be noted.</S>\n    <S sid=\"51\" ssid=\"14\">The first relates to the inside probability 0(N:4).</S>\n    <S sid=\"52\" ssid=\"15\">C&amp;C approximate it with the sum of the probabilities of all the parses for Nlk found at that point in the parse.</S>\n    <S sid=\"53\" ssid=\"16\">This in turn requires a somewhat complicated scheme to avoid repeatedly re-evaluating Equation 4 whenever a new parse is found.</S>\n    <S sid=\"54\" ssid=\"17\">In this paper we adopt a slightly simpler method.</S>\n    <S sid=\"55\" ssid=\"18\">We approximate )3(Nk) by the most probable parse for Ni,k, rather than the sum of all the parses. j We justify this on the grounds that our parser eventually returns the most probable parse, so it seems reasonable to base our metric on its value.</S>\n    <S sid=\"56\" ssid=\"19\">This also simplifies updating 13(N3,k) when new parses are found for Nk.</S>\n    <S sid=\"57\" ssid=\"20\">Our algorithm compares the probability of the new parse to the best already found for Ni 4.</S>\n    <S sid=\"58\" ssid=\"21\">If the old one is higher, nothing need be done.</S>\n    <S sid=\"59\" ssid=\"22\">If the new one is higher, it is simply added to the agenda.</S>\n    <S sid=\"60\" ssid=\"23\">The second complexity has to do with the fact that in Equation 4 the probability of the tags t3,k are approximated using two different distributions, once in the numerator where we use the PCFG probabilities, and once in the denominator, where we use the bi-tag probabilities.</S>\n    <S sid=\"61\" ssid=\"24\">One fact noted by C&amp;C, but not discussed in their paper, is that typically the bitag model gives higher probabilities for a tag sequence than does the PCFG distribution.</S>\n    <S sid=\"62\" ssid=\"25\">For any single tag t3, the difference is not much, but as we use Equation 4 to compute our FOM for larger constituents, the numerator becomes smaller and smaller with respect to the denominator, effectively favoring smaller constituents.</S>\n    <S sid=\"63\" ssid=\"26\">To avoid this one needs to normalize the two distributions to produce more similar results.</S>\n    <S sid=\"64\" ssid=\"27\">We have empirically measured the normalization factor and found that the bi-tag distribution produces probabilities that are approximately 1.3 times those produced by the PCFG distribution, on a per-word basis.</S>\n    <S sid=\"65\" ssid=\"28\">We correct for this by making the PCFG probability of a known tag ri &gt; 1.</S>\n    <S sid=\"66\" ssid=\"29\">This has the effect of multiplying the inside probability /3(N.4) by rik-J.</S>\n    <S sid=\"67\" ssid=\"30\">In Section 4 we show how the behavior of our algorithm changes for is between 1.0 and 2.4.</S>\n  </SECTION>\n  <SECTION title=\"3 Chart parsing and binarization\" number=\"3\">\n    <S sid=\"68\" ssid=\"1\">Informally, our algorithm differs from the one presented in C&amp;C primarily in that we rank all edges, incomplete as well as complete, with respect to the FOM.</S>\n    <S sid=\"69\" ssid=\"2\">A straight-forward way to extend C&amp;C in this fashion is to transform the grammar so that all productions are either unary or binary.</S>\n    <S sid=\"70\" ssid=\"3\">Once this has been done there is no need for incomplete edges at all in bottomup parsing, and parsing can be performed using the CKY algorithm, suitably extended to handle unary productions.</S>\n    <S sid=\"71\" ssid=\"4\">One way to convert a PCFG into this form is left-factoring (Hoperoft and Ullman, 1979).</S>\n    <S sid=\"72\" ssid=\"5\">Left-factoring replaces each production A -4 : p, where p is the production probability and 101 = n&gt; 2, with the following set of binary productions: In these productions A is the ith element of /3 and `13i,j' is the subsequence A of 0, but treated as a 'new' single non-terminal in the left-factored grammar (the quote marks indicate that this subsequence is to be considered a single symbol).</S>\n    <S sid=\"73\" ssid=\"6\">For example, the production VP -4 V NP NP PP :0.7 left-factors to the following productions: VP --4 NP NP' PP :0.7 NP NP' NP' PP : 1.0 NP' -4 V NP :1.0 It is not difficult to show that the left-factored grammar defines the same probability distribution over strings as the original grammar, and to devise a tree transformation that maps each parse tree of the original grammar into a unique parse tree of the left-factored grammar of the same probability.</S>\n    <S sid=\"74\" ssid=\"7\">In fact, the assumption that all productions are at most binary is not extraordinary, since tabular parsers that construct complete parse forests in worst-case 0(n3) time explicitly or implicitly convert their grammars into binary branching form (Lang, 1974; Lang, 1991).</S>\n    <S sid=\"75\" ssid=\"8\">Sikkel and Nijholt (1997) describe in detail the close relationship between the CKY algorithm, the Earley algorithm and a bottom-up variant of the Earley algorithm.</S>\n    <S sid=\"76\" ssid=\"9\">The key observation is that the 'new' non-terminals `01,i' in a CKY parse using a left-factored grammar correspond to the set of non-empty incomplete edges A -4 01,z.</S>\n    <S sid=\"77\" ssid=\"10\">13z+1,n in the bottom-up variant of the Earley algorithm, where A -&gt; 131,, is a production of the original grammar.</S>\n    <S sid=\"78\" ssid=\"11\">Specifically, the fundamental rule of chart parsing (Kay, 1980), which combines an incomplete edge A --* a • BO with a complete edge B 7- to yield the edge A -+ a B • 0, corresponds to the left-factored productions `aB' --+ a B if /3 is non-empty or A 'a' B if i3 is empty.</S>\n    <S sid=\"79\" ssid=\"12\">Thus in general a single 'new' non-terminal in a CKY parse using the left-factored grammar abbreviates several incomplete edges in the Earley algorithm.</S>\n  </SECTION>\n  <SECTION title=\"4 The Experiment\" number=\"4\">\n    <S sid=\"80\" ssid=\"1\">For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing.</S>\n    <S sid=\"81\" ssid=\"2\">All sentences of length greater than 40 were ignored for testing purposes as done in both C&amp;C and Goodman (1997).</S>\n    <S sid=\"82\" ssid=\"3\">We applied the binarization technique described above to the grammar.</S>\n    <S sid=\"83\" ssid=\"4\">We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse.</S>\n    <S sid=\"84\" ssid=\"5\">This method has the advantage of being platform independent, as well as providing a measure of &amp;quot;perfection&amp;quot;.</S>\n    <S sid=\"85\" ssid=\"6\">Here, perfection is the minimum number of edges we would need to pop off the agenda in order to create the correct parse.</S>\n    <S sid=\"86\" ssid=\"7\">For the binarized grammar, where each popped edge is a completed constituent, this number is simply the number of terminals plus nonterminals in the sentence— on average, 47.5.</S>\n    <S sid=\"87\" ssid=\"8\">Our algorithm includes some measures to reduce the number of items on the agenda, and thus (presumably) the number of popped edges.</S>\n    <S sid=\"88\" ssid=\"9\">Each time we add a constituent to the chart, we combine it with the constituents on either side of it, potentially creating several new edges.</S>\n    <S sid=\"89\" ssid=\"10\">For each of these new edges, we check to see if a matching constituent (i.e. a constituent with the same head, start, and end points) already exists in either the agenda or the chart.</S>\n    <S sid=\"90\" ssid=\"11\">If there is no match, we simply add the new edge to the agenda.</S>\n    <S sid=\"91\" ssid=\"12\">If there is a match but the old parse of Nild, is better than the new one, we discard the new parse.</S>\n    <S sid=\"92\" ssid=\"13\">Finally, if we have found a better parse of N.4, we add the new edge to the agenda, removing the old one if it has not already been popped.</S>\n    <S sid=\"93\" ssid=\"14\">We tested the parser on section section 22 of the WSJ text with various normalization constants .77, working on each sentence only until we reached the first full parse.</S>\n    <S sid=\"94\" ssid=\"15\">For each sentence we recorded the number of popped edges needed to reach the first parse, and the precision and recall of that parse.</S>\n    <S sid=\"95\" ssid=\"16\">The average number of popped edges to first parse as a function of q is shown in Figure 1, and the average precision and recall are shown in Figure 2.</S>\n    <S sid=\"96\" ssid=\"17\">The number of popped edges decreases as ij increases from 1.0 to 1.7, then begins to increase again.</S>\n    <S sid=\"97\" ssid=\"18\">See Section 5 for discussion of these results.</S>\n    <S sid=\"98\" ssid=\"19\">The precision and recall also decrease as 77 increases.</S>\n    <S sid=\"99\" ssid=\"20\">Note that, because we used a binarized grammer for parsing, the trees produced by the parser contain binarized labels rather than the labels in the treebank.</S>\n    <S sid=\"100\" ssid=\"21\">In order to calculate precision and recall. we &amp;quot;debinarized&amp;quot; the parser's output and then calculated the figures as usual.</S>\n    <S sid=\"101\" ssid=\"22\">These results suggest two further questions: Is the higher accuracy with lower 77 due in part to the higher number of edges popped?</S>\n    <S sid=\"102\" ssid=\"23\">If so, can we gain accuracy with higher i by letting the parser continue past the first parse (i.e. pop more edges)?</S>\n    <S sid=\"103\" ssid=\"24\">To answer these questions, we ran the parser again, this time allowing it to continue parsing until it had popped 20 times as many edges as needed to reach the first parse.</S>\n    <S sid=\"104\" ssid=\"25\">The results of this experiment are shown in Figure 3, where we plot (precision + recall)/2 (henceforth &amp;quot;accuracy&amp;quot;) as a function of edges.</S>\n    <S sid=\"105\" ssid=\"26\">Note that regardless of 7/ the accuracy of the parse increases given extra time, but that all of the increase is achieved with only 1.5 to 2 times as many edges as needed for the first parse.</S>\n    <S sid=\"106\" ssid=\"27\">For ij between 1.0 and 1.2, the highest accuracy is almost the same, about 75.2, but this value is reached with an average of slightly under 400 edges when 77 = 1.2, compared to about 650 when 7/ = 1.0.</S>\n  </SECTION>\n  <SECTION title=\"5 Results\" number=\"5\">\n    <S sid=\"107\" ssid=\"1\">To better understand the experimental results it first behooves us to compare them to those achieved previously.</S>\n    <S sid=\"108\" ssid=\"2\">Goodman's results (1997) are hard to compare against ours because his parser returns more than a singe best parse and because he measures processing time, not edges.</S>\n    <S sid=\"109\" ssid=\"3\">However he does give edges/second for one of his 2000 4000 6000 8000 10000 parsers and this plus his parsing times suggests that for him edges/sentence will measure in the tens of thousands — a far cry from our hundreds.</S>\n    <S sid=\"110\" ssid=\"4\">Ratnaparki's (1997) beam search parsing procedure produces higher accuracy results than our PCFG model, and achieves this with a beam width of 20.</S>\n    <S sid=\"111\" ssid=\"5\">Unfortunately his paper does not give statistics which can be directly compared with ours.</S>\n    <S sid=\"112\" ssid=\"6\">The work by C&amp;C is easier to compare.</S>\n    <S sid=\"113\" ssid=\"7\">In Figure 4 we reproduce C&amp;C's results on the percentage of sentences (length 18-26) parsed as a function of number of edges used.</S>\n    <S sid=\"114\" ssid=\"8\">We performed the same experiment, and our results are included there as well.</S>\n    <S sid=\"115\" ssid=\"9\">This figure makes dramatic the order of magnitude improvement provided by our new scheme, but it is not too easy to read numbers off of it.</S>\n    <S sid=\"116\" ssid=\"10\">Such numbers are provided in Table 1.</S>\n    <S sid=\"117\" ssid=\"11\">Our figures were obtained using ri = 1.2.</S>\n    <S sid=\"118\" ssid=\"12\">As can be seen, our parser requires about one twentieth the number of edges required by C&amp;C.</S>\n    <S sid=\"119\" ssid=\"13\">Indeed, the low average number of edges to first parse is probably the most striking thing about our results.</S>\n    <S sid=\"120\" ssid=\"14\">Even allowing for the fact that considerably more edges must be pushed than are popped, the total number of edges required to first parse is quite small.</S>\n    <S sid=\"121\" ssid=\"15\">Since the average number of edges required to construct just the (left-factored) test corpus trees is 47.5, our parsing system considers as few as 3 times as many edges as are required to actually produce the output tree.</S>\n    <S sid=\"122\" ssid=\"16\">Almost as interesting, if i is below 1.4, the precision and recall scores of the first parse are better than those obtained by running the parser to exhaustion, even though the probability of the first parses our algorithm returns cannot be higher than that found by the exhaustive version.</S>\n    <S sid=\"123\" ssid=\"17\">Furthermore, as seen in Figure 3, running our parser past the first parse by a small amount (150% of the edges required for the first parse) produces still more accurate parses.</S>\n    <S sid=\"124\" ssid=\"18\">At 150% of the minimum number of edges and I) = 1.2 the precision/recall figures are about 2% above those for the maximum likelihood parse.</S>\n    <S sid=\"125\" ssid=\"19\">We have two (possibly related) theories of these phenomona.</S>\n    <S sid=\"126\" ssid=\"20\">It may be that the FOM metric used to select constituents forces our parser to concentrate on edges which are plausible given their surrounding preterminals; information which is ignored by the exhaustive maximum likelihood parser.</S>\n    <S sid=\"127\" ssid=\"21\">Alternatively, it may be that because our FOM causes our parser to prefer edges with a high inside times (estimated) outside probability, it is in fact partially mimicking Goodman's (Goodman, 1996) 'Labelled Recall' parsing algorithm, which does not return the highest probability parse but attempts to maximize labelled bracket recall with the test set.</S>\n    <S sid=\"128\" ssid=\"22\">Finally, it is interesting to note that the minimum number of edges per parse is reached when 77 1.65, which is considerably larger than the theoretical estimate of 1.3 given earlier.</S>\n    <S sid=\"129\" ssid=\"23\">Notice that one effect of increasing r/ is to raise the FOM for longer constituents.</S>\n    <S sid=\"130\" ssid=\"24\">It may be that on average a partial parse is completed fastest if larger constituents receive more attention since they are more likely to lead quickly to a complete analysis, which would be one consequence of the larger than expected n. This last hypothesis is also consistent with the observation that average precision and recall sharply falls off when 77 is increased beyond its theoretically optimal value, since then the parser is presumably focusing on relatively larger constituents and ignoring other, strictly more plausible, smaller ones.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion\" number=\"6\">\n    <S sid=\"131\" ssid=\"1\">It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of PCFG parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability.</S>\n    <S sid=\"132\" ssid=\"2\">One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs (Charniak, 1997), or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997).</S>\n    <S sid=\"133\" ssid=\"3\">Clearly further research is warranted.</S>\n    <S sid=\"134\" ssid=\"4\">Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C&amp;C To the best of our knowledge this is currently the most effecient parsing technique for PCFG grammars induced from large tree-banks.</S>\n    <S sid=\"135\" ssid=\"5\">As such we strongly recommend this technique to others interested in PCFG parsing.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "d7c3aa39aa8762c70b07d82c3b4cc252",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Edge-Based Best-First Chart Parsing"
                }
            }
        },
        {
            "index": 4,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "investigate for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system’s error trends.",
                    "filename": "Prototype-Driven_Learning_For_Sequence_Models",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/N06-1041/Documents_xml/N06-1041.xml",
                    "title": "Prototype-Driven Learning For Sequence Models"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents the concept of prototype-driven learning for sequence models, focusing on part-of-speech tagging and information extraction tasks. The authors propose a semi-supervised learning approach where prototypical examples for each target label or label configuration are specified, without necessarily labeling any documents or sentences. The paper discusses the advantages of this approach, presents results from experiments, and demonstrates substantial error rate reductions using distributional prototype features. The findings suggest that distributional prototype features can effectively link words to prototypes based on distributional similarity, leading to significant error reduction in various induction tasks. The proposed approach provides a compact and declarative way to specify a target labeling scheme.",
                        "This paper introduces prototype-driven learning for sequence models, which aims to learn models with the least effort by using a certain kind of semi-supervised learning. The authors argue for the effectiveness of prototype-driven learning, where prototypical examples for each target label are specified, but no documents or sentences are necessarily labeled. The approach is applied to three sequence modeling tasks: part-of-speech tagging in English and Chinese, and classified ads information extraction. The paper demonstrates substantial error rate reductions using distributional prototype features, providing a detailed analysis of the results and comparisons with other systems. Overall, the approach allows for the specification of a target labeling scheme in a compact and declarative way, contributing to substantial error reduction on several induction tasks.",
                        "This paper introduces prototype-driven learning for sequence models, focusing on sequence modeling tasks such as part-of-speech tagging and information extraction. The proposed approach uses prototype lists to specify prior knowledge about target labels and leverages distributional similarity features to link words to prototypes. Experimental results on English and Chinese part-of-speech tagging, as well as classified advertisement segmentation, demonstrate substantial error rate reductions using the prototype-driven approach. The paper concludes that distributional prototype features allow for a compact and declarative specification of a target labeling scheme and reconcile the success of unsupervised word clustering with the success of sequence models in supervised settings.",
                        "This paper presents a prototype-driven learning approach for sequence models, specifically focusing on part-of-speech tagging in English and Chinese, as well as classified ad information extraction. The proposed approach uses prototype lists to specify prior knowledge about the target structure, which are then incorporated as features in a log-linear generative model. Additionally, the paper introduces distributional similarity features to link words to prototypes based on their similarity, resulting in substantial error rate reductions on all three tasks. The findings suggest that this approach allows for a compact and declarative way to specify a target labeling scheme, while reconciling the success of distributional methods with that of sequence models in supervised settings."
                    ],
                    "filename": "prototypedriven_learning_for_seq-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 104 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Prototype-Driven Learning For Sequence Models</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Learning, broadly taken, involves choosing a good model from a large space of possible models.</S>\n    <S sid=\"8\" ssid=\"2\">In supervised learning, model behavior is primarily determined by labeled examples, whose production requires a certain kind of expertise and, typically, a substantial commitment of resources.</S>\n    <S sid=\"9\" ssid=\"3\">In unsupervised learning, model behavior is largely determined by the structure of the model.</S>\n    <S sid=\"10\" ssid=\"4\">Designing models to exhibit a certain target behavior requires another, rare kind of expertise and effort.</S>\n    <S sid=\"11\" ssid=\"5\">Unsupervised learning, while minimizing the usage of labeled data, does not necessarily minimize total effort.</S>\n    <S sid=\"12\" ssid=\"6\">We therefore consider here how to learn models with the least effort.</S>\n    <S sid=\"13\" ssid=\"7\">In particular, we argue for a certain kind of semi-supervised learning, which we call prototype-driven learning.</S>\n    <S sid=\"14\" ssid=\"8\">In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.</S>\n    <S sid=\"15\" ssid=\"9\">For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).</S>\n    <S sid=\"16\" ssid=\"10\">This manner of specifying prior knowledge about the task has several advantages.</S>\n    <S sid=\"17\" ssid=\"11\">First, is it certainly compact (though it remains to be proven that it is effective).</S>\n    <S sid=\"18\" ssid=\"12\">Second, it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task).</S>\n    <S sid=\"19\" ssid=\"13\">Indeed, prototype lists have been used pedagogically to summarize tagsets to students (Manning and Sch¨utze, 1999).</S>\n    <S sid=\"20\" ssid=\"14\">Finally, natural language does exhibit proform and prototype effects (Radford, 1988), which suggests that learning by analogy to prototypes may be effective for language tasks.</S>\n    <S sid=\"21\" ssid=\"15\">In this paper, we consider three sequence modeling tasks: part-of-speech tagging in English and Chinese and a classified ads information extraction task.</S>\n    <S sid=\"22\" ssid=\"16\">Our general approach is to use distributional similarity to link any given word to similar prototypes.</S>\n    <S sid=\"23\" ssid=\"17\">For example, the word reported may be linked to said, which is in turn a prototype for the part-of-speech VBD.</S>\n    <S sid=\"24\" ssid=\"18\">We then encode these prototype links as features in a log-linear generative model, which is trained to fit unlabeled data (see section 4.1).</S>\n    <S sid=\"25\" ssid=\"19\">Distributional prototype features provide substantial error rate reductions on all three tasks.</S>\n    <S sid=\"26\" ssid=\"20\">For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.</S>\n  </SECTION>\n  <SECTION title=\"2 Tasks and Related Work: Tagging\" number=\"2\">\n    <S sid=\"27\" ssid=\"1\">For our part-of-speech tagging experiments, we used data from the English and Chinese Penn treebanks (Marcus et al., 1994; Ircs, 2002).</S>\n    <S sid=\"28\" ssid=\"2\">Example sentences are shown in figure 1(a) and (b).</S>\n    <S sid=\"29\" ssid=\"3\">A great deal of research has investigated the unsupervised and semisupervised induction of part-of-speech models, especially in English, and there is unfortunately only space to mention some highly related work here.</S>\n    <S sid=\"30\" ssid=\"4\">One approach to unsupervised learning of partof-speech models is to induce HMMs from unlabeled data in a maximum-likelihood framework.</S>\n    <S sid=\"31\" ssid=\"5\">For example, Merialdo (1991) presents experiments learning HMMs using EM.</S>\n    <S sid=\"32\" ssid=\"6\">Merialdo’s results most famously show that re-estimation degrades accuracy unless almost no examples are labeled.</S>\n    <S sid=\"33\" ssid=\"7\">Less famously, his results also demonstrate that reestimation can improve tagging accuracies to some degree in the fully unsupervised case.</S>\n    <S sid=\"34\" ssid=\"8\">One recent and much more successful approach to part-of-speech learning is contrastive estimation, presented in Smith and Eisner (2005).</S>\n    <S sid=\"35\" ssid=\"9\">They utilize task-specific comparison neighborhoods for part-ofspeech tagging to alter their objective function.</S>\n    <S sid=\"36\" ssid=\"10\">Both of these works require specification of the legal tags for each word.</S>\n    <S sid=\"37\" ssid=\"11\">Such dictionaries are large and embody a great deal of lexical knowledge.</S>\n    <S sid=\"38\" ssid=\"12\">A prototype list, in contrast, is extremely compact.</S>\n  </SECTION>\n  <SECTION title=\"3 Tasks and Related Work: Extraction\" number=\"3\">\n    <S sid=\"39\" ssid=\"1\">Grenager et al. (2005) presents an unsupervised approach to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see figure 1(c)).</S>\n    <S sid=\"40\" ssid=\"2\">Labels in this domain tend to be “sticky” in that the correct annotation tends to consist of multi-element fields of the same label.</S>\n    <S sid=\"41\" ssid=\"3\">The overall approach of Grenager et al. (2005) typifies the process involved in fully unsupervised learning on new domain: they first alter the structure of their HMM so that diagonal transitions are preferred, then modify the transition structure to explicitly model boundary tokens, and so on.</S>\n    <S sid=\"42\" ssid=\"4\">Given enough refinement set of the CLASSIFIEDS data.</S>\n    <S sid=\"43\" ssid=\"5\">The BOUNDARY field is not present in the original annotation, but added to model boundaries (see Section 5.3).</S>\n    <S sid=\"44\" ssid=\"6\">The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.</S>\n    <S sid=\"45\" ssid=\"7\">In section 5.3, we discuss an approach to this task which does not require customization of model structure, but rather centers on feature engineering.</S>\n  </SECTION>\n  <SECTION title=\"4 Approach\" number=\"4\">\n    <S sid=\"46\" ssid=\"1\">In the present work, we consider the problem of learning sequence models over text.</S>\n    <S sid=\"47\" ssid=\"2\">For each document x = [xi], we would like to predict a sequence of labels y = [yi], where xi E X and yi E Y.</S>\n    <S sid=\"48\" ssid=\"3\">We construct a generative model, p(x, y|0), where 0 are the model’s parameters, and choose parameters to maximize the log-likelihood of our observed data D: We take our model family to be chain-structured Markov random fields (MRFs), the undirected equivalent of HMMs.</S>\n    <S sid=\"49\" ssid=\"4\">Our joint probability model over (x, y) is given by where φ(c) is a potential over a clique c, taking the form exp {θT f(c)}, and f(c) is the vector of features active over c. In our sequence models, the cliques are over the edges/transitions (yi−1, yi) and nodes/emissions (xi, yi).</S>\n    <S sid=\"50\" ssid=\"5\">See figure 3 for an example from the English POS tagging domain.</S>\n    <S sid=\"51\" ssid=\"6\">Note that the only way an MRF differs from a conditional random field (CRF) (Lafferty et al., 2001) is that the partition function is no longer observation dependent; we are modeling the joint probability of x and y instead of y given x.</S>\n    <S sid=\"52\" ssid=\"7\">As a result, learning an MRF is slightly harder than learning a CRF; we discuss this issue in section 4.4.</S>\n    <S sid=\"53\" ssid=\"8\">We assume prior knowledge about the target structure via a prototype list, which specifies the set of target labels Y and, for each label y E Y, a set of prototypes words, py E Py.</S>\n    <S sid=\"54\" ssid=\"9\">See figures 2 and 4 for examples of prototype lists.1 1Note that this setting differs from the standard semisupervised learning setup, where a small number of fully labeled examples are given and used in conjunction with a larger amount of unlabeled data.</S>\n    <S sid=\"55\" ssid=\"10\">In our prototype-driven approach, we never provide a single fully labeled example sequence.</S>\n    <S sid=\"56\" ssid=\"11\">See section 5.3 for further comparison of this setting to semi-supervised learning.</S>\n    <S sid=\"57\" ssid=\"12\">Broadly, we would like to learn sequence models which both explain the observed data and meet our prior expectations about target structure.</S>\n    <S sid=\"58\" ssid=\"13\">A straightforward way to implement this is to constrain each prototype word to take only its given label(s) at training time.</S>\n    <S sid=\"59\" ssid=\"14\">As we show in section 5, this does not work well in practice because this constraint on the model is very sparse.</S>\n    <S sid=\"60\" ssid=\"15\">In providing a prototype, however, we generally mean something stronger than a constraint on that word.</S>\n    <S sid=\"61\" ssid=\"16\">In particular, we may intend that words which are in some sense similar to a prototype generally be given the same label(s) as that prototype.</S>\n    <S sid=\"62\" ssid=\"17\">In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words (Sch¨utze, 1995; Clark, 2001).</S>\n    <S sid=\"63\" ssid=\"18\">The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness (Radford, 1988).</S>\n    <S sid=\"64\" ssid=\"19\">We present more details in section 5, but for now assume that a similarity function over word types is given.</S>\n    <S sid=\"65\" ssid=\"20\">Suppose further that for each non-prototype word type w, we have a subset of prototypes, Sw, which are known to be distributionally similar to w (above some threshold).</S>\n    <S sid=\"66\" ssid=\"21\">We would like our model to relate the tags of w to those of Sw. One approach to enforcing the distributional assumption in a sequence model is by supplementing the training objective (here, data likelihood) with a penalty term that encourages parameters for which each w’s posterior distribution over tags is compatible with it’s prototypes Sw. For example, we might maximize, where t|w is the model’s distribution of tags for word w. The disadvantage of a penalty-based approach is that it is difficult to construct the penalty term in a way which produces exactly the desired behavior.</S>\n    <S sid=\"67\" ssid=\"22\">Instead, we introduce distributional prototypes into the learning process as features in our log-linear model.</S>\n    <S sid=\"68\" ssid=\"23\">Concretely, for each prototype z, we introduce a predicate PROTO = z which becomes active at each w for which z E Sw (see figure 3).</S>\n    <S sid=\"69\" ssid=\"24\">One advantage of this approach is that it allows the strength of the distributional constraint to be calibrated along with any other features; it was also more successful in our experiments.</S>\n    <S sid=\"70\" ssid=\"25\">So far we have ignored the issue of how we learn model parameters θ which maximize L(θ; D).</S>\n    <S sid=\"71\" ssid=\"26\">If our model family were HMMs, we could use the EM algorithm to perform a local search.</S>\n    <S sid=\"72\" ssid=\"27\">Since we have a log-linear formulation, we instead use a gradientbased search.</S>\n    <S sid=\"73\" ssid=\"28\">In particular, we use L-BFGS (Liu and Nocedal, 1989), a standard numerical optimization technique, which requires the ability to evaluate L(θ; D) and its gradient at a given θ.</S>\n    <S sid=\"74\" ssid=\"29\">The density p(x|θ) is easily calculated up to the global constant Z(θ) using the forward-backward algorithm (Rabiner, 1989).</S>\n    <S sid=\"75\" ssid=\"30\">The partition function is given by sumptions about the clique potentials, but can in all cases be bounded by Where K is a suitably chosen large constant.</S>\n    <S sid=\"76\" ssid=\"31\">We can efficiently compute ˆZ`(θ) for fixed ` using a generalization of the forward-backward algorithm to the lattice of all observations x of length ` (see Smith and Eisner (2005) for an exposition).</S>\n    <S sid=\"77\" ssid=\"32\">Similar to supervised maximum entropy problems, the partial derivative of L(θ; D) with respect to each parameter θj (associated with feature fj) is given by a difference in feature expectations: The first expectation is the expected count of the feature under the model’s p(y|x, θ) and is again easily computed with the forward-backward algorithm, just as for CRFs or HMMs.</S>\n    <S sid=\"78\" ssid=\"33\">The second expectation is the expectation of the feature under the model’s joint distribution over all x, y pairs, and is harder to calculate.</S>\n    <S sid=\"79\" ssid=\"34\">Again assuming that sentences beyond a certain length have negligible mass, we calculate the expectation of the feature for each fixed length ` and take a (truncated) weighted sum: For fixed `, we can calculate Ex,y|`,θfj using the lattice of all inputs of length `.</S>\n    <S sid=\"80\" ssid=\"35\">The quantity p(|x |= `) is simply ˆZ`(θ)/ ˆZ(θ).</S>\n    <S sid=\"81\" ssid=\"36\">As regularization, we use a diagonal Gaussian prior with variance σ2 = 0.5, which gave relatively good performance on all tasks.</S>\n  </SECTION>\n  <SECTION title=\"5 Experiments\" number=\"5\">\n    <S sid=\"82\" ssid=\"1\">We experimented with prototype-driven learning in three domains: English and Chinese part-of-speech tagging and classified advertisement field segmentation.</S>\n    <S sid=\"83\" ssid=\"2\">At inference time, we used maximum posterior decoding,2 which we found to be uniformly but slightly superior to Viterbi decoding.</S>\n    <S sid=\"84\" ssid=\"3\">For our English part-of-speech tagging experiments, we used the WSJ portion of the English Penn treebank (Marcus et al., 1994).</S>\n    <S sid=\"85\" ssid=\"4\">We took our data to be either the first 48K tokens (2000 sentences) or 193K tokens (8000 sentences) starting from section 2.</S>\n    <S sid=\"86\" ssid=\"5\">We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in Smith and Eisner (2005): exact word type, character suffixes of length up to 3, initial-capital, contains-hyphen, and contains-digit.</S>\n    <S sid=\"87\" ssid=\"6\">Our only edge features were tag trigrams.</S>\n    <S sid=\"88\" ssid=\"7\">With just these features (our baseline BASE) the problem is symmetric in the 45 model labels.</S>\n    <S sid=\"89\" ssid=\"8\">In order to break initial symmetry we initialized our potentials to be near one, with some random noise.</S>\n    <S sid=\"90\" ssid=\"9\">To evaluate in this setting, model labels must be mapped to target labels.</S>\n    <S sid=\"91\" ssid=\"10\">We followed the common approach in the literature, greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset.</S>\n    <S sid=\"92\" ssid=\"11\">The results of BASE, reported in table 1, depend upon random initialization; averaging over 10 runs gave an average per-position accuracy of 41.3% on the larger training set.</S>\n    <S sid=\"93\" ssid=\"12\">We automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often.</S>\n    <S sid=\"94\" ssid=\"13\">This resulted in 116 prototypes for the 193K token setting.3 For comparison, there are 18,423 word types occurring in this data.</S>\n    <S sid=\"95\" ssid=\"14\">Incorporating the prototype list in the simplest possible way, we fixed prototype occurrences in the data to their respective annotation labels.</S>\n    <S sid=\"96\" ssid=\"15\">In this case, the model is no longer symmetric, and we no longer require random initialization or post-hoc mapping of labels.</S>\n    <S sid=\"97\" ssid=\"16\">Adding prototypes in this way gave an accuracy of 68.8% on all tokens, but only 47.7% on non-prototype occurrences, which is only a marginal improvement over BASE.</S>\n    <S sid=\"98\" ssid=\"17\">It appears as though the prototype information is not spreading to non-prototype words.</S>\n    <S sid=\"99\" ssid=\"18\">In order to remedy this, we incorporated distributional similarity features.</S>\n    <S sid=\"100\" ssid=\"19\">Similar to (Sch¨utze, 1995), we collect for each word type a context vector of the counts of the most frequent 500 words, conjoined with a direction and distance (e.g +1,-2).</S>\n    <S sid=\"101\" ssid=\"20\">We then performed an SVD on the matrix to obtain a reduced rank approximation.</S>\n    <S sid=\"102\" ssid=\"21\">We used the dot product between left singular vectors as a measure of distributional similarity.</S>\n    <S sid=\"103\" ssid=\"22\">For each word w, we find the set of prototype words with similarity exceeding a fixed threshold of 0.35.</S>\n    <S sid=\"104\" ssid=\"23\">For each of these prototypes z, we add a predicate PROTO = z to each occurrence of w. For example, we might add PROTO = said to each token of reported (as in figure 3).4 Each prototype word is also its own prototype (since a word has maximum similarity to itself), so when we lock the prototype to a label, we are also pushing all the words distributionally similar to that prototype towards that label.5 3To be clear: this method of constructing a prototype list required statistics from the labeled data.</S>\n    <S sid=\"105\" ssid=\"24\">However, we believe it to be a fair and necessary approach for several reasons.</S>\n    <S sid=\"106\" ssid=\"25\">First, we wanted our results to be repeatable.</S>\n    <S sid=\"107\" ssid=\"26\">Second, we did not want to overly tune this list, though experiments below suggest that tuning could greatly reduce the error rate.</S>\n    <S sid=\"108\" ssid=\"27\">Finally, it allowed us to run on Chinese, where the authors have no expertise.</S>\n    <S sid=\"109\" ssid=\"28\">4Details of distributional similarity features: To extract context vectors, we used a window of size 2 in either direction and use the first 250 singular vectors.</S>\n    <S sid=\"110\" ssid=\"29\">We collected counts from all the WSJ portion of the Penn Treebank as well as the entire BLIPP corpus.</S>\n    <S sid=\"111\" ssid=\"30\">We limited each word to have similarity features for its top 5 most similar prototypes.</S>\n    <S sid=\"112\" ssid=\"31\">5Note that the presence of a prototype feature does not ensure every instance of that word type will be given its prototype’s label; pressure from “edge” features or other prototype features can cause occurrences of a word type to be given different labels.</S>\n    <S sid=\"113\" ssid=\"32\">However, rare words with a single prototype feature are almost always given that prototype’s label.</S>\n    <S sid=\"114\" ssid=\"33\">This setting, PROTO+SIM, brings the all-tokens accuracy up to 80.5%, which is a 37.5% error reduction over PROTO.</S>\n    <S sid=\"115\" ssid=\"34\">For non-prototypes, the accuracy increases to 67.8%, an error reduction of 38.4% over PROTO.</S>\n    <S sid=\"116\" ssid=\"35\">The overall error reduction from BASE to PROTO+SIM on all-token accuracy is 66.7%.</S>\n    <S sid=\"117\" ssid=\"36\">Table 5 lists the most common confusions for PROTO+SIM.</S>\n    <S sid=\"118\" ssid=\"37\">The second, third, and fourth most common confusions are characteristic of fully supervised taggers (though greater in number here) and are difficult.</S>\n    <S sid=\"119\" ssid=\"38\">For instance, both JJs and NNs tend to occur after determiners and before nouns.</S>\n    <S sid=\"120\" ssid=\"39\">The CD and DT confusion is a result of our prototype list not containing a contains-digit prototype for CD, so the predicate fails to be linked to CDs.</S>\n    <S sid=\"121\" ssid=\"40\">Of course in a realistic, iterative design setting, we could have altered the prototype list to include a contains-digit prototype for CD and corrected this confusion.</S>\n    <S sid=\"122\" ssid=\"41\">Figure 6 shows the marginal posterior distribution over label pairs (roughly, the bigram transition matrix) according to the treebank labels and the PROTO+SIM model run over the training set (using a collapsed tag set for space).</S>\n    <S sid=\"123\" ssid=\"42\">Note that the broad structure is recovered to a reasonable degree.</S>\n    <S sid=\"124\" ssid=\"43\">It is difficult to compare our results to other systems which utilize a full or partial tagging dictionary, since the amount of provided knowledge is substantially different.</S>\n    <S sid=\"125\" ssid=\"44\">The best comparison is to Smith and Eisner (2005) who use a partial tagging dictionary.</S>\n    <S sid=\"126\" ssid=\"45\">In order to compare with their results, we projected the tagset to the coarser set of 17 that they used in their experiments.</S>\n    <S sid=\"127\" ssid=\"46\">On 24K tokens, our PROTO+SIM model scored 82.2%.</S>\n    <S sid=\"128\" ssid=\"47\">When Smith and Eisner (2005) limit their tagging dictionary to words which occur at least twice, their best performing neighborhood model achieves 79.5%.</S>\n    <S sid=\"129\" ssid=\"48\">While these numbers seem close, for comparison, their tagging dictionary contained information about the allowable tags for 2,125 word types (out of 5,406 types) and the their system must only choose, on average, between 4.4 tags for a word.</S>\n    <S sid=\"130\" ssid=\"49\">Our prototype list, however, contains information about only 116 word types and our tagger must on average choose between 16.9 tags, a much harder task.</S>\n    <S sid=\"131\" ssid=\"50\">When Smith and Eisner (2005) include tagging dictionary entries for all words in the first half of their 24K tokens, giving tagging knowledge for 3,362 word types, they do achieve a higher accuracy of 88.1%.</S>\n    <S sid=\"132\" ssid=\"51\">We also tested our POS induction system on the Chinese POS data in the Chinese Treebank (Ircs, 2002).</S>\n    <S sid=\"133\" ssid=\"52\">The model is wholly unmodified from the English version except that the suffix features are removed since, in Chinese, suffixes are not a reliable indicator of part-of-speech as in English (Tseng et al., 2005).</S>\n    <S sid=\"134\" ssid=\"53\">Since we did not have access to a large auxiliary unlabeled corpus that was segmented, our distributional model was built only from the treebank text, and the distributional similarities are presumably degraded relative to the English.</S>\n    <S sid=\"135\" ssid=\"54\">On 60K word tokens, BASE gave an accuracy of 34.4, PROTO gave 39.0, and PROTO+SIM gave 57.4, similar in order if not magnitude to the English case.</S>\n    <S sid=\"136\" ssid=\"55\">We believe the performance for Chinese POS tagging is not as high as English for two reasons: the general difficulty of Chinese POS tagging (Tseng et al., 2005) and the lack of a larger segmented corpus from which to build distributional models.</S>\n    <S sid=\"137\" ssid=\"56\">Nonetheless, the addition of distributional similarity features does reduce the error rate by 35% from BASE.</S>\n    <S sid=\"138\" ssid=\"57\">We tested our framework on the CLASSIFIEDS data described in Grenager et al. (2005) under conditions similar to POS tagging.</S>\n    <S sid=\"139\" ssid=\"58\">An important characteristic of this domain (see figure 1(a)) is that the hidden labels tend to be “sticky,” in that fields tend to consist of runs of the same label, as in figure 1(c), in contrast with part-of-speech tagging, where we rarely see adjacent tokens given the same label.</S>\n    <S sid=\"140\" ssid=\"59\">Grenager et al. (2005) report that in order to learn this “sticky” structure, they had to alter the structure of their HMM so that a fixed mass is placed on each diagonal transition.</S>\n    <S sid=\"141\" ssid=\"60\">In this work, we learned this structure automatically though prototype similarity features without manually constraining the model (see</S>\n  </SECTION>\n  <SECTION title=\"ROOMATES ROOMATES ROOMATES UTILITIES UTILITIES UTILITIES RESTRICTIONS RESTRICTIONS RESTRICTIONS AVAILABLE AVAILABLE AVAILABLE SIZE SIZE SIZE PHOTOS PHOTOS PHOTOS RENT RENT RENT FEATURES FEATURES FEATURES CONTACT CONTACT CONTACT NEIGHBORHOOD NEIGHBORHOOD NEIGHBORHOOD ADDRESS ADDRESS ADDRESS\" number=\"6\">\n    <S sid=\"142\" ssid=\"1\">figure 8), though we did change the similarity function (see below).</S>\n    <S sid=\"143\" ssid=\"2\">On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)’s unsupervised HMM baseline.</S>\n    <S sid=\"144\" ssid=\"3\">Adding the prototype list (see figure 2) without distributional features yielded a slightly improved accuracy of 53.7%.</S>\n    <S sid=\"145\" ssid=\"4\">For this domain, we utilized a slightly different notion of distributional similarity: we are not interested in the syntactic behavior of a word type, but its topical content.</S>\n    <S sid=\"146\" ssid=\"5\">Therefore, when we collect context vectors for word types in this domain, we make no distinction by direction or distance and collect counts from a wider window.</S>\n    <S sid=\"147\" ssid=\"6\">This notion of distributional similarity is more similar to latent semantic indexing (Deerwester et al., 1990).</S>\n    <S sid=\"148\" ssid=\"7\">A natural consequence of this definition of distributional similarity is that many neighboring words will share the same prototypes.</S>\n    <S sid=\"149\" ssid=\"8\">Therefore distributional prototype features will encourage labels to persist, naturally giving the “sticky” effect of the domain.</S>\n    <S sid=\"150\" ssid=\"9\">Adding distributional similarity features to our model (PROTO+SIM) improves accuracy substantially, yielding 71.5%, a 38.4% error reduction over BASE.6 Another feature of this domain that Grenager et al. (2005) take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one.</S>\n    <S sid=\"151\" ssid=\"10\">Grenager et al. (2005) experiment with manually adding boundary states and biasing transitions from these states to not self-loop.</S>\n    <S sid=\"152\" ssid=\"11\">We capture this “boundary” effect by simply adding a line to our protoype-list, adding a new BOUNDARY state (see figure 2) with a few (hand-chosen) prototypes.</S>\n    <S sid=\"153\" ssid=\"12\">Since we utilize a trigram tagger, we are able to naturally capture the effect that the BOUNDARY tokens typically indicate transitions between the fields before and after the boundary token.</S>\n    <S sid=\"154\" ssid=\"13\">As a post-processing step, when a token is tagged as a BOUNDARY</S>\n  </SECTION>\n  <SECTION title=\"FEATURES SIZE 11.2 FEATURES NBRHD 9.0 SIZE FEATURES 7.7 NBRHD FEATURES 6.4 ADDRESS NBRHD 5.3 UTILITIES FEATURES 5.3\" number=\"7\">\n    <S sid=\"155\" ssid=\"1\">token it is given the same label as the previous non-BOUNDARY token, which reflects the annotational convention that boundary tokens are given the same label as the field they terminate.</S>\n    <S sid=\"156\" ssid=\"2\">Adding the BOUNDARY label yields significant improvements, as indicated by the PROTO+SIM+BOUND setting in Table 5.3, surpassing the best unsupervised result of Grenager et al. (2005) which is 72.4%.</S>\n    <S sid=\"157\" ssid=\"3\">Furthermore, our PROTO+SIM+BOUND model comes close to the supervised HMM accuracy of 74.4% reported in Grenager et al. (2005).</S>\n    <S sid=\"158\" ssid=\"4\">We also compared our method to the most basic semi-supervised setting, where fully labeled documents are provided along with unlabeled ones.</S>\n    <S sid=\"159\" ssid=\"5\">Roughly 25% of the data had to be labeled in order to achieve an accuracy equal to our PROTO+SIM+BOUND model, suggesting that the use of prior knowledge in the prototype system is particularly efficient.</S>\n    <S sid=\"160\" ssid=\"6\">In table 5.3, we provide the top confusions made by our PROTO+SIM+BOUND model.</S>\n    <S sid=\"161\" ssid=\"7\">As can be seen, many of our confusions involve the FEATURE field, which serves as a general purpose background state, which often differs subtly from other fields such as SIZE.</S>\n    <S sid=\"162\" ssid=\"8\">For instance, the parenthical comment: ( master has walk - in closet with vanity ) is labeled as a SIZE field in the data, but our model proposed it as a FEATURE field.</S>\n    <S sid=\"163\" ssid=\"9\">NEIGHBORHOOD and ADDRESS is another natural confusion resulting from the fact that the two fields share much of the same vocabulary (e.g [ADDRESS 2525 Telegraph Ave.] vs. [NBRHD near Telegraph]).</S>\n    <S sid=\"164\" ssid=\"10\">Acknowledgments We would like to thank the anonymous reviewers for their comments.</S>\n    <S sid=\"165\" ssid=\"11\">This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Intel.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusions\" number=\"8\">\n    <S sid=\"166\" ssid=\"1\">We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.</S>\n    <S sid=\"167\" ssid=\"2\">These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.</S>\n    <S sid=\"168\" ssid=\"3\">Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "195c9d61979045ddc680687ae36da6d9",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Prototype-Driven Learning For Sequence Models"
                }
            }
        },
        {
            "index": 5,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.",
                    "filename": "Linguistic_Regularities_in_Continuous_Space_Word_Representations",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/N13-1090/Documents_xml/N13-1090.xml",
                    "title": "Linguistic Regularities in Continuous Space Word Representations"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores linguistic regularities in continuous space word representations generated by neural network language models. The authors utilize a vector offset method to identify syntactic and semantic regularities in word representations, finding that recurrent neural network language models (RNNLM) excel at capturing these regularities. The study includes a new dataset for measuring syntactic performance, achieving nearly 40% accuracy, and also evaluates semantic generalization on the SemEval 2012 task, outperforming the previous state-of-the-art. The results highlight the effectiveness of RNNLM in capturing linguistic regularities, with the model achieving better results in both syntactic and semantic tasks than previously best performing systems. The authors note that these results are achieved through unsupervised maximum likelihood training on a large amount of text data.",
                        "This paper explores linguistic regularities in continuous space word representations using neural network language models. The authors demonstrate that the learned word representations capture meaningful syntactic and semantic regularities, which are observed as constant vector offsets between pairs of words sharing a particular relationship. The paper presents a vector offset method for identifying these regularities and evaluates its performance on both syntactic and semantic tasks. The results show that the word representations learned by a recurrent neural network language model (RNNLM) are particularly effective in capturing these regularities, outperforming previous state-of-the-art methods. The study provides new insights into the capabilities of word representations learned through unsupervised maximum likelihood training, highlighting their ability to generalize syntactic and semantic relationships.",
                        "This paper explores the linguistic regularities captured in continuous space word representations learned by recurrent neural network language models. The authors highlight the ability of these models to generalize and capture syntactic and semantic regularities in a simple way. They propose a vector offset method based on cosine distance that is remarkably effective for identifying these regularities. The paper presents a test set for measuring syntactic performance and evaluates the model's semantic generalization using the SemEval 2012 task, achieving state-of-the-art results in both cases. The authors also compare the performance of their method with other systems and find that the RNN-based representations outperform previous methods. Surprisingly, these results are achieved through unsupervised maximum likelihood training on a large amount of text data. Overall, this study demonstrates the effectiveness of RNN language models in capturing and utilizing linguistic regularities in continuous space word representations.",
                        "This paper explores the linguistic regularities in continuous space word representations learned by a recurrent neural network language model. The study finds that the learned word representations capture meaningful syntactic and semantic regularities in a simple way. The paper presents a vector offset method based on cosine distance, which proves to be remarkably effective in solving analogy questions related to both syntactic and semantic relations. Experimental results show that the word representations generated by the recurrent neural network toolkit outperform other methods in capturing syntactic regularities and semantic generalization. The study also presents a new dataset for measuring syntactic performance and evaluates semantic generalization on the SemEval 2012 task, outperforming previous state-of-the-art methods. The findings suggest that the linguistic regularities in the word representations are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data."
                    ],
                    "filename": "linguistic_regularities_in_conti-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 154 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Linguistic Regularities in Continuous Space Word Representations</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"8\" ssid=\"1\">A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.</S>\n    <S sid=\"9\" ssid=\"2\">In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.</S>\n    <S sid=\"10\" ssid=\"3\">As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.</S>\n    <S sid=\"11\" ssid=\"4\">Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.</S>\n    <S sid=\"12\" ssid=\"5\">By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks.</S>\n    <S sid=\"13\" ssid=\"6\">This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.</S>\n    <S sid=\"14\" ssid=\"7\">In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way.</S>\n    <S sid=\"15\" ssid=\"8\">Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.</S>\n    <S sid=\"16\" ssid=\"9\">For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple−xapples ≈ xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and so on.</S>\n    <S sid=\"17\" ssid=\"10\">Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.</S>\n    <S sid=\"18\" ssid=\"11\">Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows.</S>\n    <S sid=\"19\" ssid=\"12\">In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"20\" ssid=\"1\">Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990).</S>\n    <S sid=\"21\" ssid=\"2\">More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words.</S>\n    <S sid=\"22\" ssid=\"3\">These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S>\n    <S sid=\"23\" ssid=\"4\">This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models.</S>\n    <S sid=\"24\" ssid=\"5\">This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S>\n    <S sid=\"25\" ssid=\"6\">Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations.</S>\n  </SECTION>\n  <SECTION title=\"3 Recurrent Neural Network Model\" number=\"3\">\n    <S sid=\"26\" ssid=\"1\">The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1.</S>\n    <S sid=\"27\" ssid=\"2\">This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices.</S>\n    <S sid=\"28\" ssid=\"3\">The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words.</S>\n    <S sid=\"29\" ssid=\"4\">The hidden layer s(t) maintains a representation of the sentence history.</S>\n    <S sid=\"30\" ssid=\"5\">The input vector w(t) and the output vector y(t) have dimensionality of the vocabulary.</S>\n    <S sid=\"31\" ssid=\"6\">The values in the hidden and output layers are computed as follows: where In this framework, the word representations are found in the columns of U, with each column representing a word.</S>\n    <S sid=\"32\" ssid=\"7\">The RNN is trained with backpropagation to maximize the data log-likelihood under the model.</S>\n    <S sid=\"33\" ssid=\"8\">The model itself has no knowledge of syntax or morphology or semantics.</S>\n    <S sid=\"34\" ssid=\"9\">Remarkably, training such a purely lexical model to maximize likelihood will induce word representations with striking syntactic and semantic properties.</S>\n  </SECTION>\n  <SECTION title=\"4 Measuring Linguistic Regularity\" number=\"4\">\n    <S sid=\"35\" ssid=\"1\">To understand better the syntactic regularities which are inherent in the learned representation, we created a test set of analogy questions of the form “a is to b as c is to ” testing base/comparative/superlative forms of adjectives; singular/plural forms of common nouns; possessive/non-possessive forms of common nouns; and base, past and 3rd person present tense forms of verbs.</S>\n    <S sid=\"36\" ssid=\"2\">More precisely, we tagged 267M words of newspaper text with Penn Treebank POS tags (Marcus et al., 1993).</S>\n    <S sid=\"37\" ssid=\"3\">We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB).</S>\n    <S sid=\"38\" ssid=\"4\">We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1.</S>\n    <S sid=\"39\" ssid=\"5\">The total test set size is 8000.</S>\n    <S sid=\"40\" ssid=\"6\">The test set is available online.</S>\n    <S sid=\"41\" ssid=\"7\">1 In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information.</S>\n    <S sid=\"42\" ssid=\"8\">The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing.</S>\n    <S sid=\"43\" ssid=\"9\">Each relation is exemplified by 3 or 4 gold word pairs.</S>\n    <S sid=\"44\" ssid=\"10\">Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds.</S>\n    <S sid=\"45\" ssid=\"11\">This can be viewed as another analogy problem.</S>\n    <S sid=\"46\" ssid=\"12\">For example, take the ClassInclusion:Singular Collective relation with the prototypical word pair clothing:shirt.</S>\n    <S sid=\"47\" ssid=\"13\">To measure the degree that a target word pair dish:bowl has the same relation, we form the analogy “clothing is to shirt as dish is to bowl,” and ask how valid it is.</S>\n  </SECTION>\n  <SECTION title=\"5 The Vector Offset Method\" number=\"5\">\n    <S sid=\"48\" ssid=\"1\">As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions.</S>\n    <S sid=\"49\" ssid=\"2\">We have found that a simple vector offset method based on cosine distance is remarkably effective in solving these questions.</S>\n    <S sid=\"50\" ssid=\"3\">In this method, we assume relationships are present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset.</S>\n    <S sid=\"51\" ssid=\"4\">This is illustrated in Figure 2.</S>\n    <S sid=\"52\" ssid=\"5\">In this model, to answer the analogy question a:b c:d where d is unknown, we find the embedding vectors xa, xb, xc (all normalized to unit norm), and compute y = xb − xa + xc. y is the continuous space representation of the word we expect to be the best answer.</S>\n    <S sid=\"53\" ssid=\"6\">Of course, no word might exist at that exact position, so we then search for the word whose embedding vector has the greatest cosine similarity to y and output it: provided.</S>\n    <S sid=\"54\" ssid=\"7\">We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations.</S>\n    <S sid=\"55\" ssid=\"8\">We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd).</S>\n  </SECTION>\n  <SECTION title=\"6 Experimental Results\" number=\"6\">\n    <S sid=\"56\" ssid=\"1\">To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012).</S>\n    <S sid=\"57\" ssid=\"2\">Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.</S>\n    <S sid=\"58\" ssid=\"3\">The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.</S>\n    <S sid=\"59\" ssid=\"4\">Table 2 shows results for both RNNLM and LSA vectors on the syntactic task.</S>\n    <S sid=\"60\" ssid=\"5\">LSA was trained on the same data as the RNN.</S>\n    <S sid=\"61\" ssid=\"6\">We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.</S>\n    <S sid=\"62\" ssid=\"7\">2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.</S>\n    <S sid=\"63\" ssid=\"8\">This resulted in a 36k word vocabulary, and a test set with 6632 questions.</S>\n    <S sid=\"64\" ssid=\"9\">Turian’s Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of (Mnih and Hinton, 2009) do essentially as well as the RNN vectors.</S>\n    <S sid=\"65\" ssid=\"10\">These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method.</S>\n    <S sid=\"66\" ssid=\"11\">We conducted similar experiments with the semantic test set.</S>\n    <S sid=\"67\" ssid=\"12\">For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score.</S>\n    <S sid=\"68\" ssid=\"13\">The results are evaluated using the two standard metrics defined in the task, Spearman’s rank correlation coefficient p and MaxDiff accuracy.</S>\n    <S sid=\"69\" ssid=\"14\">In both cases, larger values are better.</S>\n    <S sid=\"70\" ssid=\"15\">To compare to previous systems, we report the average over all 69 relations in the test set.</S>\n    <S sid=\"71\" ssid=\"16\">From Table 4, we see that as with the syntactic regularity study, the RNN-based representations perform best.</S>\n    <S sid=\"72\" ssid=\"17\">In this case, however, Turian’s CW vectors are comparable in performance to the HLBL vectors.</S>\n    <S sid=\"73\" ssid=\"18\">With the RNN vectors, the performance improves as the number of dimensions increases.</S>\n    <S sid=\"74\" ssid=\"19\">Surprisingly, we found that even though the RNN vectors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN640 &amp; RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"7\">\n    <S sid=\"75\" ssid=\"1\">We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations.</S>\n    <S sid=\"76\" ssid=\"2\">We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities.</S>\n    <S sid=\"77\" ssid=\"3\">We present a new dataset for measuring syntactic performance, and achieve almost 40% correct.</S>\n    <S sid=\"78\" ssid=\"4\">We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.</S>\n    <S sid=\"79\" ssid=\"5\">Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "dace8ac16046887958a96d6bd49b1028",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Linguistic Regularities in Continuous Space Word Representations"
                }
            }
        },
        {
            "index": 6,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech tagging with fairly high accuracy. The same method can be applied at a higher level of textual interpretation for locating chunks in the tagged text, including non-recursive &quot;baseNP&quot; chunks. For this purpose, it is convenient to view chunking as a tagging problem by encoding the chunk structure in new tags attached to each word. In automatic tests using Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some interesting adaptations to the transformation-based learning approach are also suggested by this application.",
                    "filename": "Text_Chunking_Using_Transformation-Based_Learning",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W95-0107/Documents_xml/W95-0107.xml",
                    "title": "Text Chunking Using Transformation-Based Learning"
                },
                "llm": {
                    "descriptions": [
                        "This academic paper discusses the application of transformation-based learning to text chunking. Text chunking involves segmenting sentences into nonoverlapping segments based on superficial analysis. The paper explores the usage of transformational learning mechanism for a higher level of chunk structure and the challenges and optimizations required for this task. The authors present the results of their experiments, including the use of chunk structure tags, lexical rule templates, and rule disabling to improve system performance. The paper concludes by outlining future directions for improving the system's power and exploring larger scale structures. Overall, the study demonstrates the feasibility and usefulness of transformation-based learning for text chunking.",
                        "This academic paper discusses the application of transformation-based learning to text chunking. Text chunking involves dividing sentences into nonoverlapping segments, and this paper explores how to target a higher level of chunk structure using transformation-based learning. The authors discuss various algorithm design issues and the results obtained from their experiments. They also outline future directions for possible improvements in the system's ability to distinguish linguistic contexts. The paper concludes by demonstrating that transformation-based learning can be used to automatically induce a chunking model that achieves high recall and precision for different types of text chunks.",
                        "This academic paper discusses text chunking using transformation-based learning. It outlines the approach and applies it to two different chunking tasks - baseNP and partitioning N and V chunks. The paper describes the transformation-based learning paradigm, algorithm design issues, and presents the results of the experiments conducted. The results indicate that this approach achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks, showing promise for textual interpretation and extraction of subunits like index terms. The paper concludes by suggesting future directions and acknowledging the contributions of others in the field.",
                        "This academic paper discusses the application of transformation-based learning to the text chunking process. The study focuses on the encoding of text chunking as a tagging problem, allowing for the automatic induction of a chunking model that achieves high recall and precision for baseNP and partitioning N and V chunks. The paper examines the algorithm design issues, results, and future directions for the technique. Furthermore, it suggests potential variations in the transformation-based learning algorithm that may be useful in other settings. The findings demonstrate the feasibility and usefulness of chunking models as a foundation for textual interpretation and subunit extraction."
                    ],
                    "filename": "text_chunking_using_transformati-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 110 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Text Chunking Using Transformation-Based Learning</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Text chunking involves dividing sentences into nonoverlapping segments on the basis of fairly superficial analysis.</S>\n    <S sid=\"7\" ssid=\"2\">Abney (1991) has proposed this as a useful and relatively tractable precursor to full parsing, since it provides a foundation for further levels of analysis including verb-argument identification, while still allowing more complex attachment decisions to be postponed to a later phase.</S>\n    <S sid=\"8\" ssid=\"3\">Since chunking includes identifying the non-recursive portions of noun phrases, it can also be useful for other purposes including index term generation.</S>\n    <S sid=\"9\" ssid=\"4\">Most efforts at superficially extracting segments from sentences have focused on identifying low-level noun groups, either using hand-built grammars and finite state techniques or using statistical models like HMMs trained from corpora.</S>\n    <S sid=\"10\" ssid=\"5\">In this paper, we target a somewhat higher level of chunk structure using Brill's (1993b) transformation-based learning mechanism, in which a sequence of transformational rules is learned from a corpus; this sequence iteratively improves upon a baseline model for some interpretive feature of the text.</S>\n    <S sid=\"11\" ssid=\"6\">This technique has previously been used not only for part-of-speech tagging (Brill, 1994), but also for prepositional phrase attachment disambiguation (Brill and Resnik, 1994), and assigning unlabeled binary-branching tree structure to sentences (Brill, 1993a).</S>\n    <S sid=\"12\" ssid=\"7\">Because transformation-based learning uses patternaction rules based on selected features of the local context, it is helpful for the values being predicted to also be encoded locally.</S>\n    <S sid=\"13\" ssid=\"8\">In the text-chunking application, encoding the predicted chunk structure in tags attached to the words, rather than as brackets between words, avoids many of the difficulties with unbalanced bracketings that would result if such local rules were allowed to insert or alter inter-word brackets directly.</S>\n    <S sid=\"14\" ssid=\"9\">In this study, training and test sets marked with two different types of chunk structure were derived algorithmically from the parsed data in the Penn Treebank corpus of Wall Street Journal text (Marcus et al., 1994).</S>\n    <S sid=\"15\" ssid=\"10\">The source texts were then run through Brill's part-of-speech tagger (Brill, 1993c), and, as a baseline heuristic, chunk structure tags were assigned to each word based on its part-of-speech tag.</S>\n    <S sid=\"16\" ssid=\"11\">Rules were then automatically learned that updated these chunk structure tags based on neighboring words and their part-of-speech and chunk tags.</S>\n    <S sid=\"17\" ssid=\"12\">Applying transformation-based learning to text chunking turns out to be different in interesting ways from its use for part-of-speech tagging.</S>\n    <S sid=\"18\" ssid=\"13\">The much smaller tagset calls for a different organization of the computation, and the fact that part-of-speech assignments as well as word identities are fixed suggests different optimizations.</S>\n  </SECTION>\n  <SECTION title=\"2 Text Chunking\" number=\"2\">\n    <S sid=\"19\" ssid=\"1\">Abney (1991) has proposed text chunking as a useful preliminary step to parsing.</S>\n    <S sid=\"20\" ssid=\"2\">His chunks are inspired in part by psychological studies of Gee and Grosjean (1983) that link pause durations in reading and naive sentence diagraming to text groupings that they called 0-phrases, which very roughly correspond to breaking the string after each syntactic head that is a content word.</S>\n    <S sid=\"21\" ssid=\"3\">Abney's other motivation for chunking is procedural, based on the hypothesis that the identification of chunks can be done fairly dependably by finite state methods, postponing the decisions that require higher-level analysis to a parsing phase that chooses how to combine the chunks.</S>\n    <S sid=\"22\" ssid=\"4\">Existing efforts at identifying chunks in text have been focused primarily on low-level noun group identification, frequently as a step in deriving index terms, motivated in part by the limited coverage of present broad-scale parsers when dealing with unrestricted text.</S>\n    <S sid=\"23\" ssid=\"5\">Some researchers have applied grammar-based methods, combining lexical data with finite state or other grammar constraints, while others have worked on inducing statistical models either directly from the words or from automatically assigned part-of-speech classes.</S>\n    <S sid=\"24\" ssid=\"6\">On the grammar-based side, Bourigault (1992) describes a system for extracting &amp;quot;terminological noun phrases&amp;quot; from French text.</S>\n    <S sid=\"25\" ssid=\"7\">This system first uses heuristics to find &amp;quot;maximal length noun phrases&amp;quot;, and then uses a grammar to extract &amp;quot;terminological units.&amp;quot; For example, from the maximal NP le disque dur de la station de travail it extracts the two terminological phrases disque dur, and station de travail.</S>\n    <S sid=\"26\" ssid=\"8\">Bourigault claims that the grammar can parse &amp;quot;around 95% of the maximal length noun phrases&amp;quot; in a test corpus into possible terminological phrases, which then require manual validation.</S>\n    <S sid=\"27\" ssid=\"9\">However, because its goal is terminological phrases, it appears that this system ignores NP chunk-initial determiners and other initial prenominal modifiers, somewhat simplifying the parsing task.</S>\n    <S sid=\"28\" ssid=\"10\">Voutilainen (1993), in his impressive NPtool system, uses an approach that is in some ways similar to the one used here, in that he adds to his part-of-speech tags a new kind of tag that shows chunk structure; the chunk tag &amp;quot;©&gt;N&amp;quot;, for example, is used for determiners and premodifiers, both of which group with the following noun head.</S>\n    <S sid=\"29\" ssid=\"11\">He uses a lexicon that lists all the possible chunk tags for each word combined with hand-built constraint grammar patterns.</S>\n    <S sid=\"30\" ssid=\"12\">These patterns eliminate impossible readings to identify a somewhat idiosyncratic kind of target noun group that does not include initial determiners but does include postmodifying prepositional phrases (including determiners).</S>\n    <S sid=\"31\" ssid=\"13\">Voutilainen claims recall rates of 98.5% or better with precision of 95% or better.</S>\n    <S sid=\"32\" ssid=\"14\">However, the sample NPtool analysis given in the appendix of (Voutilainen, 1993), appears to be less accurate than claimed in general, with 5 apparent mistakes (and one unresolved ambiguity) out of the 32 NP chunks in that sample, as listed in Table 1.</S>\n    <S sid=\"33\" ssid=\"15\">These putative errors, combined with the claimed high performance, suggest that NPtool's definition of NP chunk is also tuned for extracting terminological phrases, and thus excludes many kinds of NP premodifiers, again simplifying the chunking task.</S>\n    <S sid=\"34\" ssid=\"16\">NPtool parse Apparent correct parse less [time] [less time] the other hand • the [other hand] many [advantages] [many advantages] [binary addressing] [binary addressing and and [instruction formats] instruction formats] a purely [binary computer] a [purely binary computer] Kupiec (1993) also briefly mentions the use of finite state NP recognizers for both English and French to prepare the input for a program that identified the correspondences between NPs in bilingual corpora, but he does not directly discuss their performance.</S>\n    <S sid=\"35\" ssid=\"17\">Using statistical methods, Church's Parts program (1988), in addition to identifying parts of speech, also inserted brackets identifying core NPs.</S>\n    <S sid=\"36\" ssid=\"18\">These brackets were placed using a statistical model trained on Brown corpus material in which NP brackets had been inserted semi-automatically.</S>\n    <S sid=\"37\" ssid=\"19\">In the small test sample shown, this system achieved 98% recall for correct brackets.</S>\n    <S sid=\"38\" ssid=\"20\">At about the same time, Ejerhed (1988), working with Church, performed comparisons between finite state methods and Church's stochastic models for identifying both non-recursive clauses and non-recursive NPs in English text.</S>\n    <S sid=\"39\" ssid=\"21\">In those comparisons, the stochastic methods outperformed the hand built finite-state models, with claimed accuracies of 93.5% (clauses) and 98.6% (NPs) for the statistical models compared to to 87% (clauses) and 97.8% (NPs) for the finite-state methods.</S>\n    <S sid=\"40\" ssid=\"22\">Running Church's program on test material, however, reveals that the definition of NP embodied in Church's program is quite simplified in that it does not include, for example, structures or words conjoined within NP by either explicit conjunctions like &amp;quot;and&amp;quot; and &amp;quot;or&amp;quot;, or implicitly by commas.</S>\n    <S sid=\"41\" ssid=\"23\">Church's chunker thus assigns the following NP chunk structures: [a Skokie] , [Ill.] , [subsidiary] [newer] , [big-selling prescriptions drugs] [the inefficiency] , [waste] and [lack] of [coordination] [Kidder] , [Peabody] Sz [Co] It is difficult to compare performance figures between studies; the definitions of the target chunks and the evaluation methodologies differ widely and are frequently incompletely specified.</S>\n    <S sid=\"42\" ssid=\"24\">All of the cited performance figures above also appear to derive from manual checks by the investigators of the system's predicted output, and it is hard to estimate the impact of the system's suggested chunking on the judge's determination.</S>\n    <S sid=\"43\" ssid=\"25\">We believe that the work reported here is the first study which has attempted to find NP chunks subject only to the limitation that the structures recognized do not include recursively embedded NPs, and which has measured performance by automatic comparison with a preparsed corpus.</S>\n    <S sid=\"44\" ssid=\"26\">We performed experiments using two different chunk structure targets, one that tried to bracket non-recursive &amp;quot;baseNPs&amp;quot; and one that partitioned sentences into non-overlapping N-type and V-type chunks, loosely following Abney's model.</S>\n    <S sid=\"45\" ssid=\"27\">Training and test materials with chunk tags encoding each of these kinds of structure were derived automatically from the parsed Wall Street Journal text in the Penn Treebank (Marcus et al., 1994).</S>\n    <S sid=\"46\" ssid=\"28\">While this automatic derivation process introduced a small percentage of errors of its own, it was the only practical way both to provide the amount of training data required and to allow for fully-automatic testing.</S>\n    <S sid=\"47\" ssid=\"29\">The goal of the &amp;quot;baseNP&amp;quot; chunks was to identify essentially the initial portions of nonrecursive noun phrases up to the head, including determiners but not including postmodifying prepositional phrases or clauses.</S>\n    <S sid=\"48\" ssid=\"30\">These chunks were extracted from the Treebank parses, basically by selecting NPs that contained no nested NPs1.</S>\n    <S sid=\"49\" ssid=\"31\">The handling of conjunction followed that of the Treebank annotators as to whether to show separate baseNPs or a single baseNP spanning the conjunction2.</S>\n    <S sid=\"50\" ssid=\"32\">Possessives were treated as a special case, viewing the possessive marker as the first word of a new baseNP, thus flattening the recursive structure in a useful way.</S>\n    <S sid=\"51\" ssid=\"33\">The following sentences give examples of this baseNP chunk structure: During [N the third quarter N] , [N Compaq N] purchased [N a former Wang Laboratories manufacturing facility N] in [N Sterling N], [N Scotland N] , which will be used for [N international service and repair operations N] .</S>\n    <S sid=\"52\" ssid=\"34\">[N The government NJ has [N other agencies and instruments N] for pursuing [N these other objectives N] .</S>\n    <S sid=\"53\" ssid=\"35\">Even [N Mao Tse-tung iv] [N 's China Ad began in [N 1949 N] with [N a partnership N] between [N the communists N] and [N a number N] of [N smaller , non-communist parties N] • The chunks in the partitioning chunk experiments were somewhat closer to Abney's model, where the prepositions in prepositional phrases are included with the object NP up to the head in a single N-type chunk.</S>\n    <S sid=\"54\" ssid=\"36\">This created substantial additional ambiguity for the system, which had to distinguish prepositions from particles.</S>\n    <S sid=\"55\" ssid=\"37\">The handling of conjunction again follows the Treebank parse with nominal conjuncts parsed in the Treebank as a single NP forming a single N chunk, while those parsed as conjoined NPs become separate chunks, with any coordinating conjunctions attached like prepositions to the following N chunk.</S>\n    <S sid=\"56\" ssid=\"38\">The portions of the text not involved in N-type chunks were grouped as chunks termed Vtype, though these &amp;quot;V&amp;quot; chunks included many elements that were not verbal, including adjective phrases.</S>\n    <S sid=\"57\" ssid=\"39\">The internal structure of these V-type chunks loosely followed the Treebank parse, though V chunks often group together elements that were sisters in the underlying parse tree.</S>\n    <S sid=\"58\" ssid=\"40\">Again, the possessive marker was viewed as initiating a new N-type chunk.</S>\n    <S sid=\"59\" ssid=\"41\">The following sentences are annotated with these partitioning N and V chunks: [N Some bankers NI [v are reporting v] [N more inquiries than usual N] [N about CDs N] [N since Friday NJ .</S>\n    <S sid=\"60\" ssid=\"42\">'This heuristic fails in some cases.</S>\n    <S sid=\"61\" ssid=\"43\">For example, Treebank uses the label NAC for some NPs functioning as premodifiers, like &amp;quot;Bank of England&amp;quot; in &amp;quot;Robin Leigh-Pemberton, Bank of England governor, conceded..&amp;quot;; in such cases, &amp;quot;governor&amp;quot; is not included in any baseNP chunk.</S>\n    <S sid=\"62\" ssid=\"44\">'Non-constituent NP conjunction, which Treebank labels NX, is another example that still causes problems.</S>\n    <S sid=\"63\" ssid=\"45\">[N Eastern Airlines N] [N 'creditors N] [v have begun exploring v] [N alternative approaches N] [N to a Chapter 11 reorganization N] [v because v] [N they N][v are unhappy v] [N with the carrier N] [N 's latest proposal N] .</S>\n    <S sid=\"64\" ssid=\"46\">[N Indexing NI [N for the most part N] [v has involved simply buying v] [v and then holding v] [N stocks NI [N in the correct mix N] [v to mirror Id [N a stock market barometer N] .</S>\n    <S sid=\"65\" ssid=\"47\">These two kinds of chunk structure derived from the Treebank data were encoded as chunk tags attached to each word and provided the targets for the transformation-based learning.</S>\n  </SECTION>\n  <SECTION title=\"3 The Transformation-based Learning Paradigm\" number=\"3\">\n    <S sid=\"66\" ssid=\"1\">As shown in Fig.</S>\n    <S sid=\"67\" ssid=\"2\">1, transformation-based learning starts with a supervised training corpus that specifies the correct values for some linguistic feature of interest, a baseline heuristic for predicting initial values for that feature, and a set of rule templates that determine a space of possible transformational rules.</S>\n    <S sid=\"68\" ssid=\"3\">The patterns of the learned rules match to particular combinations of features in the neighborhood surrounding a word, and their action is to change the system's current guess as to the feature for that word.</S>\n    <S sid=\"69\" ssid=\"4\">To learn a model, one first applies the baseline heuristic to produce initial hypotheses for each site in the training corpus.</S>\n    <S sid=\"70\" ssid=\"5\">At each site where this baseline prediction is not correct, the templates are then used to form instantiated candidate rules with patterns that test selected features in the neighborhood of the word and actions that correct the currently incorrect tag assignment.</S>\n    <S sid=\"71\" ssid=\"6\">This process eventually identifies all the rule candidates generated by that template set that would have a positive effect on the current tag assignments anywhere in the corpus.</S>\n    <S sid=\"72\" ssid=\"7\">Those candidate rules are then tested against the rest of corpus, to identify at how many locations they would cause negative changes.</S>\n    <S sid=\"73\" ssid=\"8\">One of those rules whose net score (positive changes minus negative changes) is maximal is then selected, applied to the corpus, and also written out as the first rule in the learned sequence.</S>\n    <S sid=\"74\" ssid=\"9\">This entire learning process is then repeated on the transformed corpus: deriving candidate rules, scoring them, and selecting one with the maximal positive effect.</S>\n    <S sid=\"75\" ssid=\"10\">This process is iterated, leading to an ordered sequence of rules, with rules discovered first ordered before those discovered later.</S>\n    <S sid=\"76\" ssid=\"11\">The predictions of the model on new text are determined by beginning with the baseline heuristic prediction and then applying each rule in the learned rule sequence in turn.</S>\n  </SECTION>\n  <SECTION title=\"4 Transformational Text Chunking\" number=\"4\">\n    <S sid=\"77\" ssid=\"1\">This section discusses how text chunking can be encoded as a tagging problem that can be conveniently addressed using transformational learning.</S>\n    <S sid=\"78\" ssid=\"2\">We also note some related adaptations in the procedure for learning rules that improve its performance, taking advantage of ways in which this task differs from the learning of part-of-speech tags.</S>\n    <S sid=\"79\" ssid=\"3\">Applying transformational learning to text chunking requires that the system's current hypotheses about chunk structure be represented in a way that can be matched against the pattern parts of rules.</S>\n    <S sid=\"80\" ssid=\"4\">One way to do this would be to have patterns match tree fragments and actions modify tree geometries, as in Brill's transformational parser (1993a).</S>\n    <S sid=\"81\" ssid=\"5\">In this work, we have found it convenient to do so by encoding the chunking using an additional set of tags, so that each word carries both a part-of-speech tag and also a &amp;quot;chunk tag&amp;quot; from which the chunk structure can be derived.</S>\n    <S sid=\"82\" ssid=\"6\">In the baseNP experiments aimed at non-recursive NP structures, we use the chunk tag set {I, 0, B}, where words marked I are inside some baseNP, those marked 0 are outside, and the B tag is used to mark the left most item of a baseNP which immediately follows another baseNP.</S>\n    <S sid=\"83\" ssid=\"7\">In these tests, punctuation marks were tagged in the same way as words.</S>\n    <S sid=\"84\" ssid=\"8\">In the experiments that partitioned text into N and V chunks, we use the chunk tag set {BN, N, By, V, P}, where BN marks the first word and N the succeeding words in an N-type group while BV and V play the same role for V-type groups.</S>\n    <S sid=\"85\" ssid=\"9\">Punctuation marks, which are ignored in Abney's chunk grammar, but which the Treebank data treats as normal- lexical items with their own part-of-speech tags, are unambiguously assigned the chunk tag P. Items tagged P are allowed to appear within N or V chunks; they are irrelevant as far as chunk boundaries are concerned, but they are still available to be matched against as elements of the left hand sides of rules.</S>\n    <S sid=\"86\" ssid=\"10\">Encoding chunk structure with tags attached to words rather than non-recursive bracket markers inserted between words has the advantage that it limits the dependence between different elements of the encoded representation.</S>\n    <S sid=\"87\" ssid=\"11\">While brackets must be correctly paired in order to derive a chunk structure, it is easy to define a mapping that can produce a valid chunk structure from any sequence of chunk tags; the few hard cases that arise can be handled completely locally.</S>\n    <S sid=\"88\" ssid=\"12\">For example, in the baseNP tag set, whenever a B tag immediately follows an 0, it must be treated as an I, and, in the partitioning chunk tag set, wherever a V tag immediately follows an N tag without any intervening By, it must be treated as a By.</S>\n    <S sid=\"89\" ssid=\"13\">Transformational learning begins with some initial &amp;quot;baseline&amp;quot; prediction, which here means a baseline assignment of chunk tags to words.</S>\n    <S sid=\"90\" ssid=\"14\">Reasonable suggestions for baseline heuristics after a text has been tagged for part-of-speech might include assigning to each word the chunk tag that it carried most frequently in the training set, or assigning each part-of-speech tag the chunk tag that was most frequently associated with that part-of-speech tag in the training.</S>\n    <S sid=\"91\" ssid=\"15\">We tested both approaches, and the baseline heuristic using part-of-speech tags turned out to do better, so it was the one used in our experiments.</S>\n    <S sid=\"92\" ssid=\"16\">The part-of-speech tags used by this baseline heuristic, and then later also matched against by transformational rule patterns, were derived by running the raw texts in a prepass through Brill's transformational part-of-speech tagger (Brill, 1993c).</S>\n    <S sid=\"93\" ssid=\"17\">In transformational learning, the space of candidate rules to be searched is defined by a set of rule templates that each specify a small number of particular feature sets as the relevant factors that a rule's left-hand-side pattern should examine, for example, the part-of-speech tag of the word two to the left combined with the actual word one to the left.</S>\n    <S sid=\"94\" ssid=\"18\">In the preliminary scan of the corpus for each learning pass, it is these templates that are applied to each location whose current tag is not correct, generating a candidate rule that would apply at least at that one location, matching those factors and correcting the chunk tag assignment.</S>\n    <S sid=\"95\" ssid=\"19\">When this approach is applied to part-of-speech tagging, the possible sources of evidence for templates involve the identities of words within a neighborhood of some appropriate size and their current part-of-speech tag assignments.</S>\n    <S sid=\"96\" ssid=\"20\">In the text chunking application, the tags being assigned are chunk structure tags, while the part-of-speech tags are a fixed part of the environment, like the lexical identities of the words themselves.</S>\n    <S sid=\"97\" ssid=\"21\">This additional class of available information causes a significant increase in the number of reasonable templates if templates for a wide range of the possible combinations of evidence are desired.</S>\n    <S sid=\"98\" ssid=\"22\">The distributed version of Brill's tagger (Brill, 1993c) makes use of 26 templates, involving various mixes of word and part-of-speech tests on neighboring words.</S>\n    <S sid=\"99\" ssid=\"23\">Our tests were performed using 100 templates; these included almost all of Brill's combinations, and extended them to include references to chunk tags as well as to words and part-of-speech tags.</S>\n    <S sid=\"100\" ssid=\"24\">The set of 100 rule templates used here was built from repetitions of 10 basic patterns, shown on the left side of Table 2 as they apply to words.</S>\n    <S sid=\"101\" ssid=\"25\">The same 10 patterns can also be used to match against part-of-speech tags, encoded as Po, P_1, etc.</S>\n    <S sid=\"102\" ssid=\"26\">(In other tests, we have explored mixed templates, that match against both word and part-of-speech values, but no mixed templates were used in these experiments.)</S>\n    <S sid=\"103\" ssid=\"27\">These 20 word and part-of-speech patterns were then combined with each of the 5 different chunk tag patterns shown on the right side of the table.</S>\n    <S sid=\"104\" ssid=\"28\">The cross product of the 20 word and part-of-speech patterns with the 5 chunk tag patterns determined the full set of 100 templates used.</S>\n  </SECTION>\n  <SECTION title=\"5 Algorithm Design Issues\" number=\"5\">\n    <S sid=\"105\" ssid=\"1\">The large increase in the number of rule templates in the text chunking application when compared to part-of-speech tagging pushed the training process against the available limits in terms of both space and time, particularly when combined with the desire to work with the largest possible training sets.</S>\n    <S sid=\"106\" ssid=\"2\">Various optimizations proved to be crucial to make the tests described feasible.</S>\n    <S sid=\"107\" ssid=\"3\">One change in the algorithm is related to the smaller size of the tag set.</S>\n    <S sid=\"108\" ssid=\"4\">In Brill's tagger (Brill, 1993c), an initial calculation in each pass computes the confusion matrix for the current tag assignments and sorts the entries of that [old-tag x new-tag] matrix, so that candidate rules can then be processed in decreasing order of the maximum possible benefit for any rule changing, say, old tag I to new tag J.</S>\n    <S sid=\"109\" ssid=\"5\">The search for the best-scoring rule can then be halted when a cell of the confusion matrix is reached whose maximum possible benefit is less than the net benefit of some rule already encountered.</S>\n    <S sid=\"110\" ssid=\"6\">The power of that approach is dependent on the fact that the confusion matrix for part-ofspeech tagging partitions the space of candidate rules into a relatively large number of classes, so that one is likely to be able to exclude a reasonably large portion of the search space.</S>\n    <S sid=\"111\" ssid=\"7\">In a chunk tagging application, with only 3 or 4 tags in the effective tagset, this approach based on the confusion matrix offers much less benefit.</S>\n    <S sid=\"112\" ssid=\"8\">However, even though the confusion matrix does not usefully subdivide the space of possible rules when the tag set is this small, it is still possible to apply a similar optimization by sorting the entire list of candidate rules on the basis of their positive scores, and then processing the candidate rules (which means determining their negative scores and thus their net scores) in order of decreasing positive scores.</S>\n    <S sid=\"113\" ssid=\"9\">By keeping track of the rule with maximum benefit seen so far, one can be certain of having found one of the globally best rules when one reaches candidate rules in the sorted list whose positive score is not greater than the net score of the best rule so far.</S>\n    <S sid=\"114\" ssid=\"10\">In earlier work on transformational part-of-speech tagging (Ramshaw and Marcus, 1994), we noted that it is possible to greatly speed up the learning process by constructing a full, bidirectional index linking each candidate rule to those locations in the corpus at which it applies and each location in the corpus to those candidate rules that apply there.</S>\n    <S sid=\"115\" ssid=\"11\">Such an index allows the process of applying rules to be performed without having to search through the corpus.</S>\n    <S sid=\"116\" ssid=\"12\">Unfortunately, such complete indexing proved to be too costly in terms of physical memory to be feasible in this application.</S>\n    <S sid=\"117\" ssid=\"13\">However, it is possible to construct a limited index that lists for each candidate rule those locations in the corpus at which the static portions of its left-hand-side pattern match.</S>\n    <S sid=\"118\" ssid=\"14\">Because this index involves only the stable word identity and part-of-speech tag values, it does not require updating; thus it can be stored more compactly, and it is also not necessary to maintain back pointers from corpus locations to the applicable rules.</S>\n    <S sid=\"119\" ssid=\"15\">This kind of partial static index proved to be a significant advantage in the portion of the program where candidate rules with relatively high positive scores are being tested to determine their negative scores, since it avoids the necessity of testing such rules against every location in the corpus.</S>\n    <S sid=\"120\" ssid=\"16\">We also investigated a new heuristic to speed up the computation: After each pass, we disable all rules whose positive score is significantly lower than the net score of the best rule for the current pass.</S>\n    <S sid=\"121\" ssid=\"17\">A disabled rule is then reenabled whenever enough other changes have been made to the corpus that it seems possible that the score of that rule might have changed enough to bring it back into contention for the top place.</S>\n    <S sid=\"122\" ssid=\"18\">This is done by adding some fraction of the changes made in each pass to the positive scores of the disabled rules, and reenabling rules whose adjusted positive scores came within a threshold of the net score of the successful rule on some pass.</S>\n    <S sid=\"123\" ssid=\"19\">Note that this heuristic technique introduces some risk of missing the actual best rule in a pass, due to its being incorrectly disabled at the time.</S>\n    <S sid=\"124\" ssid=\"20\">However, empirical comparisons between runs with and without rule disabling suggest that conservative use of this technique can produce an order of magnitude speedup while imposing only a very slight cost in terms of suboptimality of the resulting learned rule sequence.</S>\n  </SECTION>\n  <SECTION title=\"6 Results\" number=\"6\">\n    <S sid=\"125\" ssid=\"1\">The automatic derivation of training and testing data from the Treebank analyses allowed for fully automatic scoring, though the scores are naturally subject to any remaining systematic errors in the data derivation process as well as to bona fide parsing errors in the Treebank source.</S>\n    <S sid=\"126\" ssid=\"2\">Table 3 shows the results for the baseNP tests, and Table 4 shows the results for the partitioning chunks task.</S>\n    <S sid=\"127\" ssid=\"3\">Since training set size has a significant effect on the results, values are shown for three different training set sizes.</S>\n    <S sid=\"128\" ssid=\"4\">(The test set in all cases was 50K words.</S>\n    <S sid=\"129\" ssid=\"5\">Training runs were halted after the first 500 rules; rules learned after that point affect relatively few locations in the training set and have only a very slight effect for good or ill on test set performance.)</S>\n    <S sid=\"130\" ssid=\"6\">The first line in each table gives the performance of the baseline system, which assigned a baseNP or chunk tag to each word on the basis of the POS tag assigned in the prepass.</S>\n    <S sid=\"131\" ssid=\"7\">Performance is stated in terms of recall (percentage of correct chunks found) and precision (percentage of chunks found that are correct), where both ends of a chunk had to match exactly for it to be counted.</S>\n    <S sid=\"132\" ssid=\"8\">The raw percentage of correct chunk tags is also given for each run, and for each performance measure, the relative error reduction compared to the baseline is listed.</S>\n    <S sid=\"133\" ssid=\"9\">The partitioning chunks do appear to be somewhat harder to predict than baseNP chunks.</S>\n    <S sid=\"134\" ssid=\"10\">The higher error reduction for the former is partly due to the fact that the part-of-speech baseline for that task is much lower.</S>\n    <S sid=\"135\" ssid=\"11\">To give a sense of the kinds of rules being learned-, the first 10 rules from the 200K baseNP run are shown in Table 5.</S>\n    <S sid=\"136\" ssid=\"12\">It is worth glossing the rules, since one of the advantages of transformationbased learning is exactly that the resulting model is easily interpretable.</S>\n    <S sid=\"137\" ssid=\"13\">In the first of the baseNP rules, adjectives (with part-of-speech tag JJ) that are currently tagged I but that are followed by words tagged 0 have their tags changed to 0.</S>\n    <S sid=\"138\" ssid=\"14\">In Rule 2, determiners that are preceded by two words both tagged I have their own tag changed to B, marking the beginning of a baseNP that happens to directly follow another.</S>\n    <S sid=\"139\" ssid=\"15\">(Since the tag B is only used when baseNPs abut, the baseline system tags determiners as I.)</S>\n    <S sid=\"140\" ssid=\"16\">Rule 3 takes words which immediately follow determiners tagged I that in turn follow something tagged 0 and changes their tag to also be I.</S>\n    <S sid=\"141\" ssid=\"17\">Rules 4-6 are similar to Rule 2, marking the initial words of baseNPs that directly follow another baseNP.</S>\n    <S sid=\"142\" ssid=\"18\">Rule 7 marks conjunctions (with part-of-speech tag CC) as I if they follow an I and precede a noun, since such conjunctions are more likely to be embedded in a single baseNP than to separate two baseNPs, and Rules 8 and 9 do the same.</S>\n    <S sid=\"143\" ssid=\"19\">(The word &amp;quot;&amp;&amp;quot; in rule 8 comes mostly from company names in the Wall St. Journal source data.)</S>\n    <S sid=\"144\" ssid=\"20\">Finally, Rule 10 picks up cases like &amp;quot;including about four million shares&amp;quot; where &amp;quot;about&amp;quot; is used as a quantifier rather than preposition.</S>\n    <S sid=\"145\" ssid=\"21\">A similar list of the first ten rules for the chunk task can be seen in Table 6.</S>\n    <S sid=\"146\" ssid=\"22\">To gloss a few of these, in the first rule here, determiners (with part-of-speech tag DT), which usually begin N chunks and thus are assigned the baseline tag BN, have their chunk tags changed to N if they follow a word whose tag is also BN.</S>\n    <S sid=\"147\" ssid=\"23\">In Rule 2, sites currently tagged N but which fall at the beginning of a sentence have their tags switched to BN.</S>\n    <S sid=\"148\" ssid=\"24\">(The dummy tag Z and word ZZZ indicate that the locations one to the left are beyond the sentence boundaries.)</S>\n    <S sid=\"149\" ssid=\"25\">Rule 3 changes N to BN after a comma (which is tagged P), and in Rule 4, locations tagged BN are switched to BV if the following location is tagged V and has the part-of-speech tag VB.</S>\n    <S sid=\"150\" ssid=\"26\">The fact that this system includes lexical rule templates that refer to actual words sets it apart from approaches that rely only on part-of-speech tags to predict chunk structure.</S>\n    <S sid=\"151\" ssid=\"27\">To explore how much difference in performance those lexical rule templates make, we repeated the above test runs omitting templates that refer to specific words.</S>\n    <S sid=\"152\" ssid=\"28\">The results for these runs, in Tables 7 and 8, suggest that the lexical rules improve performance on the baseNP chunk task by about 1% (roughly 5% of the overall error reduction) and on the partitioning chunk task by about 5% (roughly 10% of the error reduction).</S>\n    <S sid=\"153\" ssid=\"29\">Thus lexical rules appear to be making a limited contribution in determining baseNP chunks, but a more significant one for the partitioning chunks.</S>\n    <S sid=\"154\" ssid=\"30\">A rough hand categorization of a sample of the errors from a baseNP run indicates that many fall into classes that are understandably difficult for any process using only local word and partof-speech patterns to resolve.</S>\n    <S sid=\"155\" ssid=\"31\">The most frequent single confusion involved words tagged VBG and VBN, whose baseline prediction given their part-of-speech tag was 0, but which also occur frequently inside baseNPs.</S>\n    <S sid=\"156\" ssid=\"32\">The system did discover some rules that allowed it to fix certain classes of VBG and VBN mistaggings, for example, rules that retagged VBNs as I when they preceded an NN or NNS tagged I.</S>\n    <S sid=\"157\" ssid=\"33\">However, many also remained unresolved, and many of those appear to be cases that would require more than local word and part-of-speech patterns to resolve.</S>\n    <S sid=\"158\" ssid=\"34\">The second most common class of errors involved conjunctions, which, combined with the former class, make up half of all the errors in the sample.</S>\n    <S sid=\"159\" ssid=\"35\">The Treebank tags the words &amp;quot;and&amp;quot; and frequently &amp;quot;,&amp;quot; with the part-of-speech tag CC, which the baseline system again predicted would fall most often outside of a baseNP3.</S>\n    <S sid=\"160\" ssid=\"36\">However, the Treebank parses do also frequently classify conjunctions of Ns or NPs as a single baseNP, and again there appear to be insufficient clues in the word and tag contexts for the current system to make the distinction.</S>\n    <S sid=\"161\" ssid=\"37\">Frequently, in fact, the actual choice of structure assigned by the Treebank annotators seemed largely dependent on semantic indications unavailable to the transformational learner.</S>\n  </SECTION>\n  <SECTION title=\"7 Future Directions\" number=\"7\">\n    <S sid=\"162\" ssid=\"1\">We are planning to explore several different paths that might increase the system's power to distinguish the linguistic contexts in which particular changes would be useful.</S>\n    <S sid=\"163\" ssid=\"2\">One such direction is to expand the template set by adding templates that are sensitive to the chunk structure.</S>\n    <S sid=\"164\" ssid=\"3\">For example, instead of referring to the word two to the left, a rule pattern could refer to the first word in the current chunk, or the last word of the previous chunk.</S>\n    <S sid=\"165\" ssid=\"4\">Another direction would be to enrich the vocabulary of chunk tags, so that they could be used during the learning process to encode contextual features for use by later rules in the sequence.</S>\n    <S sid=\"166\" ssid=\"5\">We would also like to explore applying these same kinds of techniques to building larger scale structures, in which larger units are assembled or predicate/argument structures derived by combining chunks.</S>\n    <S sid=\"167\" ssid=\"6\">One interesting direction here would be to explore the use of chunk structure tags that encode a form of dependency grammar, where the tag &amp;quot;N+2&amp;quot; might mean that the current word is to be taken as part.of the unit headed by the N two words to the right.</S>\n  </SECTION>\n  <SECTION title=\"8 Conclusions\" number=\"8\">\n    <S sid=\"168\" ssid=\"1\">By representing text chunking as a kind of tagging problem, it becomes possible to easily apply transformation-based learning.</S>\n    <S sid=\"169\" ssid=\"2\">We have shown that this approach is able to automatically induce a chunking model from supervised training that achieves recall and precision of 92% for baseNP chunks and 88% for partitioning N and V chunks.</S>\n    <S sid=\"170\" ssid=\"3\">Such chunking models provide a useful and feasible next step in textual interpretation that goes beyond part-of-speech tagging, and that serve as a foundation both for larger-scale grouping and for direct extraction of subunits like index terms.</S>\n    <S sid=\"171\" ssid=\"4\">In addition, some variations in the transformation-based learning algorithm are suggested by this application that may also be useful in other settings.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"9\">\n    <S sid=\"172\" ssid=\"1\">We would like to thank Eric Brill for making his system widely available, and Ted Briscoe and David Yarowsky for helpful comments, including the suggestion to test the system's performance without lexical rule templates.</S>\n    <S sid=\"173\" ssid=\"2\">'Note that this is one of the cases where Church's chunker allows separate NP fragments to count as chunks.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "44c8e72ff846b6b13bc4167e4f7ffb62",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Text Chunking Using Transformation-Based Learning"
                }
            }
        },
        {
            "index": 7,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We demonstrate how supervised discriminative machine learning techniques can be used to automate the assessment of ‘English as a Second or Other Language’ (ESOL) examination scripts. In particular, we use rank preference learning to explicitly model the grade relationships between scripts. A number of different features are extracted and ablation tests are used to investigate their contribution to overall performance. A comparison between regression and rank preference models further supports our method. Experimental results on the first publically available dataset show that our system can achieve levels of performance close to the upper bound for the task, as defined by the agreement between human examiners on the same corpus. Finally, using a set of ‘outlier’ texts, we test the validity of our model and identify cases where the model’s scores diverge from that of a human examiner.",
                    "filename": "A_New_Dataset_and_Method_for_Automatically_Grading_ESOL_Texts",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P11-1019/Documents_xml/P11-1019.xml",
                    "title": "A New Dataset and Method for Automatically Grading ESOL Texts"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a new dataset and method for automatically grading ESOL texts. The authors explore the task of automated assessment and describe the use of Support Vector Machines (SVMs) as a rank preference learning problem. They discuss linguistic features, complexity measures, and error-rate estimation as components of their approach. The paper evaluates the system's performance using correlation measures and ablation tests to assess the contribution of different feature types. Additionally, the authors conduct validity tests to determine the system's robustness to subversion by writers. The paper also discusses previous work in the field of automated assessment and concludes by highlighting the potential for future research using the dataset. The release of the dataset is intended to facilitate further research in this area.",
                        "This academic paper introduces a new dataset and method for automatically grading ESOL texts. The paper discusses the task of automated assessment of free text, focusing on the analysis and assessment of writing competence. The authors highlight the advantages of automated assessment systems, such as reduced workload in marking texts and consistent application of marking criteria. The paper describes the dataset of ESOL examination scripts used for training and testing, the novel approach to the task, and the results for the system on this dataset. The paper also addresses the robustness of the system to subversion by writers who understand its workings. The authors hope that the release of the dataset will facilitate further research on the automated assessment of ESOL free text. The paper concludes with plans for future work and acknowledgements to contributors.",
                        "This academic paper discusses a new dataset and method for automatically grading ESOL texts. The authors treat automated assessment of ESOL text as a rank preference learning problem, using Support Vector Machines and a variety of linguistic features. The paper provides details of the dataset used, the system developed, evaluation methodology, and ablation experiments. The results show that the rank preference model improves correlation significantly over regression models and correlates well with human examiners' scores. Additionally, the paper discusses validity tests, previous work, and future research directions. The dataset and system described in the paper aim to facilitate further research on automated assessment of ESOL free text and enable precise comparison of different systems and feature types.",
                        "This paper introduces a new dataset and method for automatically grading ESOL (English for Speakers of Other Languages) texts. The study focuses on the automated assessment of free text, aiming to evaluate the writing competence and quality. The dataset used is the Cambridge Learner Corpus, which contains anonymized scripts from English language learners taking the First Certificate in English (FCE) exam. The study applies a rank preference learning approach using Support Vector Machines (SVMs) and investigates the contribution of various linguistic features to the overall performance of the automated assessment system. The results show that the system achieves performance close to the upper bound for the task, and the study also evaluates the robustness of the system to potential subversion by writers. Additionally, the paper discusses previous work in the field and highlights the need for cross-system comparisons and further research in this area."
                    ],
                    "filename": "a_new_dataset_and_method_for_aut-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 137 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A New Dataset and Method for Automatically Grading ESOL Texts</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">The task of automated assessment of free text focuses on automatically analysing and assessing the quality of writing competence.</S>\n    <S sid=\"8\" ssid=\"2\">Automated assessment systems exploit textual features in order to measure the overall quality and assign a score to a text.</S>\n    <S sid=\"9\" ssid=\"3\">The earliest systems used superficial features, such as word and sentence length, as proxies for understanding the text.</S>\n    <S sid=\"10\" ssid=\"4\">More recent systems have used more sophisticated automated text processing techniques to measure grammaticality, textual coherence, prespecified errors, and so forth.</S>\n    <S sid=\"11\" ssid=\"5\">Deployment of automated assessment systems gives a number of advantages, such as the reduced workload in marking texts, especially when applied to large-scale assessments.</S>\n    <S sid=\"12\" ssid=\"6\">Additionally, automated systems guarantee the application of the same marking criteria, thus reducing inconsistency, which may arise when more than one human examiner is employed.</S>\n    <S sid=\"13\" ssid=\"7\">Often, implementations include feedback with respect to the writers’ writing abilities, thus facilitating self-assessment and self-tutoring.</S>\n    <S sid=\"14\" ssid=\"8\">Implicitly or explicitly, previous work has mostly treated automated assessment as a supervised text classification task, where training texts are labelled with a grade and unlabelled test texts are fitted to the same grade point scale via a regression step applied to the classifier output (see Section 6 for more details).</S>\n    <S sid=\"15\" ssid=\"9\">Different techniques have been used, including cosine similarity of vectors representing text in various ways (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003), generative machine learning models (Rudner and Liang, 2002), domain-specific feature extraction (Attali and Burstein, 2006), and/or modified syntactic parsers (Lonsdale and Strong-Krause, 2003).</S>\n    <S sid=\"16\" ssid=\"10\">A recent review identifies twelve different automated free-text scoring systems (Williamson, 2009).</S>\n    <S sid=\"17\" ssid=\"11\">Examples include e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003), IntelliMetric (Elliot, 2003; Rudner et al., 2006) and Project Essay Grade (PEG) (Page, 2003).</S>\n    <S sid=\"18\" ssid=\"12\">Several of these are now deployed in highstakes assessment of examination scripts.</S>\n    <S sid=\"19\" ssid=\"13\">Although there are many published analyses of the performance of individual systems, as yet there is no publically available shared dataset for training and testing such systems and comparing their performance.</S>\n    <S sid=\"20\" ssid=\"14\">As it is likely that the deployment of such systems will increase, standardised and independent evaluation methods are important.</S>\n    <S sid=\"21\" ssid=\"15\">We make such a dataset of ESOL examination scripts available1 (see Section 2 for more details), describe our novel approach to the task, and provide results for our system on this dataset.</S>\n    <S sid=\"22\" ssid=\"16\">We address automated assessment as a supervised discriminative machine learning problem and particularly as a rank preference problem (Joachims, 2002).</S>\n    <S sid=\"23\" ssid=\"17\">Our reasons are twofold: Discriminative classification techniques often outperform non-discriminative ones in the context of text classification (Joachims, 1998).</S>\n    <S sid=\"24\" ssid=\"18\">Additionally, rank preference techniques (Joachims, 2002) allow us to explicitly learn an optimal ranking model of text quality.</S>\n    <S sid=\"25\" ssid=\"19\">Learning a ranking directly, rather than fitting a classifier score to a grade point scale after training, is both a more generic approach to the task and one which exploits the labelling information in the training data efficiently and directly.</S>\n    <S sid=\"26\" ssid=\"20\">Techniques such as LSA (Landauer and Foltz, 1998) measure, in addition to writing competence, the semantic relevance of a text written in response to a given prompt.</S>\n    <S sid=\"27\" ssid=\"21\">However, although our corpus of manually-marked texts was produced by learners of English in response to prompts eliciting free-text answers, the marking criteria are primarily based on the accurate use of a range of different linguistic constructions.</S>\n    <S sid=\"28\" ssid=\"22\">For this reason, we believe that an approach which directly measures linguistic competence will be better suited to ESOL text assessment, and will have the additional advantage that it may not require retraining for new prompts or tasks.</S>\n    <S sid=\"29\" ssid=\"23\">As far as we know, this is the first application of a rank preference model to automated assessment (hereafter AA).</S>\n    <S sid=\"30\" ssid=\"24\">In this paper, we report experiments on rank preference Support Vector Machines (SVMs) trained on a relatively small amount of data, on identification of appropriate feature types derived automatically from generic text processing tools, on comparison with a regression SVM model, and on the robustness of the best model to ‘outlier’ texts.</S>\n    <S sid=\"31\" ssid=\"25\">We report a consistent, comparable and replicable set of results based entirely on the new dataset and on public-domain tools and data, whilst also experimentally motivating some novel feature types for the AA task, thus extending the work described in (Briscoe et al., 2010).</S>\n    <S sid=\"32\" ssid=\"26\">In the following sections we describe in more detail the dataset used for training and testing, the system developed, the evaluation methodology, as well as ablation experiments aimed at studying the contribution of different feature types to the AA task.</S>\n    <S sid=\"33\" ssid=\"27\">We show experimentally that discriminative models with appropriate feature types can achieve performance close to the upper bound, as defined by the agreement between human examiners on the same test corpus.</S>\n  </SECTION>\n  <SECTION title=\"2 Cambridge Learner Corpus\" number=\"2\">\n    <S sid=\"34\" ssid=\"1\">The Cambridge Learner Corpus2 (CLC), developed as a collaborative project between Cambridge University Press and Cambridge Assessment, is a large collection of texts produced by English language learners from around the world, sitting Cambridge Assessment’s English as a Second or Other Language (ESOL) examinations3.</S>\n    <S sid=\"35\" ssid=\"2\">For the purpose of this work, we extracted scripts produced by learners taking the First Certificate in English (FCE) exam, which assesses English at an upper-intermediate level.</S>\n    <S sid=\"36\" ssid=\"3\">The scripts, which are anonymised, are annotated using XML and linked to meta-data about the question prompts, the candidate’s grades, native language and age.</S>\n    <S sid=\"37\" ssid=\"4\">The FCE writing component consists of two tasks asking learners to write either a letter, a report, an article, a composition or a short story, between 200 and 400 words.</S>\n    <S sid=\"38\" ssid=\"5\">Answers to each of these tasks are annotated with marks (in the range 1–40), which have been fitted to a RASCH model (Fischer and Molenaar, 1995) to correct for inter-examiner inconsistency and comparability.</S>\n    <S sid=\"39\" ssid=\"6\">In addition, an overall mark is assigned to both tasks, which is the one we use in our experiments.</S>\n    <S sid=\"40\" ssid=\"7\">Each script has been also manually tagged with information about the linguistic errors committed, using a taxonomy of approximately 80 error types (Nicholls, 2003).</S>\n    <S sid=\"41\" ssid=\"8\">The following is an example errorcoded sentence: In the morning, you are &lt;NS type = “TV”&gt; waken|woken&lt;/NS&gt; up by a singing puppy.</S>\n    <S sid=\"42\" ssid=\"9\">In this sentence, TV denotes an incorrect tense of verb error, where waken can be corrected to woken.</S>\n    <S sid=\"43\" ssid=\"10\">Our data consists of 1141 scripts from the year 2000 for training written by 1141 distinct learners, and 97 scripts from the year 2001 for testing written by 97 distinct learners.</S>\n    <S sid=\"44\" ssid=\"11\">The learners’ ages follow a bimodal distribution with peaks at approximately 16–20 and 26–30 years of age.</S>\n    <S sid=\"45\" ssid=\"12\">The prompts eliciting the free text are provided with the dataset.</S>\n    <S sid=\"46\" ssid=\"13\">However, in this paper we make no use of prompt information and do not make any attempt to check that the text answer is appropriate to the prompt.</S>\n    <S sid=\"47\" ssid=\"14\">Our focus is on developing an accurate AA system for ESOL text that does not require prompt-specific or topic-specific training.</S>\n    <S sid=\"48\" ssid=\"15\">There is no overlap between the prompts used in 2000 and in 2001.</S>\n    <S sid=\"49\" ssid=\"16\">A typical prompt taken from the 2000 training dataset is shown below: Your teacher has asked you to write a story for the school’s English language magazine.</S>\n    <S sid=\"50\" ssid=\"17\">The story must begin with the following words: “Unfortunately, Pat wasn’t very good at keeping secrets”.</S>\n  </SECTION>\n  <SECTION title=\"3 Approach\" number=\"3\">\n    <S sid=\"51\" ssid=\"1\">We treat automated assessment of ESOL text (see Section 2) as a rank preference learning problem (see Section 1).</S>\n    <S sid=\"52\" ssid=\"2\">In the experiments reported here we use Support Vector Machines (SVMs) (Vapnik, 1995) through the SVMlight package (Joachims, 1999).</S>\n    <S sid=\"53\" ssid=\"3\">Using the dataset described in Section 2, a number of linguistic features are automatically extracted and their contribution to overall performance is investigated.</S>\n    <S sid=\"54\" ssid=\"4\">SVMs have been extensively used for learning classification, regression and ranking functions.</S>\n    <S sid=\"55\" ssid=\"5\">In its basic form, a binary SVM classifier learns a linear threshold function that discriminates data points of two categories.</S>\n    <S sid=\"56\" ssid=\"6\">By using a different loss function, the e-insensitive loss function (Smola, 1996), SVMs can also perform regression.</S>\n    <S sid=\"57\" ssid=\"7\">SVMs in regression mode estimate a function that outputs a real number based on the training data.</S>\n    <S sid=\"58\" ssid=\"8\">In both cases, the model generalises by computing a hyperplane that has the largest (soft-)margin.</S>\n    <S sid=\"59\" ssid=\"9\">In rank preference SVMs, the goal is to learn a ranking function which outputs a score for each data point, from which a global ordering of the data is constructed.</S>\n    <S sid=\"60\" ssid=\"10\">This procedure requires a set R consisting of training samples xn and their target rankings rn: such that xi &gt;-R xj when ri &lt; rj, where 1 5 i,j 5 n and i =� j.</S>\n    <S sid=\"61\" ssid=\"11\">A rank preference model is not trained directly on this set of data objects and their labels; rather a set of pair-wise difference vectors is created.</S>\n    <S sid=\"62\" ssid=\"12\">The goal of a linear ranking model is to compute a weight vector w that maximises the number of correctly ranked pairs: This is equivalent to solving the following optimisation problem: The factor C allows a trade-off between the training error and the margin size, while �ij are nonnegative slack variables that measure the degree of misclassification.</S>\n    <S sid=\"63\" ssid=\"13\">The optimisation problem is equivalent to that for the classification model on pair-wise difference vectors.</S>\n    <S sid=\"64\" ssid=\"14\">In this case, generalisation is achieved by maximising the differences between closely-ranked data pairs.</S>\n    <S sid=\"65\" ssid=\"15\">The principal advantage of applying rank preference learning to the AA task is that we explicitly model the grade relationships between scripts and do not need to apply a further regression step to fit the classifier output to the scoring scheme.</S>\n    <S sid=\"66\" ssid=\"16\">The results reported in this paper are obtained by learning a linear classification function.</S>\n    <S sid=\"67\" ssid=\"17\">We parsed the training and test data (see Section 2) using the Robust Accurate Statistical Parsing (RASP) system with the standard tokenisation and sentence boundary detection modules (Briscoe et al., 2006) in order to broaden the space of candidate features suitable for the task.</S>\n    <S sid=\"68\" ssid=\"18\">The features used in our experiments are mainly motivated by the fact that lexical and grammatical features should be highly discriminative for the AA task.</S>\n    <S sid=\"69\" ssid=\"19\">Our full feature set is as follows: Word unigrams and bigrams are lower-cased and used in their inflected forms.</S>\n    <S sid=\"70\" ssid=\"20\">PoS unigrams, bigrams and trigrams are extracted using the RASP tagger, which uses the CLAWS4 tagset.</S>\n    <S sid=\"71\" ssid=\"21\">The most probable posterior tag per word is used to construct PoS ngram features, but we use the RASP parser’s option to analyse words assigned multiple tags when the posterior probability of the highest ranked tag is less than 0.9, and the next n tags have probability greater than 150 of it.</S>\n    <S sid=\"72\" ssid=\"22\">Based on the most likely parse for each identified sentence, we extract the rule names from the phrase structure (PS) tree.</S>\n    <S sid=\"73\" ssid=\"23\">RASP’s rule names are semiautomatically generated and encode detailed information about the grammatical constructions found (e.g.</S>\n    <S sid=\"74\" ssid=\"24\">V1/modal bse/+-, ‘a VP consisting of a modal auxiliary head followed by an (optional) adverbial phrase, followed by a VP headed by a verb with base inflection’).</S>\n    <S sid=\"75\" ssid=\"25\">Moreover, rule names explicitly represent information about peripheral or rare constructions (e.g.</S>\n    <S sid=\"76\" ssid=\"26\">S/pp-ap s-r, ‘a S with preposed PP with adjectival complement, e.g. for better or worse, he left’), as well as about fragmentary and likely extragrammatical sequences (e.g.</S>\n    <S sid=\"77\" ssid=\"27\">T/txt-frag, ‘a text unit consisting of 2 or more subanalyses that cannot be combined using any rule in the grammar’).</S>\n    <S sid=\"78\" ssid=\"28\">Therefore, we believe that many (longer-distance) grammatical constructions and errors found in texts can be (implicitly) captured by this feature type.</S>\n    <S sid=\"79\" ssid=\"29\">In developing our AA system, a number of different grammatical complexity measures were extracted from parses, and their impact on the accuracy of the system was explored.</S>\n    <S sid=\"80\" ssid=\"30\">For the experiments reported here, we use complexity measures representing the sum of the longest distance in word tokens between a head and dependent in a grammatical relation (GR) from the RASP GR output, calculated for each GR graph from the top 10 parses per sentence.</S>\n    <S sid=\"81\" ssid=\"31\">In particular, we extract the mean and median values of these distances per sentence and use the maximum values per script.</S>\n    <S sid=\"82\" ssid=\"32\">Intuitively, this feature captures information about the grammatical sophistication of the writer.</S>\n    <S sid=\"83\" ssid=\"33\">However, it may also be confounded in cases where sentence boundaries are not identified through, for example, poor punctuation.</S>\n    <S sid=\"84\" ssid=\"34\">Although the CLC contains information about the linguistic errors committed (see Section 2), we try to extract an error-rate in a way that doesn’t require manually tagged data.</S>\n    <S sid=\"85\" ssid=\"35\">However, we also use an error-rate calculated from the CLC error tags to obtain an upper bound for the performance of an automated error estimator (true CLC error-rate).</S>\n    <S sid=\"86\" ssid=\"36\">In order to estimate the error-rate, we build a trigram language model (LM) using ukWaC (ukWaC LM) (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens.</S>\n    <S sid=\"87\" ssid=\"37\">Next, we extend our language model with trigrams extracted from a subset of the texts contained in the CLC (CLC LM).</S>\n    <S sid=\"88\" ssid=\"38\">As the CLC contains texts produced by second language learners, we only extract frequently occurring trigrams from highly ranked scripts to avoid introducing erroneous ones to our language model.</S>\n    <S sid=\"89\" ssid=\"39\">A word trigram in test data is counted as an error if it is not found in the language model.</S>\n    <S sid=\"90\" ssid=\"40\">We compute presence/absence efficiently using a Bloom filter encoding of the language models (Bloom, 1970).</S>\n    <S sid=\"91\" ssid=\"41\">Feature instances of types i and ii are weighted using the tf*idf scheme and normalised by the L2 norm.</S>\n    <S sid=\"92\" ssid=\"42\">Feature type iii is weighted using frequency counts, while iii and iv are scaled so that their final value has approximately the same order of magnitude as i and ii.</S>\n    <S sid=\"93\" ssid=\"43\">The script length is based on the number of words and is mainly added to balance the effect the length of a script has on other features.</S>\n    <S sid=\"94\" ssid=\"44\">Finally, features whose overall frequency is lower than four are discarded from the model.</S>\n  </SECTION>\n  <SECTION title=\"4 Evaluation\" number=\"4\">\n    <S sid=\"95\" ssid=\"1\">In order to evaluate our AA system, we use two correlation measures, Pearson’s product-moment correlation coefficient and Spearman’s rank correlation coefficient (hereafter Pearson’s and Spearman’s correlation respectively).</S>\n    <S sid=\"96\" ssid=\"2\">Pearson’s correlation determines the degree to which two linearly dependent variables are related.</S>\n    <S sid=\"97\" ssid=\"3\">As Pearson’s correlation is sensitive to the distribution of data and, due to outliers, its value can be misleading, we also report Spearman’s correlation.</S>\n    <S sid=\"98\" ssid=\"4\">The latter is a nonparametric robust measure of association which is sensitive only to the ordinal arrangement of values.</S>\n    <S sid=\"99\" ssid=\"5\">As our data contains some tied values, we calculate Spearman’s correlation by using Pearson’s correlation on the ranks.</S>\n    <S sid=\"100\" ssid=\"6\">Table 1 presents the Pearson’s and Spearman’s correlation between the CLC scores and the AA system predicted values, when incrementally adding to the model the feature types described in Section 3.2.</S>\n    <S sid=\"101\" ssid=\"7\">Each feature type improves the model’s performance.</S>\n    <S sid=\"102\" ssid=\"8\">Extending our language model with frequent trigrams extracted from the CLC improves Pearson’s and Spearman’s correlation by 0.006 and 0.015 respectively.</S>\n    <S sid=\"103\" ssid=\"9\">The addition of the error-rate obtained from the manually annotated CLC error tags on top of all the features further improves performance by 0.01 and 0.016.</S>\n    <S sid=\"104\" ssid=\"10\">An evaluation of our best error detection method shows a Pearson correlation of 0.611 between the estimated and the true CLC error counts.</S>\n    <S sid=\"105\" ssid=\"11\">This suggests that there is room for improvement in the language models we developed to estimate the error-rate.</S>\n    <S sid=\"106\" ssid=\"12\">In the experiments reported hereafter, we use the ukWaC+CLC LM to calculate the error-rate.</S>\n    <S sid=\"107\" ssid=\"13\">In order to assess the independent as opposed to the order-dependent additive contribution of each feature type to the overall performance of the system, we run a number of ablation tests.</S>\n    <S sid=\"108\" ssid=\"14\">An ablation test consists of removing one feature of the system at a time and re-evaluating the model on the test set.</S>\n    <S sid=\"109\" ssid=\"15\">Table 2 presents Pearson’s and Spearman’s correlation between the CLC and our system, when removing one feature at a time.</S>\n    <S sid=\"110\" ssid=\"16\">All features have a positive effect on performance, while the error-rate has a big impact, as its absence is responsible for a 0.061 decrease of Spearman’s correlation.</S>\n    <S sid=\"111\" ssid=\"17\">In addition, the removal of either the word ngrams, the PS rules, or the error-rate estimate contributes to a large decrease in Pearson’s correlation.</S>\n    <S sid=\"112\" ssid=\"18\">In order to test the significance of the improved correlations, we ran one-tailed t-tests with a = 0.05 for the difference between dependent correlations (Williams, 1959; Steiger, 1980).</S>\n    <S sid=\"113\" ssid=\"19\">The results showed that PoS ngrams, PS rules, the complexity measures, and the estimated error-rate contribute significantly to the improvement of Spearman’s correlation, while PS rules also contribute significantly to the improvement of Pearson’s correlation.</S>\n    <S sid=\"114\" ssid=\"20\">One of the main approaches adopted by previous systems involves the identification of features that measure writing skill, and then the application of linear or stepwise regression to find optimal feature weights so that the correlation with manually assigned scores is maximised.</S>\n    <S sid=\"115\" ssid=\"21\">We trained a SVM regression model with our full set of feature types and compared it to the SVM rank preference model.</S>\n    <S sid=\"116\" ssid=\"22\">The results are given in Table 3.</S>\n    <S sid=\"117\" ssid=\"23\">The rank preference model improves Pearson’s and Spearman’s correlation by 0.044 and 0.067 respectively, and these differences are significant, suggesting that rank preference is a more appropriate model for the AA task.</S>\n    <S sid=\"118\" ssid=\"24\">Four senior and experienced ESOL examiners remarked the 97 FCE test scripts drawn from 2001 exams, using the marking scheme from that year (see Section 2).</S>\n    <S sid=\"119\" ssid=\"25\">In order to obtain a ceiling for the performance of our system, we calculate the average correlation between the CLC and the examiners’ scores, and find an upper bound of 0.796 and 0.792 Pearson’s and Spearman’s correlation respectively.</S>\n    <S sid=\"120\" ssid=\"26\">In order to evaluate the overall performance of our system, we calculate its correlation with the four senior examiners in addition to the RASCH-adjusted CLC scores.</S>\n    <S sid=\"121\" ssid=\"27\">Tables 4 and 5 present the results obtained.</S>\n    <S sid=\"122\" ssid=\"28\">The average correlation of the AA system with the CLC and the examiner scores shows that it is close dicted values with the CLC and the examiners’ scores, where E1 refers to the first examiner, E2 to the second etc. to the upper bound for the task.</S>\n    <S sid=\"123\" ssid=\"29\">Human–machine agreement is comparable to that of human–human agreement, with the exception of Pearson’s correlation with examiner E4 and Spearman’s correlation with examiners E1 and E4, where the discrepancies are higher.</S>\n    <S sid=\"124\" ssid=\"30\">It is likely that a larger training set and/or more consistent grading of the existing training data would help to close this gap.</S>\n    <S sid=\"125\" ssid=\"31\">However, our system is not measuring some properties of the scripts, such as discourse cohesion or relevance to the prompt eliciting the text, that examiners will take into account.</S>\n  </SECTION>\n  <SECTION title=\"5 Validity tests\" number=\"5\">\n    <S sid=\"126\" ssid=\"1\">The practical utility of an AA system will depend strongly on its robustness to subversion by writers who understand something of its workings and attempt to exploit this to maximise their scores (independently of their underlying ability).</S>\n    <S sid=\"127\" ssid=\"2\">Surprisingly, there is very little published data on the robustness of existing systems.</S>\n    <S sid=\"128\" ssid=\"3\">However, Powers et al. (2002) invited writing experts to trick the scoring capabilities of an earlier version of e-Rater (Burstein et al., 1998). e-Rater (see Section 6 for more details) assigns a score to a text based on linguistic feature types extracted using relatively domain-specific techniques.</S>\n    <S sid=\"129\" ssid=\"4\">Participants were given a description of these techniques as well as of the cue words that the system uses.</S>\n    <S sid=\"130\" ssid=\"5\">The results showed that it was easier to fool the system into assigning higher than lower scores.</S>\n    <S sid=\"131\" ssid=\"6\">Our goal here is to determine the extent to which knowledge of the feature types deployed poses a threat to the validity of our system, where certain text generation strategies may give rise to large positive discrepancies.</S>\n    <S sid=\"132\" ssid=\"7\">As mentioned in Section 2, the marking criteria for FCE scripts are primarily based on the accurate use of a range of different grammatical constructions relevant to specific communicative goals, but our system assesses this indirectly.</S>\n    <S sid=\"133\" ssid=\"8\">We extracted 6 high-scoring FCE scripts from the CLC that do not overlap with our training and test data.</S>\n    <S sid=\"134\" ssid=\"9\">Based on the features used by our system and without bias towards any modification, we modified each script in one of the following ways: ii.</S>\n    <S sid=\"135\" ssid=\"10\">Swap words that have the same PoS within a sentence Although the above modifications do not exhaust the potential challenges a deployed AA system might face, they represent a threat to the validity of our system since we are using a highly related feature set.</S>\n    <S sid=\"136\" ssid=\"11\">In total, we create 30 such ‘outlier’ texts, which were given to an ESOL examiner for marking.</S>\n    <S sid=\"137\" ssid=\"12\">Using the ‘outlier’ scripts as well as their original/unmodified versions, we ran our system on each modification separately and calculated the correlation between the predicted values and the examiner’s scores.</S>\n    <S sid=\"138\" ssid=\"13\">Table 6 presents the results.</S>\n    <S sid=\"139\" ssid=\"14\">The predicted values of the system have a high correlation with the examiner’s scores when tested on ‘outlier’ texts of modification types i(a), i(b) and i(c).</S>\n    <S sid=\"140\" ssid=\"15\">However, as i(c) has a lower correlation compared to i(a) and i(b), it is likely that a random ordering of ngrams with N &gt; 3 will further decrease performance.</S>\n    <S sid=\"141\" ssid=\"16\">A modification of type ii, where words with the same PoS within a sentence are swapped, results in a Pearson and Spearman correlation of 0.634 and 0.761 respectively.</S>\n    <S sid=\"142\" ssid=\"17\">Analysis of the results showed that our system predicted higher scores than the ones assigned by the examiner.</S>\n    <S sid=\"143\" ssid=\"18\">This can be explained by the fact that texts produced using modification type ii contain a small portion of correct sentences.</S>\n    <S sid=\"144\" ssid=\"19\">However, the marking criteria are based on the overall writing quality.</S>\n    <S sid=\"145\" ssid=\"20\">The final case, where correct sentences are randomly ordered, receives the lowest correlation.</S>\n    <S sid=\"146\" ssid=\"21\">As our system is not measuring discourse cohesion, discrepancies are much higher; the system’s predicted scores are high whilst the ones assigned by the examiner are very low.</S>\n    <S sid=\"147\" ssid=\"22\">However, for a writer to be able to generate text of this type already requires significant linguistic competence, whilst a number of generic methods for assessing text and/or discourse cohesion have been developed and could be deployed in an extended version of our system.</S>\n    <S sid=\"148\" ssid=\"23\">It is also likely that highly creative ‘outlier’ essays may give rise to large negative discrepancies.</S>\n    <S sid=\"149\" ssid=\"24\">Recent comments in the British media have focussed on this issue, reporting that, for example, one deployed essay marking system assigned Winston Churchill’s speech ‘We Shall Fight on the Beaches’ a low score because of excessive repetition5.</S>\n    <S sid=\"150\" ssid=\"25\">Our model predicted a high passing mark for this text, but not the highest one possible, that some journalists clearly feel it deserves.</S>\n  </SECTION>\n  <SECTION title=\"6 Previous work\" number=\"6\">\n    <S sid=\"151\" ssid=\"1\">In this section we briefly discuss a number of the more influential and/or better described approaches.</S>\n    <S sid=\"152\" ssid=\"2\">P´erez-Marin et al. (2009), Williamson (2009), Dikli (2006) and Valenti et al.</S>\n    <S sid=\"153\" ssid=\"3\">(2003) provide a more detailed overview of existing AA systems.</S>\n    <S sid=\"154\" ssid=\"4\">Project Essay Grade (PEG) (Page, 2003), one of the earliest systems, uses a number of manuallyidentified mostly shallow textual features, which are considered to be proxies for intrinsic qualities of writing competence.</S>\n    <S sid=\"155\" ssid=\"5\">Linear regression is used to assign optimal feature weights that maximise the correlation with the examiner’s scores.</S>\n    <S sid=\"156\" ssid=\"6\">The main issue with this system is that features such as word length and script length are easy to manipulate independently of genuine writing ability, potentially undermining the validity of the system.</S>\n    <S sid=\"157\" ssid=\"7\">In e-Rater (Attali and Burstein, 2006), texts are represented using vectors of weighted features.</S>\n    <S sid=\"158\" ssid=\"8\">Each feature corresponds to a different property of texts, such as an aspect of grammar, style, discourse and topic similarity.</S>\n    <S sid=\"159\" ssid=\"9\">Additional features, representing stereotypical grammatical errors for example, are extracted using manually-coded task-specific detectors based, in part, on typical marking criteria.</S>\n    <S sid=\"160\" ssid=\"10\">An unmarked text is scored based on the cosine similarity between its weighted vector and the ones in the training set.</S>\n    <S sid=\"161\" ssid=\"11\">Feature weights and/or scores can be fitted to a marking scheme by stepwise or linear regression.</S>\n    <S sid=\"162\" ssid=\"12\">Unlike our approach, e-Rater models discourse structure, semantic coherence and relevance to the prompt.</S>\n    <S sid=\"163\" ssid=\"13\">However, the system contains manually developed task-specific components and requires retraining or tuning for each new prompt and assessment task.</S>\n    <S sid=\"164\" ssid=\"14\">Intelligent Essay Assessor (IEA) (Landauer et al., 2003) uses Latent Semantic Analysis (LSA) (Landauer and Foltz, 1998) to compute the semantic similarity between texts, at a specific grade point, and a test text.</S>\n    <S sid=\"165\" ssid=\"15\">In LSA, text is represented by a matrix, where rows correspond to words and columns to context (texts).</S>\n    <S sid=\"166\" ssid=\"16\">Singular Value Decomposition (SVD) is used to obtain a reduced dimension matrix clustering words and contexts.</S>\n    <S sid=\"167\" ssid=\"17\">The system is trained on topic and/or prompt specific texts while test texts are assigned a score based on the ones in the training set that are most similar.</S>\n    <S sid=\"168\" ssid=\"18\">The overall score, which is calculated using regression techniques, is based on the content score as well as on other properties of texts, such as style, grammar, and so forth, though the methods used to assess these are not described in any detail in published work.</S>\n    <S sid=\"169\" ssid=\"19\">Again, the system requires retraining or tuning for new prompts and assessment tasks.</S>\n    <S sid=\"170\" ssid=\"20\">Lonsdale and Strong-Krause (2003) use a modified syntactic parser to analyse and score texts.</S>\n    <S sid=\"171\" ssid=\"21\">Their method is based on a modified version of the Link Grammar parser (Sleator and Templerley, 1995) where the overall score of a text is calculated as the average of the scores assigned to each sentence.</S>\n    <S sid=\"172\" ssid=\"22\">Sentences are scored on a five-point scale based on the parser’s cost vector, which roughly measures the complexity and deviation of a sentence from the parser’s grammatical model.</S>\n    <S sid=\"173\" ssid=\"23\">This approach bears some similarities to our use of grammatical complexity and extragrammaticality features, but grammatical features represent only one component of our overall system, and of the task.</S>\n    <S sid=\"174\" ssid=\"24\">The Bayesian Essay Test Scoring sYstem (BETSY) (Rudner and Liang, 2002) uses multinomial or Bernoulli Naive Bayes models to classify texts into different classes (e.g. pass/fail, grades A– F) based on content and style features such as word unigrams and bigrams, sentence length, number of verbs, noun–verb pairs etc.</S>\n    <S sid=\"175\" ssid=\"25\">Classification is based on the conditional probability of a class given a set of features, which is calculated using the assumption that each feature is independent of the other.</S>\n    <S sid=\"176\" ssid=\"26\">This system shows that treating AA as a text classification problem is viable, but the feature types are all fairly shallow, and the approach doesn’t make efficient use of the training data as a separate classifier is trained for each grade point.</S>\n    <S sid=\"177\" ssid=\"27\">Recently, Chen et al. (2010) has proposed an unsupervised approach to AA of texts addressing the same topic, based on a voting algorithm.</S>\n    <S sid=\"178\" ssid=\"28\">Texts are clustered according to their grade and given an initial Z-score.</S>\n    <S sid=\"179\" ssid=\"29\">A model is trained where the initial score of a text changes iteratively based on its similarity with the rest of the texts as well as their Zscores.</S>\n    <S sid=\"180\" ssid=\"30\">The approach might be better described as weakly supervised as the distribution of text grades in the training data is used to fit the final Z-scores to grades.</S>\n    <S sid=\"181\" ssid=\"31\">The system uses a bag-of-words representation of text, so would be easy to subvert.</S>\n    <S sid=\"182\" ssid=\"32\">Nevertheless, exploration of the trade-offs between degree of supervision required in training and grading accuracy is an important area for future research.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusions and future work\" number=\"7\">\n    <S sid=\"183\" ssid=\"1\">Finally, we hope that the release of the training and test dataset described here will facilitate further research on the AA task for ESOL free text and, in particular, precise comparison of different systems, feature types, and grade fitting methods.</S>\n    <S sid=\"184\" ssid=\"2\">Though many of the systems described in Section 6 have been shown to correlate well with examiners’ marks on test data in many experimental contexts, no cross-system comparisons are available because of the lack of a shared training and test dataset.</S>\n    <S sid=\"185\" ssid=\"3\">Furthermore, none of the published work of which we are aware has systematically compared the contribution of different feature types to the AA task, and only one (Powers et al., 2002) assesses the ease with which the system can be subverted given some knowledge of the features deployed.</S>\n    <S sid=\"186\" ssid=\"4\">We have shown experimentally how rank preference models can be effectively deployed for automated assessment of ESOL free-text answers.</S>\n    <S sid=\"187\" ssid=\"5\">Based on a range of feature types automatically extracted using generic text processing techniques, our system achieves performance close to the upper bound for the task.</S>\n    <S sid=\"188\" ssid=\"6\">Ablation tests highlight the contribution of each feature type to the overall performance, while significance of the resulting improvements in correlation with human scores has been calculated.</S>\n    <S sid=\"189\" ssid=\"7\">A comparison between regression and rank preference models further supports our approach.</S>\n    <S sid=\"190\" ssid=\"8\">Preliminary experiments based on a set of ‘outlier’ texts have shown the types of texts for which the system’s scoring capability can be undermined.</S>\n    <S sid=\"191\" ssid=\"9\">We plan to experiment with better error detection techniques, since the overall error-rate of a script is one of the most discriminant features.</S>\n    <S sid=\"192\" ssid=\"10\">Briscoe et al. (2010) describe an approach to automatic offprompt detection which does not require retraining for each new question prompt and which we plan to integrate with our system.</S>\n    <S sid=\"193\" ssid=\"11\">It is clear from the ‘outlier’ experiments reported here that our system would benefit from features assessing discourse coherence, and to a lesser extent from features assessing semantic (selectional) coherence over longer bounds than those captured by ngrams.</S>\n    <S sid=\"194\" ssid=\"12\">The addition of an incoherence metric to the feature set of an AA system has been shown to improve performance significantly (Miltsakaki and Kukich, 2000; Miltsakaki and Kukich, 2004).</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgements\" number=\"8\">\n    <S sid=\"195\" ssid=\"1\">We would like to thank Cambridge ESOL, a division of Cambridge Assessment, for permission to use and distribute the examination scripts.</S>\n    <S sid=\"196\" ssid=\"2\">We are also grateful to Cambridge Assessment for arranging for the test scripts to be remarked by four of their senior examiners.</S>\n    <S sid=\"197\" ssid=\"3\">Finally, we would like to thank Marek Rei, Øistein Andersen and the anonymous reviewers for their useful comments.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "e32c95acd3f0ba183e8399a5721482ba",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "A New Dataset and Method for Automatically Grading ESOL Texts"
                }
            }
        },
        {
            "index": 8,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.",
                    "filename": "Guiding_Semi-Supervision_with_Constraint-Driven_Learning",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P07-1036/Documents_xml/P07-1036.xml",
                    "title": "Guiding Semi-Supervision with Constraint-Driven Learning"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces a novel constraints-based learning protocol for guiding semi-supervised learning. The authors develop a formalism for constraints-based learning that unifies several kinds of constraints and allows for different levels of constraint violation. The proposed approach is shown to significantly reduce the number of training examples required in two information extraction problems. The paper provides experimental results that demonstrate the effectiveness of incorporating constraints in the learning process, especially when the amount of labeled training data is small. The authors also emphasize the importance of using soft constraints rather than hard constraints, as well as the choice of Hamming distance for approximating constraint violation. Overall, the paper presents a general and intuitive framework for guiding semi-supervised learning with constraints.",
                        "This paper introduces a novel constraints-based learning protocol for guiding semi-supervised learning. The authors propose a formalism for constraints-based learning that unifies several kinds of constraints, including unary, dictionary-based, and n-ary constraints which encode structural information and interdependencies among possible labels. The paper presents an algorithm for using constraints to guide the semi-supervised learning process, which results in significant improvements in model performance. Experimental results on two information extraction problems demonstrate the effectiveness of the proposed approach. The paper also discusses the importance of using soft constraints over hard constraints and provides insights into the choice of distance functions for capturing constraint violations. Overall, the paper provides a valuable framework for incorporating domain knowledge into the semi-supervised learning process.",
                        "This paper proposes a novel framework for semi-supervised learning, using constraints to guide the learning process. The authors develop a formalism for constraints-based learning that unifies various kinds of constraints, allowing for different levels of constraint violation. They demonstrate the effectiveness of their approach in two information extraction problems, using Hidden Markov Models (HMMs) as the underlying model. The results indicate significant improvement in model performance with a small amount of labeled data and a large unlabeled pool. The paper also discusses the importance of using soft constraints over hard constraints and provides insights into the choice of distance function for constraint approximation. Overall, the proposed framework shows promise for improving semi-supervised learning, especially with limited labeled training data.",
                        "This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning. The formalism developed unifies several kinds of constraints and allows capturing different levels of constraint violation. The proposed approach is demonstrated to significantly outperform traditional semi-supervised frameworks. The paper presents empirical results on two domains, citations and advertisements, showing that constraints improve the performance of both the final inference stage and the semi-supervised learning process. The framework is general and can be used when domain knowledge cannot be expressed by the model. The study also discusses the importance of using soft constraints rather than hard constraints and provides insights for future work in this area."
                    ],
                    "filename": "guiding_semisupervision_with_con-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 121 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Guiding Semi-Supervision with Constraint-Driven Learning</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"4\" ssid=\"1\">Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance.</S>\n    <S sid=\"5\" ssid=\"2\">Acquiring labeled data is a difficult and expensive task.</S>\n    <S sid=\"6\" ssid=\"3\">Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002).</S>\n    <S sid=\"7\" ssid=\"4\">The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets.</S>\n    <S sid=\"8\" ssid=\"5\">However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000).</S>\n    <S sid=\"9\" ssid=\"6\">In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002).</S>\n    <S sid=\"10\" ssid=\"7\">On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005).</S>\n    <S sid=\"11\" ssid=\"8\">This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning.</S>\n    <S sid=\"12\" ssid=\"9\">We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels.</S>\n    <S sid=\"13\" ssid=\"10\">One advantage of our formalism is that it allows capturing different levels of constraint violation.</S>\n    <S sid=\"14\" ssid=\"11\">Our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data while learning (see Section 5.</S>\n    <S sid=\"15\" ssid=\"12\">In the experimental part of this paper we use HMMs as the underlying model, and exhibit significant reduction in the number of training examples required in two information extraction problems.</S>\n    <S sid=\"16\" ssid=\"13\">As is often the case in semi-supervised learning, the algorithm can be viewed as a process that improves the model by generating feedback through labeling unlabeled examples.</S>\n    <S sid=\"17\" ssid=\"14\">Our algorithm pushes this intuition further, in that the use of constraints allows us to better exploit domain information as a way to label, along with the current learned model, unlabeled examples.</S>\n    <S sid=\"18\" ssid=\"15\">Given a small amount of labeled data and a large unlabeled pool, our framework initializes the model with the labeled data and then repeatedly: This way, we can generate better “training” examples during the semi-supervised learning process.</S>\n    <S sid=\"19\" ssid=\"16\">The core of our approach, (1), is described in Section 5.</S>\n    <S sid=\"20\" ssid=\"17\">The task is described in Section 3 and the Experimental study in Section 6.</S>\n    <S sid=\"21\" ssid=\"18\">It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">In the semi-supervised domain there are two main approaches for injecting domain specific knowledge.</S>\n    <S sid=\"23\" ssid=\"2\">One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure.</S>\n    <S sid=\"24\" ssid=\"3\">For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels.</S>\n    <S sid=\"25\" ssid=\"4\">This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF.</S>\n    <S sid=\"26\" ssid=\"5\">However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol.</S>\n    <S sid=\"27\" ssid=\"6\">A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning.</S>\n    <S sid=\"28\" ssid=\"7\">This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging.</S>\n    <S sid=\"29\" ssid=\"8\">(Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity.</S>\n    <S sid=\"30\" ssid=\"9\">This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier.</S>\n    <S sid=\"31\" ssid=\"10\">In our framework, dictionary lookup approaches are viewed as unary constraints on the output states.</S>\n    <S sid=\"32\" ssid=\"11\">We extend these kinds of constraints and allow for more general, n-ary constraints.</S>\n    <S sid=\"33\" ssid=\"12\">In the supervised learning setting it has been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling.</S>\n    <S sid=\"34\" ssid=\"13\">(Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005).</S>\n    <S sid=\"35\" ssid=\"14\">Our formalism is most related to this last work.</S>\n    <S sid=\"36\" ssid=\"15\">But, we develop a semi-supervised learning protocol based on this formalism.</S>\n    <S sid=\"37\" ssid=\"16\">We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints’ violation.</S>\n    <S sid=\"38\" ssid=\"17\">Conceptually, although not technically, the most related work to ours is (Shen et al., 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking.</S>\n    <S sid=\"39\" ssid=\"18\">To the best of our knowledge, we are the first to suggest a general semi-supervised protocol that is driven by soft constraints.</S>\n    <S sid=\"40\" ssid=\"19\">We propose learning with constraints - a framework that combines the approaches described above in a unified and intuitive way.</S>\n  </SECTION>\n  <SECTION title=\"3 Tasks, Examples and Datasets\" number=\"3\">\n    <S sid=\"41\" ssid=\"1\">In Section 4 we will develop a general framework for semi-supervised learning with constraints.</S>\n    <S sid=\"42\" ssid=\"2\">However, it is useful to illustrate the ideas on concrete problems.</S>\n    <S sid=\"43\" ssid=\"3\">Therefore, in this section, we give a brief introduction to the two domains on which we tested our algorithms.</S>\n    <S sid=\"44\" ssid=\"4\">We study two information extraction problems in each of which, given text, a set of pre-defined fields is to be identified.</S>\n    <S sid=\"45\" ssid=\"5\">Since the fields are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours.1 The first task is to identify fields from citations (McCallum et al., 2000) .</S>\n    <S sid=\"46\" ssid=\"6\">The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005).</S>\n    <S sid=\"47\" ssid=\"7\">Given a citation, the task is to extract the each open bracket.</S>\n    <S sid=\"48\" ssid=\"8\">The correct assignment was shown in (a).</S>\n    <S sid=\"49\" ssid=\"9\">While the predicted label assignment (b) is generally coherent, some constraints are violated.</S>\n    <S sid=\"50\" ssid=\"10\">Most obviously, punctuation marks are ignored as cues for state transitions.</S>\n    <S sid=\"51\" ssid=\"11\">The constraint “Fields cannot end with stop words (such as “the”)” may be also good. fields that appear in the given reference.</S>\n    <S sid=\"52\" ssid=\"12\">See Fig.</S>\n    <S sid=\"53\" ssid=\"13\">1.</S>\n    <S sid=\"54\" ssid=\"14\">There are 13 possible fields including author, title, location, etc.</S>\n    <S sid=\"55\" ssid=\"15\">To gain an insight to how the constraints can guide semi-supervised learning, assume that the sentence shown in Figure 1 appears in the unlabeled data pool.</S>\n    <S sid=\"56\" ssid=\"16\">Part (a) of the figure shows the correct labeled assignment and part (b) shows the assignment labeled by a HMM trained on 30 labels.</S>\n    <S sid=\"57\" ssid=\"17\">However, if we apply the constraint that state transition can occur only on punctuation marks, the same HMM model parameters will result in the correct labeling (a).</S>\n    <S sid=\"58\" ssid=\"18\">Therefore, by adding the improved labeled assignment we can generate better training samples during semi-supervised learning.</S>\n    <S sid=\"59\" ssid=\"19\">In fact, the punctuation marks are only some of the constraints that can be applied to this problem.</S>\n    <S sid=\"60\" ssid=\"20\">The set of constraints we used in our experiments appears in Table 1.</S>\n    <S sid=\"61\" ssid=\"21\">Note that some of the constraints are non-local and are very intuitive for people, yet it is very difficult to inject this knowledge into most models.</S>\n    <S sid=\"62\" ssid=\"22\">The second problem we consider is extracting fields from advertisements (Grenager et al., 2005).</S>\n    <S sid=\"63\" ssid=\"23\">The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website.</S>\n    <S sid=\"64\" ssid=\"24\">In the dataset, only 302 entries have been labeled with 12 fields, including size, rent, neighborhood, features, and so on.</S>\n    <S sid=\"65\" ssid=\"25\">The data was preprocessed using regular expressions for phone numbers, email addresses and URLs.</S>\n    <S sid=\"66\" ssid=\"26\">The list of the constraints for this domain is given in Table 1.</S>\n    <S sid=\"67\" ssid=\"27\">We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006).</S>\n    <S sid=\"68\" ssid=\"28\">We slightly modified the seedwords due to difference in preprocessing.</S>\n  </SECTION>\n  <SECTION title=\"4 Notation and Definitions\" number=\"4\">\n    <S sid=\"69\" ssid=\"1\">given an input sequence x = (x1,... , xN), the task is to find the best assignment to the output variables y = (y1, ... , yM).</S>\n    <S sid=\"70\" ssid=\"2\">We denote X to be the space of the possible input sequences and Y to be the set of possible output sequences.</S>\n    <S sid=\"71\" ssid=\"3\">We define a structured output classifier as a function h : X  Y that uses a global scoring function f : X × Y  R to assign scores to each possible input/output pair.</S>\n    <S sid=\"72\" ssid=\"4\">Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs.</S>\n    <S sid=\"73\" ssid=\"5\">The global scoring function is often decomposed as a weighted sum of feature functions, This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case).</S>\n    <S sid=\"74\" ssid=\"6\">Even when not dictated by the model, the feature functions fi(x, y) used are local to allow inference tractability.</S>\n    <S sid=\"75\" ssid=\"7\">Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference.</S>\n    <S sid=\"76\" ssid=\"8\">Now, consider a scenario where we have a set of constraints C1, ... , CK.</S>\n    <S sid=\"77\" ssid=\"9\">We define a constraint C : X × Y  {0, 1} as a function that indicates whether the input/output sequence violates some desired properties.</S>\n    <S sid=\"78\" ssid=\"10\">When the constraints are hard, the solution is given by from citations and advertisements.</S>\n    <S sid=\"79\" ssid=\"11\">Some constraints (represented in the first block of each domain) are global and are relatively difficult to inject into traditional models.</S>\n    <S sid=\"80\" ssid=\"12\">While all the constraints hold for the vast majority of the data, some of them are violated by some correct labeled assignments. where 1C(x) is a subset of Y for which all Ci assign the value 1 for the given (x, y).</S>\n    <S sid=\"81\" ssid=\"13\">When the constraints are soft, we want to incur some penalty for their violation.</S>\n    <S sid=\"82\" ssid=\"14\">Moreover, we want to incorporate into our cost function a measure for the amount of violation incurred by violating the constraint.</S>\n    <S sid=\"83\" ssid=\"15\">A generic way to capture this intuition is to introduce a distance function d(y, 1Ci(x)) between the space of outputs that respect the constraint,1Ci(x), and the given output sequence y.</S>\n    <S sid=\"84\" ssid=\"16\">One possible way to implement this distance function is as the minimal Hamming distance to a sequence that respects the constraint Ci, that is: d(y, 1Ci(x)) = min(y'E1C(.))</S>\n    <S sid=\"85\" ssid=\"17\">H(y, y').</S>\n    <S sid=\"86\" ssid=\"18\">If the penalty for violating the soft constraint Ci is pi, we write the score function as: We refer to d(y, 1C(x)) as the valuation of the constraint C on (x, y).</S>\n    <S sid=\"87\" ssid=\"19\">The intuition behind (1) is as follows.</S>\n    <S sid=\"88\" ssid=\"20\">Instead of merely maximizing the model’s likelihood, we also want to bias the model using some knowledge.</S>\n    <S sid=\"89\" ssid=\"21\">The first term of (1) is used to learn from data.</S>\n    <S sid=\"90\" ssid=\"22\">The second term biases the mode by using the knowledge encoded in the constraints.</S>\n    <S sid=\"91\" ssid=\"23\">Note that we do not normalize our objective function to be a true probability distribution.</S>\n  </SECTION>\n  <SECTION title=\"5 Learning and Inference with Constraints\" number=\"5\">\n    <S sid=\"92\" ssid=\"1\">In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S>\n    <S sid=\"93\" ssid=\"2\">The task is to learn the parameter vector A by using the new objective function (1).</S>\n    <S sid=\"94\" ssid=\"3\">While our formulation allows us to train also the coefficients of the constraints valuation, pi, we choose not to do it, since we view this as a way to bias (or enforce) the prior knowledge into the learned model, rather than allowing the data to brush it away.</S>\n    <S sid=\"95\" ssid=\"4\">Our experiments demonstrate that the proposed approach is robust to inaccurate approximation of the prior knowledge (assigning the same penalty to all the pi ).</S>\n    <S sid=\"96\" ssid=\"5\">We note that in the presence of constraints, the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding, see (Toutanova et al., 2005; Roth and Yih, 2005) for a discussion), we chose beamsearch decoding.</S>\n    <S sid=\"97\" ssid=\"6\">The semi-supervised learning with constraints is done with an EM-like procedure.</S>\n    <S sid=\"98\" ssid=\"7\">We initialize the model with traditional supervised learning (ignoring the constraints) on a small labeled set.</S>\n    <S sid=\"99\" ssid=\"8\">Given an unlabeled set U, in the estimation step, the traditional EM algorithm assigns a distribution over labeled assignments Y of each x E U, and in the maximization step, the set of model parameters is learned from the distributions assigned in the estimation step.</S>\n    <S sid=\"100\" ssid=\"9\">However, in the presence of constraints, assigning the complete distributions in the estimation step is infeasible since the constraints reshape the distribution in an arbitrary way.</S>\n    <S sid=\"101\" ssid=\"10\">As in existing methods for training a model by maximizing a linear cost function (maximize likelihood or discriminative maximization), the distribution over y is represented as the set of scores assigned to it; rather than considering the score assigned to all y's, we truncate the distribution to the top K assignments as returned by the search.</S>\n    <S sid=\"102\" ssid=\"11\">Given a set of K top assignments yi, , yK, we approximate the estimation step by assigning uniform probability to the top K candidates, and zero to the other output sequences.</S>\n    <S sid=\"103\" ssid=\"12\">We denote this algorithm top-K hard EM.</S>\n    <S sid=\"104\" ssid=\"13\">In this paper, we use beamsearch to generate K candidates according to (1).</S>\n    <S sid=\"105\" ssid=\"14\">Our training algorithm is summarized in Figure 2.</S>\n    <S sid=\"106\" ssid=\"15\">Several things about the algorithm should be clarified: the Top-K-Inference procedure in line 7, the learning procedure in line 9, and the new parameter estimation in line 9.</S>\n    <S sid=\"107\" ssid=\"16\">The Top-K-Inference is a procedure that returns the K labeled assignments that maximize the new objective function (1).</S>\n    <S sid=\"108\" ssid=\"17\">In our case we used the topK elements in the beam, but this could be applied to any other inference procedure.</S>\n    <S sid=\"109\" ssid=\"18\">The fact that the constraints are used in the inference procedure (in particular, for generating new training examples) allows us to use a learning algorithm that ignores the constraints, which is a lot more efficient (although algorithms that do take the constraints into account can be used too).</S>\n    <S sid=\"110\" ssid=\"19\">We used maximum likelihood estimation of A but, in general, perceptron or quasiNewton can also be used.</S>\n    <S sid=\"111\" ssid=\"20\">It is known that traditional semi-supervised training can degrade the learned model’s performance.</S>\n    <S sid=\"112\" ssid=\"21\">(Nigam et al., 2000) has suggested to balance the contribution of labeled and unlabeled data to the parameters.</S>\n    <S sid=\"113\" ssid=\"22\">The intuition is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model.</S>\n    <S sid=\"114\" ssid=\"23\">The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter -y which controls the convex combination of models induced by the labeled and the unlabeled data.</S>\n    <S sid=\"115\" ssid=\"24\">Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms.</S>\n    <S sid=\"116\" ssid=\"25\">Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006).</S>\n    <S sid=\"117\" ssid=\"26\">Similarly to self-training, we use the current model to generate new training examples from the unlaTop-K-Inference, we use beamsearch to find the Kbest solution according to Eq.</S>\n    <S sid=\"118\" ssid=\"27\">(1). beled set.</S>\n    <S sid=\"119\" ssid=\"28\">However, there are two important differences.</S>\n    <S sid=\"120\" ssid=\"29\">One is that in self-training, once an unlabeled sample was labeled, it is never labeled again.</S>\n    <S sid=\"121\" ssid=\"30\">In our case all the samples are relabeled in each iteration.</S>\n    <S sid=\"122\" ssid=\"31\">In self-training it is often the case that only high-confidence samples are added to the labeled data pool.</S>\n    <S sid=\"123\" ssid=\"32\">While we include all the samples in the training pool, we could also limit ourselves to the high-confidence samples.</S>\n    <S sid=\"124\" ssid=\"33\">The second difference is that each unlabeled example generates K labeled instances.</S>\n    <S sid=\"125\" ssid=\"34\">The case of one iteration of top-1 hard EM is equivalent to self training, where all the unlabeled samples are added to the labeled pool.</S>\n    <S sid=\"126\" ssid=\"35\">There are several possible benefits to using K &gt; 1 samples.</S>\n    <S sid=\"127\" ssid=\"36\">(1) It effectively increases the training set by a factor of K (albeit by somewhat noisy examples).</S>\n    <S sid=\"128\" ssid=\"37\">In the structured scenario, each of the top-K assignments is likely to have some good components so generating top-K assignments helps leveraging the noise.</S>\n    <S sid=\"129\" ssid=\"38\">(2) Given an assignment that does not satisfy some constraints, using top-K allows for multiple ways to correct it.</S>\n    <S sid=\"130\" ssid=\"39\">For example, consider the output 11101000 with the constraint that it should belong to the language 1*0*.</S>\n    <S sid=\"131\" ssid=\"40\">If the two top scoring corrections are 11111000 and 11100000, considering only one of those can negatively bias the model.</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments and Results\" number=\"6\">\n    <S sid=\"132\" ssid=\"1\">In this section, we present empirical results of our algorithms on two domains: citations and advertisements.</S>\n    <S sid=\"133\" ssid=\"2\">Both problems are modeled with a simple token-based HMM.</S>\n    <S sid=\"134\" ssid=\"3\">We stress that token-based HMM cannot represent many of our constraints.</S>\n    <S sid=\"135\" ssid=\"4\">The function d(y, 1C(x)) used is an approximation of a Hamming distance function, discussed in Section 7.</S>\n    <S sid=\"136\" ssid=\"5\">For both domains, and all the experiments, -y was set to 0.1.</S>\n    <S sid=\"137\" ssid=\"6\">The constraints violation penalty p is set to − log 10−4 and − log 10−1 for citations and advertisements, resp.2 Note that all constraints share the same penalty.</S>\n    <S sid=\"138\" ssid=\"7\">The number of semi-supervised training cycles (line 3 of Figure 2) was set to 5.</S>\n    <S sid=\"139\" ssid=\"8\">The constraints for the two domains are listed in Table 1.</S>\n    <S sid=\"140\" ssid=\"9\">We trained models on training sets of size varying from 5 to 300 for the citations and from 5 to 100 for the advertisements.</S>\n    <S sid=\"141\" ssid=\"10\">Additionally, in all the semi-supervised experiments, 1000 unlabeled examples are used.</S>\n    <S sid=\"142\" ssid=\"11\">We report token-based3 accuracy on 100 held-out examples (which do not overlap neither with the training nor with the unlabeled data).</S>\n    <S sid=\"143\" ssid=\"12\">We ran 5 experiments in each setting, randomly choosing the training set.</S>\n    <S sid=\"144\" ssid=\"13\">The results reported below are the averages over these 5 runs.</S>\n    <S sid=\"145\" ssid=\"14\">To verify our claims we implemented several baselines.</S>\n    <S sid=\"146\" ssid=\"15\">The first baseline is the supervised learning protocol denoted by sup.</S>\n    <S sid=\"147\" ssid=\"16\">The second baseline was a traditional top-1 Hard EM also known as truncated EM4 (denoted by H for Hard).</S>\n    <S sid=\"148\" ssid=\"17\">In the third baseline, denoted H&amp;W, we balanced the weight of the supervised and unsupervised models as described in line 9 of Figure 2.</S>\n    <S sid=\"149\" ssid=\"18\">We compare these baselines to our proposed protocol, H&amp;W&amp;C, where we added the constraints to guide the H&amp;W protocol.</S>\n    <S sid=\"150\" ssid=\"19\">We experimented with two flavors of the algorithm: the top-1 and the top-K version.</S>\n    <S sid=\"151\" ssid=\"20\">In the top-K version, the algorithm uses K-best predictions (K=50) for each instance in order to update the model as described in Figure 2.</S>\n    <S sid=\"152\" ssid=\"21\">The experimental results for both domains are in given Table 2.</S>\n    <S sid=\"153\" ssid=\"22\">As hypothesized, hard EM sometimes from citations and advertisements.</S>\n    <S sid=\"154\" ssid=\"23\">N is the number of labeled samples.</S>\n    <S sid=\"155\" ssid=\"24\">H is the traditional hard-EM and H&amp;W weighs labeled and unlabeled data as mentioned in Sec.</S>\n    <S sid=\"156\" ssid=\"25\">5.</S>\n    <S sid=\"157\" ssid=\"26\">Our proposed model is H&amp;W&amp;C, which uses constraints in the learning procedure.</S>\n    <S sid=\"158\" ssid=\"27\">I refers to using constraints during inference at evaluation time.</S>\n    <S sid=\"159\" ssid=\"28\">Note that adding constraints improves the accuracy during both learning and inference. degrade the performance.</S>\n    <S sid=\"160\" ssid=\"29\">Indeed, with 300 labeled examples in the citations domain, the performance decreases from 86.1 to 80.7.</S>\n    <S sid=\"161\" ssid=\"30\">The usefulness of injecting constraints in semi-supervised learning is exhibited in the two right most columns: using constraints H&amp;W&amp;C improves the performance over H&amp;W quite significantly.</S>\n    <S sid=\"162\" ssid=\"31\">We carefully examined the contribution of using constraints to the learning stage and the testing stage, and two separate results are presented: testing with constraints (denoted I for inference) and without constraints (no I).</S>\n    <S sid=\"163\" ssid=\"32\">The I results are consistently better.</S>\n    <S sid=\"164\" ssid=\"33\">And, it is also clear from Table 2, that using constraints in training always improves the model and the amount of improvement depends on the amount of labeled data.</S>\n    <S sid=\"165\" ssid=\"34\">Figure 3 compares two protocols on the advertisements domain: H&amp;W+I, where we first run the H&amp;W protocol and then apply the constraints during testing stage, and H&amp;W&amp;C+I, which uses constraints to guide the model during learning and uses it also in testing.</S>\n    <S sid=\"166\" ssid=\"35\">Although injecting constraints in the learning process helps, testing with constraints is more important than using constraints during learning, especially when the labeled data size is large.</S>\n    <S sid=\"167\" ssid=\"36\">This confirms results reported for the supervised learning case in (Punyakanok et al., 2005; Roth and Yih, 2005).</S>\n    <S sid=\"168\" ssid=\"37\">However, as shown, our proposed algorithm H&amp;W&amp;C for training with constraints is critical when the amount labeled data is small.</S>\n    <S sid=\"169\" ssid=\"38\">Figure 4 further strengthens this point.</S>\n    <S sid=\"170\" ssid=\"39\">In the citations domain, H&amp;W&amp;C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples.</S>\n    <S sid=\"171\" ssid=\"40\">(Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains.</S>\n    <S sid=\"172\" ssid=\"41\">However, due to different preprocessing, the comparison is not straightforward.</S>\n    <S sid=\"173\" ssid=\"42\">For the citation domain, when 20 labeled and 300 unlabeled samples are available, (Grenager et al., 2005) observed an increase from 65.2% to 71.3%.</S>\n    <S sid=\"174\" ssid=\"43\">Our improvement is from 70.1% to 79.4%.</S>\n    <S sid=\"175\" ssid=\"44\">For the advertisement domain, they observed no improvement, while our model improves from 68.1% to 74.6% with 20 labeled samples.</S>\n    <S sid=\"176\" ssid=\"45\">Moreover, we successfully use out-of-domain data (web data) to improve our model, while they report that this data did not improve their unsupervised model.</S>\n    <S sid=\"177\" ssid=\"46\">(Haghighi and Klein, 2006) also worked on one of our data sets.</S>\n    <S sid=\"178\" ssid=\"47\">Their underlying model, Markov Random Fields, allows more expressive features.</S>\n    <S sid=\"179\" ssid=\"48\">Nevertheless, when they use only unary constraints they get 53.75%.</S>\n    <S sid=\"180\" ssid=\"49\">When they use their final model, along with a mechanism for extending the prototypes to other tokens, they get results that are comparable to our model with 10 labeled examples.</S>\n    <S sid=\"181\" ssid=\"50\">Additionally, in their framework, it is not clear how to use small amounts of labeled data when available.</S>\n    <S sid=\"182\" ssid=\"51\">Our model outperforms theirs once we add 10 more examples.</S>\n  </SECTION>\n  <SECTION title=\"7 Soft Constraints\" number=\"7\">\n    <S sid=\"183\" ssid=\"1\">This section discusses the importance of using soft constraints rather than hard constraints, the choice of Hamming distance for d(y, 1C(x)) and how we approximate it.</S>\n    <S sid=\"184\" ssid=\"2\">We use two constraints to illustrate the ideas.</S>\n    <S sid=\"185\" ssid=\"3\">(C1): “state transitions can only occur on punctuation marks or newlines”, and (C2): “the field TITLE must appear”.</S>\n    <S sid=\"186\" ssid=\"4\">First, we claim that defining d(y, 1C(x)) to be the Hamming distance is superior to using a binary value, d(y, 1C(x)) = 0 if y E 1C(x) and 1 otherwise.</S>\n    <S sid=\"187\" ssid=\"5\">Consider, for example, the constraint C1 in the advertisements domain.</S>\n    <S sid=\"188\" ssid=\"6\">While the vast majority of the instances satisfy the constraint, some violate it in more than one place.</S>\n    <S sid=\"189\" ssid=\"7\">Therefore, once the binary distance is set to 1, the algorithm looses the ability to discriminate constraint violations in other locations of the same instance.</S>\n    <S sid=\"190\" ssid=\"8\">This may hurt the performance in both the inference and the learning stage.</S>\n    <S sid=\"191\" ssid=\"9\">Computing the Hamming distance exactly can be a computationally hard problem.</S>\n    <S sid=\"192\" ssid=\"10\">Furthermore, it is unreasonable to implement the exact computation for each constraint.</S>\n    <S sid=\"193\" ssid=\"11\">Therefore, we implemented a generic approximation for the hamming distance assuming only that we are given a boolean function OC(yN) that returns whether labeling the token xN with state yN violates constraint with respect to an already labeled consider the prefix x1, x2, x3, x4, which contains no punctuation or newlines and was labeled AUTH, AUTH, DATE, DATE.</S>\n    <S sid=\"194\" ssid=\"12\">This labeling violates C1, the minimal hamming distance is 2, and our approximation gives 1, (since there is only one transition that violates the constraint.)</S>\n    <S sid=\"195\" ssid=\"13\">For constraints which cannot be validated based on prefix information, our approximation resorts to binary violation count.</S>\n    <S sid=\"196\" ssid=\"14\">For instance, the constraint C2 cannot be implemented with prefix information when the assignment is not complete.</S>\n    <S sid=\"197\" ssid=\"15\">Otherwise, it would mean that the field TITLE should appear as early as possible in the assignment.</S>\n    <S sid=\"198\" ssid=\"16\">While (Roth and Yih, 2005) showed the significance of using hard constraints, our experiments show that using soft constraints is a superior option.</S>\n    <S sid=\"199\" ssid=\"17\">For example, in the advertisements domain, C1 holds for the large majority of the gold-labeled instances, but is sometimes violated.</S>\n    <S sid=\"200\" ssid=\"18\">In supervised training with 100 labeled examples on this domain, sup gave 76.3% accuracy.</S>\n    <S sid=\"201\" ssid=\"19\">When the constraint violation penalty p was infinity (equivalent to hard constraint), the accuracy improved to 78.7%, but when the penalty was set to −log(0.1), the accuracy of the model jumped to 80.6%.</S>\n  </SECTION>\n  <SECTION title=\"8 Conclusions and Future Work\" number=\"8\">\n    <S sid=\"202\" ssid=\"1\">We proposed to use constraints as a way to guide semi-supervised learning.</S>\n    <S sid=\"203\" ssid=\"2\">The framework developed is general both in terms of the representation and expressiveness of the constraints, and in terms of the underlying model being learned – HMM in the current implementation.</S>\n    <S sid=\"204\" ssid=\"3\">Moreover, our framework is a useful tool when the domain knowledge cannot be expressed by the model.</S>\n    <S sid=\"205\" ssid=\"4\">The results show that constraints improve not only the performance of the final inference stage but also propagate useful information during the semisupervised learning process and that training with the constraints is especially significant when the number of labeled training data is small.</S>\n    <S sid=\"206\" ssid=\"5\">Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing.</S>\n    <S sid=\"207\" ssid=\"6\">Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "44d2fb16ea22c2ea9cc01a347e91ff7f",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Guiding Semi-Supervision with Constraint-Driven Learning"
                }
            }
        },
        {
            "index": 9,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.",
                    "filename": "Joint_Unsupervised_Coreference_Resolution_with_Markov_Logic",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D08-1068/Documents_xml/D08-1068.xml",
                    "title": "Joint Unsupervised Coreference Resolution with Markov Logic"
                },
                "llm": {
                    "descriptions": [
                        "This academic paper presents a joint unsupervised coreference resolution system that leverages Markov logic for accurate coreference resolution. The paper introduces a model that performs joint inference among mentions, using relations like apposition and predicate nominals. The system is found to be as accurate as supervised systems and outperforms state-of-the-art unsupervised systems. The paper also discusses the learning and inference processes used in the system, as well as the results of experiments conducted on MUC-6 and ACE-2004 datasets. The system is shown to be competitive and performs well in comparison to previous approaches. The paper concludes with potential future directions for incorporating additional knowledge and combining coreference resolution with other NLP tasks.",
                        "This paper introduces an unsupervised coreference resolution system that utilizes joint inference among mentions, incorporating relations such as apposition and predicate nominals. The system uses Markov logic as a representation language, allowing for easy extension to include additional linguistic and world knowledge. The system's results demonstrate improved coreference accuracy compared to state-of-the-art systems, both supervised and unsupervised, on datasets such as MUC-6 and ACE-2004. The paper discusses the impact of various components of the system, such as better head identification and leveraging syntactic relations. Overall, the system achieves competitive accuracy as a fully unsupervised approach, paving the way for future directions in incorporating additional knowledge and combining coreference resolution with other NLP tasks.",
                        "This academic paper introduces a joint unsupervised coreference resolution system using Markov logic. The system leverages relations like apposition and predicate nominals for joint inference among mentions. The paper compares this system to previous approaches on MUC-6, ACE-2004, and ACE-2 datasets, demonstrating that the unsupervised system is as accurate as supervised systems. The results show significant improvements in coreference accuracy, outperforming state-of-the-art unsupervised and supervised systems. The system is found to be competitive and effective, with potential for future extensions to incorporate additional linguistic and world knowledge. The research was funded by DARPA contracts, NSF grant, and ONR grant. The paper concludes by highlighting future directions for research in coreference resolution.",
                        "This paper introduces a joint unsupervised coreference resolution system that leverages Markov logic for inference. The system is designed to identify mentions that refer to the same entities, a key task in NLP applications. The paper compares the proposed system with existing supervised and unsupervised approaches, achieving significant improvements in coreference resolution accuracy. By incorporating linguistic features such as apposition and predicate nominals, the system outperforms previous state-of-the-art models. The authors also discuss future directions for the system, including the incorporation of additional knowledge and joint entity detection. The system's accuracy and potential for extension make it a promising contribution to the field of coreference resolution.",
                        "This academic paper introduces a novel unsupervised coreference resolution system that performs joint inference among mentions, leveraging relations like apposition and predicate nominals. The system uses Markov logic as a representation language, allowing it to be easily extended to incorporate additional linguistic and world knowledge. The paper presents experiments comparing the system with previous approaches, demonstrating that it outperforms state-of-the-art unsupervised and supervised systems on various datasets. The results show that the proposed system achieves coreference accuracy comparable to the best supervised systems. The paper concludes by discussing future directions, such as incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.",
                        "This paper introduces an unsupervised coreference resolution system that leverages Markov logic for joint inference among mentions, using relations such as apposition and predicate nominals. The system is as accurate as supervised systems and outperforms the state-of-the-art unsupervised system. The Markov logic representation allows for easy extension to incorporate additional linguistic and world knowledge. The approach is tested on standard MUC and ACE datasets, showing significant improvements over existing systems. The paper highlights the potential for future work in incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks. The research was funded by DARPA, NSF, and ONR contracts and grants.",
                        "This academic paper introduces a new unsupervised coreference resolution system that performs joint inference among mentions using relations like apposition and predicate nominals. Leveraging Markov logic as a representation language, the system easily incorporates additional linguistic and world knowledge. The paper details the model's development, learning, and inference processes, and presents experiments comparing the system with previous approaches on MUC-6 and ACE-2004 datasets. The results show that the new system outperforms state-of-the-art unsupervised and supervised systems, achieving the best coreference accuracy reported to date. The paper concludes by outlining future directions, including the incorporation of additional knowledge and combining coreference resolution with other NLP tasks.",
                        "This academic paper introduces an unsupervised coreference resolution system that performs joint inference among mentions using Markov logic. The system leverages relations like apposition and predicate nominals to achieve high accuracy, matching or exceeding the performance of supervised systems. The paper includes detailed descriptions of the Markov logic network used for joint unsupervised coreference resolution, learning and inference algorithms, and experiments conducted on standard datasets. The results show significant improvements in coreference accuracy compared to state-of-the-art unsupervised and supervised systems. The paper concludes by discussing future directions for incorporating additional knowledge, conducting joint entity detection, and combining coreference resolution with other natural language processing tasks.",
                        "This paper introduces a joint unsupervised coreference resolution system using Markov logic. The system leverages relations like apposition and predicate nominals, and performs joint inference among mentions. The paper presents a detailed description of the Markov logic network for joint unsupervised coreference resolution, the learning and inference algorithms used, and the results of experiments conducted on standard MUC and ACE datasets. The system outperforms existing unsupervised and supervised systems, achieving state-of-the-art accuracy. The paper also discusses future directions for the research, including incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks. The research was funded by DARPA contracts, NSF grant, and ONR grant.",
                        "This paper introduces an unsupervised coreference resolution system that performs joint inference among mentions, leveraging relations like apposition and predicate nominals. The system uses Markov logic as a representation language, allowing for easy extension to incorporate additional linguistic and world knowledge. The approach significantly outperforms existing unsupervised and supervised systems on MUC-6 and ACE-2004 datasets. Notably, the system's full model achieves the best coreference accuracy reported on MUC-6 to date. The paper discusses the results, compares the system to previous approaches, and highlights future directions for incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks."
                    ],
                    "filename": "joint_unsupervised_coreference_r-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 118 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Joint Unsupervised Coreference Resolution with Markov Logic</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">The goal of coreference resolution is to identify mentions (typically noun phrases) that refer to the same entities.</S>\n    <S sid=\"7\" ssid=\"2\">This is a key subtask in many NLP applications, including information extraction, question answering, machine translation, and others.</S>\n    <S sid=\"8\" ssid=\"3\">Supervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g., McCallum &amp; Wellner (2005)).</S>\n    <S sid=\"9\" ssid=\"4\">While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them.</S>\n    <S sid=\"10\" ssid=\"5\">This limits their applicability.</S>\n    <S sid=\"11\" ssid=\"6\">Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text.</S>\n    <S sid=\"12\" ssid=\"7\">However, unsupervised coreference resolution is much more difficult.</S>\n    <S sid=\"13\" ssid=\"8\">Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags supervised ones by a substantial margin.</S>\n    <S sid=\"14\" ssid=\"9\">Extending it appears difficult, due to the limitations of its Dirichlet process-based representation.</S>\n    <S sid=\"15\" ssid=\"10\">The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones.</S>\n    <S sid=\"16\" ssid=\"11\">Relations that have been exploited in supervised coreference resolution include transitivity (McCallum &amp; Wellner, 2005) and anaphoricity (Denis &amp; Baldridge, 2007).</S>\n    <S sid=\"17\" ssid=\"12\">However, there is little work to date on joint inference for unsupervised resolution.</S>\n    <S sid=\"18\" ssid=\"13\">We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic (Richardson &amp; Domingos, 2006).</S>\n    <S sid=\"19\" ssid=\"14\">Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals.</S>\n    <S sid=\"20\" ssid=\"15\">By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution.</S>\n    <S sid=\"21\" ssid=\"16\">We test our approach on standard MUC and ACE datasets.</S>\n    <S sid=\"22\" ssid=\"17\">Our basic model, trained on a minimum of data, suffices to outperform Haghighi and Klein’s (2007) one.</S>\n    <S sid=\"23\" ssid=\"18\">Our full model, using apposition and other relations for joint inference, is often as accurate as the best supervised models, or more.</S>\n    <S sid=\"24\" ssid=\"19\">We begin by reviewing the necessary background on Markov logic.</S>\n    <S sid=\"25\" ssid=\"20\">We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.</S>\n    <S sid=\"26\" ssid=\"21\">Finally, we present our experiments and results.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"27\" ssid=\"1\">Most existing supervised learning approaches for coreference resolution are suboptimal since they resolve each mention pair independently, only imposing transitivity in postprocessing (Ng, 2005).</S>\n    <S sid=\"28\" ssid=\"2\">Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007).</S>\n    <S sid=\"29\" ssid=\"3\">Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions.</S>\n    <S sid=\"30\" ssid=\"4\">Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations.</S>\n    <S sid=\"31\" ssid=\"5\">While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date.</S>\n    <S sid=\"32\" ssid=\"6\">Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp; Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes.</S>\n    <S sid=\"33\" ssid=\"7\">At the heart of their system is a mixture model with a few linguistically motivated features such as head words, entity properties and salience.</S>\n    <S sid=\"34\" ssid=\"8\">Their approach is a major step forward in unsupervised coreference resolution, but extending it is challenging.</S>\n    <S sid=\"35\" ssid=\"9\">The main advantage of Dirichlet processes is that they are exchangeable, allowing parameters to be integrated out, but Haghighi and Klein forgo this when they introduce salience.</S>\n    <S sid=\"36\" ssid=\"10\">Their model thus requires Gibbs sampling over both assignments and parameters, which can be very expensive.</S>\n    <S sid=\"37\" ssid=\"11\">Haghighi and Klein circumvent this by making approximations that potentially hurt accuracy.</S>\n    <S sid=\"38\" ssid=\"12\">At the same time, the Dirichlet process prior favors skewed cluster sizes and a number of clusters that grows logarithmically with the number of data points, neither of which seems generally appropriate for coreference resolution.</S>\n    <S sid=\"39\" ssid=\"13\">Further, deterministic or strong non-deterministic dependencies cause Gibbs sampling to break down (Poon &amp; Domingos, 2006), making it difficult to leverage many linguistic regularities.</S>\n    <S sid=\"40\" ssid=\"14\">For example, apposition (as in “Bill Gates, the chairman of Microsoft”) suggests coreference, and thus the two mentions it relates should always be placed in the same cluster.</S>\n    <S sid=\"41\" ssid=\"15\">However, Gibbs sampling can only move one mention at a time from one cluster to another, and this is unlikely to happen, because it would require breaking the apposition rule.</S>\n    <S sid=\"42\" ssid=\"16\">Blocked sampling can alleviate this problem by sampling multiple mentions together, but it requires that the block size be predetermined to a small fixed number.</S>\n    <S sid=\"43\" ssid=\"17\">When we incorporate apposition and other regularities the blocks can become arbitrarily large, making this infeasible.</S>\n    <S sid=\"44\" ssid=\"18\">For example, suppose we also want to leverage predicate nominals (i.e., the subject and the predicating noun of a copular verb are likely coreferent).</S>\n    <S sid=\"45\" ssid=\"19\">Then a sentence like “He is Bill Gates, the chairman of Microsoft” requires a block of four mentions: “He”, “Bill Gates”, “the chairman of Microsoft”, and “Bill Gates, the chairman of Microsoft”.</S>\n    <S sid=\"46\" ssid=\"20\">Similar difficulties occur with other inference methods.</S>\n    <S sid=\"47\" ssid=\"21\">Thus, extending Haghighi and Klein’s model to include richer linguistic features is a challenging problem.</S>\n    <S sid=\"48\" ssid=\"22\">Our approach is instead based on Markov logic, a powerful representation for joint inference with uncertainty (Richardson &amp; Domingos, 2006).</S>\n    <S sid=\"49\" ssid=\"23\">Like Haghighi and Klein’s, our model is cluster-based rather than pairwise, and implicitly imposes transitivity.</S>\n    <S sid=\"50\" ssid=\"24\">We do not predetermine anaphoricity of a mention, but rather fuse it into the integrated resolution process.</S>\n    <S sid=\"51\" ssid=\"25\">As a result, our model is inherently joint among mentions and subtasks.</S>\n    <S sid=\"52\" ssid=\"26\">It shares several features with Haghighi &amp; Klein’s model, but removes or refines features where we believe it is appropriate to.</S>\n    <S sid=\"53\" ssid=\"27\">Most importantly, our model leverages apposition and predicate nominals, which Haghighi &amp; Klein did not use.</S>\n    <S sid=\"54\" ssid=\"28\">We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains.</S>\n    <S sid=\"55\" ssid=\"29\">It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum &amp; Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp; Domingos, 2008)).</S>\n  </SECTION>\n  <SECTION title=\"3 Markov Logic\" number=\"3\">\n    <S sid=\"56\" ssid=\"1\">In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor &amp; Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy.</S>\n    <S sid=\"57\" ssid=\"2\">One of the most powerful representations for joint inference is Markov logic, a probabilistic extension of first-order logic (Richardson &amp; Domingos, 2006).</S>\n    <S sid=\"58\" ssid=\"3\">A Markov logic network (MLN) is a set of weighted first-order clauses.</S>\n    <S sid=\"59\" ssid=\"4\">Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause.</S>\n    <S sid=\"60\" ssid=\"5\">The weight of a feature is the weight of the firstorder clause that originated it.</S>\n    <S sid=\"61\" ssid=\"6\">The probability of a state x in such a network is given by P(x) = (1/Z) exp (Ez wzfz(x)), where Z is a normalization constant, wz is the weight of the ith clause, fz = 1 if the ith clause is true, and fz = 0 otherwise.</S>\n    <S sid=\"62\" ssid=\"7\">Markov logic makes it possible to compactly specify probability distributions over complex relational domains.</S>\n    <S sid=\"63\" ssid=\"8\">Efficient inference can be performed using MC-SAT (Poon &amp; Domingos, 2006).</S>\n    <S sid=\"64\" ssid=\"9\">MC-SAT is a “slice sampling” Markov chain Monte Carlo algorithm.</S>\n    <S sid=\"65\" ssid=\"10\">Slice sampling introduces auxiliary variables u that decouple the original ones x, and alternately samples u conditioned on x and viceversa.</S>\n    <S sid=\"66\" ssid=\"11\">To sample from the slice (the set of states x consistent with the current u), MC-SAT calls SampleSAT (Wei et al., 2004), which uses a combination of satisfiability testing and simulated annealing.</S>\n    <S sid=\"67\" ssid=\"12\">The advantage of using a satisfiability solver (WalkSAT) is that it efficiently finds isolated modes in the distribution, and as a result the Markov chain mixes very rapidly.</S>\n    <S sid=\"68\" ssid=\"13\">The slice sampling scheme ensures that detailed balance is (approximately) preserved.</S>\n    <S sid=\"69\" ssid=\"14\">MC-SAT is orders of magnitude faster than previous MCMC algorithms like Gibbs sampling, making efficient sampling possible on a scale that was previAlgorithm 1 MC-SAT(clauses, weights, num samples) x(c) Satisfy(hard clauses) for i 1 to num samples do M 0 for all ck E clauses satisfied by x(z−1) do With probability 1 — e−lk add ck to M ously out of reach.</S>\n    <S sid=\"70\" ssid=\"15\">Algorithm 1 gives pseudo-code for MC-SAT.</S>\n    <S sid=\"71\" ssid=\"16\">At iteration i — 1, the factor 0k for clause ck is either elk if ck is satisfied in x(z−1), or 1 otherwise.</S>\n    <S sid=\"72\" ssid=\"17\">MC-SAT first samples the auxiliary variable uk uniformly from (0, 0k), then samples a new state uniformly from the set of states that satisfy 0k &gt; uk for all k (the slice).</S>\n    <S sid=\"73\" ssid=\"18\">Equivalently, for each k, with probability 1 — e−lk the next state must satisfy ck.</S>\n    <S sid=\"74\" ssid=\"19\">In general, we can factorize the probability distribution in any way that facilitates inference, sample the uk’s, and make sure that the next state is drawn uniformly from solutions that satisfy 0k &gt; uk for all factors.</S>\n    <S sid=\"75\" ssid=\"20\">MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities.</S>\n    <S sid=\"76\" ssid=\"21\">We developed a general method for producing a “lazy” version of relational inference algorithms (Poon &amp; Domingos, 2008), which carries exactly the same inference steps as the original algorithm, but only maintains a small subset of “active” predicates/clauses, grounding more as needed.</S>\n    <S sid=\"77\" ssid=\"22\">We showed that Lazy-MC-SAT, the lazy version of MC-SAT, reduced memory and time by orders of magnitude in several domains.</S>\n    <S sid=\"78\" ssid=\"23\">We use Lazy-MC-SAT in this paper.</S>\n    <S sid=\"79\" ssid=\"24\">Supervised learning for Markov logic maximizes the conditional log-likelihood L(x, y) = log P(Y = yJX = x), where Y represents the non-evidence predicates, X the evidence predicates, and x, y their values in the training data.</S>\n    <S sid=\"80\" ssid=\"25\">For simplicity, from now on we omit X, whose values are fixed and always conditioned on.</S>\n    <S sid=\"81\" ssid=\"26\">The optimization problem is convex and a global optimum can be found using gradient descent, with the gradient being where ni is the number of true groundings of clause i.</S>\n    <S sid=\"82\" ssid=\"27\">The expected count can be approximated as where yk are samples generated by MC-SAT.</S>\n    <S sid=\"83\" ssid=\"28\">To combat overfitting, a Gaussian prior is imposed on all weights.</S>\n    <S sid=\"84\" ssid=\"29\">In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses.</S>\n    <S sid=\"85\" ssid=\"30\">Lowd &amp; Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.</S>\n    <S sid=\"86\" ssid=\"31\">This estimates the optimal step size in each step as where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent.</S>\n    <S sid=\"87\" ssid=\"32\">H is the Hessian matrix, with the (i, j)th entry being The Hessian can be approximated with the same samples used for the gradient.</S>\n    <S sid=\"88\" ssid=\"33\">Its negative inverse diagonal is used as the preconditioner.1 The open-source Alchemy package (Kok et al., 2007) provides implementations of existing algorithms for Markov logic.</S>\n    <S sid=\"89\" ssid=\"34\">In Section 5, we develop the first general-purpose unsupervised learning algorithm for Markov logic by extending the existing algorithms to handle hidden predicates.2 1Lowd &amp; Domingos showed that α can be computed more efficiently, without explicitly approximating or storing the Hessian.</S>\n    <S sid=\"90\" ssid=\"35\">Readers are referred to their paper for details.</S>\n    <S sid=\"91\" ssid=\"36\">2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates.</S>\n    <S sid=\"92\" ssid=\"37\">Kok &amp; Domingos (2007) applied Markov logic to relational clustering, but they used hard EM.</S>\n  </SECTION>\n  <SECTION title=\"4 An MLN for Joint Unsupervised Coreference Resolution\" number=\"4\">\n    <S sid=\"93\" ssid=\"1\">In this section, we present our MLN for joint unsupervised coreference resolution.</S>\n    <S sid=\"94\" ssid=\"2\">Our model deviates from Haghighi &amp; Klein’s (2007) in several important ways.</S>\n    <S sid=\"95\" ssid=\"3\">First, our MLN does not model saliences for proper nouns or nominals, as their influence is marginal compared to other features; for pronoun salience, it uses a more intuitive and simpler definition based on distance, and incorporated it as a prior.</S>\n    <S sid=\"96\" ssid=\"4\">Another difference is in identifying heads.</S>\n    <S sid=\"97\" ssid=\"5\">For the ACE datasets, Haghighi and Klein used the gold heads; for the MUC-6 dataset, where labels are not available, they crudely picked the rightmost token in a mention.</S>\n    <S sid=\"98\" ssid=\"6\">We show that a better way is to determine the heads using head rules in a parser.</S>\n    <S sid=\"99\" ssid=\"7\">This improves resolution accuracy and is always applicable.</S>\n    <S sid=\"100\" ssid=\"8\">Crucially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein.</S>\n    <S sid=\"101\" ssid=\"9\">In our approach, what it takes is just adding two formulas to the MLN.</S>\n    <S sid=\"102\" ssid=\"10\">As common in previous work, we assume that true mention boundaries are given.</S>\n    <S sid=\"103\" ssid=\"11\">We do not assume any other labeled information.</S>\n    <S sid=\"104\" ssid=\"12\">In particular, we do not assume gold name entity recognition (NER) labels, and unlike Haghighi &amp; Klein (2007), we do not assume gold mention types (for ACE datasets, they also used gold head words).</S>\n    <S sid=\"105\" ssid=\"13\">We determined the head of a mention either by taking its rightmost token, or by using the head rules in a parser.</S>\n    <S sid=\"106\" ssid=\"14\">We detected pronouns using a list.</S>\n    <S sid=\"107\" ssid=\"15\">The main query predicate is InClust(m, c!</S>\n    <S sid=\"108\" ssid=\"16\">), which is true iff mention m is in cluster c. The “t!” notation signifies that for each m, this predicate is true for a unique value of c. The main evidence predicate is Head(m, t!</S>\n    <S sid=\"109\" ssid=\"17\">), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head mixture model, where the mixture component priors are represented by the unit clause InClust(+m, +c) and the head distribution is represented by the head prediction rule All free variables are implicitly universally quantified.</S>\n    <S sid=\"110\" ssid=\"18\">The “+” notation signifies that the MLN contains an instance of the rule, with a separate weight, for each value combination of the variables with a plus sign.</S>\n    <S sid=\"111\" ssid=\"19\">By convention, at each inference step we name each non-empty cluster after the earliest mention it contains.</S>\n    <S sid=\"112\" ssid=\"20\">This helps break the symmetry among mentions, which otherwise produces multiple optima and makes learning unnecessarily harder.</S>\n    <S sid=\"113\" ssid=\"21\">To encourage clustering, we impose an exponential prior on the number of non-empty clusters with weight −1.</S>\n    <S sid=\"114\" ssid=\"22\">The above model only clusters mentions with the same head, and does not work well for pronouns.</S>\n    <S sid=\"115\" ssid=\"23\">To address this, we introduce the predicate IsPrn(m), which is true iff the mention m is a pronoun, and adapt the head prediction rule as follows: This is always false when m is a pronoun, and thus applies only to non-pronouns.</S>\n    <S sid=\"116\" ssid=\"24\">Pronouns tend to resolve with mentions that are semantically compatible with them.</S>\n    <S sid=\"117\" ssid=\"25\">Thus we introduce predicates that represent entity type, number, and gender: Type(x, e!</S>\n    <S sid=\"118\" ssid=\"26\">), Number(x, n!</S>\n    <S sid=\"119\" ssid=\"27\">), Gender(x, g!</S>\n    <S sid=\"120\" ssid=\"28\">), where x can be either a cluster or mention, e E {Person,Organization,Location,Other}, n E {Singular,Plural} and g E {Male, Female, Neuter}.</S>\n    <S sid=\"121\" ssid=\"29\">Many of these are known for pronouns, and some can be inferred from simple linguistic cues (e.g., “Ms.</S>\n    <S sid=\"122\" ssid=\"30\">Galen” is a singular female person, while “XYZ Corp.” is an organization).3 Entity type assignment is represented by the unit clause Type(+x, +e) and similarly for number and gender.</S>\n    <S sid=\"123\" ssid=\"31\">A mention should agree with its cluster in entity type.</S>\n    <S sid=\"124\" ssid=\"32\">This is ensured by the hard rule (which has infinite weight and must be satisfied) InClust(m, c) ==&gt; (Type(m, e) &lt;---&gt; Type(c, e)) 3We used the following cues: Mr., Ms., Jr., Inc., Corp., corporation, company.</S>\n    <S sid=\"125\" ssid=\"33\">The proportions of known properties range from 14% to 26%.</S>\n    <S sid=\"126\" ssid=\"34\">There are similar hard rules for number and gender.</S>\n    <S sid=\"127\" ssid=\"35\">Different pronouns prefer different entity types, as represented by which only applies to pronouns, and whose weight is positive if pronoun t is likely to assume entity type e and negative otherwise.</S>\n    <S sid=\"128\" ssid=\"36\">There are similar rules for number and gender.</S>\n    <S sid=\"129\" ssid=\"37\">Aside from semantic compatibility, pronouns tend to resolve with nearby mentions.</S>\n    <S sid=\"130\" ssid=\"38\">To model this, we impose an exponential prior on the distance (number of mentions) between a pronoun and its antecedent, with weight −1.4 This is similar to Haghighi and Klein’s treatment of salience, but simpler.</S>\n    <S sid=\"131\" ssid=\"39\">Syntactic relations among mentions often suggest coreference.</S>\n    <S sid=\"132\" ssid=\"40\">Incorporating such relations into our MLN is straightforward.</S>\n    <S sid=\"133\" ssid=\"41\">We illustrate this with two examples: apposition and predicate nominals.</S>\n    <S sid=\"134\" ssid=\"42\">We introduce a predicate for apposition, Appo(x, y), where x, y are mentions, and which is true iff y is an appositive of x.</S>\n    <S sid=\"135\" ssid=\"43\">We then add the rule Appo(x, y) ==&gt; (InClust(x, c) &lt;---&gt; InClust(y, c)) which ensures that x, y are in the same cluster if y is an appositive of x.</S>\n    <S sid=\"136\" ssid=\"44\">Similarly, we introduce a predicate for predicate nominals, PredNom(x, y), and the corresponding rule.5 The weights of both rules can be learned from data with a positive prior mean.</S>\n    <S sid=\"137\" ssid=\"45\">For simplicity, in this paper we treat them as hard constraints.</S>\n    <S sid=\"138\" ssid=\"46\">We also consider a rule-based system that clusters non-pronouns by their heads, and attaches a pronoun to the cluster which has no known conflicting type, number, or gender, and contains the closest antecedent for the pronoun.</S>\n    <S sid=\"139\" ssid=\"47\">This system can be encoded in an MLN with just four rules.</S>\n    <S sid=\"140\" ssid=\"48\">Three of them are the ones for enforcing agreement in type, number, and gender between a cluster and its members, as defined in the base MLN.</S>\n    <S sid=\"141\" ssid=\"49\">The fourth rule is With a large but not infinite weight (e.g., 100), this rule has the effect of clustering non-pronouns by their heads, except when it violates the hard rules.</S>\n    <S sid=\"142\" ssid=\"50\">The MLN can also include the apposition and predicate-nominal rules.</S>\n    <S sid=\"143\" ssid=\"51\">As in the base MLN, we impose the same exponential prior on the number of non-empty clusters and that on the distance between a pronoun and its antecedent.</S>\n    <S sid=\"144\" ssid=\"52\">This simple MLN is remarkably competitive, as we will see in the experiment section.</S>\n  </SECTION>\n  <SECTION title=\"5 Learning and Inference\" number=\"5\">\n    <S sid=\"145\" ssid=\"1\">Unsupervised learning in Markov logic maximizes the conditional log-likelihood where Z are unknown predicates.</S>\n    <S sid=\"146\" ssid=\"2\">In our coreference resolution MLN, Y includes Head and known groundings of Type, Number and Gender, Z includes InClust and unknown groundings of Type, Number, Gender, and X includes IsPrn, Appo and PredNom.</S>\n    <S sid=\"147\" ssid=\"3\">(For simplicity, from now on we drop X from the formula.)</S>\n    <S sid=\"148\" ssid=\"4\">With Z, the optimization problem is no longer convex.</S>\n    <S sid=\"149\" ssid=\"5\">However, we can still find a local optimum using gradient descent, with the gradient being where ni is the number of true groundings of the ith clause.</S>\n    <S sid=\"150\" ssid=\"6\">We extended PSCG for unsupervised learning.</S>\n    <S sid=\"151\" ssid=\"7\">The gradient is the difference of two expectations, each of which can be approximated using samples generated by MC-SAT.</S>\n    <S sid=\"152\" ssid=\"8\">The (i, j)th entry of and the step size can be computed accordingly.</S>\n    <S sid=\"153\" ssid=\"9\">Since our problem is no longer convex, the negative diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner.</S>\n    <S sid=\"154\" ssid=\"10\">We also adjusted A more conservatively than Lowd &amp; Domingos (2007).</S>\n    <S sid=\"155\" ssid=\"11\">Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning.</S>\n    <S sid=\"156\" ssid=\"12\">We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI).</S>\n    <S sid=\"157\" ssid=\"13\">Learning in MUC-6 took only one hour, and in ACE-2004 two and a half.</S>\n    <S sid=\"158\" ssid=\"14\">To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses.</S>\n    <S sid=\"159\" ssid=\"15\">In the existing implementation in Alchemy (Kok et al., 2007), SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m, c!)).</S>\n    <S sid=\"160\" ssid=\"16\">Such predicates can be viewed as multi-valued predicates (e.g., Head(m) with value ranging over all c’s) and are prevalent in NLP applications.</S>\n    <S sid=\"161\" ssid=\"17\">We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied.</S>\n    <S sid=\"162\" ssid=\"18\">By default, MC-SAT treats each ground clause as a separate factor while determining the slice.</S>\n    <S sid=\"163\" ssid=\"19\">This can be very inefficient for highly correlated clauses.</S>\n    <S sid=\"164\" ssid=\"20\">For example, given a non-pronoun mention m currently in cluster c and with head t, among the mixture prior rules involving m InClust(m, c) is the only one that is satisfied, and among those head-prediction rules involving m, ¬IsPrn(m)∧InClust(m, c)∧Head(m, t) is the only one that is satisfied; the factors for these rules multiply to = exp(wm, + wm,�,t), where wm,r is the weight for InClust(m, c), and wm,�,t is the weight for ¬IsPrn(m) ∧ InClust(m, c) ∧ Head(m, t), since an unsatisfied rule contributes a factor of e0 = 1.</S>\n    <S sid=\"165\" ssid=\"21\">We extended MC-SAT to treat each set of mutually exclusive and exhaustive rules as a single factor.</S>\n    <S sid=\"166\" ssid=\"22\">E.g., for the above m, MC-SAT now samples u uniformly from (0, 0), and requires that in the next state 0' be no less than u. Equivalently, the new cluster and head for m should satisfy wm,c, + wm,c,,t, &gt; log(u).</S>\n    <S sid=\"167\" ssid=\"23\">We extended SampleSAT so that when it considers flipping any variable involved in such constraints (e.g., c or t above), it ensures that their new values still satisfy these constraints.</S>\n    <S sid=\"168\" ssid=\"24\">The final clustering is found using the MaxWalkSAT weighted satisfiability solver (Kautz et al., 1997), with the appropriate extensions.</S>\n    <S sid=\"169\" ssid=\"25\">We first ran a MaxWalkSAT pass with only finite-weight formulas, then ran another pass with all formulas.</S>\n    <S sid=\"170\" ssid=\"26\">We found that this significantly improved the quality of the results that MaxWalkSAT returned.</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments\" number=\"6\">\n    <S sid=\"171\" ssid=\"1\">We implemented our method as an extension to the Alchemy system (Kok et al., 2007).</S>\n    <S sid=\"172\" ssid=\"2\">Since our learning uses sampling, all results are the average of five runs using different random seeds.</S>\n    <S sid=\"173\" ssid=\"3\">Our optimization problem is not convex, so initialization is important.</S>\n    <S sid=\"174\" ssid=\"4\">The core of our model (head mixture) tends to cluster non-pronouns with the same head.</S>\n    <S sid=\"175\" ssid=\"5\">Therefore, we initialized by setting all weights to zero, and running the same learning algorithm on the base MLN, while assuming that in the ground truth, nonpronouns are clustered by their heads.</S>\n    <S sid=\"176\" ssid=\"6\">(Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.)</S>\n    <S sid=\"177\" ssid=\"7\">We used 30 iterations of PSCG for learning.</S>\n    <S sid=\"178\" ssid=\"8\">(In preliminary experiments, additional iterations had little effect on coreference accuracy.)</S>\n    <S sid=\"179\" ssid=\"9\">We generated 100 samples using MC-SAT for each expectation approximation.6 We conducted experiments on MUC-6, ACE-2004, and ACE Phrase-2 (ACE-2).</S>\n    <S sid=\"180\" ssid=\"10\">We evaluated our systems using two commonly-used scoring programs: MUC (Vilain et al., 1995) and B3 (Amit &amp; Baldwin, 1998).</S>\n    <S sid=\"181\" ssid=\"11\">To gain more insight, we also report pairwise resolution scores and mean absolute error in the number of clusters.</S>\n    <S sid=\"182\" ssid=\"12\">The MUC-6 dataset consists of 30 documents for testing and 221 for training.</S>\n    <S sid=\"183\" ssid=\"13\">To evaluate the contribution of the major components in our model, we conducted five experiments, each differing from the previous one in a single aspect.</S>\n    <S sid=\"184\" ssid=\"14\">We emphasize that our approach is unsupervised, and thus the data only contains raw text plus true mention boundaries.</S>\n    <S sid=\"185\" ssid=\"15\">MLN-1 In this experiment, the base MLN was used, and the head was chosen crudely as the rightmost token in a mention.</S>\n    <S sid=\"186\" ssid=\"16\">Our system was run on each test document separately, using a minimum of training data (the document itself).</S>\n    <S sid=\"187\" ssid=\"17\">MLN-30 Our system was trained on all 30 test documents together.</S>\n    <S sid=\"188\" ssid=\"18\">This tests how much can be gained by pooling information.</S>\n    <S sid=\"189\" ssid=\"19\">MLN-H The heads were determined using the head rules in the Stanford parser (Klein &amp; Manning, 2003), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added.</S>\n    <S sid=\"190\" ssid=\"20\">MLN-HAN The predicate-nominal rule was added.</S>\n    <S sid=\"191\" ssid=\"21\">This is our full model.</S>\n    <S sid=\"192\" ssid=\"22\">We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule.</S>\n    <S sid=\"193\" ssid=\"23\">Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set.</S>\n    <S sid=\"194\" ssid=\"24\">In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007).</S>\n    <S sid=\"195\" ssid=\"25\">Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007).</S>\n    <S sid=\"196\" ssid=\"26\">The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively.</S>\n    <S sid=\"197\" ssid=\"27\">ACE-2 contains a training set and a test set.</S>\n    <S sid=\"198\" ssid=\"28\">In our experiments, we only used the test set, which contains three sections, BNEWS, NWIRE, and NPAPER, with 51, 29, and 17 documents, respectively.</S>\n  </SECTION>\n  <SECTION title=\"6.3 Results\" number=\"7\">\n    <S sid=\"199\" ssid=\"1\">Table 1 compares our system with previous approaches on the MUC-6 dataset, in MUC scores.</S>\n    <S sid=\"200\" ssid=\"2\">Our approach greatly outperformed Haghighi &amp; Klein (2007), the state-of-the-art unsupervised system.</S>\n    <S sid=\"201\" ssid=\"3\">Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outperformed it trained on 381 documents.</S>\n    <S sid=\"202\" ssid=\"4\">Training on the 30 test documents together resulted in a significant gain.</S>\n    <S sid=\"203\" ssid=\"5\">(We also ran experiments using more documents, and the results were similar.)</S>\n    <S sid=\"204\" ssid=\"6\">Better head identification (MLN-H) led to a large improvement in accuracy, which is expected since for mentions with a right modifier, the rightmost tokens confuse rather than help coreference (e.g., “the chairman of Microsoft”).</S>\n    <S sid=\"205\" ssid=\"7\">Notice that with this improvement our system already outperforms a state-of-theart supervised system (McCallum &amp; Wellner, 2005).</S>\n    <S sid=\"206\" ssid=\"8\">Leveraging apposition resulted in another large improvement, and predicate nominals also helped.</S>\n    <S sid=\"207\" ssid=\"9\">Our full model scores about 9% higher than Haghighi &amp; Klein (2007), and about 6% higher than McCallum &amp; Wellner (2005).</S>\n    <S sid=\"208\" ssid=\"10\">To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1).</S>\n    <S sid=\"209\" ssid=\"11\">(The other systems did not report B3.)</S>\n    <S sid=\"210\" ssid=\"12\">Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp; Wellner (2005), the supervised system.</S>\n    <S sid=\"211\" ssid=\"13\">The MLNs with learning (MLN-30 and MLN-HAN), on the other hand, substantially outperformed the corresponding rule-based ones.</S>\n    <S sid=\"212\" ssid=\"14\">Table 2 compares our system to Haghighi &amp; Klein (2007) on the ACE-2004 training set in MUC scores.</S>\n    <S sid=\"213\" ssid=\"15\">Again, our system outperformed theirs by a large margin.</S>\n    <S sid=\"214\" ssid=\"16\">The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE.</S>\n    <S sid=\"215\" ssid=\"17\">(Haghighi &amp; Klein (2007) did not report B3.)</S>\n    <S sid=\"216\" ssid=\"18\">Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set.</S>\n    <S sid=\"217\" ssid=\"19\">Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007).</S>\n    <S sid=\"218\" ssid=\"20\">Our approach significantly outperformed Ng (2005).</S>\n    <S sid=\"219\" ssid=\"21\">It tied with Denis &amp; Baldridge (2007) on NWIRE, and was somewhat less accurate on BNEWS and NPAPER.</S>\n    <S sid=\"220\" ssid=\"22\">Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together.</S>\n    <S sid=\"221\" ssid=\"23\">B3 suffers less from this problem but is not perfect.</S>\n    <S sid=\"222\" ssid=\"24\">Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute error in the number of clusters (Table 6).</S>\n    <S sid=\"223\" ssid=\"25\">Systems that simply merge all mentions will have exceedingly low pairwise precision (far below 50%), and very large errors in the number of clusters.</S>\n    <S sid=\"224\" ssid=\"26\">Our system has fairly good pairwise precisions and small mean error in the number of clusters, which verifies that our results are sound.</S>\n    <S sid=\"225\" ssid=\"27\">Many of our system’s remaining errors involve nominals.</S>\n    <S sid=\"226\" ssid=\"28\">Additional features should be considered to distinguish mentions that have the same head but are different entities.</S>\n    <S sid=\"227\" ssid=\"29\">For pronouns, many remaining errors can be corrected using linguistic knowledge like binding theory and salience hierarchy.</S>\n    <S sid=\"228\" ssid=\"30\">Our heuristics for identifying appositives and predicate nominals also make many errors, which often can be fixed with additional name entity recognition capabilities (e.g., given “Mike Sullivan, VOA News”, it helps to know that the former is a person and the latter an organization).</S>\n    <S sid=\"229\" ssid=\"31\">The most challenging case involves phrases with different heads that are both proper nouns (e.g., “Mr.</S>\n    <S sid=\"230\" ssid=\"32\">Bush” and “the White House”).</S>\n    <S sid=\"231\" ssid=\"33\">Handling these cases requires domain knowledge and/or more powerful joint inference.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"8\">\n    <S sid=\"232\" ssid=\"1\">This paper introduces the first unsupervised coreference resolution system that is as accurate as supervised systems.</S>\n    <S sid=\"233\" ssid=\"2\">It performs joint inference among mentions, using relations like apposition and predicate nominals.</S>\n    <S sid=\"234\" ssid=\"3\">It uses Markov logic as a representation language, which allows it to be easily extended to incorporate additional linguistic and world knowledge.</S>\n    <S sid=\"235\" ssid=\"4\">Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.</S>\n  </SECTION>\n  <SECTION title=\"8 Acknowledgements\" number=\"9\">\n    <S sid=\"236\" ssid=\"1\">We thank the anonymous reviewers for their comments.</S>\n    <S sid=\"237\" ssid=\"2\">This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.</S>\n    <S sid=\"238\" ssid=\"3\">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "0dd102b563fc25f0ca1c10eecf109e0f",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Joint Unsupervised Coreference Resolution with Markov Logic"
                }
            }
        },
        {
            "index": 10,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for",
                    "filename": "Parsing_The_Wall_Street_Journal_Using_A_Lexical-Functional_Grammar_And_Discriminative_Estimation_Techniques",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P02-1035/Documents_xml/P02-1035.xml",
                    "title": "Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques"
                },
                "llm": {
                    "descriptions": [
                        "This paper describes the development and evaluation of a stochastic parsing system for The Wall Street Journal (WSJ) treebank, using a hand-coded linguistically fine-grained grammar and a stochastic disambiguation model. The system achieves full grammar coverage through the use of constraint-based parsing techniques and partial parsing methods. Additionally, the paper discusses the use of WSJ treebank annotations for discriminative estimation of an exponential model on fine-grained parses. Experimental evaluation results show 79% F-score on WSJ data and 76% F-score on the Brown corpus test set. The paper also highlights the reduction in error rate and compares the results to other stochastic parsing systems, particularly the parser of Carroll et al. (1999).",
                        "This paper presents an attempt to scale up a stochastic parsing system using a hand-coded linguistically fine-grained grammar and a stochastic disambiguation model to the WSJ treebank. To achieve full grammar coverage, specialized constraint-based parsing techniques for LFG grammars are combined with partial parsing techniques. The paper also discusses the maximal exploitation of treebank annotations for estimating a distribution on fine-grained LFG parses. The combined system trained on WSJ data achieves full grammar coverage and a disambiguation performance of 79% F-score on WSJ data, and 76% F-score on the Brown corpus test set. The paper highlights that the disambiguator offers a narrow window to operate due to the symbolic grammar's significant restriction of possible analyses.",
                        "This paper presents an attempt to scale up a stochastic parsing system combining a hand-coded linguistically fine-grained grammar and a stochastic disambiguation model to the WSJ treebank. The approach achieves full grammar coverage and maximal exploitation of treebank annotations for estimating a distribution on fine-grained LFG parses. The combined system trained on WSJ data achieves full grammar coverage and disambiguation performance of 79% F-score on WSJ data and 76% F-score on the Brown corpus test set. The paper discusses the challenges and implications of the achieved disambiguation performance and compares the results to other stochastic parsing systems, highlighting the reduction in error rate.",
                        "This paper presents an approach to scale up stochastic parsing systems using handcoded grammars and discriminative estimation techniques. The goal is to parse the UPenn Wall Street Journal (WSJ) treebank and achieve 100% grammar coverage on unseen data. The paper describes the Lexical-Functional Grammar (LFG) and constraint-based parser used, as well as the robustness techniques employed. It also details the exponential model on LFG parses and the discriminative statistical estimation technique. Experimental results are reported, including the disambiguation performance, and a discussion of these results is provided. The paper concludes with a comparison of the results to other stochastic parsing systems, particularly the parser of Carroll et al. (1999)."
                    ],
                    "filename": "parsing_the_wall_street_journal_-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 116 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"8\" ssid=\"1\">Statistical parsing using combined systems of handcoded linguistically fine-grained grammars and stochastic disambiguation components has seen considerable progress in recent years.</S>\n    <S sid=\"9\" ssid=\"2\">However, such attempts have so far been confined to a relatively small scale for various reasons.</S>\n    <S sid=\"10\" ssid=\"3\">Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems.</S>\n    <S sid=\"11\" ssid=\"4\">Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).</S>\n    <S sid=\"12\" ssid=\"5\">Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text.</S>\n    <S sid=\"13\" ssid=\"6\">The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994).</S>\n    <S sid=\"14\" ssid=\"7\">The problem of grammar coverage, i.e. the fact that not all sentences receive an analysis, is tackled in our approach by an extension of a fullfledged Lexical-Functional Grammar (LFG) and a constraint-based parser with partial parsing techniques.</S>\n    <S sid=\"15\" ssid=\"8\">In the absence of a complete parse, a socalled “FRAGMENT grammar” allows the input to be analyzed as a sequence of well-formed chunks.</S>\n    <S sid=\"16\" ssid=\"9\">The set of fragment parses is then chosen on the basis of a fewest-chunk method.</S>\n    <S sid=\"17\" ssid=\"10\">With this combination of full and partial parsing techniques we achieve 100% grammar coverage on unseen data.</S>\n    <S sid=\"18\" ssid=\"11\">Another goal of this work is the best possible exploitation of the WSJ treebank for discriminative estimation of an exponential model on LFG parses.</S>\n    <S sid=\"19\" ssid=\"12\">We define discriminative or conditional criteria with respect to the set of grammar parses consistent with the treebank annotations.</S>\n    <S sid=\"20\" ssid=\"13\">Such data can be gathered by applying labels and brackets taken from the treebank annotation to the parser input.</S>\n    <S sid=\"21\" ssid=\"14\">The rudimentary treebank annotations are thus used to provide partially labeled data for discriminative estimation of a probability model on linguistically fine-grained parses.</S>\n    <S sid=\"22\" ssid=\"15\">Concerning empirical evaluation of disambiguation performance, we feel that an evaluation measuring matches of predicate-argument relations is more appropriate for assessing the quality of our LFGbased system than the standard measure of matching labeled bracketing on section 23 of the WSJ treebank.</S>\n    <S sid=\"23\" ssid=\"16\">The first evaluation we present measures matches of predicate-argument relations in LFG fstructures (henceforth the LFG annotation scheme) to a gold standard of manually annotated f-structures for a representative subset of the WSJ treebank.</S>\n    <S sid=\"24\" ssid=\"17\">The evaluation measure counts the number of predicateargument relations in the f-structure of the parse selected by the stochastic model that match those in the gold standard annotation.</S>\n    <S sid=\"25\" ssid=\"18\">Our parser plus stochastic disambiguator achieves 79% F-score under this evaluation regime.</S>\n    <S sid=\"26\" ssid=\"19\">Furthermore, we employ another metric which maps predicate-argument relations in LFG fstructures to the dependency relations (henceforth the DR annotation scheme) proposed by Carroll et al. (1999).</S>\n    <S sid=\"27\" ssid=\"20\">Evaluation with this metric measures the matches of dependency relations to Carroll et al.’s gold standard corpus.</S>\n    <S sid=\"28\" ssid=\"21\">For a direct comparison of our results with Carroll et al.’s system, we computed an F-score that does not distinguish different types of dependency relations.</S>\n    <S sid=\"29\" ssid=\"22\">Under this measure we obtain 76% F-score.</S>\n    <S sid=\"30\" ssid=\"23\">This paper is organized as follows.</S>\n    <S sid=\"31\" ssid=\"24\">Section 2 describes the Lexical-Functional Grammar, the constraint-based parser, and the robustness techniques employed in this work.</S>\n    <S sid=\"32\" ssid=\"25\">In section 3 we present the details of the exponential model on LFG parses and the discriminative statistical estimation technique.</S>\n    <S sid=\"33\" ssid=\"26\">Experimental results are reported in section 4.</S>\n    <S sid=\"34\" ssid=\"27\">A discussion of results is in section 5.</S>\n  </SECTION>\n  <SECTION title=\"2 Robust Parsing using LFG\" number=\"2\">\n    <S sid=\"35\" ssid=\"1\">The grammar used for this project was developed in the ParGram project (Butt et al., 1999).</S>\n    <S sid=\"36\" ssid=\"2\">It uses LFG as a formalism, producing c(onstituent)-structures (trees) and f(unctional)-structures (attribute value matrices) as output.</S>\n    <S sid=\"37\" ssid=\"3\">The c-structures encode constituency.</S>\n    <S sid=\"38\" ssid=\"4\">F-structures encode predicate-argument relations and other grammatical information, e.g., number, tense.</S>\n    <S sid=\"39\" ssid=\"5\">The XLE parser (Maxwell and Kaplan, 1993) was used to produce packed representations, specifying all possible grammar analyses of the input.</S>\n    <S sid=\"40\" ssid=\"6\">The grammar has 314 rules with regular expression right-hand sides which compile into a collection of finite-state machines with a total of 8,759 states and 19,695 arcs.</S>\n    <S sid=\"41\" ssid=\"7\">The grammar uses several lexicons and two guessers: one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized.</S>\n    <S sid=\"42\" ssid=\"8\">As such, most nouns, adjectives, and adverbs have no explicit lexical entry.</S>\n    <S sid=\"43\" ssid=\"9\">The main verb lexicon contains 9,652 verb stems and 23,525 subcategorization frame-verb stem entries; there are also lexicons for adjectives and nouns with subcategorization frames and for closed class items.</S>\n    <S sid=\"44\" ssid=\"10\">For estimation purposes using the WSJ treebank, the grammar was modified to parse part of speech tags and labeled bracketing.</S>\n    <S sid=\"45\" ssid=\"11\">A stripped down version of the WSJ treebank was created that used only those POS tags and labeled brackets relevant for determining grammatical relations.</S>\n    <S sid=\"46\" ssid=\"12\">The WSJ labeled brackets are given LFG lexical entries which constrain both the c-structure and the f-structure of the parse.</S>\n    <S sid=\"47\" ssid=\"13\">For example, the WSJ’s ADJP-PRD label must correspond to an AP in the c-structure and an XCOMP in the f-structure.</S>\n    <S sid=\"48\" ssid=\"14\">In this version of the corpus, all WSJ labels with -SBJ are retained and are restricted to phrases corresponding to SUBJ in the LFG grammar; in addition, it contains NP under VP (OBJ and OBJth in the LFG grammar), all -LGS tags (OBL-AG), all -PRD tags (XCOMP), VP under VP (XCOMP), SBAR- (COMP), and verb POS tags under VP (V in the c-structure).</S>\n    <S sid=\"49\" ssid=\"15\">For example, our labeled bracketing of wsj 1305.mrg is [NP-SBJHis credibility] is/VBZ also [PP-PRD on the line] in the investment community.</S>\n    <S sid=\"50\" ssid=\"16\">Some mismatches between the WSJ labeled bracketing and the LFG grammar remain.</S>\n    <S sid=\"51\" ssid=\"17\">These often arise when a given constituent fills a grammatical role in more than one clause.</S>\n    <S sid=\"52\" ssid=\"18\">For example, in wsj 1303.mrg Japan’s Daiwa Securities Co. named Masahiro Dozen president., the noun phrase Masahiro Dozen is labeled as an NP-SBJ.</S>\n    <S sid=\"53\" ssid=\"19\">However, the LFG grammar treats it as the OBJ of the matrix clause.</S>\n    <S sid=\"54\" ssid=\"20\">As a result, the labeled bracketed version of this sentence does not receive a full parse, even though its unlabeled, string-only counterpart is wellformed.</S>\n    <S sid=\"55\" ssid=\"21\">Some other bracketing mismatches remain, usually the result of adjunct attachment.</S>\n    <S sid=\"56\" ssid=\"22\">Such mismatches occur in part because, besides minor modifications to match the bracketing for special constructions, e.g., negated infinitives, the grammar was not altered to mirror the idiosyncrasies of the WSJ bracketing.</S>\n    <S sid=\"57\" ssid=\"23\">To increase robustness, the standard grammar has been augmented with a FRAGMENT grammar.</S>\n    <S sid=\"58\" ssid=\"24\">This grammar parses the sentence as well-formed chunks specified by the grammar, in particular as Ss, NPs, PPs, and VPs.</S>\n    <S sid=\"59\" ssid=\"25\">These chunks have both c-structures and f-structures corresponding to them.</S>\n    <S sid=\"60\" ssid=\"26\">Any token that cannot be parsed as one of these chunks is parsed as a TOKEN chunk.</S>\n    <S sid=\"61\" ssid=\"27\">The TOKENs are also recorded in the c- and f-structures.</S>\n    <S sid=\"62\" ssid=\"28\">The grammar has a fewest-chunk method for determining the correct parse.</S>\n    <S sid=\"63\" ssid=\"29\">For example, if a string can be parsed as two NPs and a VP or as one NP and an S, the NP-S option is chosen.</S>\n    <S sid=\"64\" ssid=\"30\">A sample FRAGMENT c-structure and f-structure are shown in Fig.</S>\n    <S sid=\"65\" ssid=\"31\">1 for wsj 0231.mrg (The golden share was scheduled to expire at the beginning of), an incomplete sentence; the parser builds one S chunk and then one TOKEN for the stranded preposition.</S>\n    <S sid=\"66\" ssid=\"32\">A final capability of XLE that increases coverage of the standard-plus-fragment grammar is a SKIMMING technique.</S>\n    <S sid=\"67\" ssid=\"33\">Skimming is used to avoid timeouts and memory problems.</S>\n    <S sid=\"68\" ssid=\"34\">When the amount of time or memory spent on a sentence exceeds a threshhold, XLE goes into skimming mode for the constituents whose processing has not been completed.</S>\n    <S sid=\"69\" ssid=\"35\">When XLE skims these remaining constituents, it does a bounded amount of work per subtree.</S>\n    <S sid=\"70\" ssid=\"36\">This guarantees that XLE finishes processing a sentence in a polynomial amount of time.</S>\n    <S sid=\"71\" ssid=\"37\">In parsing section 23, 7.2% of the sentences were skimmed; 26.1% of these resulted in full parses, while 73.9% were FRAGMENT parses.</S>\n    <S sid=\"72\" ssid=\"38\">The grammar coverage achieved 100% of section 23 as unseen unlabeled data: 74.7% as full parses, 25.3% FRAGMENT and/or SKIMMED parses.</S>\n  </SECTION>\n  <SECTION title=\"3 Discriminative Statistical Estimation from Partially Labeled Data\" number=\"3\">\n    <S sid=\"73\" ssid=\"1\">We employed the well-known family of exponential models for stochastic disambiguation.</S>\n    <S sid=\"74\" ssid=\"2\">In this paper we are concerned with conditional exponential models of the form: where X(y) is the set of parses for sentence y, Zλ(y) = PxEX(y) eλ'f(x) is a normalizing constant, λ = (λ1, ... , λn) E IRn is a vector of log-parameters, f = (f1, ... , fn) is a vector of property-functions fi : X IR for i = 1, ... , n on the set of parses X, and λ f(x) is the vector dot product Pni=1 λifi(x).</S>\n    <S sid=\"75\" ssid=\"3\">In our experiments, we used around 1000 complex property-functions comprising information about c-structure, f-structure, and lexical elements in parses, similar to the properties used in Johnson et al. (1999).</S>\n    <S sid=\"76\" ssid=\"4\">For example, there are property functions for c-structure nodes and c-structure subtrees, indicating attachment preferences.</S>\n    <S sid=\"77\" ssid=\"5\">High versus low attachment is indicated by property functions counting the number of recursively embedded phrases.</S>\n    <S sid=\"78\" ssid=\"6\">Other property functions are designed to refer to f-structure attributes, which correspond to grammatical functions in LFG, or to atomic attributevalue pairs in f-structures.</S>\n    <S sid=\"79\" ssid=\"7\">More complex property functions are designed to indicate, for example, the branching behaviour of c-structures and the (non)parallelism of coordinations on both c-structure and f-structure levels.</S>\n    <S sid=\"80\" ssid=\"8\">Furthermore, properties refering to lexical elements based on an auxiliary distribution approach as presented in Riezler et al. (2000) are included in the model.</S>\n    <S sid=\"81\" ssid=\"9\">Here tuples of head words, argument words, and grammatical relations are extracted from the training sections of the WSJ, and fed into a finite mixture model for clustering grammatical relations.</S>\n    <S sid=\"82\" ssid=\"10\">The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses.</S>\n    <S sid=\"83\" ssid=\"11\">Discriminative estimation techniques have recently received great attention in the statistical machine learning community and have already been applied to statistical parsing (Johnson et al., 1999; Collins, 2000; Collins and Duffy, 2001).</S>\n    <S sid=\"84\" ssid=\"12\">In discriminative estimation, only the conditional relation of an analysis given an example is considered relevant, whereas in maximum likelihood estimation the joint probability of the training data to best describe observations is maximized.</S>\n    <S sid=\"85\" ssid=\"13\">Since the discriminative task is kept in mind during estimation, discriminative methods can yield improved performance.</S>\n    <S sid=\"86\" ssid=\"14\">In our case, discriminative criteria cannot be defined directly with respect to “correct labels” or “gold standard” parses since the WSJ annotations are not sufficient to disambiguate the more complex LFG parses.</S>\n    <S sid=\"87\" ssid=\"15\">However, instead of retreating to unsupervised estimation techniques or creating small LFG treebanks by hand, we use the labeled bracketing of the WSJ training sections to guide discriminative estimation.</S>\n    <S sid=\"88\" ssid=\"16\">That is, discriminative criteria are defined with respect to the set ofparses consistent with the WSJ annotations.1 The objective function in our approach, denoted by P(λ), is the joint of the negative log-likelihood −L(λ) and a Gaussian regularization term −G(λ) on the parameters λ.</S>\n    <S sid=\"89\" ssid=\"17\">Let {(yj, zj)Imj=1 be a set of training data, consisting of pairs of sentences y and partial annotations z, let X(y, z) be the set of parses for sentence y consistent with annotation z, and let X(y) be the set of all parses produced by the grammar for sentence y.</S>\n    <S sid=\"90\" ssid=\"18\">Furthermore, let p[f] denote the expectation of function f under distribution p. Then P(λ) can be defined for a conditional exponential model pλ(zIy) as: Intuitively, the goal of estimation is to find model pa'An earlier approach using partially labeled data for estimating stochastics parsers is Pereira and Schabes’s (1992) work on training PCFG from partially bracketed data.</S>\n    <S sid=\"91\" ssid=\"19\">Their approach differs from the one we use here in that Pereira and Schabes take an EM-based approach maximizing the joint likelihood of the parses and strings of their training data, while we maximize the conditional likelihood of the sets of parses given the corresponding strings in a discriminative estimation setting. rameters which make the two expectations in the last equation equal, i.e. which adjust the model parameters to put all the weight on the parses consistent with the annotations, modulo a penalty term from the Gaussian prior for too large or too small weights.</S>\n    <S sid=\"92\" ssid=\"20\">Since a closed form solution for such parameters is not available, numerical optimization methods have to be used.</S>\n    <S sid=\"93\" ssid=\"21\">In our experiments, we applied a conjugate gradient routine, yielding a fast converging optimization algorithm where at each iteration the negative log-likelihood P(λ) and the gradient vector have to be evaluated.2 For our task the gradient takes the form: The derivatives in the gradient vector intuitively are again just a difference of two expectations Note also that this expression shares many common terms with the likelihood function, suggesting an efficient implementation of the optimization routine.</S>\n  </SECTION>\n  <SECTION title=\"4 Experimental Evaluation\" number=\"4\">\n    <S sid=\"94\" ssid=\"1\">The basic training data for our experiments are sections 02-21 of the WSJ treebank.</S>\n    <S sid=\"95\" ssid=\"2\">As a first step, all sections were parsed, and the packed parse forests unpacked and stored.</S>\n    <S sid=\"96\" ssid=\"3\">For discriminative estimation, this data set was restricted to sentences which receive a full parse (in contrast to a FRAGMENT or SKIMMED parse) for both its partially labeled and its unlabeled variant.</S>\n    <S sid=\"97\" ssid=\"4\">Furthermore, only sentences 2An alternative numerical method would be a combination of iterative scaling techniques with a conditional EM algorithm (Jebara and Pentland, 1998).</S>\n    <S sid=\"98\" ssid=\"5\">However, it has been shown experimentally that conjugate gradient techniques can outperform iterative scaling techniques by far in running time (Minka, 2001). which received at most 1,000 parses were used.</S>\n    <S sid=\"99\" ssid=\"6\">From this set, sentences of which a discriminative learner cannot possibly take advantage, i.e. sentences where the set of parses assigned to the partially labeled string was not a proper subset of the parses assigned the unlabeled string, were removed.</S>\n    <S sid=\"100\" ssid=\"7\">These successive selection steps resulted in a final training set consisting of 10,000 sentences, each with parses for partially labeled and unlabeled versions.</S>\n    <S sid=\"101\" ssid=\"8\">Altogether there were 150,000 parses for partially labeled input and 500,000 for unlabeled input.</S>\n    <S sid=\"102\" ssid=\"9\">For estimation, a simple property selection procedure was applied to the full set of around 1000 properties.</S>\n    <S sid=\"103\" ssid=\"10\">This procedure is based on a frequency cutoff on instantiations of properties for the parses in the labeled training set.</S>\n    <S sid=\"104\" ssid=\"11\">The result of this procedure is a reduction of the property vector to about half its size.</S>\n    <S sid=\"105\" ssid=\"12\">Furthermore, a held-out data set was created from section 24 of the WSJ treebank for experimental selection of the variance parameter of the prior distribution.</S>\n    <S sid=\"106\" ssid=\"13\">This set consists of 120 sentences which received only full parses, out of which the most plausible one was selected manually.</S>\n    <S sid=\"107\" ssid=\"14\">Two different sets of test data were used: (i) 700 sentences randomly extracted from section 23 of the WSJ treebank and given gold-standard f-structure annotations according to our LFG scheme, and (ii) 500 sentences from the Brown corpus given gold standard annotations by Carroll et al. (1999) according to their dependency relations (DR) scheme.3 Annotating the WSJ test set was bootstrapped by parsing the test sentences using the LFG grammar and also checking for consistency with the Penn Treebank annotation.</S>\n    <S sid=\"108\" ssid=\"15\">Starting from the (sometimes fragmentary) parser analyses and the Treebank annotations, gold standard parses were created by manual corrections and extensions of the LFG parses.</S>\n    <S sid=\"109\" ssid=\"16\">Manual corrections were necessary in about half of the cases.</S>\n    <S sid=\"110\" ssid=\"17\">The average sentence length of the WSJ f-structure bank is 19.8 words; the average number of predicate-argument relations in the goldstandard f-structures is 31.2.</S>\n    <S sid=\"111\" ssid=\"18\">Performance on the LFG-annotated WSJ test set was measured using both the LFG and DR metrics, thanks to an f-structure-to-DR annotation mapping.</S>\n    <S sid=\"112\" ssid=\"19\">Performance on the DR-annotated Brown test set was only measured using the DR metric.</S>\n    <S sid=\"113\" ssid=\"20\">The LFG evaluation metric is based on the comparison of full f-structures, represented as triples relation(predicate, argument).</S>\n    <S sid=\"114\" ssid=\"21\">The predicateargument relations of the f-structure for one parse of the sentence Meridian will pay a premium of $30.5 million to assume $2 billion in deposits. are shown in Fig.</S>\n    <S sid=\"115\" ssid=\"22\">2.</S>\n    <S sid=\"116\" ssid=\"23\">The DR annotation for our example sentence, obtained via a mapping from f-structures to Carroll et al’s annotation scheme, is shown in Fig.</S>\n    <S sid=\"117\" ssid=\"24\">3.</S>\n    <S sid=\"118\" ssid=\"25\">Superficially, the LFG and DR representations are very similar.</S>\n    <S sid=\"119\" ssid=\"26\">One difference between the annotation schemes is that the LFG representation in general specifies more relation tuples than the DR representation.</S>\n    <S sid=\"120\" ssid=\"27\">Also, multiple occurences of the same lexical item are indicated explicitly in the LFG representation but not in the DR representation.</S>\n    <S sid=\"121\" ssid=\"28\">The main conceptual difference between the two annotation schemes is the fact that the DR scheme crucially refers to phrase-structure properties and word order as well as to grammatical relations in the definition of dependency relations, whereas the LFG scheme abstracts away from serialization and phrase-structure.</S>\n    <S sid=\"122\" ssid=\"29\">Facts like this can make a correct mapping of LFG f-structures to DR relations problematic.</S>\n    <S sid=\"123\" ssid=\"30\">Indeed, we believe that we still underestimate by a few points because of DR mapping difficulties.</S>\n    <S sid=\"124\" ssid=\"31\">4 In our evaluation, we report F-scores for both types of annotation, LFG and DR, and for three types of parse selection, (i) lower bound: random choice of a parse from the set of analyses (averaged over 10 runs), (ii) upper bound: selection of the parse with the best F-score according to the annotation scheme used, and (iii) stochastic: the parse selected by the stochastic disambiguator.</S>\n    <S sid=\"125\" ssid=\"32\">The error reduction row lists the reduction in error rate relative to the upper and lower bounds obtained by the stochastic disambiguation model.</S>\n    <S sid=\"126\" ssid=\"33\">F-score is defined as 2 × precision × recall/(precision + recall).</S>\n    <S sid=\"127\" ssid=\"34\">The effect of the quality of the parses on disambiguation performance can be illustrated by breaking down the F-scores according to whether the parser yields full parses, FRAGMENT, SKIMMED, or SKIMMED+FRAGMENT parses for the test sentences.</S>\n    <S sid=\"128\" ssid=\"35\">The percentages of test examples which belong to the respective classes of quality are listed in the first row of Table 2.</S>\n    <S sid=\"129\" ssid=\"36\">F-scores broken down according to classes of parse quality are recorded in the following rows.</S>\n    <S sid=\"130\" ssid=\"37\">The first column shows F-scores for all parses in the test set, as in Table 1.</S>\n    <S sid=\"131\" ssid=\"38\">The second column shows the best F-scores when restricting attention to examples which receive only full parses.</S>\n    <S sid=\"132\" ssid=\"39\">The third column reports F-scores for examples which receive only non-full parses, i.e.</S>\n    <S sid=\"133\" ssid=\"40\">FRAGMENT or SKIMMED parses or SKIMMED+FRAGMENT parses.</S>\n    <S sid=\"134\" ssid=\"41\">Columns 4-6 break down non-full parses according to examples which receive only FRAGMENT, only SKIMMED, or only SKIMMED+FRAGMENT parses.</S>\n    <S sid=\"135\" ssid=\"42\">Results of the evaluation on Carroll et al.’s Brown test set are given in Table 3.</S>\n    <S sid=\"136\" ssid=\"43\">Evaluation results for the DR measure applied to the Brown corpus test set broken down according to parse-quality are shown in Table 2.</S>\n    <S sid=\"137\" ssid=\"44\">In Table 3 we show the DR measure along with an evaluation measure which facilitates a direct comparison of our results to those of Carroll et al. (1999).</S>\n    <S sid=\"138\" ssid=\"45\">Following Carroll et al. (1999), we count a dependency relation as correct if the gold standard has a relation with the same governor and dependent but perhaps with a different relation-type.</S>\n    <S sid=\"139\" ssid=\"46\">This dependency-only (DO) measure thus does not reflect mismatches between arguments and modifiers in a small number of cases.</S>\n    <S sid=\"140\" ssid=\"47\">Note that since for the evaluation on the Brown corpus, no heldout data were available to adjust the variance parameter of a Bayesian model, we used a plain maximumlikelihood model for disambiguation on this test set.</S>\n  </SECTION>\n  <SECTION title=\"5 Discussion\" number=\"5\">\n    <S sid=\"141\" ssid=\"1\">We have presented a first attempt at scaling up a stochastic parsing system combining a hand-coded linguistically fine-grained grammar and a stochastic disambiguation model to the WSJ treebank.</S>\n    <S sid=\"142\" ssid=\"2\">Full grammar coverage is achieved by combining specialized constraint-based parsing techniques for LFG grammars with partial parsing techniques.</S>\n    <S sid=\"143\" ssid=\"3\">Furthermore, a maximal exploitation of treebank annotations for estimating a distribution on fine-grained LFG parses is achieved by letting grammar analyses which are consistent with the WSJ labeled bracketing define a gold standard set for discriminative estimation.</S>\n    <S sid=\"144\" ssid=\"4\">The combined system trained on WSJ data achieves full grammar coverage and disambiguation performance of 79% F-score on WSJ data, and 76% F-score on the Brown corpus test set.</S>\n    <S sid=\"145\" ssid=\"5\">While disambiguation performance of around 79% F-score on WSJ data seems promising, from one perspective it only offers a 3% absolute improvement over a lower bound random baseline.</S>\n    <S sid=\"146\" ssid=\"6\">We think that the high lower bound measure highlights an important aspect of symbolic constraintbased grammars (in contrast to treebank grammars): the symbolic grammar already significantly restricts/disambiguates the range of possible analyses, giving the disambiguator a much narrower window in which to operate.</S>\n    <S sid=\"147\" ssid=\"7\">As such, it is more appropriate to assess the disambiguator in terms of reduction in error rate (36% relative to the upper bound) than in terms of absolute F-score.</S>\n    <S sid=\"148\" ssid=\"8\">Both the DR and LFG annotations broadly agree in their measure of error reduction.</S>\n    <S sid=\"149\" ssid=\"9\">The lower reduction in error rate relative to the upper bound for DR evaluation on the Brown corpus can be attributed to a corpus effect that has also been observed by Gildea (2001) for training and testing PCFGs on the WSJ and Brown corpora.5 Breaking down results according to parse quality shows that irrespective of evaluation measure and corpus, around 4% overall performance is lost due to non-full parses, i.e.</S>\n    <S sid=\"150\" ssid=\"10\">FRAGMENT, or SKIMMED, or SKIMMED+FRAGMENT parses.</S>\n    <S sid=\"151\" ssid=\"11\">Due to the lack of standard evaluation measures and gold standards for predicate-argument matching, a comparison of our results to other stochastic parsing systems is difficult.</S>\n    <S sid=\"152\" ssid=\"12\">To our knowledge, so far the only direct point of comparison is the parser of Carroll et al. (1999) which is also evaluated on Carroll et al.’s test corpus.</S>\n    <S sid=\"153\" ssid=\"13\">They report an F-score of 75.1% for a DO evaluation that ignores predicate labels, counting only dependencies.</S>\n    <S sid=\"154\" ssid=\"14\">Under this measure, our system achieves 76.1% F-score.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "48d6707f95da2aaac392fde21c42e570",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques"
                }
            }
        },
        {
            "index": 11,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We show that jointly parsing a bitext can substantially improve parse quality on both sides. In a maximum entropy bitext parsing model, we define a distribution over source trees, target trees, and node-to-node alignments between them. Features include monolingual parse scores and various measures of syntactic divergence. Using the translated portion of the Chinese treebank, our model is trained iteratively to maximize the marginal likelihood of training tree pairs, with alignments treated as latent variables. The resulting bitext parser outperforms state-of-the-art monoparser baselines by 2.5 predicting side trees and 1.8 predicting Chinese side trees (the highest published numbers on these corpora). Moreover, these improved trees yield a 2.4 BLEU increase when used in a downstream MT evaluation.",
                    "filename": "Two_Languages_are_Better_than_One_for_Syntactic_Parsing",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D08-1092/Documents_xml/D08-1092.xml",
                    "title": "Two Languages are Better than One (for Syntactic Parsing)"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a joint log-linear model over source trees, target trees, and node-to-node alignments to select an optimal tree pair from a k-best list. The model leverages mutual constraints to improve the quality of syntactic analyses over independent monolingual parsing. On Chinese treebank data, the procedure improves F1 by 1.8 on Chinese sentences and 2.5 on out-of-domain English sentences. Additionally, using this joint parsing technique to preprocess the input to a syntactic MT system results in a 2.4 BLEU improvement. The study demonstrates that jointly parsing sentences in a translation pair can substantially enhance parses and influence downstream MT quality.",
                        "This paper presents a method for improving syntactic parsing through joint parsing of translation pairs. The authors propose a log-linear model over source trees, target trees, and node-to-node alignments, showing that jointly parsing bitexts improves parses substantially. The paper discusses the model and features, training procedures, and statistical parsing experiments, demonstrating improved F1 scores on both Chinese and English sentences. Additionally, the impact of joint parsing on syntactic MT systems is evaluated, resulting in a 2.4 BLEU increase. Overall, the study shows that joint parsing of translation pairs can exploit mutual constraints and improve syntactic analyses over independent monolingual parsing, with applications in both parsing and machine translation.",
                        "This paper explores the use of joint parsing for improving the quality of syntactic analyses in machine translation. By leveraging mutual constraints in translation pairs, the authors present a joint log-linear model over source trees, target trees, and node-to-node alignments. Experimental results on Chinese treebank data demonstrate an improvement in F1 by 1.8 on Chinese sentences and 2.5 on out-of-domain English sentences. Additionally, the joint parsing technique yields a 2.4 BLEU improvement when used to preprocess the input to a syntactic MT system. The study concludes that jointly parsing sentences in translation pairs can exploit bilingual constraints to enhance the quality of syntactic analyses over independent monolingual parsing, with implications for machine translation systems.",
                        "This paper explores the benefits of jointly parsing sentences in a translation pair to improve the quality of syntactic analyses over independent monolingual parsing. The authors present a log-linear model used to select an optimal tree pair from a k-best list and show that this technique improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences. Furthermore, using joint parsing to preprocess the input to a syntactic MT system results in a 2.4 BLEU improvement. The study demonstrates that jointly parsing sentences in a translation pair can exploit mutual constraints to enhance the quality of syntactic analyses and improve machine translation quality."
                    ],
                    "filename": "two_languages_are_better_than_on-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 117 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Two Languages are Better than One (for Syntactic Parsing)</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Methods for machine translation (MT) have increasingly leveraged not only the formal machinery of syntax (Wu, 1997; Chiang, 2007; Zhang et al., 2008), but also linguistic tree structures of either the source side (Huang et al., 2006; Marton and Resnik, 2008; Quirk et al., 2005), the target side (Yamada and Knight, 2001; Galley et al., 2004; Zollmann et al., 2006; Shen et al., 2008), or both (Och et al., 2003; Aue et al., 2004; Ding and Palmer, 2005).</S>\n    <S sid=\"8\" ssid=\"2\">These methods all rely on automatic parsing of one or both sides of input bitexts and are therefore impacted by parser quality.</S>\n    <S sid=\"9\" ssid=\"3\">Unfortunately, parsing general bitexts well can be a challenge for newswiretrained treebank parsers for many reasons, including out-of-domain input and tokenization issues.</S>\n    <S sid=\"10\" ssid=\"4\">On the other hand, the presence of translation pairs offers a new source of information: bilingual constraints.</S>\n    <S sid=\"11\" ssid=\"5\">For example, Figure 1 shows a case where a state-of-the-art English parser (Petrov and Klein, 2007) has chosen an incorrect structure which is incompatible with the (correctly chosen) output of a comparable Chinese parser.</S>\n    <S sid=\"12\" ssid=\"6\">Smith and Smith (2004) previously showed that such bilingual constraints can be leveraged to transfer parse quality from a resource-rich language to a resourceimpoverished one.</S>\n    <S sid=\"13\" ssid=\"7\">In this paper, we show that bilingual constraints and reinforcement can be leveraged to substantially improve parses on both sides of a bitext, even for two resource-rich languages.</S>\n    <S sid=\"14\" ssid=\"8\">Formally, we present a log-linear model over triples of source trees, target trees, and node-tonode tree alignments between them.</S>\n    <S sid=\"15\" ssid=\"9\">We consider a set of core features which capture the scores of monolingual parsers as well as measures of syntactic alignment.</S>\n    <S sid=\"16\" ssid=\"10\">Our model conditions on the input sentence pair and so features can and do reference input characteristics such as posterior distributions from a word-level aligner (Liang et al., 2006; DeNero and Klein, 2007).</S>\n    <S sid=\"17\" ssid=\"11\">Our training data is the translated section of the Chinese treebank (Xue et al., 2002; Bies et al., 2007), so at training time correct trees are observed on both the source and target side.</S>\n    <S sid=\"18\" ssid=\"12\">Gold tree alignments are not present and so are induced as latent variables using an iterative training procedure.</S>\n    <S sid=\"19\" ssid=\"13\">To make the process efficient and modular to existing monolingual parsers, we introduce several approximations: use of k-best lists in candidate generation, an adaptive bound to avoid considering all k2 combinations, and Viterbi approximations to alignment posteriors.</S>\n    <S sid=\"20\" ssid=\"14\">We evaluate our system primarily as a parser and secondarily as a component in a machine translation pipeline.</S>\n    <S sid=\"21\" ssid=\"15\">For both English and Chinese, we begin with the state-of-the-art parsers presented in Petrov and Klein (2007) as a baseline.</S>\n    <S sid=\"22\" ssid=\"16\">Joint parse selection improves the English trees by 2.5 F1 and the Chinese trees by 1.8 F1.</S>\n    <S sid=\"23\" ssid=\"17\">While other Chinese treebank parsers do not have access to English side translations, this Chinese figure does outperform all published monolingual Chinese treebank results on an equivalent split of the data.</S>\n    <S sid=\"24\" ssid=\"18\">As MT motivates this work, another valuable evaluation is the effect of joint selection on downstream MT quality.</S>\n    <S sid=\"25\" ssid=\"19\">In an experiment using a syntactic MT system, we find that rules extracted from joint parses results in an increase of 2.4 BLEU points over rules extracted from independent parses.1 In sum, jointly parsing bitexts improves parses substantially, and does so in a way that that carries all the way through the MT pipeline.</S>\n  </SECTION>\n  <SECTION title=\"2 Model\" number=\"2\">\n    <S sid=\"26\" ssid=\"1\">In our model, we consider pairs of sentences (s, s'), where we use the convention that unprimed variables are source domain and primed variables are target domain.</S>\n    <S sid=\"27\" ssid=\"2\">These sentences have parse trees t (respectively t') taken from candidate sets T (T').</S>\n    <S sid=\"28\" ssid=\"3\">Non-terminal nodes in trees will be denoted by n (n') and we abuse notation by equating trees with their node sets.</S>\n    <S sid=\"29\" ssid=\"4\">Alignments a are simply at-mostone-to-one matchings between a pair of trees t and t' (see Figure 2a for an example).</S>\n    <S sid=\"30\" ssid=\"5\">Note that we will also mention word alignments in feature definitions; a and the unqualified term alignment will always refer to node alignments.</S>\n    <S sid=\"31\" ssid=\"6\">Words in a sentence are denoted by v (v').</S>\n    <S sid=\"32\" ssid=\"7\">Our model is a general log-linear (maximum entropy) distribution over triples (t, a, t') for sentence pairs (s, s'): Features are thus defined over (t, a, t') triples; we discuss specific features below.</S>\n  </SECTION>\n  <SECTION title=\"3 Features\" number=\"3\">\n    <S sid=\"33\" ssid=\"1\">To use our model, we need features of a triple (t, a, t') which encode both the monolingual quality of the trees as well as the quality of the alignment between them.</S>\n    <S sid=\"34\" ssid=\"2\">We introduce a variety of features in the next sections.</S>\n    <S sid=\"35\" ssid=\"3\">To capture basic monolingual parse quality, we begin with a single source and a single target feature whose values are the log likelihood of the source tree t and the target tree t', respectively, as given by our baseline monolingual parsers.</S>\n    <S sid=\"36\" ssid=\"4\">These two features are called SOURCELL and TARGETLL respectively.</S>\n    <S sid=\"37\" ssid=\"5\">It is certainly possible to augment these simple features with what would amount to monolingual reranking features, but we do not explore that option here.</S>\n    <S sid=\"38\" ssid=\"6\">Note that with only these two features, little can be learned: all positive weights w cause the jointly optimal parse pair (t, t') to comprise the two top-1 monolingual outputs (the baseline).</S>\n    <S sid=\"39\" ssid=\"7\">All other features in our model reference the entire triple (t, a, t').</S>\n    <S sid=\"40\" ssid=\"8\">In this work, such features are defined over aligned node pairs for efficiency, but generalizations are certainly possible.</S>\n    <S sid=\"41\" ssid=\"9\">Bias: The first feature is simply a bias feature which has value 1 on each aligned node pair (n, n').</S>\n    <S sid=\"42\" ssid=\"10\">This bias allows the model to learn a general preference for denser alignments.</S>\n    <S sid=\"43\" ssid=\"11\">Alignment features: Of course, some alignments are better than others.</S>\n    <S sid=\"44\" ssid=\"12\">One indicator of a good nodeto-node alignment between n and n' is that a good word alignment model thinks that there are many word-to-word alignments in their bispan.</S>\n    <S sid=\"45\" ssid=\"13\">Similarly, there should be few alignments that violate that bispan.</S>\n    <S sid=\"46\" ssid=\"14\">To compute such features, we define a(v, v') to be the posterior probability assigned to the word alignment between v and v' by an independent word aligner.2 Before defining alignment features, we need to define some additional variables.</S>\n    <S sid=\"47\" ssid=\"15\">For any node n E t (n' E t'), the inside span i(n) (i(n')) comprises the input tokens of s (s') dominated by that node.</S>\n    <S sid=\"48\" ssid=\"16\">Similarly, the complement, the outside span, will be denoted o(n) (o(n')), and comprises the tokens not dominated by that node.</S>\n    <S sid=\"49\" ssid=\"17\">See Figure 2b,c for examples of the resulting regions.</S>\n    <S sid=\"50\" ssid=\"18\">Hard alignment features: We also define the hard versions of these features, which take counts from the word aligner’s hard top-1 alignment output S: Scaled alignment features: Finally, undesirable larger bispans can be relatively sparse at the word alignment level, yet still contain many good word alignments simply by virtue of being large.</S>\n    <S sid=\"51\" ssid=\"19\">We therefore define a scaled count which measures density rather than totals.</S>\n    <S sid=\"52\" ssid=\"20\">The geometric mean of span lengths was a superior measure of bispan “area” than the true area because word-level alignments tend to be broadly one-to-one in our word alignment model.</S>\n    <S sid=\"53\" ssid=\"21\">Head word alignment features: When considering a node pair (n, n'), especially one which dominates a large area, the above measures treat all spanned words as equally important.</S>\n    <S sid=\"54\" ssid=\"22\">However, lexical heads are generally more representative than other spanned words.</S>\n    <S sid=\"55\" ssid=\"23\">Let h select the headword of a node according to standard head percolation rules (Collins, 2003; Bikel and Chiang, 2000).</S>\n    <S sid=\"56\" ssid=\"24\">We also consider features that measure correspondences between the tree structures themselves.</S>\n    <S sid=\"57\" ssid=\"25\">Span difference: We expect that, in general, aligned nodes should dominate spans of roughly the same length, and so we allow the model to learn to penalize node pairs whose inside span lengths differ greatly.</S>\n    <S sid=\"58\" ssid=\"26\">Number of children: We also expect that there will be correspondences between the rules of the CFGs that generate the trees in each language.</S>\n    <S sid=\"59\" ssid=\"27\">To encode some of this information, we compute indicators of the number of children c that the nodes have in t and t'.</S>\n    <S sid=\"60\" ssid=\"28\">NUMCHILDREN(|c(n)|, |c(n')|) = 1 For each feature above (except monolingual features), we create label-specific versions by conjoining the label pair (`(n), `(n')).</S>\n    <S sid=\"61\" ssid=\"29\">We use both the typed and untyped variants of all features.</S>\n  </SECTION>\n  <SECTION title=\"4 Training\" number=\"4\">\n    <S sid=\"62\" ssid=\"1\">Recall that our data condition supplies sentence pairs (s, s') along with gold parse pairs (g, g').</S>\n    <S sid=\"63\" ssid=\"2\">We do not observe the alignments a which link these parses.</S>\n    <S sid=\"64\" ssid=\"3\">In principle, we want to find weights which maximize the marginal log likelihood of what we do observe given our sentence pairs:3 Child labels: In addition, we also encode whether w∗ = arg max EP(g,a,g0|s,s0,w) (1) certain label pairs occur as children of matched w a nodes.</S>\n    <S sid=\"65\" ssid=\"4\">Let c(n, `) select the children of n with la- = arg max Ea exp(w&gt;φ(g, a, g0)) bel `. w � �a exp(w&gt;φ(t, a, t0)) (2) (t,t') CHILDLABEL(`, `') = |c(n, `) |· |c(n', `')| Note that the corresponding “self labels” feature is not listed because it arises in the next section as a typed variant of the bias feature.</S>\n    <S sid=\"66\" ssid=\"5\">There are several challenges.</S>\n    <S sid=\"67\" ssid=\"6\">First, the space of symmetric at-most-one-to-one matchings is #P-hard to sum over exactly (Valiant, 1979).</S>\n    <S sid=\"68\" ssid=\"7\">Second, even without matchings to worry about, standard methods for maximizing the above formulation would require summation over pairs of trees, and we want to assume a fairly generic interface to independent monolingual parsers (though deeper joint modeling and/or training is of course a potential extension).</S>\n    <S sid=\"69\" ssid=\"8\">As we have chosen to operate in a reranking mode over monolingual k-best lists, we have another issue: our k-best outputs on the data which trains our model may not include the gold tree pair.</S>\n    <S sid=\"70\" ssid=\"9\">We therefore make several approximations and modifications, which we discuss in turn.</S>\n    <S sid=\"71\" ssid=\"10\">Because summing over alignments a is intractable, we cannot evaluate (2) or its derivatives.</S>\n    <S sid=\"72\" ssid=\"11\">However, if we restrict the space of possible alignments, then we can make this optimization more feasible.</S>\n    <S sid=\"73\" ssid=\"12\">One way to do this is to stipulate in advance that for each tree pair, there is a canonical alignment a0(t, t0).</S>\n    <S sid=\"74\" ssid=\"13\">Of course, we want a0 to reflect actual correspondences between t and t0, so we want a reasonable definition that ensures the alignments are of reasonable quality.</S>\n    <S sid=\"75\" ssid=\"14\">Fortunately, it turns out that we can efficiently optimize a given a fixed tree pair and weight vector: This optimization requires only that we search for an optimal alignment.</S>\n    <S sid=\"76\" ssid=\"15\">Because all our features can be factored to individual node pairs, this can be done with the Hungarian algorithm in cubic time.4 Note that we do not enforce any kind of domination consistency in the matching: for example, the optimal alignment might in principle have the source root aligning to a target non-root and vice versa.</S>\n    <S sid=\"77\" ssid=\"16\">We then define a0(t, t0) as the alignment that maximizes w&gt;0 O(t, a, t0), where w0 is a fixed initial weight vector with a weight of 1 for INSIDEBOTH, -1 for INSRCOUTTRG and INTRGOUTSRC, and 0 for all other features.</S>\n    <S sid=\"78\" ssid=\"17\">Then, we simplify (2) by fixing the alignments a0: This optimization has no latent variables and is therefore convex and straightforward.</S>\n    <S sid=\"79\" ssid=\"18\">However, while we did use this as a rapid training procedure during development, fixing the alignments a priori is both unsatisfying and also less effective than a procedure which allows the alignments a to adapt during training.</S>\n    <S sid=\"80\" ssid=\"19\">Again, for fixed alignments a, optimizing w is easy.</S>\n    <S sid=\"81\" ssid=\"20\">Similarly, with a fixed w, finding the optimal a for any particular tree pair is also easy.</S>\n    <S sid=\"82\" ssid=\"21\">Another option is therefore to use an iterative procedure that alternates between choosing optimal alignments for a fixed w, and then reoptimizing w for those fixed alignments according to (3).</S>\n    <S sid=\"83\" ssid=\"22\">By iterating, we perform the following optimization: Note that (4) is just (2) with summation replaced by maximization.</S>\n    <S sid=\"84\" ssid=\"23\">Though we do not know of any guarantees for this EM-like algorithm, in practice it converges after a few iterations given sufficient training data.</S>\n    <S sid=\"85\" ssid=\"24\">We initialize the procedure by setting w0 as defined above.</S>\n    <S sid=\"86\" ssid=\"25\">When training our model, we approximate the sets of all trees with k-best lists, T and T0, produced by monolingual parsers.</S>\n    <S sid=\"87\" ssid=\"26\">Since these sets are not guaranteed to contain the gold trees g and g0, our next approximation is to define a set of pseudo-gold trees, following previous work in monolingual parse reranking (Charniak and Johnson, 2005).</S>\n    <S sid=\"88\" ssid=\"27\">We define Tˆ ( Tˆ0) as the F1-optimal subset of T (T0).</S>\n    <S sid=\"89\" ssid=\"28\">We then modify (4) to reflect the fact that we are seeking to maximize the likelihood of trees in this subset: To reduce the time and space requirements for training, we do not always use the full k-best lists.</S>\n    <S sid=\"90\" ssid=\"29\">To prune the set T, we rank all the trees in T from 1 to k, according to their log likelihood under the baseline parsing model, and find the rank of the least likely pseudo-gold tree: Finally, we restrict T based on rank: To prune the list of tree pairs, first we rank them according to the metric: where E is a free parameter of the pruning procedure.</S>\n    <S sid=\"91\" ssid=\"30\">The restricted set T0pruned is constructed in the same way.</S>\n    <S sid=\"92\" ssid=\"31\">When training, we replace the sum over all tree pairs in (T, T0) in the denominator of (6) with a sum over all tree pairs in (Tpruned, T0pruned).</S>\n    <S sid=\"93\" ssid=\"32\">The parameter E can be set to any value from 0 to k, with lower values resulting in more efficient training, and higher values resulting in better performance.</S>\n    <S sid=\"94\" ssid=\"33\">We set E by empirically determining a good speed/performance tradeoff (see §6.2).</S>\n  </SECTION>\n  <SECTION title=\"5 Joint Selection\" number=\"5\">\n    <S sid=\"95\" ssid=\"1\">At test time, we have a weight vector w and so selecting optimal trees for the sentence pair (s, s0) from a pair of k best lists, (T, T0) is straightforward.</S>\n    <S sid=\"96\" ssid=\"2\">We just find: Note that with no additional cost, we can also find the optimal alignment between t∗ and t0∗: Because the size of (T, T0) grows as O(k�), the time spent iterating through all these tree pairs can grow unreasonably long, particularly when reranking a set of sentence pairs the size of a typical MT corpus.</S>\n    <S sid=\"97\" ssid=\"3\">To combat this, we use a simple pruning technique to limit the number of tree pairs under consideration.</S>\n    <S sid=\"98\" ssid=\"4\">Then, we simply remove all tree pairs whose ranking falls below some empirically determined cutoff.</S>\n    <S sid=\"99\" ssid=\"5\">As we show in §6.3, by using this technique we are able to speed up reranking by a factor of almost 20 without an appreciable loss of performance.</S>\n  </SECTION>\n  <SECTION title=\"6 Statistical Parsing Experiments\" number=\"6\">\n    <S sid=\"100\" ssid=\"1\">All the data used to train the joint parsing model and to evaluate parsing performance were taken from articles 1-325 of the Chinese treebank, which all have English translations with gold-standard parse trees.</S>\n    <S sid=\"101\" ssid=\"2\">The articles were split into training, development, and test sets according to the standard breakdown for Chinese parsing evaluations.</S>\n    <S sid=\"102\" ssid=\"3\">Not all sentence pairs could be included for various reasons, including one-to-many Chinese-English sentence alignments, sentences omitted from the English translations, and low-fidelity translations.</S>\n    <S sid=\"103\" ssid=\"4\">Additional sentence pairs were dropped from the training data because they had unambiguous parses in at least one of the two languages.</S>\n    <S sid=\"104\" ssid=\"5\">Table 1 shows how many sentences were included in each dataset.</S>\n    <S sid=\"105\" ssid=\"6\">We had two training setups: rapid and full.</S>\n    <S sid=\"106\" ssid=\"7\">In the rapid training setup, only 1000 sentence pairs from the training set were used, and we used fixed alignments for each tree pair rather than iterating (see §4.1).</S>\n    <S sid=\"107\" ssid=\"8\">The full training setup used the iterative training procedure on all 2298 training sentence pairs.</S>\n    <S sid=\"108\" ssid=\"9\">We used the English and Chinese parsers in Petrov and Klein (2007)5 to generate all k-best lists and as our evaluation baseline.</S>\n    <S sid=\"109\" ssid=\"10\">Because our bilingual data is from the Chinese treebank, and the data typically used to train a Chinese parser contains the Chinese side of our bilingual training data, we had to train a new Chinese grammar using only articles 400-1151 (omitting articles 1-270).</S>\n    <S sid=\"110\" ssid=\"11\">This modified grammar was used to generate the k-best lists that we trained our model on.</S>\n    <S sid=\"111\" ssid=\"12\">However, as we tested on the same set of articles used for monolingual Chinese parser evaluation, there was no need to use a modified grammar to generate k-best lists at test time, and so we used a regularly trained Chinese parser for this purpose.</S>\n    <S sid=\"112\" ssid=\"13\">We also note that since all parsing evaluations were performed on Chinese treebank data, the Chinese test sentences were in-domain, whereas the English sentences were very far out-of-domain for the Penn Treebank-trained baseline English parser.</S>\n    <S sid=\"113\" ssid=\"14\">Hence, in these evaluations, Chinese scores tend to be higher than English ones.</S>\n    <S sid=\"114\" ssid=\"15\">Posterior word alignment probabilities were obtained from the word aligner of Liang et al. (2006) and DeNero and Klein (2007)6, trained on approximately 1.7 million sentence pairs.</S>\n    <S sid=\"115\" ssid=\"16\">For our alignment model we used an HMM in each direction, trained to agree (Liang et al., 2006), and we combined the posteriors using DeNero and Klein’s (2007) soft union method.</S>\n    <S sid=\"116\" ssid=\"17\">Unless otherwise specified, the maximum value of k was set to 100 for both training and testing, and all experiments used a value of 25 as the c parameter for training set pruning and a cutoff rank of 500 for test set pruning.</S>\n    <S sid=\"117\" ssid=\"18\">To verify that all our features were contributing to the model’s performance, we did an ablation study, removing one group of features at a time.</S>\n    <S sid=\"118\" ssid=\"19\">Table 2 shows the F1 scores on the bilingual development data resulting from training with each group of features removed.7 Note that though head word features seemed to be detrimental in our rapid training setup, earlier testing had shown a positive effect, so we reran the comparison using our full training setup, where we again saw an improvement when including these features.</S>\n    <S sid=\"119\" ssid=\"20\">To find a good value of the c parameter for training set pruning we tried several different values, using our rapid training setup and testing on the dev set.</S>\n    <S sid=\"120\" ssid=\"21\">The results are shown in Table 3.</S>\n    <S sid=\"121\" ssid=\"22\">We selected 25 as it showed the best performance/speed tradeoff, on average performing as well as if we had done no pruning at all, while requiring only a quarter the memory and CPU time.</S>\n    <S sid=\"122\" ssid=\"23\">We also tried several different values of the rank cutoff for test set pruning, using the full training setup and testing on the dev set.</S>\n    <S sid=\"123\" ssid=\"24\">The results are in Table 4.</S>\n    <S sid=\"124\" ssid=\"25\">For F1 evaluation, which is on a very small set of sentences, we selected 500 as the value with the best speed/performance tradeoff.</S>\n    <S sid=\"125\" ssid=\"26\">However, when reranking our entire MT corpus, we used a value of 200, sacrificing a tiny bit of performance for an extra factor of 2 in speed.8 Since our bitext parser currently operates as a reranker, the quality of the trees is limited by the quality of the k-best lists produced by the baseline parsers.</S>\n    <S sid=\"126\" ssid=\"27\">To test this limitation, we evaluated performance on the dev set using baseline k-best lists of varying length.</S>\n    <S sid=\"127\" ssid=\"28\">Training parameters were fixed (full training setup with k = 100) and test set pruning was disabled for these experiments.</S>\n    <S sid=\"128\" ssid=\"29\">The results are in Table 5.</S>\n    <S sid=\"129\" ssid=\"30\">The relatively modest gains with increasing k, even as the oracle scores continue to improve, indicate that performance is limited more by the model’s reliance on the baseline parsers than by search errors that result from the reranking approach.</S>\n    <S sid=\"130\" ssid=\"31\">Our final evaluation was done using the full training setup.</S>\n    <S sid=\"131\" ssid=\"32\">Here, we report F1 scores on two sets of data.</S>\n    <S sid=\"132\" ssid=\"33\">First, as before, we only include the sentence pairs from our bilingual corpus to fully demonstrate the gains made by joint parsing.</S>\n    <S sid=\"133\" ssid=\"34\">We also report scores on the full test set to allow easier comparison with past work on Chinese parsing.</S>\n    <S sid=\"134\" ssid=\"35\">For the latter evaluation, sentences that were not in the bilingual corpus were simply parsed with the baseline parsers.</S>\n    <S sid=\"135\" ssid=\"36\">The results are in Table 6.</S>\n    <S sid=\"136\" ssid=\"37\">Joint parsing improves F1 by 2.5 points on out-of-domain English sentences and by 1.8 points on in-domain Chinese sentences; this represents the best published Chinese treebank parsing performance, even after sentences that lack a translation are taken into account.</S>\n  </SECTION>\n  <SECTION title=\"7 Machine Translation\" number=\"7\">\n    <S sid=\"137\" ssid=\"1\">To test the impact of joint parsing on syntactic MT systems, we compared the results of training an MT system with two different sets of trees: those produced by the baseline parsers, and those produced by our joint parser.</S>\n    <S sid=\"138\" ssid=\"2\">For this evaluation, we used a syntactic system based on Galley et al. (2004) and Galley et al.</S>\n    <S sid=\"139\" ssid=\"3\">(2006), which extracts tree-to-string transducer rules based on target-side trees.</S>\n    <S sid=\"140\" ssid=\"4\">We trained the system on 150,000 Chinese-English sentence pairs from the training corpus of Wang et al. (2007), and used a large (close to 5 billion tokens) 4-gram lanwith trees output from either baseline monolingual parsers or our joint parser.</S>\n    <S sid=\"141\" ssid=\"5\">To facilitate relative comparison, the Moses (Koehn et al., 2007) number listed reflects the default Moses configuration, including its full distortion model, and standard training pipeline. guage model for decoding.</S>\n    <S sid=\"142\" ssid=\"6\">We tuned and evaluated BLEU (Papineni et al., 2001) on separate held-out sets of sentences of up to length 40 from the same corpus.</S>\n    <S sid=\"143\" ssid=\"7\">The results are in Table 7, showing that joint parsing yields a BLEU increase of 2.4.9</S>\n  </SECTION>\n  <SECTION title=\"8 Conclusions\" number=\"8\">\n    <S sid=\"144\" ssid=\"1\">By jointly parsing (and aligning) sentences in a translation pair, it is possible to exploit mutual constraints that improve the quality of syntactic analyses over independent monolingual parsing.</S>\n    <S sid=\"145\" ssid=\"2\">We presented a joint log-linear model over source trees, target trees, and node-to-node alignments between them, which is used to select an optimal tree pair from a k-best list.</S>\n    <S sid=\"146\" ssid=\"3\">On Chinese treebank data, this procedure improves F1 by 1.8 on Chinese sentences and by 2.5 on out-of-domain English sentences.</S>\n    <S sid=\"147\" ssid=\"4\">Furthermore, by using this joint parsing technique to preprocess the input to a syntactic MT system, we obtain a 2.4 BLEU improvement.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgements\" number=\"9\">\n    <S sid=\"148\" ssid=\"1\">We would like to thank the anonymous reviewers for helpful comments on an earlier draft of this paper and Adam Pauls and Jing Zheng for help in running our MT experiments.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "8c7ae1d39505618b86a88c27236eedb7",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Two Languages are Better than One (for Syntactic Parsing)"
                }
            }
        },
        {
            "index": 12,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.",
                    "filename": "Bayesian_Word_Sense_Induction",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/E09-1013/Documents_xml/E09-1013.xml",
                    "title": "Bayesian Word Sense Induction"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a novel Bayesian approach to sense induction, a task aimed at automatically discovering all possible senses of an ambiguous word. The model is formulated within a generative framework, incorporating lexical information, parts of speech, and dependencies in a principled manner. The study outperforms state-of-the-art systems and shows potential for adaptation to other applications where multiple levels of information need to be considered. The approach is not specific to sense induction and can be extended to tasks such as document classification, possibly incorporating additional layers such as images or captions. Future work may explore more rigorous parameter estimation techniques and the use of an infinite Dirichlet model to automatically determine the optimal number of senses.",
                        "This paper presents a Bayesian approach to sense induction, aiming to automatically identify all possible senses of an ambiguous word. The authors argue that sense induction can overcome issues associated with word sense disambiguation, such as adapting to new domains or languages with scarce lexical resources. They adopt a novel Bayesian approach to formalize the induction problem in a generative model, using a distribution over senses and generating context words accordingly. The proposed model also integrates various information sources, such as parts of speech and dependencies, in a principled manner. The model is evaluated on benchmark datasets, outperforming state-of-the-art systems. The authors also discuss the potential for future improvements in parameter estimation techniques and exploring an infinite Dirichlet model.",
                        "This paper introduces a novel Bayesian approach to sense induction, which formulates sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on the basis of latent variables. The model incorporates lexical information, parts of speech, and dependencies in a principled manner and outperforms state-of-the-art systems. Additionally, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account. The authors also discuss future directions for more rigorous parameter estimation techniques and potential improvements to the model's performance. Overall, the paper presents a comprehensive and innovative approach to sense induction, showing promising results and potential for further development.",
                        "This paper introduces a novel Bayesian approach to sense induction, which is the process of automatically discovering all possible senses of an ambiguous word. The paper formulates sense induction in a generative framework, describing how the contexts surrounding an ambiguous word might be generated based on latent variables. The model incorporates lexical information, parts of speech, and dependencies in a principled manner, outperforming state-of-the-art systems. The approach is not specific to sense induction and can be adapted for other applications where multiple levels of information need to be taken into account. The authors hope to explore more rigorous parameter estimation techniques in the future, such as integrating hyperparameter estimation into the Gibbs sampling procedure and using an infinite Dirichlet model to automatically determine the optimal number of senses.",
                        "This academic paper introduces a novel Bayesian approach to sense induction, aiming to automatically discover all possible senses of an ambiguous word. The paper formulates sense induction in a generative model, integrating lexical information, parts of speech, and dependencies in a principled manner. The model's performance is evaluated using a recently released benchmark dataset, showing improvements over state-of-the-art systems. The paper discusses the experimental set-up, data used, and the evaluation methodology. It also explores the selection of model parameters and the impact of different feature categories on the model's performance. The results indicate that the proposed model outperforms other sense induction systems, demonstrating its potential for broader applications. The paper concludes by outlining potential future research directions.",
                        "This paper presents a novel Bayesian approach to sense induction by formulating the task within a generative framework. The model incorporates lexical information, parts of speech, and dependencies in a principled manner, outperforming state-of-the-art systems. The system is evaluated on the Semeval-2007 benchmark dataset and demonstrates improvements over the most-frequent-sense baseline. A key feature of this approach is its flexibility in modeling empirical data, affording the ability to explore different parameters to optimize performance. The authors hope to further explore rigorous parameter estimation techniques in the future. This approach is not specific to sense induction and has potential for adaptation in various applications where multiple levels of information must be considered. The authors acknowledge the support of EPSRC and express gratitude to Sharon Goldwater for feedback on earlier versions of this work.",
                        "This paper presents a Bayesian approach to sense induction, which is the task of automatically discovering all possible senses of an ambiguous word. The proposed model formalizes the induction problem in a generative framework that incorporates lexical information, parts of speech, and dependencies in a principled manner. The paper evaluates the model on a benchmark dataset and demonstrates improvements over state-of-the-art systems, outperforming other sense induction methods. The results show that the model can handle different domains and adapt to new contexts by capturing both immediate local context and wider context. The paper also discusses the potential for future research, including exploring rigorous parameter estimation techniques and the incorporation of layer weighting parameters to improve performance. Overall, the proposed Bayesian approach offers a flexible and effective method for sense induction.",
                        "This paper presents a novel Bayesian approach to sense induction, formulated within a generative framework that describes how the contexts surrounding an ambiguous word might be generated based on latent variables. The model incorporates lexical information, parts of speech, and dependencies in a principled manner and outperforms state-of-the-art systems. The approach is not specific to sense induction and can be adapted for other applications where multiple levels of information need to be taken into account. The authors hope to explore more rigorous parameter estimation techniques in the future, such as integrating hyperparameter estimation into the Gibbs sampling procedure and allowing for an infinite number of senses using an infinite Dirichlet model. This work is supported by EPSRC and benefited from feedback from Sharon Goldwater.",
                        "This paper presents a novel Bayesian approach to sense induction, which aims to automatically discover all possible senses of an ambiguous word. The proposed model formalizes the induction problem in a generative framework, incorporating lexical information, parts of speech, and dependencies in a principled manner. The model outperforms state-of-the-art systems and can be adapted for other applications where multiple levels of information are desirable. The paper discusses the experimental set-up, data, evaluation methodology, and model selection process, showing that the combined model with wider and more local contextual information yields the best performance. Overall, the approach provides a flexible and effective means of sense induction, with potential for further development and application in related tasks.",
                        "This paper presents a novel Bayesian approach to sense induction, a task that aims to automatically discover all possible senses of an ambiguous word. The model incorporates lexical information, parts of speech, and dependencies in a principled manner and outperforms state-of-the-art systems. The framework is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account. The paper also discusses the model's parameter estimation and potential for using an infinite Dirichlet model to automatically determine the optimal number of senses. Overall, the paper provides a comprehensive overview of the sense induction model, its evaluation setup, and experimental results, demonstrating the effectiveness and potential of the proposed approach."
                    ],
                    "filename": "bayesian_word_sense_induction-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 133 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Bayesian Word Sense Induction</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Sense induction is the task of discovering automatically all possible senses of an ambiguous word.</S>\n    <S sid=\"8\" ssid=\"2\">It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context.</S>\n    <S sid=\"9\" ssid=\"3\">Although the bulk of previous work has been devoted to the disambiguation problem1, there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD.</S>\n    <S sid=\"10\" ssid=\"4\">Since most disambiguation methods assign senses according to, and with the aid of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce.</S>\n    <S sid=\"11\" ssid=\"5\">A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.</S>\n    <S sid=\"12\" ssid=\"6\">In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.</S>\n    <S sid=\"13\" ssid=\"7\">There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.</S>\n    <S sid=\"14\" ssid=\"8\">Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993).</S>\n    <S sid=\"15\" ssid=\"9\">Sense induction is typically treated as an unsupervised clustering problem.</S>\n    <S sid=\"16\" ssid=\"10\">The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.</S>\n    <S sid=\"17\" ssid=\"11\">In other words, contexts that are grouped together in the same class represent a specific word sense.</S>\n    <S sid=\"18\" ssid=\"12\">In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.</S>\n    <S sid=\"19\" ssid=\"13\">For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution.</S>\n    <S sid=\"20\" ssid=\"14\">It is thus assumed that different senses will correspond to distinct lexical distributions.</S>\n    <S sid=\"21\" ssid=\"15\">In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning).</S>\n    <S sid=\"22\" ssid=\"16\">Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S>\n    <S sid=\"23\" ssid=\"17\">LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words.</S>\n    <S sid=\"24\" ssid=\"18\">The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic.</S>\n    <S sid=\"25\" ssid=\"19\">Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.</S>\n    <S sid=\"26\" ssid=\"20\">Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.</S>\n    <S sid=\"27\" ssid=\"21\">In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection.</S>\n    <S sid=\"28\" ssid=\"22\">We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.</S>\n    <S sid=\"29\" ssid=\"23\">For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem.</S>\n    <S sid=\"30\" ssid=\"24\">This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.</S>\n    <S sid=\"31\" ssid=\"25\">We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.</S>\n    <S sid=\"32\" ssid=\"26\">The remainder of this paper is structured as follows.</S>\n    <S sid=\"33\" ssid=\"27\">We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).</S>\n    <S sid=\"34\" ssid=\"28\">Section 5 describes the resources and evaluation methodology used in our experiments.</S>\n    <S sid=\"35\" ssid=\"29\">We discuss our results in Section 6, and conclude in Section 7.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"36\" ssid=\"1\">Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts.</S>\n    <S sid=\"37\" ssid=\"2\">Considerable latitude is allowed in selecting and representing the cooccurring contexts.</S>\n    <S sid=\"38\" ssid=\"3\">Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003).</S>\n    <S sid=\"39\" ssid=\"4\">The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).</S>\n    <S sid=\"40\" ssid=\"5\">In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.</S>\n    <S sid=\"41\" ssid=\"6\">A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007).</S>\n    <S sid=\"42\" ssid=\"7\">Graph-based methods have also been applied to the sense induction task.</S>\n    <S sid=\"43\" ssid=\"8\">In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences.</S>\n    <S sid=\"44\" ssid=\"9\">Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003).</S>\n    <S sid=\"45\" ssid=\"10\">Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature.</S>\n    <S sid=\"46\" ssid=\"11\">The inferred document-level topics can help determine coarsegrained sense distinctions.</S>\n    <S sid=\"47\" ssid=\"12\">Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.</S>\n    <S sid=\"48\" ssid=\"13\">In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD.</S>\n    <S sid=\"49\" ssid=\"14\">Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm.</S>\n    <S sid=\"50\" ssid=\"15\">For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.</S>\n    <S sid=\"51\" ssid=\"16\">Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word.</S>\n    <S sid=\"52\" ssid=\"17\">Then, the word sense is selected based on the word, neighbor, and topic.</S>\n    <S sid=\"53\" ssid=\"18\">Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.</S>\n    <S sid=\"54\" ssid=\"19\">In this case the model discovers both the topics of the corpus and the senses assigned to each of its words.</S>\n    <S sid=\"55\" ssid=\"20\">Our own model is also inspired by LDA but crucially performs word sense induction, not disambiguation.</S>\n    <S sid=\"56\" ssid=\"21\">Unlike the work mentioned above, we do not rely on a pre-existing list of senses, and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the tendency to remain at a high level, document-like, setting.</S>\n    <S sid=\"57\" ssid=\"22\">In contrast, we make use of much smaller units of text (a few sentences, rather than a full document), and create an individual model for each (ambiguous) word type.</S>\n    <S sid=\"58\" ssid=\"23\">Our induced senses are few in number (typically less than ten).</S>\n    <S sid=\"59\" ssid=\"24\">This is in marked contrast to tens, and sometimes hundreds, of topics commonly used in document-modeling tasks.</S>\n    <S sid=\"60\" ssid=\"25\">Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch¨utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models.</S>\n    <S sid=\"61\" ssid=\"26\">Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.</S>\n    <S sid=\"62\" ssid=\"27\">Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies).</S>\n    <S sid=\"63\" ssid=\"28\">This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.</S>\n    <S sid=\"64\" ssid=\"29\">Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.</S>\n    <S sid=\"65\" ssid=\"30\">2003) have been task-specific and limited to only two layers of information.</S>\n    <S sid=\"66\" ssid=\"31\">Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.</S>\n  </SECTION>\n  <SECTION title=\"3 The Sense Induction Model\" number=\"3\">\n    <S sid=\"67\" ssid=\"1\">The core idea behind sense induction is that contextual information provides important cues regarding a word’s meaning.</S>\n    <S sid=\"68\" ssid=\"2\">The idea dates back to (at least) Firth (1957) (“You shall know a word by the company it keeps”), and underlies most WSD and lexicon acquisition work to date.</S>\n    <S sid=\"69\" ssid=\"3\">Under this premise, we should expect different senses to be signaled by different lexical distributions.</S>\n    <S sid=\"70\" ssid=\"4\">We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.</S>\n    <S sid=\"71\" ssid=\"5\">More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s. Each word wi in the context window is generated by first sampling a sense from the sense distribution, then choosing a word from the sense-context distribution.</S>\n    <S sid=\"72\" ssid=\"6\">P(si = j) denotes the probability that the jth sense was sampled for the ith word token and P(wi|si = j) the probability of context word wi under sense j.</S>\n    <S sid=\"73\" ssid=\"7\">The model thus specifies a distribution over words within a context window: where S is the number of senses.</S>\n    <S sid=\"74\" ssid=\"8\">We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.</S>\n    <S sid=\"75\" ssid=\"9\">The variables in the lower right corner refer to the number of samples. consists of Nc word tokens.</S>\n    <S sid=\"76\" ssid=\"10\">We shall write �(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c. Following Blei et al. (2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a.</S>\n    <S sid=\"77\" ssid=\"11\">The role of the hyperparameter a is to create a smoothed sense distribution.</S>\n    <S sid=\"78\" ssid=\"12\">We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002).</S>\n    <S sid=\"79\" ssid=\"13\">The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed.</S>\n    <S sid=\"80\" ssid=\"14\">Our model is represented in graphical notation in Figure 1.</S>\n    <S sid=\"81\" ssid=\"15\">The model sketched above only takes word information into account.</S>\n    <S sid=\"82\" ssid=\"16\">Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002).</S>\n    <S sid=\"83\" ssid=\"17\">The first idea that comes to mind, is to use the same model while treating various features as word-like elements.</S>\n    <S sid=\"84\" ssid=\"18\">In other words, we could simply assume that the contexts we wish to model are the union of all our features.</S>\n    <S sid=\"85\" ssid=\"19\">Although straightforward, this solution is undesirable.</S>\n    <S sid=\"86\" ssid=\"20\">It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model.</S>\n    <S sid=\"87\" ssid=\"21\">For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser).</S>\n    <S sid=\"88\" ssid=\"22\">Layers containing more elements (e.g.</S>\n    <S sid=\"89\" ssid=\"23\">10 word window) would overwhelm rectangles represent different sources (layers) of information.</S>\n    <S sid=\"90\" ssid=\"24\">All layers share the same, instancespecific, sense distribution (0), but each have their own (multinomial) sense-feature distribution (�).</S>\n    <S sid=\"91\" ssid=\"25\">Shaded nodes represent observed features f; these can be words, parts of speech, collocations or dependencies. unconditional joint distribution P(s) of the unobserved variables (provided certain criteria are fulfilled).</S>\n    <S sid=\"92\" ssid=\"26\">In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).</S>\n    <S sid=\"93\" ssid=\"27\">From these assignments, we must determine the sense distribution of the instance as a whole.</S>\n    <S sid=\"94\" ssid=\"28\">This is the purpose of the Gibbs sampling procedure.</S>\n    <S sid=\"95\" ssid=\"29\">Specifically, in order to derive the update function used in the Gibbs sampler, we must provide the conditional probability of the i-th variable being assigned sense si in layer l, given the feature value fi of the context variable and the current sense assignments of all the other variables in the data (s−i): p(si|s−i, f) — p(fi|s, f −i,R) · p(si|s−i,a) (2) The probability of a single sense assignment, si, is proportional to the product of the likelihood (of feature fi, given the rest of the data) and the prior probability of the assignment. smaller ones (e.g.</S>\n    <S sid=\"96\" ssid=\"30\">1 word window).</S>\n    <S sid=\"97\" ssid=\"31\">Our solution is to treat each information source (or feature type) individually and then combine all of them together in a unified model.</S>\n    <S sid=\"98\" ssid=\"32\">Our underlying assumption is that the context window around the target word can have multiple representations, all of which share the same sense distribution.</S>\n    <S sid=\"99\" ssid=\"33\">We illustrate this in Figure 2 where each inner rectangle (layer) corresponds to a distinct feature type.</S>\n    <S sid=\"100\" ssid=\"34\">We will naively assume independence between multiple layers, even though this is clearly not the case in our task.</S>\n    <S sid=\"101\" ssid=\"35\">The idea here is to model each layer as faithfully as possible to the empirical data while at the same time combining information from all layers in estimating the sense distribution of each target instance.</S>\n  </SECTION>\n  <SECTION title=\"4 Inference\" number=\"4\">\n    <S sid=\"102\" ssid=\"1\">Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984).</S>\n    <S sid=\"103\" ssid=\"2\">The procedure begins by randomly initializing all unobserved random variables.</S>\n    <S sid=\"104\" ssid=\"3\">At each iteration, each random variable si is sampled from the conditional distribution P(si|s−i) where s−i refers to all variables other than si.</S>\n    <S sid=\"105\" ssid=\"4\">Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) · p(O |f-i, Rt)dO _ #(fi, si) + Rl For the likelihood term p(fi|s, f −i,R), integrating over all possible values of the multinomial featuresense distribution � gives us the rightmost term in Equation 3, which has an intuitive interpretation.</S>\n    <S sid=\"106\" ssid=\"5\">The term #(fi,si) indicates the number of times the feature-value fi was assigned sense si in the rest of the data.</S>\n    <S sid=\"107\" ssid=\"6\">Similarly, #(si) indicates the number of times the sense assignment si was observed in the data.</S>\n    <S sid=\"108\" ssid=\"7\">Rl is the Dirichlet prior for the featuresense distribution � in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.</S>\n    <S sid=\"109\" ssid=\"8\">Intuitively, the probability of a feature-value given a sense is directly proportional to the number of times we have seen that value and that senseassignment together in the data, taking into account a pseudo-count prior, expressed through R. This can also be viewed as a form of smoothing.</S>\n    <S sid=\"110\" ssid=\"9\">A similar approach is taken with regards to the prior probability p(si|s−i,a).</S>\n    <S sid=\"111\" ssid=\"10\">In this case, however, all layers must be considered: Here λl is the weight for the contribution of layer l, and αl is the portion of the Dirichlet prior for the sense distribution θ in the current layer.</S>\n    <S sid=\"112\" ssid=\"11\">Treating each layer individually, we integrate over the possible values of θ, obtaining a similar count-based term: where #l(si) indicates the number of elements in layer l assigned the sense si, #l indicates the number of elements in layer l, i.e., the size of the layer and S the number of senses.</S>\n    <S sid=\"113\" ssid=\"12\">To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.</S>\n    <S sid=\"114\" ssid=\"13\">This distributes α according to the relative size of each layer in the instance.</S>\n    <S sid=\"115\" ssid=\"14\">Placing these values in Equation 4 we obtain the following: #m+S·α Putting it all together, we arrive at the final update equation for the Gibbs sampling: Note that when dealing with a single layer, Equation 8 collapses to: where #m(si) indicates the number of elements (e.g., words) in the context window assigned to sense si.</S>\n    <S sid=\"116\" ssid=\"15\">This is identical to the update equation in the original, word-based LDA model.</S>\n    <S sid=\"117\" ssid=\"16\">The sampling algorithm gives direct estimates of s for every context element.</S>\n    <S sid=\"118\" ssid=\"17\">However, in view of our task, we are more interested in estimating θ, the sense-context distribution which can be obtained as in Equation 7, but taking into account all sense assignments, without removing assignment i.</S>\n    <S sid=\"119\" ssid=\"18\">Our system labels each instance with the single, most probable sense.</S>\n  </SECTION>\n  <SECTION title=\"5 Evaluation Setup\" number=\"5\">\n    <S sid=\"120\" ssid=\"1\">In this section we discuss our experimental set-up for assessing the performance of the model presented above.</S>\n    <S sid=\"121\" ssid=\"2\">We give details on our training procedure, describe our features, and explain how our system output was evaluated.</S>\n    <S sid=\"122\" ssid=\"3\">Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words.</S>\n    <S sid=\"123\" ssid=\"4\">For example, nouns represent 45% of the content words in the British National Corpus.</S>\n    <S sid=\"124\" ssid=\"5\">Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech.</S>\n    <S sid=\"125\" ssid=\"6\">For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).</S>\n    <S sid=\"126\" ssid=\"7\">The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ).</S>\n    <S sid=\"127\" ssid=\"8\">It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns.</S>\n    <S sid=\"128\" ssid=\"9\">The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense.</S>\n    <S sid=\"129\" ssid=\"10\">This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses.</S>\n    <S sid=\"130\" ssid=\"11\">We used two corpora for training as we wanted to evaluate our model’s performance across different domains.</S>\n    <S sid=\"131\" ssid=\"12\">The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.</S>\n    <S sid=\"132\" ssid=\"13\">This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.</S>\n    <S sid=\"133\" ssid=\"14\">The second, in-domain, corpus was built from selected portions of the Wall Street Journal.</S>\n    <S sid=\"134\" ssid=\"15\">We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.</S>\n    <S sid=\"135\" ssid=\"16\">Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).</S>\n    <S sid=\"136\" ssid=\"17\">Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods.</S>\n    <S sid=\"137\" ssid=\"18\">Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).</S>\n    <S sid=\"138\" ssid=\"19\">Here, no attempt is made to match the induced senses against the labels of the gold standard.</S>\n    <S sid=\"139\" ssid=\"20\">Under the second scheme, the gold standard is partitioned into a test and training corpus.</S>\n    <S sid=\"140\" ssid=\"21\">The latter is used to derive a mapping of the induced senses to the gold standard labels.</S>\n    <S sid=\"141\" ssid=\"22\">The mapping is then used to calculate the system’s F-Score on the test corpus.</S>\n    <S sid=\"142\" ssid=\"23\">Unfortunately, the first scheme failed to discriminate among participating systems.</S>\n    <S sid=\"143\" ssid=\"24\">The onecluster-per-word baseline outperformed all systems, except one, which was only marginally better.</S>\n    <S sid=\"144\" ssid=\"25\">The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.</S>\n    <S sid=\"145\" ssid=\"26\">For the purposes of this work, therefore, we focused on the second evaluation scheme.</S>\n    <S sid=\"146\" ssid=\"27\">Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores.</S>\n    <S sid=\"147\" ssid=\"28\">Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context.</S>\n    <S sid=\"148\" ssid=\"29\">Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).</S>\n    <S sid=\"149\" ssid=\"30\">These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation).</S>\n    <S sid=\"150\" ssid=\"31\">In all cases, we use the lemmatized version of the word(s).</S>\n    <S sid=\"151\" ssid=\"32\">The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word).</S>\n    <S sid=\"152\" ssid=\"33\">This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.</S>\n    <S sid=\"153\" ssid=\"34\">For instances containing more than one occurrence of the target word, we disambiguate the first occurrence.</S>\n    <S sid=\"154\" ssid=\"35\">Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sensecluster.3</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments\" number=\"6\">\n    <S sid=\"155\" ssid=\"1\">Model Selection The framework presented in Section 3 affords great flexibility in modeling the empirical data.</S>\n    <S sid=\"156\" ssid=\"2\">This however entails that several parameters must be instantiated.</S>\n    <S sid=\"157\" ssid=\"3\">More precisely, our model is conditioned on the Dirichlet hyperparameters α and β and the number of senses S. Additional parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights.</S>\n    <S sid=\"158\" ssid=\"4\">Our strategy in this paper is to fix α and β and explore the consequences of varying S. The value for the α hyperparameter was set to 0.02.</S>\n    <S sid=\"159\" ssid=\"5\">This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets.</S>\n    <S sid=\"160\" ssid=\"6\">We experimented with α values ranging from 0.005 to 1.</S>\n    <S sid=\"161\" ssid=\"7\">The β parameter was set to 0.1 (in all layers).</S>\n    <S sid=\"162\" ssid=\"8\">This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).</S>\n    <S sid=\"163\" ssid=\"9\">For simplicity, we used uniform weights for the layers.</S>\n    <S sid=\"164\" ssid=\"10\">The Gibbs sampler was run for 2,000 iterations.</S>\n    <S sid=\"165\" ssid=\"11\">Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs.</S>\n    <S sid=\"166\" ssid=\"12\">Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive.</S>\n    <S sid=\"167\" ssid=\"13\">We experimented with values ranging from three to nine senses.</S>\n    <S sid=\"168\" ssid=\"14\">Figure 3 shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively.</S>\n    <S sid=\"169\" ssid=\"15\">Here, we are using the optimal combination of layers for each system (which we discuss in the following section in detail).</S>\n    <S sid=\"170\" ssid=\"16\">For the model trained on WSJ, performance peaks at four senses, which is similar to the average ambiguity in the test data.</S>\n    <S sid=\"171\" ssid=\"17\">For the model trained on the BNC, however, the best results are obtained using twice as many senses.</S>\n    <S sid=\"172\" ssid=\"18\">Using fewer senses with the BNC-trained system can result in a drop in accuracy of almost 2%.</S>\n    <S sid=\"173\" ssid=\"19\">This is due to the shift in domain.</S>\n    <S sid=\"174\" ssid=\"20\">As the sense-divisions of the learning domain do not match those of the target domain, finer granularity is required in order to encompass all the relevant distinctions.</S>\n    <S sid=\"175\" ssid=\"21\">Table 1 illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.</S>\n    <S sid=\"176\" ssid=\"22\">The most probable words for each sense are also shown.</S>\n    <S sid=\"177\" ssid=\"23\">Firstly, note that the model infers some plausible senses for drug on the WSJ corpus (top half of Table 1).</S>\n    <S sid=\"178\" ssid=\"24\">Sense 1 corresponds to the “enforcement” sense of drug, Sense 2 refers to “medication”, Sense 3 to the “drug industry” and Sense 4 to “drugs research”.</S>\n    <S sid=\"179\" ssid=\"25\">The inferred senses for drug on the BNC (bottom half of Table 1) are more fine grained.</S>\n    <S sid=\"180\" ssid=\"26\">For example, the model finds distinct senses for “medication” (Sense 1 and 7) and “illegal substance” (Senses 2, 4, 6, 7).</S>\n    <S sid=\"181\" ssid=\"27\">It also finds a separate sense for “drug dealing” (Sense 5) and “enforcement” (Sense 8).</S>\n    <S sid=\"182\" ssid=\"28\">Because the BNC has a broader focus, finer distinctions are needed to cover as many senses as possible that are relevant to the target domain (WSJ).</S>\n    <S sid=\"183\" ssid=\"29\">Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.</S>\n    <S sid=\"184\" ssid=\"30\">We also investigate whether their combination, through our layered model (see Figure 2), yields performance improvements.</S>\n    <S sid=\"185\" ssid=\"31\">We used 4 senses for the system trained on WSJ and 8 for the system trained on the BNC (a was set to 0.02 and b to 0.1) Table 2 (left side) shows the performance of our model when using only one layer.</S>\n    <S sid=\"186\" ssid=\"32\">The layer composed of words co-occurring within a ±10-word window (10w), and representing wider, topical, information gives the highest scores on its own.</S>\n    <S sid=\"187\" ssid=\"33\">It is followed by the ±5 (5w) and ±1 (1w) word windows, which represent more immediate, local context.</S>\n    <S sid=\"188\" ssid=\"34\">Part-of-speech n-grams (pg) and word ngrams (ng), on their own, achieve lower scores, largely due to over-generalization and data sparseness, respectively.</S>\n    <S sid=\"189\" ssid=\"35\">The lowest-scoring single layer is the dependency layer (dp), with performance only slightly above the most-frequent-sense baseline (MFS).</S>\n    <S sid=\"190\" ssid=\"36\">Dependency information is very informative when present, but extremely sparse.</S>\n    <S sid=\"191\" ssid=\"37\">Table 2 (middle) also shows the results obtained when running the layered model with all but one of the layers as input.</S>\n    <S sid=\"192\" ssid=\"38\">We can use this information to determine the contribution of each layer by comparing to the combined model with all layers (all).</S>\n    <S sid=\"193\" ssid=\"39\">Because we are dealing with multiple layers, there is an element of overlap involved.</S>\n    <S sid=\"194\" ssid=\"40\">Therefore, each of the word-window layers, despite relatively high informativeness on its own, does not cause as much damage when it is absent, since the other layers compensate for the topical and local information.</S>\n    <S sid=\"195\" ssid=\"41\">The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.</S>\n    <S sid=\"196\" ssid=\"42\">Finally, we can see that the extremely sparse dependency layer is detrimental to the multi-layer model as a whole, and its removal increases performance.</S>\n    <S sid=\"197\" ssid=\"43\">The sparsity of the data in this layer means that there is often little information on which to base a decision.</S>\n    <S sid=\"198\" ssid=\"44\">In these cases, the layer contributes a close-to-uniform estimation of the sense distribution, which confuses the combined model.</S>\n    <S sid=\"199\" ssid=\"45\">Other layer combinations obtained similar results.</S>\n    <S sid=\"200\" ssid=\"46\">Table 2 (right side) shows the most informative two and three layer combinations.</S>\n    <S sid=\"201\" ssid=\"47\">Again, dependencies tend to decrease performance.</S>\n    <S sid=\"202\" ssid=\"48\">On the other hand, combining features that have similar performance on their own is beneficial.</S>\n    <S sid=\"203\" ssid=\"49\">We obtain the best performance overall with a two layered model combining topical (+10w) and local (+5w) contexts.</S>\n    <S sid=\"204\" ssid=\"50\">Table 3 replicates the same suite of experiments on the BNC corpus.</S>\n    <S sid=\"205\" ssid=\"51\">The general trends are similar.</S>\n    <S sid=\"206\" ssid=\"52\">Some interesting differences are apparent, however.</S>\n    <S sid=\"207\" ssid=\"53\">The sparser layers, notably word n-grams and dependencies, fare comparatively worse.</S>\n    <S sid=\"208\" ssid=\"54\">This is expected, since the more precise, local, information is likely to vary strongly across domains.</S>\n    <S sid=\"209\" ssid=\"55\">Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.</S>\n    <S sid=\"210\" ssid=\"56\">Another observable difference is that the combined model without the dependency layer does slightly better than each of the single layers.</S>\n    <S sid=\"211\" ssid=\"57\">The 1w+pg combination improves over its components, which have similar individual performance.</S>\n    <S sid=\"212\" ssid=\"58\">Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side).</S>\n    <S sid=\"213\" ssid=\"59\">Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition.</S>\n    <S sid=\"214\" ssid=\"60\">IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.</S>\n    <S sid=\"215\" ssid=\"61\">These models and our own model significantly outperform the most-frequent-sense baseline (p &lt; 0.01 using a x2 test).</S>\n    <S sid=\"216\" ssid=\"62\">Our best system (10w+5w on WSJ) is significantly better than UMND2 (p &lt; 0.01) and quantitatively better than IR2, although the difference is not statistically significant.</S>\n  </SECTION>\n  <SECTION title=\"7 Discussion\" number=\"7\">\n    <S sid=\"217\" ssid=\"1\">This paper presents a novel Bayesian approach to sense induction.</S>\n    <S sid=\"218\" ssid=\"2\">We formulated sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on the basis of latent variables.</S>\n    <S sid=\"219\" ssid=\"3\">Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.</S>\n    <S sid=\"220\" ssid=\"4\">Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account.</S>\n    <S sid=\"221\" ssid=\"5\">For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text.</S>\n    <S sid=\"222\" ssid=\"6\">In the future, we hope to explore more rigorous parameter estimation techniques.</S>\n    <S sid=\"223\" ssid=\"7\">Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.</S>\n    <S sid=\"224\" ssid=\"8\">Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.</S>\n    <S sid=\"225\" ssid=\"9\">In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal.</S>\n    <S sid=\"226\" ssid=\"10\">This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.</S>\n    <S sid=\"227\" ssid=\"11\">Acknowledgments The authors acknowledge the support of EPSRC (grant EP/C538447/1).</S>\n    <S sid=\"228\" ssid=\"12\">We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "e4c31afd66e83eb0ed6323a7fd3ef86e",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Bayesian Word Sense Induction"
                }
            }
        },
        {
            "index": 13,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "In this paper we address the problem of modeling compositional meaning for phrases and sentences using distributional methods. We experiment with several possible combinations of representation and composition, exhibiting varying degrees of sophistication. Some are shallow while others operate over syntactic structure, rely on parameter learning, or require access to very large corpora. We find that shallow approaches are as good as more computationally intensive alternatives with regards to two particular tests: (1) phrase similarity and (2) paraphrase detection. The sizes of the involved training corpora and the generated vectors are not as important as the fit between the meaning representation and compositional method.",
                    "filename": "A_Comparison_of_Vector-based_Representations_for_Semantic_Composition",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D12-1050/Documents_xml/D12-1050.xml",
                    "title": "A Comparison of Vector-based Representations for Semantic Composition"
                },
                "llm": {
                    "descriptions": [
                        "This paper systematically compares three types of distributional representation and their impact on semantic composition. The representations include a simple distributional semantic space, word embeddings computed with a neural language model, and a representation based on weighted word-link-word tuples arranged into a third-order tensor. These representations are evaluated with three composition methods involving addition, multiplication, and a deep recursive autoencoder. The evaluation is conducted on phrase similarity and paraphrase detection tasks. The findings suggest that simpler models, such as the simple semantic space, outperform more sophisticated models in tasks involving both phrase similarity and paraphrase detection. This suggests that the choice of lexical representation is critical for the success of distributional semantic models.",
                        "This paper compares three types of distributional representations and their impact on semantic composition, examining a simple semantic space, word embeddings computed with a neural language model, and a representation based on weighted word-link-word tuples. The paper evaluates nine models on phrase similarity and paraphrase detection tasks, finding that simple distributional semantics representations outperform more complex models. The findings suggest that the problem of vector composition cannot be pursued independently of the choice of lexical representation, highlighting the importance of mutually learning representation and composition. The study provides insight into the future of compositional models for distributional semantics.",
                        "This paper compares vector-based representations for semantic composition. It examines three types of distributional representation: a simple semantic space, syntax-aware space, and word embeddings computed with a neural language model. The study evaluates these representations using different compositional models and assesses their effectiveness in two evaluation protocols: phrase similarity and paraphrase detection. The results suggest that simple distributional representations, such as the semantic space, perform competitively without requiring sophisticated linguistic processing. The findings indicate that the choice of lexical representation and vector composition methods are interdependent and must be mutually learned in a good model of distributive semantics.",
                        "This paper compares three types of distributional representations and their impact on semantic composition. The representations include a simple semantic space, word embeddings computed with a neural language model, and a representation based on weighted word-link-word tuples arranged into a third-order tensor. These representations are evaluated using three composition methods: addition, multiplication, and a deep recursive autoencoder. The paper assesses the effectiveness of these models using two evaluation protocols: phrase similarity and paraphrase detection. The results show that simple representations are as effective as more complex ones, suggesting that the problem of vector composition cannot be pursued independent of the choice of lexical representation.",
                        "This paper compares three types of distributional representations and their effect on semantic composition. It examined simple distributional semantic space, word embeddings computed with a neural language model, and a representation based on weighted word-link-word tuples. These representations were evaluated using three composition methods involving addition, multiplication, and a deep recursive autoencoder. The evaluation involved phrase similarity and paraphrase detection tasks. The findings suggest that simple distributional semantic space performed competitively, while the more sophisticated representations did not outperform it. The study highlights the importance of the mutual learning of lexical representation and vector composition. Overall, the results indicate that simple approaches can be effective in modeling semantic composition.",
                        "This academic paper compares three types of distributional representation and their effects on semantic composition. The study examines a simple distributional semantic space, word embeddings from a neural language model, and a representation based on weighted word-link-word tuples. These representations are evaluated using three composition methods: addition, multiplication, and a deep recursive autoencoder. The study assesses the effectiveness of these models using phrase similarity and paraphrase detection tasks. The findings indicate that simpler representations, despite being syntax-oblivious, perform competitively on the tasks, challenging the need for more complex representations in distributional semantics. The paper emphasizes the importance of mutually learning lexical representation and composition methods.",
                        "This paper compares three types of distributional representations and their impact on semantic composition. The representations include a simple distributional semantic space, word embeddings from a neural language model, and a representation based on weighted word-link-word tuples arranged into a third-order tensor. Nine models were evaluated on phrase similarity and paraphrase detection tasks. The findings indicate that the simple distributional semantic space performs comparably to more sophisticated representations, suggesting that simple models may be as effective as more complex ones for semantic tasks. The study emphasizes the importance of a mutual learning process for both lexical representation and composition in distributive semantics models.",
                        "This paper compares three types of distributional representations and their impact on semantic composition. The representations include a simple semantic space, word embeddings from a neural language model, and a representation based on weighted word-link-word tuples. These representations are evaluated with three composition methods: addition, multiplication, and a deep recursive autoencoder. The evaluation involves phrase similarity and paraphrase detection tasks. The findings indicate that simple representations often outperform more sophisticated ones in both tasks, suggesting that the choice of lexical representation must be considered in conjunction with vector composition methods. These results provide insights into the future of compositional models for distributional semantics.",
                        "This paper compares three types of distributional representations and their impact on semantic composition. The study evaluates simple distributional semantic space, word embeddings from a neural language model, and a representation using weighted word-link-word tuples. These representations are then tested using three composition methods: addition, multiplication, and a deep recursive autoencoder. The study assesses the models' performance on phrase similarity and paraphrase detection tasks. The findings indicate that simpler representations are generally as effective as more complex ones, suggesting that a good model of distributional semantics requires a combination of mutually learned representation and composition.",
                        "This paper compares three types of distributional representation and their effect on semantic composition. The representations include a simple distributional semantic space, word embeddings computed with a neural language model, and a representation based on weighted word-link-word tuples arranged into a third-order tensor. These representations are evaluated using three methods of vector composition: addition, multiplication, and a deep recursive autoencoder. The paper evaluates these models on phrase similarity and paraphrase detection tasks and finds that the simple semantic space performs competitively without requiring access to large corpora or sophisticated linguistic processing. The findings suggest that in a good model of distributive semantics, representation and composition must be mutually learned."
                    ],
                    "filename": "a_comparison_of_vectorbased_repr-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 104 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Comparison of Vector-based Representations for Semantic Composition</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Distributional models of semantics have seen considerable success at simulating a wide range of behavioral data in tasks involving semantic cognition and also in practical applications.</S>\n    <S sid=\"7\" ssid=\"2\">For example, they have been used to model judgments of semantic similarity (McDonald, 2000) and association (Denhire and Lemaire, 2004; Griffiths et al., 2007) and have been shown to achieve human level performance on synonymy tests (Landauer and Dumais, 1997; Griffiths et al., 2007) such as those included in the Test of English as a Foreign Language (TOEFL).</S>\n    <S sid=\"8\" ssid=\"3\">This ability has been put to practical use in numerous natural language processing tasks such as automatic thesaurus extraction (Grefenstette, 1994), word sense discrimination (Sch¨utze, 1998), language modeling (Bellegarda, 2000), and the identification of analogical relations (Turney, 2006).</S>\n    <S sid=\"9\" ssid=\"4\">While much research has been directed at the most effective ways of constructing representations for individual words, there has been far less consensus regarding the representation of larger constructions such as phrases and sentences.</S>\n    <S sid=\"10\" ssid=\"5\">The problem has received some attention in the connectionist literature, particularly in response to criticisms of the ability of connectionist representations to handle complex structures (Smolensky, 1990; Plate, 1995).</S>\n    <S sid=\"11\" ssid=\"6\">More recently, several proposals have been put forward for computing the meaning of word combinations in vector spaces.</S>\n    <S sid=\"12\" ssid=\"7\">This renewed interest is partly due to the popularity of distributional methods and their application potential to tasks that require an understanding of larger phrases or complete sentences.</S>\n    <S sid=\"13\" ssid=\"8\">For example, Mitchell and Lapata (2010) introduce a general framework for studying vector composition, which they formulate as a function f of two vectors u and v. Different composition models arise, depending on how f is chosen.</S>\n    <S sid=\"14\" ssid=\"9\">Assuming that composition is a linear function of the Cartesian product of u and v allows to specify additive models which are by far the most common method of vector combination in the literature (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001).</S>\n    <S sid=\"15\" ssid=\"10\">Alternatively, assuming that composition is a linear function of the tensor product of u and v, gives rise to models based on multiplication.</S>\n    <S sid=\"16\" ssid=\"11\">One of the most sophisticated proposals for semantic composition is that of Clark et al. (2008) and the more recent implementation of Grefenstette and Sadrzadeh (2011a).</S>\n    <S sid=\"17\" ssid=\"12\">Using techniques from logic, category theory, and quantum information they develop a compositional distributional semantics that brings type-logical and distributional vector space models together.</S>\n    <S sid=\"18\" ssid=\"13\">In their framework, words belong to different type-based categories and different categories exist in different dimensional spaces.</S>\n    <S sid=\"19\" ssid=\"14\">The category of a word is decided by the number and type of adjoints (arguments) it can take and the composition of a sentence results in a vector which exists in sentential space.</S>\n    <S sid=\"20\" ssid=\"15\">Verbs, adjectives and adverbs act as relational functions, are represented by matrices, and modify the properties of nouns, that are represented by vectors (see also Baroni and Zamparelli (2010) for a proposal similar in spirit).</S>\n    <S sid=\"21\" ssid=\"16\">Clarke (2012) introduces context-theoretic semantics, a general framework for combining vector representations, based on a mathematical theory of meaning as context, and shows that it can be used to describe a variety of models including that of Clark et al. (2008).</S>\n    <S sid=\"22\" ssid=\"17\">Socher et al. (2011a) and Socher et al.</S>\n    <S sid=\"23\" ssid=\"18\">(2011b) present a framework based on recursive neural networks that learns vector space representations for multi-word phrases and sentences.</S>\n    <S sid=\"24\" ssid=\"19\">The network is given a list of word vectors as input and a binary tree representing their syntactic structure.</S>\n    <S sid=\"25\" ssid=\"20\">Then, it computes an n-dimensional representation p of two n-dimensional children and the process is repeated at every parent node until a representation for a full tree is constructed.</S>\n    <S sid=\"26\" ssid=\"21\">Parent representations are computed essentially by concatenating the representations of their children.</S>\n    <S sid=\"27\" ssid=\"22\">During training, the model tries to minimize the reconstruction errors between the n-dimensional parent vectors and those representing their children.</S>\n    <S sid=\"28\" ssid=\"23\">This model can also compute compositional representations when the tree structure is not given, e.g., by greedily inferring a binary tree.</S>\n    <S sid=\"29\" ssid=\"24\">Although the type of function used for vector composition has attracted much attention, relatively less emphasis has been placed on the basic distributional representations on which the composition functions operate.</S>\n    <S sid=\"30\" ssid=\"25\">In this paper, we examine three types of distributional representation of increasing sophistication and their effect on semantic composition.</S>\n    <S sid=\"31\" ssid=\"26\">These include a simple semantic space, where a word’s vector represents its co-occurrence with neighboring words (Mitchell and Lapata, 2010), a syntax-aware space based on weighted distributional tuples that encode typed co-occurrence relations among words (Baroni and Lenci, 2010), and word embeddings computed with a neural language model (Bengio, 2001; Collobert and Weston, 2008).</S>\n    <S sid=\"32\" ssid=\"27\">Word embeddings are distributed representations, low-dimensional and real-valued.</S>\n    <S sid=\"33\" ssid=\"28\">Each dimension of the embedding represents a latent feature of the word, hopefully capturing useful syntactic and semantic properties.</S>\n    <S sid=\"34\" ssid=\"29\">Using these representations, we construct several compositional models, based on addition, multiplication, and recursive neural networks.</S>\n    <S sid=\"35\" ssid=\"30\">We assess the effectiveness of these models using two evaluation protocols.</S>\n    <S sid=\"36\" ssid=\"31\">The first one involves modeling similarity judgments for short phrases gathered in human experiments (Mitchell and Lapata, 2010).</S>\n    <S sid=\"37\" ssid=\"32\">The second one is paraphrase detection, i.e., the task of examining two sentences and determining whether they have the same meaning (Socher et al., 2011a).</S>\n    <S sid=\"38\" ssid=\"33\">We find that shallow approaches are as good as more computationally intensive alternatives.</S>\n    <S sid=\"39\" ssid=\"34\">They achieve considerable semantic expressivity without any learning, sophisticated linguistic processing, or access to very large corpora.</S>\n    <S sid=\"40\" ssid=\"35\">Our contributions in this work are three-fold: an empirical comparison of a broad range of compositional models, some of which are introduced here for the first time; the use of an evaluation methodology that takes into account the full spectrum of compositionality from phrases to sentences; and the empirical finding that relatively simple compositional models can be used to perform competitively on the paraphrase detection and phrase similarity tasks.</S>\n  </SECTION>\n  <SECTION title=\"2 Modeling\" number=\"2\">\n    <S sid=\"41\" ssid=\"1\">The elementary objects that we operate on are vectors associated with words.</S>\n    <S sid=\"42\" ssid=\"2\">We instantiate these word representations following three distinct semantic space models which we describe in Section 2.1 below.</S>\n    <S sid=\"43\" ssid=\"3\">Analogously, in Section 2.2 we consider three methods of vector composition, i.e., how a phrase or a sentence can be represented as a vector using the vectors of its constituent words.</S>\n    <S sid=\"44\" ssid=\"4\">Combining different vector representations and composition methods gives rise to several compositional models whose performance we evaluate in Sections 3 and 4.</S>\n    <S sid=\"45\" ssid=\"5\">For all of our experiments we employ column vectors from a Cartesian, finitely-dimensional space.</S>\n    <S sid=\"46\" ssid=\"6\">The dimensionality will depend on the source of the vectors involved.</S>\n    <S sid=\"47\" ssid=\"7\">Similarly, the component values inside each source’s vectors are not to be interpreted in the same manner.</S>\n    <S sid=\"48\" ssid=\"8\">Nonetheless, they have in common that they originate from distributive corpus statistics. meaning is commonly represented in a highdimensional space, where each component corresponds to some contextual element in which the word is found.</S>\n    <S sid=\"49\" ssid=\"9\">The contextual elements can be words themselves, or larger linguistic units such as sentences or documents, or even more complex linguistic representations such as the argument slots of predicates.</S>\n    <S sid=\"50\" ssid=\"10\">A semantic space that is often employed in studying compositionality across a variety of tasks (Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011a) uses a context window of five words on either side of the target word, and 2,000 vector dimensions.</S>\n    <S sid=\"51\" ssid=\"11\">These are the common context words in the British National Corpus (BNC), a corpus of about 100 million tokens.</S>\n    <S sid=\"52\" ssid=\"12\">Their values are set to the ratio of the probability of the context word given the target word to the probability of the context word overall.</S>\n    <S sid=\"53\" ssid=\"13\">More formally, let us consider the BNC as a set of sentences: ni ) from the BNC’s vocabulary VocBNC.</S>\n    <S sid=\"54\" ssid=\"14\">Then f reqw is the amount of times that each word w ∈ VocBNC appears in the BNC.</S>\n    <S sid=\"55\" ssid=\"15\">Mitchell and Lapata (2010) collect the M most frequent non-stoplist words in the set ctxttop = {w(top)1, , wM(top) } and let them consitute the word vectors’ dimensions.</S>\n    <S sid=\"56\" ssid=\"16\">Each dimension’s value is obtained from a co-occurrence count: for w ∈ VocBNC and j = 1,...,M. Using these counts, they define word vectors component-wise. for j = 1,...,M, where totalCount is the total number of words in the BNC.</S>\n    <S sid=\"57\" ssid=\"17\">This space is relatively simple, it has few parameters, requires no preprocessing other than tokenization and involves no syntactic information or parameter learning.</S>\n    <S sid=\"58\" ssid=\"18\">Despite its simplicity, it is a good starting point for studying representations for compositional models as a baseline against which to evaluate more elaborate models.</S>\n    <S sid=\"59\" ssid=\"19\">Neural Language Model Another perhaps less well-known approach to meaning representation is to represent words as continuous vectors of parameters.</S>\n    <S sid=\"60\" ssid=\"20\">Such word vectors can be obtained with an unsupervised neural language model (NLM, Bengio (2001); Collobert and Weston (2008)) which jointly learns an embedding of words into a vector space and uses these vectors to predict how likely a word is, given its context.</S>\n    <S sid=\"61\" ssid=\"21\">We induced word embeddings with Collobert and Weston (2008)’s neural language model.</S>\n    <S sid=\"62\" ssid=\"22\">The model is discriminative and non-probabilistic.</S>\n    <S sid=\"63\" ssid=\"23\">Each word i ∈ D (the vocabulary) is embedded into a d-dimensional space using a lookup table LTW(·): where W ∈ Rd×|D |is a matrix of parameters to be learned.</S>\n    <S sid=\"64\" ssid=\"24\">Wi ∈ Rd is the i-th column of W and d is the word vector size to be chosen by the user.</S>\n    <S sid=\"65\" ssid=\"25\">The parameters W are automatically trained during the learning process using backpropagation.</S>\n    <S sid=\"66\" ssid=\"26\">Specifically, at each training update, the model reads an n-gram x = (w1,...,wn) from the corpus.</S>\n    <S sid=\"67\" ssid=\"27\">The n-gram is paired with a corrupted n-gram x˜ = (w1,..., ˜wn) where ˜wn =6 wn is chosen uniformly from the vocabulary.</S>\n    <S sid=\"68\" ssid=\"28\">The model concatenates the learned embeddings of the n words and predicts a score for the n-gram sequence using the learned embeddings as features.</S>\n    <S sid=\"69\" ssid=\"29\">The training criterion is that n-grams that are present in the training corpus must have a score at least some margin higher than the corrupted n-grams.</S>\n    <S sid=\"70\" ssid=\"30\">The model learns via gradient descent over the neural network parameters and the embedding lookup table.</S>\n    <S sid=\"71\" ssid=\"31\">Word vectors are stored in a word embedding matrix which captures syntactic and semantic information from co-occurrence statistics.</S>\n    <S sid=\"72\" ssid=\"32\">As these representations are learned, albeit in an unsupervised manner, one would hope that they capture word meanings more succinctly, compared to the simpler distributional representations that are merely based on co-occurrence.</S>\n    <S sid=\"73\" ssid=\"33\">We trained the neural language model on the BNC.</S>\n    <S sid=\"74\" ssid=\"34\">We optimized the model’s parameters on a word similarity task using 4% of the BNC as development data.</S>\n    <S sid=\"75\" ssid=\"35\">Specifically, we used WordSim353, a benchmark dataset (Finkelstein et al., 2001), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.</S>\n    <S sid=\"76\" ssid=\"36\">We experimented with vectors of varying dimensionality (ranging from 50 to 200, with a step size of 50).</S>\n    <S sid=\"77\" ssid=\"37\">The size of the target word’s context window was 2, 3 and 4 in turn.</S>\n    <S sid=\"78\" ssid=\"38\">The rate at which embeddings were learned ranged from 3.4 x 10−10 to 6.7 x 10−10 to 10−9.</S>\n    <S sid=\"79\" ssid=\"39\">We ran each training process for 1.1 x 108 to 2.7 x 108 iterations (ca.</S>\n    <S sid=\"80\" ssid=\"40\">2 days).</S>\n    <S sid=\"81\" ssid=\"41\">We obtained the best results with 50 dimensions, a context window of size 4, and a embedding learning rate of 10−9.</S>\n    <S sid=\"82\" ssid=\"42\">The NLM with these parameters was then trained for 1.51x109 iterations (ca.</S>\n    <S sid=\"83\" ssid=\"43\">2 weeks).</S>\n    <S sid=\"84\" ssid=\"44\">Figure 1 illustrates a two-dimensional projection of the embeddings for the 500 most common words in the BNC.</S>\n    <S sid=\"85\" ssid=\"45\">We only show two out of the actual 50 dimensions involved, but one can already begin to see clusterings of a syntactic and semantic nature.</S>\n    <S sid=\"86\" ssid=\"46\">In one corner, for example, we encounter a grouping of possessive pronouns together with the possessive clitic ’s.</S>\n    <S sid=\"87\" ssid=\"47\">The singular ones my, her and his are closely positioned, as are the plural ones our, your and their.</S>\n    <S sid=\"88\" ssid=\"48\">Also, there is a clustering of sociopolitical terms, such as international, country, national, government, and council.</S>\n    <S sid=\"89\" ssid=\"49\">Distributional Memory Tensor Baroni and Lenci (2010) present Distributional Memory, a generalized framework for distributional semantics from which several special-purpose models can be derived.</S>\n    <S sid=\"90\" ssid=\"50\">In their framework distributional information is extracted from the corpus once, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor.</S>\n    <S sid=\"91\" ssid=\"51\">Different matrices are then generated from the tensor, and their rows and columns give rise to different semantic spaces appropriate for capturing different semantic problems.</S>\n    <S sid=\"92\" ssid=\"52\">In this way, the same distributional information can be shared across tasks such as word similarity or analogical learning.</S>\n    <S sid=\"93\" ssid=\"53\">More formally, Baroni and Lenci (2010) construct a 3-dimensional tensor T assigning a value c to instances of word pairs w,v and a connecting link-word l. This representation operates over a dependency-parsed corpus and the scores c are obtained via counting the occurrences of tuples, and weighting the raw counts by mutual information.</S>\n    <S sid=\"94\" ssid=\"54\">Table 1 presents examples of tensor entries.</S>\n    <S sid=\"95\" ssid=\"55\">These were taken from a distributional memory tensor1 that Baroni and Lenci obtained via preprocessing several corpora: the web-derived ukWac corpus of about 1.915 billion words, a mid-2009 dump of the English Wikipedia containing about 820 million words, and the BNC.</S>\n    <S sid=\"96\" ssid=\"56\">Extracting a 3-dimensional tensor from the BNC alone would create very sparse representations.</S>\n    <S sid=\"97\" ssid=\"57\">We therefore extract so-called word-fibres, essentially projections onto a lower-dimensional subspace, from the same tensor Baroni and Lenci (2010) collectively derived from the 3 billion word corpus just described (henceforth 3-BWC).</S>\n    <S sid=\"98\" ssid=\"58\">We view the 3-dimensional tensor as a mapping which assigns each target word w a non-zero value c, given the context (l,v).</S>\n    <S sid=\"99\" ssid=\"59\">All wordcontext combinations not listed in T are implicitly assigned a zero value.</S>\n    <S sid=\"100\" ssid=\"60\">Now we consider two possible approaches for obtaining vectors, depending on their application.</S>\n    <S sid=\"101\" ssid=\"61\">First, we let the D most frequent contexts constitute the D dimensions that each word vector will have.</S>\n    <S sid=\"102\" ssid=\"62\">Table 2 shows the 11 contexts (l,v) that appear most frequently in T. Thus, each target word’s vector is defined component-wise as: for j = 1,...,D. This approach is used when a fixed vector dimensionality is necessary.</S>\n    <S sid=\"103\" ssid=\"63\">A more dynamic approach is possible when very few words w1,...,wn are involved in a test.</S>\n    <S sid=\"104\" ssid=\"64\">Their representations can then have a denser format, that is, with no zero-valued components.</S>\n    <S sid=\"105\" ssid=\"65\">For this we identify the set of contexts common to the words involved, ctxtdyn = {(l(dyn) Each context (l,v) again constitutes a vector dimension.</S>\n    <S sid=\"106\" ssid=\"66\">The dimensionality varies strongly depending on the selection of words, but if n does not exceed 4, the dimensionality |ctxtdyn |will typically be substantial enough.</S>\n    <S sid=\"107\" ssid=\"67\">In this approach, each word’s vector consists of the values c found along with that word and its context in the tensor.</S>\n    <S sid=\"108\" ssid=\"68\">In our experiments we compose word vectors to create representations for phrase vectors and sentence vectors.</S>\n    <S sid=\"109\" ssid=\"69\">The phrases we are interested in consist of two words each: an adjective and a noun like black hair, a compound noun made up of two nouns such as oil industry, or a verbal phrase with a transitive verb and an object noun, e.g., pour tea.</S>\n    <S sid=\"110\" ssid=\"70\">Conceiving of a phrase phr = (w1,w2) as a binary tuple of words, we obtain its vector from its words’ vectors either by addition: In the same way we acquire a vector senVeci representing a sentence Seni = (w(i) ni ) from the vectors for w1,...,wni.</S>\n    <S sid=\"111\" ssid=\"71\">We simply sum the existing word vectors, that is, vectors obtained via the respective corpus for words that are not on our stoplist: And do the same with point-wise multiplication: The multiplication model in (13) can be seen as an instantiation of the categorical compositional framework put forward by Clark et al. (2008).</S>\n    <S sid=\"112\" ssid=\"72\">In fact, a variety of multiplication-based models can be derived from this framework; and comparisons against component-wise multiplication on phrase similarity tasks yield comparable results (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b).</S>\n    <S sid=\"113\" ssid=\"73\">We thus opt for the model (13) as an example of compositional models based on multiplication due to its good performance across a variety of tasks, including language modeling and prediction of reading difficulty (Mitchell, 2011).</S>\n    <S sid=\"114\" ssid=\"74\">Our third method, for creating phrase and sentence vectors alike, is the application of Socher et al. (2011a)’s model.</S>\n    <S sid=\"115\" ssid=\"75\">They use the Stanford parser (Klein and Manning, 2003) to create a binary parse tree for each input phrase or sentence.</S>\n    <S sid=\"116\" ssid=\"76\">This tree is then used as the basis for a deep recursive autoencoder (RAE).</S>\n    <S sid=\"117\" ssid=\"77\">The aim is to construct a vector representation for the tree’s root bottom-up where the leaves contain word vectors.</S>\n    <S sid=\"118\" ssid=\"78\">The latter can in theory be provided by any type of semantic space, however Socher et al. use word embeddings provided by the neural language model (Collobert and Weston, 2008).</S>\n    <S sid=\"119\" ssid=\"79\">Given the binary tree input structure, the model computes parent representations p from their children (c1,c2) using a standard neural network layer: where [c1;c2] is the concatenation of the two children, f is an element-wise activation function such as tanh, b is a bias term, and W E Rnx2n is an encoding matrix that we want to learn during training.</S>\n    <S sid=\"120\" ssid=\"80\">One way of assessing how well p represents its direct children is to decode their vectors in a reconstruction layer: During training, the goal is to minimize the reconstruction errors of all input pairs at nonterminal nodes p in a given parse tree by computing the square of the Euclidean distance between the original input and its reconstruction: Socher et al. (2011a) extend the standard recursive autoencoder sketched above in two ways.</S>\n    <S sid=\"121\" ssid=\"81\">Firstly, they present an unfolding autoencoder that tries to reconstruct all leaf nodes underneath each node rather than only its direct children.</S>\n    <S sid=\"122\" ssid=\"82\">And secondly, instead of transforming the two children directly into a parent p, they introduce another hidden layer inbetween.</S>\n    <S sid=\"123\" ssid=\"83\">We obtained three compositional models per representation resulting in nine compositional models overall.</S>\n    <S sid=\"124\" ssid=\"84\">Plugging different representations into the additive and multiplicative models is relatively straightforward.</S>\n    <S sid=\"125\" ssid=\"85\">The RAE can also be used with arbitrary word vectors.</S>\n    <S sid=\"126\" ssid=\"86\">Socher et al. (2011a) obtain best results with 100-dimensional vectors which we also used in our experiments.</S>\n    <S sid=\"127\" ssid=\"87\">NLM vectors were trained with this dimensionality on the BNC for 7.9 x 108 iterations (with window size 4 and an embedding learning rate of 10−9).</S>\n    <S sid=\"128\" ssid=\"88\">We constructed a simple distributional space with M = 100 dimensions, i.e., those connected to the 100 most frequent co-occurrence words.</S>\n    <S sid=\"129\" ssid=\"89\">In the case of vectors obtained from Baroni and Lenci (2010)’s DM tensor, we differentiated between phrases and sentences, due to the disparate amount of words contained in them (see Section 2.1).</S>\n    <S sid=\"130\" ssid=\"90\">To represent phrases, we used vectors of dynamic dimensionality, since these form a richer and denser representation.</S>\n    <S sid=\"131\" ssid=\"91\">The sentences considered in Section 4 are too large for this approach and all word vectors must be members of the same vector space.</S>\n    <S sid=\"132\" ssid=\"92\">Hence, these sentence vectors have fixed dimensionality D = 100, consisting of the “most significant” 100 dimensions, i.e., those reflecting the 100 most frequent contexts.</S>\n  </SECTION>\n  <SECTION title=\"3 Experiment 1: Phrase Similarity\" number=\"3\">\n    <S sid=\"133\" ssid=\"1\">Our first experiment focused on modeling similarity judgments for short phrases gathered in human experiments.</S>\n    <S sid=\"134\" ssid=\"2\">Distributional representations of individual words are commonly evaluated on tasks based on their ability to model semantic similarity relations, e.g., synonymy or priming.</S>\n    <S sid=\"135\" ssid=\"3\">Thus, it seems appropriate to evaluate phrase representations in a similar manner.</S>\n    <S sid=\"136\" ssid=\"4\">Specifically, we used the dataset from Mitchell and Lapata (2010) which contains similarity judgments for adjective-noun, noun-noun and verb-object phrases, respectively.2 Each item is a phrase pair phr1, phr2 which has a human rating from 1 (very low similarity) to 7 (very high similarity).</S>\n    <S sid=\"137\" ssid=\"5\">Using the composition models described above, we compute the cosine similarity of phr1 and phr2: Model similarities were evaluated against the human similarity ratings using Spearman’s p correlation coefficient.</S>\n    <S sid=\"138\" ssid=\"6\">Table 3 summarizes the performance of the various models on the phrase similarity dataset.</S>\n    <S sid=\"139\" ssid=\"7\">Rows in the table correspond to different vector representations: the simple distributional semantic space (SDS) from Mitchell and Lapata (2010), Baroni and Lenci’s (2010) distributional memory tensor (DM) and the neural language model (NLM), for each phrase combination: adjective noun (Adj-N), nounnoun (N-N) and verb object (V-Obj).</S>\n    <S sid=\"140\" ssid=\"8\">For each phrase type we report results for each compositional model, namely additive (+), multiplicative (0) and recursive autoencoder (RAE).</S>\n    <S sid=\"141\" ssid=\"9\">The table also shows the dimensionality of the input vectors next to the vector representation.</S>\n    <S sid=\"142\" ssid=\"10\">As can be seen, for SDS the best performing model is multiplication, as it is mostly for DM.</S>\n    <S sid=\"143\" ssid=\"11\">With regard to NLM, vector addition yields overall better results.</S>\n    <S sid=\"144\" ssid=\"12\">In general, neither DM or NLM in any compositional configuration are able to outperform SDS with multiplication.</S>\n    <S sid=\"145\" ssid=\"13\">All models in Table 3 are significantly correlated with the human similarity judgments (p &lt; 0.01).</S>\n    <S sid=\"146\" ssid=\"14\">Spearman’s p differences of 0.3 or more are significant at the 0.01 level, using a ttest (Cohen and Cohen, 1983).</S>\n  </SECTION>\n  <SECTION title=\"4 Experiment 2: Paraphrase Detection\" number=\"4\">\n    <S sid=\"147\" ssid=\"1\">Although the phrase similarity task gives a fairly direct insight into semantic similarity and compositional representations, it is somewhat limited in scope as it only considers two-word constructions rather than naturally occurring sentences.</S>\n    <S sid=\"148\" ssid=\"2\">Ideally, we would like to augment our evaluation with a task which is based on large quantities of natural data and for which vector composition has practical consequences.</S>\n    <S sid=\"149\" ssid=\"3\">For these reasons, we used the Microsoft Research Paraphrase Corpus (MSRPC) introduced by Dolan et al. (2004).</S>\n    <S sid=\"150\" ssid=\"4\">The corpus consists of sentence pairs Seni1,Seni2 and labels indicating whether they are in a paraphrase relationship or not.</S>\n    <S sid=\"151\" ssid=\"5\">The vector representations obtained from our various compositional models were used as features for the paraphrase classification task.</S>\n    <S sid=\"152\" ssid=\"6\">The MSRPC dataset contains 5,801 sentence pairs, we used the standard split of 4,076 training pairs (67.5% of which are paraphrases) and 1,725 test pairs (66.5% of which are paraphrases).</S>\n    <S sid=\"153\" ssid=\"7\">In order to judge whether two sentences have the same meaning we employ Fan et al. (2008)’s liblinear classifier.</S>\n    <S sid=\"154\" ssid=\"8\">For each of our three vector sources and three different compositional methods, we create the following features: (a) a vector representing the pair of input sentences either via concatenation (“con”) or subtraction (“sub”); (b) a vector encoding which words appear therein (“enc”); and (c) a vector made up of the following four other pieces of information: the cosine similarity of the sentence vectors, the length of Seni1, the length of Seni2, and the unigram overlap among the two sentences.</S>\n    <S sid=\"155\" ssid=\"9\">In order to encode which words appear in each sentence and how often, we define a vector wdCounti for sentence Seni and enumerate all words occuring in the MSRPC: giving the word count vectors nMSRPC dimensions.</S>\n    <S sid=\"156\" ssid=\"10\">Thus the k-th component of wdCounti is the frequency with which the word w(MSRPC) appears in for k = 1,...,nMSRPC.</S>\n    <S sid=\"157\" ssid=\"11\">Even though nMSRPC may be large, the computer files storing our feature vectors do not explode in size because wdCount contains many zeros and the classifier allows a sparse notation of (non-zero) feature values.</S>\n    <S sid=\"158\" ssid=\"12\">Regarding the last four features, we measured the similarity between sentences the same way as we did with phrases in section 3.</S>\n    <S sid=\"159\" ssid=\"13\">Note that this is the cosine of the angle between senVeci1 and senVeci2.</S>\n    <S sid=\"160\" ssid=\"14\">This enables us to observe the similarity or dissimilarity of two sentences independent of their sentence length.</S>\n    <S sid=\"161\" ssid=\"15\">Even though each contained word increases or decreases the norm of the resulting sentence vector, this does not distort the overall similarity value, due to normalization.</S>\n    <S sid=\"162\" ssid=\"16\">The lengths of Seni1 and Seni2 are simply the number of words they contain.</S>\n    <S sid=\"163\" ssid=\"17\">The unigram overlap feature value may be viewed as the cardinality of the intersection of each sentence’s multisetbag-of-words.</S>\n    <S sid=\"164\" ssid=\"18\">The latter is encoded in the alreadyintroduced wdCount vectors.</S>\n    <S sid=\"165\" ssid=\"19\">Therefore, In order to establish which features work best for each representation and composition method, we exhaustively explored all combinations on a development set (20% of the original MSRPC training set).</S>\n    <S sid=\"166\" ssid=\"20\">Tables 4 (accuracy) and 5 (F1) show our results on the test set with the best feature combinations for each model (shown in parentheses).</S>\n    <S sid=\"167\" ssid=\"21\">Each row corresponds to a different type of composition and each column to a different word representation model.</S>\n    <S sid=\"168\" ssid=\"22\">As can be seen, the distributional memory (DM) is the best performing representation for the additive composition model.</S>\n    <S sid=\"169\" ssid=\"23\">The neural language model (NLM) gives best results for the recursive autoencoder (RAE), although the other two representations come close.</S>\n    <S sid=\"170\" ssid=\"24\">And finally the simple distributional semantic space (SDS) works best with multiplication.</S>\n    <S sid=\"171\" ssid=\"25\">Also note that the best performing models, namely DM with addition and SDS with multiplication, use a basic feature space consisting only of the cosine similarity of the composed sentence vectors, the length of the two sentences involved, and their unigram word overlap.</S>\n    <S sid=\"172\" ssid=\"26\">Although our intention was to use the paraphrase detection task as a test-bed for evaluating compositional models rather than achieving state-of-the-art results, Table 6 compares our approach against previous work on the same task and dataset.</S>\n    <S sid=\"173\" ssid=\"27\">Initial research concentrated on individual words rather than sentential representations.</S>\n    <S sid=\"174\" ssid=\"28\">Several approaches used WordNet in conjunction with distributional similarity in an attempt to detect meaning conveyed by synonymous words (Islam and Inkpen, 2007; Mihalcea et al., 2006; Fernando and Stevenson, 2008).</S>\n    <S sid=\"175\" ssid=\"29\">More recently, the addition of syntactic features based on dependency parse trees (Wan et al., 2006; Das and Smith, 2009) has been shown to substantially boost performance.</S>\n    <S sid=\"176\" ssid=\"30\">The model of Das and Smith (2009), for example, uses quasi-synchronous dependency grammar to model the structure of the sentences involved in the comparison and their correspondences.</S>\n    <S sid=\"177\" ssid=\"31\">Socher et al. (2011a) obtain an accuracy that is higher than previously published results.</S>\n    <S sid=\"178\" ssid=\"32\">This model is more sophisticated than the one we used in our experiments (see Table 4 and 5).</S>\n    <S sid=\"179\" ssid=\"33\">Rather than using the output of the RAE as features for the classifier, it applies dynamic pooling, a procedure that takes a similarity matrix as input (e.g., created by sentences with differing lengths) and maps it to a matrix of fixed size that represents more faithfully the global similarity structure.3 Overall, we observe that our own models do as well as some of the models that employ WordNet and more sophisticated syntactic features.</S>\n    <S sid=\"180\" ssid=\"34\">With regard to F1, we are comparable with Das and Smith (2009) and Socher et al. (2011a) without using elaborate features, or any additional manipulations over and above the output of the composition functions 3Without dynamic pooling, their model yields an accuracy of 74.2. which if added could increase performance.</S>\n  </SECTION>\n  <SECTION title=\"5 Discussion\" number=\"5\">\n    <S sid=\"181\" ssid=\"1\">In this paper we systematically compared three types of distributional representation and their effect on semantic composition.</S>\n    <S sid=\"182\" ssid=\"2\">Our comparisons involved a simple distributional semantic space (Mitchell and Lapata, 2010), word embeddings computed with a neural language model (Collobert and Weston, 2008) and a representation based on weighted word-link-word tuples arranged into a third-order tensor (Baroni and Lenci, 2010).</S>\n    <S sid=\"183\" ssid=\"3\">These representations vary in many respects: the amount of preprocessing and linguistic information involved (the third-order tensor computes semantic representations over parsed corpora), whether the semantic space is the by-product of a learning process (in the neural language model the parameters of the lookup table must be learned), and data requirements (the third-order tensor involves processing billions of words).</S>\n    <S sid=\"184\" ssid=\"4\">These representations served as input to three composition methods involving addition, multiplication and a deep recursive autoencoder.</S>\n    <S sid=\"185\" ssid=\"5\">Again these methods differ in terms of how they implement compositionality: addition and multiplication are commutative and associative operations and thus ignore word order and, more generally, syntactic structure.</S>\n    <S sid=\"186\" ssid=\"6\">In contrast, the recursive autoencoder is syntax-aware as it operates over a parse tree.</S>\n    <S sid=\"187\" ssid=\"7\">However, the composed representations must be learned with a neural network.</S>\n    <S sid=\"188\" ssid=\"8\">We evaluated nine models on the complementary tasks of phrase similarity and paraphrase detection.</S>\n    <S sid=\"189\" ssid=\"9\">The former task simplifies the challenge of finding an adequate method of composition and places more emphasis on the representation, whereas the latter poses, in a sense, the ultimate challenge for composition models.</S>\n    <S sid=\"190\" ssid=\"10\">It involves entire sentences exhibiting varied syntactic constructions and in the limit involves genuine natural language undertanding.</S>\n    <S sid=\"191\" ssid=\"11\">Across both tasks our results deliver a consistent message: simple is best.</S>\n    <S sid=\"192\" ssid=\"12\">Despite being in theory more expressive, the representations obtained by the neural language model and the third-order tensor cannot match the simple semantic space on the phrase similarity task.</S>\n    <S sid=\"193\" ssid=\"13\">In this task syntax-oblivious composition models are superior to the more sophisticated recursive autoencoder.</S>\n    <S sid=\"194\" ssid=\"14\">The latter performs better on the paraphrase detection task when its output is fed to a classifier.</S>\n    <S sid=\"195\" ssid=\"15\">The simple semantic space may not take word order or sentence structure into account, but nevertheless achieves considerable semantic expressivity: it is on par with the third-order tensor without having access to as much data (3 billion words) or a syntactically parsed corpus.</S>\n    <S sid=\"196\" ssid=\"16\">What do these findings tell us about the future of compositional models for distributional semantics?</S>\n    <S sid=\"197\" ssid=\"17\">The problem of finding the right methods of vector composition cannot be pursued independent of the choice of lexical representation.</S>\n    <S sid=\"198\" ssid=\"18\">Having tested many model combinations, we argue that in a good model of distributive semantics representation and composition must go hand in hand, i.e., they must be mutually learned.</S>\n    <S sid=\"199\" ssid=\"19\">Acknowledgments We are grateful to Jeff Mitchell for his help with the re-implementation of his models.</S>\n    <S sid=\"200\" ssid=\"20\">Thanks to Frank Keller and Micha Elsner for their input on earlier versions of this work and to Richard Socher for technical assistance.</S>\n    <S sid=\"201\" ssid=\"21\">We acknowledge the support of EPSRC through project grant EP/I032916/1.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "f3ff7456109af6f325d2dc1762d3f673",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "A Comparison of Vector-based Representations for Semantic Composition"
                }
            }
        },
        {
            "index": 14,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "In this paper we explore the computational modelling of compositionality in distributional models of semantics. In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens. We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model. We propose two evaluation methods for the implemented models. Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.",
                    "filename": "A_Regression_Model_of_Adjective-Noun_Compositionality_in_Distributional_Semantics",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W10-2805/Documents_xml/W10-2805.xml",
                    "title": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a regression model for the compositionality of adjectives and nouns in distributional semantics. The authors propose a method using Partial Least Squares Regression (PLSR) to model the composition of word meanings based on independent vectors in a multidimensional space. The study compares the performance of this method with simple additive and multiplicative models. The evaluation results show that the PLSR model outperforms the other two models in approximating the composed meaning of adjective-noun pairs. The paper concludes that PLSR is a promising approach for modelling compositionality in distributional models of semantics. Future work will focus on developing better evaluation methods and exploring the role of lexical association in prediction models.",
                        "This paper presents a novel method using Partial Least Squares Regression (PLSR) to model the compositionality of meaning in distributional models of semantics, specifically focusing on adjacent Adjective-Noun pairs. The study evaluates the performance of the additive model, multiplicative model, and PLSR using different evaluation methods, including comparison to a gold standard. The results show that PLSR outperforms the other models in approximation of compositional vectors. Despite some mixed results in the evaluation, the paper concludes that PLSR is the model that produces the most consistent results in this experimental stage. The study also highlights the need for better evaluation methods and potential future developments, such as exploring other techniques and addressing the role of lexical association in the prediction models.",
                        "This paper presents a novel method for modeling the compositionality of meaning in distributional models of semantics, focusing on adjacent Adjective-Noun pairs. The method, Partial Least Squares Regression (PLSR), is applied and compared with simple additive and multiplicative models. The study evaluates the performance of these models using two different procedures based on the Euclidean measure of geometric distance. The results show that PLSR outperformed the other models in the first proposed evaluation method, while the simple additive model performed best in the second evaluation test. Overall, PLSR shows the most consistent results and is considered the most promising model. The paper concludes with future directions for development and considerations for linguistic phenomena beyond the scope of this study.",
                        "This paper presents a novel method for modeling the compositionality of meaning in distributional models of semantics, focusing on adjacent Adjective-Noun pairs. The method, Partial Least Squares Regression (PLSR), is applied and compared with simple additive and multiplicative models. The study evaluates the performance of each model using different procedures based on the Euclidean measure of geometric distance. Results indicate that PLSR outperforms the other models in approximating the composed meaning of Adjective-Noun pairs. However, the evaluation based on comparison to a gold standard yields mixed results, with the simple additive model performing best. The study concludes that PLSR shows the most consistent results at this experimental stage, but further research is needed to develop better evaluation methods and extend the experiments to other techniques."
                    ],
                    "filename": "a_regression_model_of_adjectiven-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 121 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Word-space vector models or distributional models of semantics (henceforth DSMs), are computational models that build contextual semantic representations for lexical items from corpus data.</S>\n    <S sid=\"7\" ssid=\"2\">DSMs have been successfully used in the recent years for a number of different computational tasks involving semantic relations between words (e.g. synonym identification, computation of semantic similarity, modelling selectional preferences, etc., for a thorough discussion of the field, cf.</S>\n    <S sid=\"8\" ssid=\"3\">Sahlgren, 2006).</S>\n    <S sid=\"9\" ssid=\"4\">The theoretical foundation of DSMs is to be found in the “distributional hypothesis of meaning”, attributed to Z. Harris, which maintains that meaning is susceptible to distributional analysis and, in particular, that differences in meaning between words or morphemes in a language correlate with differences in their distribution (Harris 1970, pp.</S>\n    <S sid=\"10\" ssid=\"5\">784–787).</S>\n    <S sid=\"11\" ssid=\"6\">While the vector-based representation of word meaning has been used for a long time in computational linguistics, the techniques that are currently used have not seen much development with regards to one of the main aspects of semantics in natural language: compositionality.</S>\n    <S sid=\"12\" ssid=\"7\">To be fair, the study of semantic compositionality in DSMs has seen a slight revival in the recent times, cf.</S>\n    <S sid=\"13\" ssid=\"8\">Widdows (2008), Mitchell &amp; Lapata (2008), Giesbrecht (2009), Baroni &amp; Lenci (2009), who propose various DSM approaches to represent argument structure, subject-verb and verb-object co-selection.</S>\n    <S sid=\"14\" ssid=\"9\">Current approaches to compositionality in DSMs are based on the application of a simple geometric operation on the basis of individual vectors (vector addition, pointwisemultiplication of corresponding dimensions, tensor product) which should in principle approximate the composition of any two given vectors.</S>\n    <S sid=\"15\" ssid=\"10\">On the contrary, since the the very nature of compositionality depends on the semantic relation being instantiated in a syntactic structure, we propose that the composition of vector representations must be modelled as a relation-specific phenomenon.</S>\n    <S sid=\"16\" ssid=\"11\">In particular, we propose that the usual procedures from machine learning tasks must be implemented also in the search for semantic compositionality in DSM.</S>\n    <S sid=\"17\" ssid=\"12\">In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.</S>\n    <S sid=\"18\" ssid=\"13\">We extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and, finally, a multinomial multiple regression model by Partial Least Squares Regression (PLSR).</S>\n  </SECTION>\n  <SECTION title=\"2 Compositionality of meaning in DSMs\" number=\"2\">\n    <S sid=\"19\" ssid=\"1\">Previous work in the field has produced a small number of operations to represent the composition of vectorial representations of word meaning.</S>\n    <S sid=\"20\" ssid=\"2\">In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: In the DSM literature, the additive model has become a de facto standard approach to approximate the composed meaning of a group of words (or a document) as the sum of their vectors (which results in the centroid of the starting vectors).</S>\n    <S sid=\"21\" ssid=\"3\">This has been successfully applied to document-based applications such as the computation of document similarity in information retrieval.</S>\n    <S sid=\"22\" ssid=\"4\">Mitchell &amp; Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).</S>\n    <S sid=\"23\" ssid=\"5\">Widdows (2008) Obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model.</S>\n    <S sid=\"24\" ssid=\"6\">For the sake of simplifying the implementation of evaluation methods, in this paper we will compare the first two approaches, vector addition and vector pointwise-multiplication, with regression modelling by partial least squares.</S>\n  </SECTION>\n  <SECTION title=\"3 Partial least squares regression of compositionality\" number=\"3\">\n    <S sid=\"25\" ssid=\"1\">We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.</S>\n    <S sid=\"26\" ssid=\"2\">This is, in principle, a tractable problem that can be solved by standard machine learning techniques such as multilayer perceptrons or support vector machines.</S>\n    <S sid=\"27\" ssid=\"3\">However, given that sequences of words tend to be of very low frequency (and thus difficult to represent in a DSM), suitable data sets will inevitably suffer the curse of dimensionality: we will often have many more variables (dimensions) than observations.</S>\n    <S sid=\"28\" ssid=\"4\">Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.</S>\n    <S sid=\"29\" ssid=\"5\">PLSR is widely used in in unrelated fields such as spectroscopy, medical chemistry, brain-imaging and marketing (Mevik &amp; Wehrens, 2007).</S>\n  </SECTION>\n  <SECTION title=\"4 Materials and tools\" number=\"4\">\n    <S sid=\"30\" ssid=\"1\">We use a general-purpose vector space extracted from the British National Corpus.</S>\n    <S sid=\"31\" ssid=\"2\">We used the Infomap software to collect co-occurrence statistics for lemmas within a rectangular 5L–5R window.</S>\n    <S sid=\"32\" ssid=\"3\">The corpus was pre-processed to represent frequent Adjective-Noun lemma pairs as a single token (e.g. while in the original corpus the A-N phrase nice house consists in two separate lemmas (nice and house), in the processed corpus it appears as a single entry nice_house).</S>\n    <S sid=\"33\" ssid=\"4\">The corpus was also processed by stop-word removal.</S>\n    <S sid=\"34\" ssid=\"5\">We extracted a list of A-N candidate pairs with simple regex-based queries targeting adjacent sequences composed of [Det/Art–A–N] (e.g. that little house).</S>\n    <S sid=\"35\" ssid=\"6\">We filtered the candidate list by frequency (&gt; 400) obtaining 1,380 different A-N pairs.</S>\n    <S sid=\"36\" ssid=\"7\">The vector space was built with the 40,000 most frequent tokens in the corpus (a cut-off point that included all the extracted A-N pairs).</S>\n    <S sid=\"37\" ssid=\"8\">The original dimensions were the 3,000 most frequent content words in the BNC.</S>\n    <S sid=\"38\" ssid=\"9\">The vector space was reduced to the first 500 “latent” dimensions by SVD as implemented by the Infomap software.</S>\n    <S sid=\"39\" ssid=\"10\">Thus, the resulting space consists in a matrix with 40, 000 x 500 dimensions.</S>\n    <S sid=\"40\" ssid=\"11\">We then extracted the vector representation for each A-N candidate as well as for each independent constituent, e.g. vectors for nice_house (v3), as well as for nice (v1) and house (v2) were saved.</S>\n    <S sid=\"41\" ssid=\"12\">The resulting vector subspace was imported into the R statistical computing environment for the subsequent model building and evaluation.</S>\n    <S sid=\"42\" ssid=\"13\">In particular, we produced our regression analysis with the pls package (Mevik &amp; Wehrens, 2007), which implements PLSR and a number of very useful functions for cross-validation, prediction, error analysis, etc.</S>\n    <S sid=\"43\" ssid=\"14\">By simply combining the vector representations of the independent Adjectives and Nouns in our data-set (v1 and v2) we built an additive prediction model (v1 + v2) and a simplified pointwise multiplicative prediction model (v1 x v2) for each candidate pair.</S>\n    <S sid=\"44\" ssid=\"15\">We also fitted a PLSR model using v1 and v2 as predictors and the corresponding observed pair v3 as dependent variable.</S>\n    <S sid=\"45\" ssid=\"16\">The data were divided into a training set (1,000 A-N pairs) and a testing set (the remaining 380 A-N pairs).</S>\n    <S sid=\"46\" ssid=\"17\">The model’s parameters were estimated by performing 10-fold cross-validation during the training phase.</S>\n    <S sid=\"47\" ssid=\"18\">In what follows we briefly evaluate the three resulting models of compositionality.</S>\n  </SECTION>\n  <SECTION title=\"5 Evaluation\" number=\"5\">\n    <S sid=\"48\" ssid=\"1\">In order to evaluate the three models of compositionality that were built, we devised two different procedures based on the Euclidean measure of geometric distance.</S>\n    <S sid=\"49\" ssid=\"2\">The first method draws a direct comparison of the different predicted vectors for each candidate A-N pair by computing the Euclidean distance between the observed vector and the modelled predictions.</S>\n    <S sid=\"50\" ssid=\"3\">We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.</S>\n    <S sid=\"51\" ssid=\"4\">We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.</S>\n    <S sid=\"52\" ssid=\"5\">The idea here is that the best models should produce predictions that are as close as possible to the originally observed A-N vector.</S>\n    <S sid=\"53\" ssid=\"6\">Our second evaluation method uses the 10 nearest neighbours of each of the observed A-N pairs in the test set as gold-standard (excluding any modelled predictions), and compares them with the 10 nearest neighbours of each of the corresponding predictions as generated by the models.</S>\n    <S sid=\"54\" ssid=\"7\">The aim is to assess if the predictions made by each model share any top-10 neighbours with their corresponding gold-standard.</S>\n    <S sid=\"55\" ssid=\"8\">We award 1 point for every shared neighbour.</S>\n    <S sid=\"56\" ssid=\"9\">We calculated the Euclidean distance between each observed A-N pair and the corresponding prediction made by each model.</S>\n    <S sid=\"57\" ssid=\"10\">On general inspection, it is clear that the approximation of A-N compositional vectors made by PLSR is considerably closer than those produced by the additive and multiplicative models, cf.</S>\n    <S sid=\"58\" ssid=\"11\">Table 1.</S>\n    <S sid=\"59\" ssid=\"12\">We also computed in detail which of the three predicted composed vectors was closest to the corresponding observation.</S>\n    <S sid=\"60\" ssid=\"13\">To this effect we extracted the 10 nearest neighbours for each A-N pair in the test set using the whole compositionality subspace (all the predicted and the original vectors).</S>\n    <S sid=\"61\" ssid=\"14\">In 94 cases out of 380, the PLSR intended prediction was the nearest neighbour.</S>\n    <S sid=\"62\" ssid=\"15\">Cumulatively, PLSR’s predictions were in the top-10 nearest neighbour list in 219 out of 380 cases (57.6%).</S>\n    <S sid=\"63\" ssid=\"16\">The other models’ performance in this test was negligible, cf.</S>\n    <S sid=\"64\" ssid=\"17\">Table 2.</S>\n    <S sid=\"65\" ssid=\"18\">Overall, 223 items in the test set had at least one predicted vector in the top-10 list; of these, 219 (98%) were generated by PLSR and the remaining 4 (1%) by the multiplicative model.</S>\n    <S sid=\"66\" ssid=\"19\">Since the main use of DSMs is to extract similar vectors from a multidimensional space (representing related documents, distributional synonyms, etc.</S>\n    <S sid=\"67\" ssid=\"20\">), we would like to test if the modelling of semantic compositionality is able to produce predictions that are as similar as possible to the originally observed data.</S>\n    <S sid=\"68\" ssid=\"21\">A very desirable result would be if any predicted compositional A-N vector could be reliably used instead of the extracted bigram.</S>\n    <S sid=\"69\" ssid=\"22\">This could only be achieved if a model’s predictions show a similar distributional behaviour with respect to the observed vector.</S>\n    <S sid=\"70\" ssid=\"23\">To test this idea using our data, we took the 10 nearest neighbours of each of the observed AN pairs in the test set as gold standard.</S>\n    <S sid=\"71\" ssid=\"24\">These gold neighbours were extracted from the observation testing subspace, thus excluding any modelled predictions.</S>\n    <S sid=\"72\" ssid=\"25\">This is a very restrictive setting: it means that the gold standard for each of the 380 test items is composed of the 10 nearest neighbours from the same 380 items (which may turn out to be not very close at all).</S>\n    <S sid=\"73\" ssid=\"26\">We then extracted the 10 nearest neighbours for each of the three modelled predictions, but this time the subspace included all predictions, as well as all the original observations (380 x 4 = 1520 items).</S>\n    <S sid=\"74\" ssid=\"27\">Finally, we tested if the predictions made by each model shared any top-10 neighbours with their corresponding gold-standard.</S>\n    <S sid=\"75\" ssid=\"28\">We awarded 1 point for every shared neighbour.</S>\n    <S sid=\"76\" ssid=\"29\">The results obtained with these evaluation settings were very poor.</S>\n    <S sid=\"77\" ssid=\"30\">Only the additive model scored points (48), although the performance was rather disappointing (maximum potential score for the test was 3,800 points).</S>\n    <S sid=\"78\" ssid=\"31\">Both the pointwise multiplicative model and the PLSR model failed to retrieve any of the gold standard neighbours.</S>\n    <S sid=\"79\" ssid=\"32\">This poor results can be attributed to the very restrictive nature of our gold standard and, also, to the asymmetrical composition of the compared data (gold standard: 3,800 neighbours from a pool of just 380 different items; prediction space: 11,400 neighbours from a pool of 1,520 items).</S>\n    <S sid=\"80\" ssid=\"33\">However, given the that DSMs are known for their ability to extract similar items from the same space, we decided to relax our test settings by awarding points not only to shared neighbours, but also to the same model’s predictions of those neighbours.</S>\n    <S sid=\"81\" ssid=\"34\">Thus, given a target neighbour such as good_deal, in our second setting we awarded points not only to the gold standard good_deal, but also to the predictions good_deal_ADD, good_deal_MUL and good_deal_PLSR when evaluating each corresponding model.</S>\n    <S sid=\"82\" ssid=\"35\">With these settings the compared spaces become less asymmetrical (gold standard: 7,600 neighbours from a pool of just 380 different items plus predictions; prediction space: 11,400 neighbours from a pool of 1,520 items).</S>\n    <S sid=\"83\" ssid=\"36\">The obtained results show a great improvement (max. potential score 7,600 points): Once again, the additive model showed the best performance, followed by PLSR.</S>\n    <S sid=\"84\" ssid=\"37\">The multiplicative model’s performance was negligible.</S>\n    <S sid=\"85\" ssid=\"38\">While carrying out these experiments, an unexpected fact became evident.</S>\n    <S sid=\"86\" ssid=\"39\">Each of the models in turn produces predictions that are relatively close to each other, regardless of the independent words that were used to calculate the compositional vectors.</S>\n    <S sid=\"87\" ssid=\"40\">This has the consequence that the nearest neighbour lists for each model’s predictions are, by and large, populated by items generated in the same model, as shown in Table 4.</S>\n    <S sid=\"88\" ssid=\"41\">Neighbours of predictions from the multiplicative model are all multiplicative.</S>\n    <S sid=\"89\" ssid=\"42\">The additive model has the most varied set of neighbours, but the majority of them are additive-neighbours.</S>\n    <S sid=\"90\" ssid=\"43\">PLSR shows a mixed behaviour.</S>\n    <S sid=\"91\" ssid=\"44\">However, PLSR produced neighbours that find their way into the neighbour sets of both the additive model and the observations.</S>\n    <S sid=\"92\" ssid=\"45\">These remarks point in the same direction: every model is a simplified and specialised version of the original space, somewhat more orderly than the observed data, and may give different results depending on the task at stake.</S>\n    <S sid=\"93\" ssid=\"46\">PLSR (and to a lesser extent also the multiplicative model) is particularly efficient as generator of neighbours for real vectors, a characteristic that could be applied to guess distributional synonyms of unseen A-N pairs.</S>\n    <S sid=\"94\" ssid=\"47\">On the other hand, the additive model (and to a lesser extent PLSR) is especially successful in attracting gold standard neighbours.</S>\n    <S sid=\"95\" ssid=\"48\">Overall, even at this experimental stage, PLSR is clearly the model that produces the most consistent results.</S>\n  </SECTION>\n  <SECTION title=\"6 Concluding remarks\" number=\"6\">\n    <S sid=\"96\" ssid=\"1\">This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.</S>\n    <S sid=\"97\" ssid=\"2\">The method, Partial Least Squares Regression, is well known in other dataintensive fields of research, but to our knowledge had never been put to work in computational distributional semantics.</S>\n    <S sid=\"98\" ssid=\"3\">Its main advantage is the fact that it is designed to approximate functions in problems of multivariate multiple regression where the number of observations is relatively small if compared to the number of variables (dimensions).</S>\n    <S sid=\"99\" ssid=\"4\">We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.</S>\n    <S sid=\"100\" ssid=\"5\">The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.</S>\n    <S sid=\"101\" ssid=\"6\">Our second evaluation test (using comparison to a gold standard) gave mixed results: the best performance was obtained by the simple additive model, with PLSR coming in second place.</S>\n    <S sid=\"102\" ssid=\"7\">This is work in progress, but the results look very promising.</S>\n    <S sid=\"103\" ssid=\"8\">Future developments will certainly focus on the creation of better evaluation methods, as well as on extending the experiments to other techniques (e.g. convolution product as discussed by Widdows, 2008 and Giesbrecht, 2009).</S>\n    <S sid=\"104\" ssid=\"9\">Another important issue that we still have not touched is the role played by lexical association (collocations) in the prediction models.</S>\n    <S sid=\"105\" ssid=\"10\">We would like to make sure that we are not modelling the compositionality of noncompositional examples.</S>\n    <S sid=\"106\" ssid=\"11\">A last word on the view of semantic compositionality suggested by our approach.</S>\n    <S sid=\"107\" ssid=\"12\">Modelling compositionality as a machine learning task implies that a great number of different “types” of composition (functions combining vectors) may be learned from natural language samples.</S>\n    <S sid=\"108\" ssid=\"13\">In principle, any semantic relation instantiated by any syntactic structure could be learned if sufficient data is provided.</S>\n    <S sid=\"109\" ssid=\"14\">This approach must be confronted with other linguistic phenomena, also of greater complexity than just a set of bigrams.</S>\n    <S sid=\"110\" ssid=\"15\">Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgements\" number=\"7\">\n    <S sid=\"111\" ssid=\"1\">Thanks are due to Marco Baroni, Stefan Evert, Roberto Zamparelli and the three anonymous reviewers for their assistance and helpful comments.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "45cb7ae167b4c102957a16dd12a61910",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics"
                }
            }
        },
        {
            "index": 15,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.",
                    "filename": "A_Composite_Kernel_To_Extract_Relations_Between_Entities_With_Both_Flat_And_Structured_Features",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P06-1104/Documents_xml/P06-1104.xml",
                    "title": "A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces a novel composite kernel for relation extraction, combining flat and structured features using kernel methods. The composite kernel, consisting of an entity kernel and a convolution parse tree kernel, is shown to be highly effective in capturing diverse knowledge for relation extraction. The paper presents experiments on the ACE corpus, comparing the performance of the composite kernel with previous methods and demonstrating its superiority. The discussion highlights the advantages of the composite kernel over feature-based and previous kernel methods. The paper concludes by suggesting future work to further improve relation detection accuracy and address data imbalance and sparseness issues. Overall, the study demonstrates the potential of kernel methods in relation extraction and presents a promising direction for future research.",
                        "This academic paper introduces a composite kernel for relation extraction, which effectively combines flat entity features and structured syntactic features. The composite kernel is shown to outperform previous best-reported feature-based methods on the ACE corpus. The research demonstrates that without extensive feature engineering, an individual tree kernel achieves comparable performance with feature-based methods. The paper highlights the importance of relation instance representation and suggests future work to improve relation detection and address data imbalance and sparseness issues. Overall, the composite kernel is proven to be effective for relation extraction, and the study provides insights into the potential of kernel methods for capturing diverse knowledge.",
                        "This paper introduces a composite kernel for relation extraction, combining flat and structured features. It presents a novel approach to relation extraction using kernel methods, demonstrating the effectiveness of the composite kernel. The paper compares the method with previous work, showing significant performance improvements. The composite kernel consists of an entity kernel and a convolution parse tree kernel, effectively capturing diverse knowledge for relation extraction. The paper also discusses experiments and observations, highlighting the potential for further improvements. Overall, the composite kernel shows promise for relation extraction, outperforming previous methods and providing a foundation for future work in the field. The paper concludes by outlining potential extensions and acknowledging contributions from others.",
                        "This paper proposes a composite kernel for relation extraction that combines flat entity features and structured syntactic features to outperform previous methods on the ACE corpus. The composite kernel is composed of an entity kernel and a convolution parse tree kernel. The study demonstrates that the composite kernel effectively captures diverse knowledge for relation extraction without the need for extensive feature engineering. The paper also discusses the effectiveness of the composite kernel in relation to previous methods and the potential for future work, including improving relation detection through the inclusion of more individual kernels and addressing data imbalance and sparseness issues. Overall, the paper provides valuable insights and contributions to relation extraction in natural language processing."
                    ],
                    "filename": "a_composite_kernel_to_extract_re-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 117 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text.</S>\n    <S sid=\"7\" ssid=\"2\">The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 19871998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005).</S>\n    <S sid=\"8\" ssid=\"3\">According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities.</S>\n    <S sid=\"9\" ssid=\"4\">For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION.</S>\n    <S sid=\"10\" ssid=\"5\">Commercial).</S>\n    <S sid=\"11\" ssid=\"6\">In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola, 2001).</S>\n    <S sid=\"12\" ssid=\"7\">Many feature-based learning algorithms involve only the dot-product between feature vectors.</S>\n    <S sid=\"13\" ssid=\"8\">Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects.</S>\n    <S sid=\"14\" ssid=\"9\">A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.</S>\n    <S sid=\"15\" ssid=\"10\">Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects.</S>\n    <S sid=\"16\" ssid=\"11\">For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP.</S>\n    <S sid=\"17\" ssid=\"12\">In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005).</S>\n    <S sid=\"18\" ssid=\"13\">This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.</S>\n    <S sid=\"19\" ssid=\"14\">The composite kernel consists of an entity kernel and a convolution parse tree kernel.</S>\n    <S sid=\"20\" ssid=\"15\">Our study demonstrates that the composite kernel is very effective for relation extraction.</S>\n    <S sid=\"21\" ssid=\"16\">It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively.</S>\n    <S sid=\"22\" ssid=\"17\">An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels.</S>\n    <S sid=\"23\" ssid=\"18\">Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.</S>\n    <S sid=\"24\" ssid=\"19\">The rest of the paper is organized as follows.</S>\n    <S sid=\"25\" ssid=\"20\">In Section 2, we review the previous work.</S>\n    <S sid=\"26\" ssid=\"21\">Section 3 discusses our composite kernel.</S>\n    <S sid=\"27\" ssid=\"22\">Section 4 reports the experimental results and our observations.</S>\n    <S sid=\"28\" ssid=\"23\">Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure by Haussler (1999) in the machine learning field.</S>\n    <S sid=\"29\" ssid=\"24\">This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. previous work from the viewpoint of feature exploration.</S>\n    <S sid=\"30\" ssid=\"25\">We conclude our work and indicate the future work in Section 6.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"31\" ssid=\"1\">Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.</S>\n    <S sid=\"32\" ssid=\"2\">Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns.</S>\n    <S sid=\"33\" ssid=\"3\">Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model.</S>\n    <S sid=\"34\" ssid=\"4\">Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.</S>\n    <S sid=\"35\" ssid=\"5\">These methods are very effective for relation extraction and show the bestreported performance on the ACE corpus.</S>\n    <S sid=\"36\" ssid=\"6\">However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks.</S>\n    <S sid=\"37\" ssid=\"7\">Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features.</S>\n    <S sid=\"38\" ssid=\"8\">Zelenko et al. (2003) developed a kernel over parse trees for relation extraction.</S>\n    <S sid=\"39\" ssid=\"9\">The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner.</S>\n    <S sid=\"40\" ssid=\"10\">Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.</S>\n    <S sid=\"41\" ssid=\"11\">Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes.</S>\n    <S sid=\"42\" ssid=\"12\">The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus.</S>\n    <S sid=\"43\" ssid=\"13\">Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.</S>\n    <S sid=\"44\" ssid=\"14\">Their kernel simply counts the number of common word classes at each position in the shortest paths between two entities in dependency trees.</S>\n    <S sid=\"45\" ssid=\"15\">The kernel requires the two paths to have the same length; otherwise the kernel value is zero.</S>\n    <S sid=\"46\" ssid=\"16\">Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus.</S>\n    <S sid=\"47\" ssid=\"17\">The above discussion shows that, although kernel methods can explore the huge amounts of implicit (structured) features, until now the feature-based methods enjoy more success.</S>\n    <S sid=\"48\" ssid=\"18\">One may ask: how can we make full use of the nice properties of kernel methods and define an effective kernel for relation extraction?</S>\n    <S sid=\"49\" ssid=\"19\">In this paper, we study how relation extraction can benefit from the elegant properties of kernel methods: 1) implicitly exploring (structured) features in a high dimensional space; and 2) the nice mathematical properties, for example, the sum, product, normalization and polynomial expansion of existing kernels is a valid kernel (Schölkopf and Smola, 2001).</S>\n    <S sid=\"50\" ssid=\"20\">We also demonstrate how our composite kernel effectively captures the diverse knowledge for relation extraction.</S>\n  </SECTION>\n  <SECTION title=\"3 Composite Kernel for Relation Extraction\" number=\"3\">\n    <S sid=\"51\" ssid=\"1\">In this section, we define the composite kernel and study the effective representation of a relation instance.</S>\n    <S sid=\"52\" ssid=\"2\">Our composite kernel consists of an entity kernel and a convolution parse tree kernel.</S>\n    <S sid=\"53\" ssid=\"3\">To our knowledge, convolution kernels have not been explored for relation extraction.</S>\n    <S sid=\"54\" ssid=\"4\">(1) Entity Kernel: The ACE 2003 data defines four entity features: entity headword, entity type and subtype (only for GPE), and mention type while the ACE 2004 data makes some modifications and introduces a new feature “LDC mention type”.</S>\n    <S sid=\"55\" ssid=\"5\">Our statistics on the ACE data reveals that the entity features impose a strong constraint on relation types.</S>\n    <S sid=\"56\" ssid=\"6\">Therefore, we design a linear kernel to explicitly capture such features: where R1 and R2 stands for two relation instances, Ei means the ith entity of a relation instance, and KE(•,•) is a simple kernel function over the features of entities: where fi represents the ith entity feature, and the function C(•,•) returns 1 if the two feature values are identical and 0 otherwise.</S>\n    <S sid=\"57\" ssid=\"7\">KE(•,•) returns the number of feature values in common of two entities.</S>\n    <S sid=\"58\" ssid=\"8\">(2) Convolution Parse Tree Kernel: A convolution kernel aims to capture structured information in terms of substructures.</S>\n    <S sid=\"59\" ssid=\"9\">Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling.</S>\n    <S sid=\"60\" ssid=\"10\">Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = (# subtree1(T), ..., # subtreei(T), ..., # subtreen(T) ) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vectorφ(T) .</S>\n    <S sid=\"61\" ssid=\"11\">To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly. where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and ( ) I subtree i n is a function that is 1 iff the subtreei occurs with root at node n and zero otherwise, and ∆(n1,n2) is the number of the common subtrees rooted at n1 and n2, i.e.</S>\n    <S sid=\"62\" ssid=\"12\">∆(n1, n2) = ∑i Isubtreei (n1 ) ⋅ Isubtreei (n2 ) ∆(n1 , n2) can be computed by the following recursive rules: where nc(n1) is the child number of n1, ch(n,j) is the jth child of node n andλ (0&lt;λ &lt;1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes.</S>\n    <S sid=\"63\" ssid=\"13\">In addition, the recursive rule (3) holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring.</S>\n    <S sid=\"64\" ssid=\"14\">The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two relation instances.</S>\n    <S sid=\"65\" ssid=\"15\">The time complexity for computing this kernel is O( |N1  |⋅  |N2|) .</S>\n    <S sid=\"66\" ssid=\"16\">In this paper, two composite kernels are defined by combing the above two individual kernels in the following ways: is the coefficient.</S>\n    <S sid=\"67\" ssid=\"17\">Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.4.</S>\n    <S sid=\"68\" ssid=\"18\">Here, Kˆ(•,•) is the normalizedK(•,•), Kp(•,•) is the polynomial expansion of K(•,•) with degree d=2, i.e.</S>\n    <S sid=\"69\" ssid=\"19\">Kp(•,•) = (K(•,•)+1)2 , and α is the coefficient.</S>\n    <S sid=\"70\" ssid=\"20\">Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.23.</S>\n    <S sid=\"71\" ssid=\"21\">The polynomial expansion aims to explore the entity bi-gram features, esp. the combined features from the first and second entities, respectively.</S>\n    <S sid=\"72\" ssid=\"22\">In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination.</S>\n    <S sid=\"73\" ssid=\"23\">This can avoid one kernel value being overwhelmed by that of another one.</S>\n    <S sid=\"74\" ssid=\"24\">The entity kernel formulated by eqn.</S>\n    <S sid=\"75\" ssid=\"25\">(1) is a proper kernel since it simply calculates the dot product of the entity feature vectors.</S>\n    <S sid=\"76\" ssid=\"26\">The tree kernel formulated by eqn.</S>\n    <S sid=\"77\" ssid=\"27\">(3) is proven to be a proper kernel (Collins and Duffy, 2001).</S>\n    <S sid=\"78\" ssid=\"28\">Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels.</S>\n    <S sid=\"79\" ssid=\"29\">A relation instance is encapsulated by a parse tree.</S>\n    <S sid=\"80\" ssid=\"30\">Thus, it is critical to understand which portion of a parse tree is important in the kernel calculation.</S>\n    <S sid=\"81\" ssid=\"31\">We study five cases as shown in Fig.1. mon sub-tree including the two entities.</S>\n    <S sid=\"82\" ssid=\"32\">In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).</S>\n    <S sid=\"83\" ssid=\"33\">Fig.</S>\n    <S sid=\"84\" ssid=\"34\">1 illustrates different representations of an example relation instance.</S>\n    <S sid=\"85\" ssid=\"35\">T1 is MCT for the relation instance, where the sub-tree circled by a dashed line is PT, which is also shown in T2 for clarity.</S>\n    <S sid=\"86\" ssid=\"36\">The only difference between MCT and PT lies in that MCT does not allow partial production rules (for example, NP4PP is a partial production rule while NP4NP+PP is an entire production rule in the top of T2).</S>\n    <S sid=\"87\" ssid=\"37\">For instance, only the most-right child in the most-left sub-tree [NP [CD 200] [JJ domestic] [E1-PER ...]] of T1 is kept in T2.</S>\n    <S sid=\"88\" ssid=\"38\">By comparing the performance of T1 and T2, we can evaluate the effect of sub-trees with partial production rules as shown in T2 and the necessity of keeping the whole left and right context sub-trees as shown in T1 in relation extraction.</S>\n    <S sid=\"89\" ssid=\"39\">T3 is CPT, where the two sub-trees circled by dashed lines are included as the context to T2 and make T3 context-sensitive.</S>\n    <S sid=\"90\" ssid=\"40\">This is to evaluate whether the limited context information in CPT can boost performance.</S>\n    <S sid=\"91\" ssid=\"41\">FPT in T4 is formed by removing the two circled nodes in T2.</S>\n    <S sid=\"92\" ssid=\"42\">This is to study whether and how the elimination of single non-terminal nodes affects the performance of relation extraction. fits to 200 domestic partners of their own workers in New York”, where the phrase type “E1-PER” denotes that the current node is the 1st entity with type “PERSON”, and likewise for the others.</S>\n    <S sid=\"93\" ssid=\"43\">The relation instance is excerpted from the ACE 2003 corpus, where a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and “workers” (PER).</S>\n    <S sid=\"94\" ssid=\"44\">We use Charniak’s parser (Charniak, 2001) to parse the example sentence.</S>\n    <S sid=\"95\" ssid=\"45\">To save space, the FCPT is not shown here.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiments\" number=\"4\">\n    <S sid=\"96\" ssid=\"1\">Data: We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments.</S>\n    <S sid=\"97\" ssid=\"2\">In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances.</S>\n    <S sid=\"98\" ssid=\"3\">The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes.</S>\n    <S sid=\"99\" ssid=\"4\">The ACE 2004 data contains 451 documents and 5702 relation instances.</S>\n    <S sid=\"100\" ssid=\"5\">It redefines 7 entity types, 7 major relation types and 23 subtypes.</S>\n    <S sid=\"101\" ssid=\"6\">Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.</S>\n    <S sid=\"102\" ssid=\"7\">Both corpora are parsed using Charniak’s parser (Charniak, 2001).</S>\n    <S sid=\"103\" ssid=\"8\">We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances.</S>\n    <S sid=\"104\" ssid=\"9\">In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators).</S>\n    <S sid=\"105\" ssid=\"10\">Implementation: We formalize relation extraction as a multi-class classification problem.</S>\n    <S sid=\"106\" ssid=\"11\">SVM is selected as our classifier.</S>\n    <S sid=\"107\" ssid=\"12\">We adopt the one vs. others strategy and select the one with the largest margin as the final answer.</S>\n    <S sid=\"108\" ssid=\"13\">The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)).</S>\n    <S sid=\"109\" ssid=\"14\">In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004).</S>\n    <S sid=\"110\" ssid=\"15\">Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance.</S>\n    <S sid=\"111\" ssid=\"16\">In this subsection, we report the experiments of different kernel setups for different purposes. parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (“E1PER” and so on as shown in Fig.</S>\n    <S sid=\"112\" ssid=\"17\">1) with “NP”.</S>\n    <S sid=\"113\" ssid=\"18\">Table 1 compares the performance of 5 tree kernel setups on the ACE 2003 data using the tree structure information only.</S>\n    <S sid=\"114\" ssid=\"19\">It shows that: ACE 2003 five major types using the parse tree structure information only (regardless of any entity-related information) kernel setups over the ACE major types of both the 2003 data (the numbers in parentheses) and the 2004 data (the numbers outside parentheses) the 2003 data although the ACE 2003 data is two times larger than the ACE 2004 data.</S>\n    <S sid=\"115\" ssid=\"20\">This may be due to two reasons: 1) The ACE 2004 data defines two new entity types and re-defines the relation types and subtypes in order to reduce the inconsistency between LDC annotators.</S>\n    <S sid=\"116\" ssid=\"21\">2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data.</S>\n    <S sid=\"117\" ssid=\"22\">The detailed classification in the 2004 data leads to significant performance improvement of 6.2 (54.4-48.2) in Fmeasure over that on the 2003 data.</S>\n    <S sid=\"118\" ssid=\"23\">Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003/2004 major types, respectively. compare our method with previous work on the ACE 2002/2003/2004 data, respectively.</S>\n    <S sid=\"119\" ssid=\"24\">They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4.</S>\n    <S sid=\"120\" ssid=\"25\">This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints.</S>\n    <S sid=\"121\" ssid=\"26\">That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version.</S>\n    <S sid=\"122\" ssid=\"27\">Culotta and Sorensen (2004) used a generic ACE corpus including about 800 documents (no corpus version is specified).</S>\n    <S sid=\"123\" ssid=\"28\">Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compare these methods exactly and fairly.</S>\n    <S sid=\"124\" ssid=\"29\">Therefore Table 3 is only for reference purpose.</S>\n    <S sid=\"125\" ssid=\"30\">We just hope that we can get a few clues from this table. not restricted by the two constraints of the two dependency kernels (identical layer and ancestors for the matchable nodes and identical length of two shortest paths, as discussed in Section 2).</S>\n    <S sid=\"126\" ssid=\"31\">The above experiments verify the effectiveness of our composite kernels for relation extraction.</S>\n    <S sid=\"127\" ssid=\"32\">They suggest that the parse tree kernel can effectively explore the syntactic features which are critical for relation extraction. both the 2003 and 2004 data for the composite kernel by polynomial expansion (4) Error Analysis: Table 5 reports the error distribution of the polynomial composite kernel over the major types on the ACE data.</S>\n    <S sid=\"128\" ssid=\"33\">It shows that 83.5%(198+115/198+115+62) / 85.8%(416 +171/416+171+96) of the errors result from relation detection and only 16.5%/14.2% of the errors result from relation characterization.</S>\n    <S sid=\"129\" ssid=\"34\">This may be due to data imbalance and sparseness issues since we find that the negative samples are 8 times more than the positive samples in the training set.</S>\n    <S sid=\"130\" ssid=\"35\">Nevertheless, it clearly directs our future work.</S>\n  </SECTION>\n  <SECTION title=\"5 Discussion\" number=\"5\">\n    <S sid=\"131\" ssid=\"1\">In this section, we compare our method with the previous work from the feature engineering viewpoint and report some other observations and issues in our experiments.</S>\n    <S sid=\"132\" ssid=\"2\">This is to explain more about why our method performs better and significantly outperforms the previous two dependency tree kernels from the theoretical viewpoint.</S>\n    <S sid=\"133\" ssid=\"3\">(1) Compared with Feature-based Methods: The basic difference lies in the relation instance representation (parse tree vs. feature vector) and the similarity calculation mechanism (kernel function vs. dot-product).</S>\n    <S sid=\"134\" ssid=\"4\">The main difference is the different feature spaces.</S>\n    <S sid=\"135\" ssid=\"5\">Regarding the parse tree features, our method implicitly represents a parse tree by a vector of integer counts of each sub-tree type, i.e., we consider the entire sub-tree types and their occurring frequencies.</S>\n    <S sid=\"136\" ssid=\"6\">In this way, the parse tree-related features (the path features and the chunking features) used in the featurebased methods are embedded (as a subset) in our feature space.</S>\n    <S sid=\"137\" ssid=\"7\">Moreover, the in-between word features and the entity-related features used in the feature-based methods are also captured by the tree kernel and the entity kernel, respectively.</S>\n    <S sid=\"138\" ssid=\"8\">Therefore our method has the potential of effectively capturing not only most of the previous flat features but also the useful syntactic structure features.</S>\n    <S sid=\"139\" ssid=\"9\">(2) Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004).</S>\n    <S sid=\"140\" ssid=\"10\">Moreover, the difference between our method and Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire subtrees.</S>\n    <S sid=\"141\" ssid=\"11\">However, the path does not maintain the tree structure information.</S>\n    <S sid=\"142\" ssid=\"12\">In addition, their kernel requires the two paths to have the same length.</S>\n    <S sid=\"143\" ssid=\"13\">Such constraint is too strict.</S>\n    <S sid=\"144\" ssid=\"14\">(1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers.</S>\n    <S sid=\"145\" ssid=\"15\">In this paper, the speed issue is solved in three ways.</S>\n    <S sid=\"146\" ssid=\"16\">First, the inclusion of the entity kernel makes the composite kernel converge fast.</S>\n    <S sid=\"147\" ssid=\"17\">Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance.</S>\n    <S sid=\"148\" ssid=\"18\">This significantly improves the speed.</S>\n    <S sid=\"149\" ssid=\"19\">Finally, the parse tree kernel requires exact match between two subtrees, which normally does not occur very frequently.</S>\n    <S sid=\"150\" ssid=\"20\">Collins and Duffy (2001) report that in practice, running time for the parse tree kernel is more close to linear (O(JN1J+JN2J), rather than O(JN1J*JN2J ).</S>\n    <S sid=\"151\" ssid=\"21\">As a result, using the PC with Intel P4 3.0G CPU and 2G RAM, our system only takes about 110 minutes and 30 minutes to do training on the ACE 2003 (~77k training instances) and 2004 (~33k training instances) data, respectively.</S>\n    <S sid=\"152\" ssid=\"22\">(2) Further Improvement: One of the potential problems in the parse tree kernel is that it carries out exact matches between sub-trees, so that this kernel fails to handle sparse phrases (i.e.</S>\n    <S sid=\"153\" ssid=\"23\">“a car” vs. “a red car”) and near-synonymic grammar tags (for example, the variations of a verb (i.e. go, went, gone)).</S>\n    <S sid=\"154\" ssid=\"24\">To some degree, it could possibly lead to over-fitting and compromise the performance.</S>\n    <S sid=\"155\" ssid=\"25\">However, the above issues can be handled by allowing grammar-driven partial rule matching and other approximate matching mechanisms in the parse tree kernel calculation.</S>\n    <S sid=\"156\" ssid=\"26\">Finally, it is worth noting that by introducing more individual kernels our method can easily scale to cover more features from a multitude of sources (e.g.</S>\n    <S sid=\"157\" ssid=\"27\">Wordnet, gazetteers, etc) that can be brought to bear on the task of relation extraction.</S>\n    <S sid=\"158\" ssid=\"28\">In addition, we can also easily implement the feature weighting scheme by adjusting the eqn.</S>\n    <S sid=\"159\" ssid=\"29\">(2) and the rule (2) in calculating ∆(n1,n2) (see subsection 3.1).</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion and Future Work\" number=\"6\">\n    <S sid=\"160\" ssid=\"1\">Kernel functions have nice properties.</S>\n    <S sid=\"161\" ssid=\"2\">In this paper, we have designed a composite kernel for relation extraction.</S>\n    <S sid=\"162\" ssid=\"3\">Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous bestreported feature-based methods on the ACE corpus.</S>\n    <S sid=\"163\" ssid=\"4\">To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.</S>\n    <S sid=\"164\" ssid=\"5\">This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.</S>\n    <S sid=\"165\" ssid=\"6\">In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.</S>\n    <S sid=\"166\" ssid=\"7\">The most immediate extension of our work is to improve the accuracy of relation detection.</S>\n    <S sid=\"167\" ssid=\"8\">This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.</S>\n    <S sid=\"168\" ssid=\"9\">We can also benefit from machine learning algorithms to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint.</S>\n    <S sid=\"169\" ssid=\"10\">In the future work, we will design a more flexible tree kernel for more accurate similarity measure.</S>\n    <S sid=\"170\" ssid=\"11\">Acknowledgements: We would like to thank Dr. Alessandro Moschitti for his great help in using his Tree Kernel Toolkits and fine-tuning the system.</S>\n    <S sid=\"171\" ssid=\"12\">We also would like to thank the three anonymous reviewers for their invaluable suggestions.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "1b16241979cebafc77e92c6cd31ed7bf",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features"
                }
            }
        },
        {
            "index": 16,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon.",
                    "filename": "A_Corpus-Based_Approach_For_Building_Semantic_Lexicons",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W97-0313/Documents_xml/W97-0313.xml",
                    "title": "A Corpus-Based Approach For Building Semantic Lexicons"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a corpus-based approach for building semantic lexicons. The authors aim to automate the creation of semantic knowledge, which is essential for natural language processing. They propose a method that utilizes a text corpus and a small set of seed words for a category to identify other words that belong to the same category. The algorithm uses statistical measures and a bootstrapping mechanism to generate a ranked list of potential category words, which are then reviewed by a human to select the best ones for the dictionary. Experimental results with five categories demonstrate the effectiveness of the approach, with the output being a ranked list of potential category words that a user can review to quickly create a semantic lexicon. The study suggests that this method offers a promising approach for corpus-based semantic knowledge acquisition.",
                        "This paper presents a corpus-based approach for constructing semantic lexicons, allowing fast and automatic acquisition of domain-specific semantic knowledge. The authors propose a statistical bootstrapping algorithm, which uses a text corpus and a small set of seed words for a category to identify other words that belong to that category. The algorithm generates a ranked list of potential category words, which is then reviewed and refined by a human. The paper discusses experimental results for five categories, evaluating the effectiveness and generality of the approach. The results indicate that a core semantic lexicon can be built for each category with only 10-15 minutes of human interaction, making the approach promising for corpus-based semantic knowledge acquisition. The system is the first of its kind aimed at building semantic lexicons from raw text without using any additional semantic knowledge.",
                        "This academic paper presents a corpus-based approach for building semantic lexicons, aiming to construct semantic knowledge for natural language processing systems. The system uses a text corpus and a small set of seed words for a category to identify other words that belong to the category. The approach is designed for fast semantic lexicon construction and involves a bootstrapping mechanism to generate a ranked list of potential category words. The paper discusses experimental results for five categories, the selection of categories and seed words, and the challenges and benefits of the approach. The findings suggest that a core semantic lexicon can be built for each category with minimal human interaction, providing a promising approach for corpus-based semantic knowledge acquisition. The research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee.",
                        "This paper presents a corpus-based approach for building semantic lexicons semi-automatically, aiming to minimize ambiguity problems by identifying the most probable word sense in a restricted domain. The system uses a text corpus and a small set of seed words for a category, and then employs a bootstrapping mechanism to generate a ranked list of potential category words. The top words are reviewed and selected by a human to create a semantic lexicon quickly. Experimental results with five categories demonstrate the effectiveness of the approach, with the system being able to identify relevant category words that a user might not think to include in a semantic lexicon on their own. The paper concludes that this approach shows promise for fast semantic knowledge acquisition and could be refined for broader applications."
                    ],
                    "filename": "a_corpusbased_approach_for_build-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 142 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Corpus-Based Approach For Building Semantic Lexicons</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"8\" ssid=\"1\">Semantic information can be helpful in almost all aspects of natural language understanding, including word sense disambiguation, selectional restrictions, attachment decisions, and discourse processing.</S>\n    <S sid=\"9\" ssid=\"2\">Semantic knowledge can add a great deal of power and accuracy to natural language processing systems.</S>\n    <S sid=\"10\" ssid=\"3\">But semantic information is difficult to obtain.</S>\n    <S sid=\"11\" ssid=\"4\">In most cases, semantic knowledge is encoded manually for each application.</S>\n    <S sid=\"12\" ssid=\"5\">There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986).</S>\n    <S sid=\"13\" ssid=\"6\">While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge.</S>\n    <S sid=\"14\" ssid=\"7\">Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.</S>\n    <S sid=\"15\" ssid=\"8\">Representing all sublanguages in a single knowledge base would be nearly impossible.</S>\n    <S sid=\"16\" ssid=\"9\">Furthermore, domain-specific semantic lexicons are useful for minimizing ambiguity problems.</S>\n    <S sid=\"17\" ssid=\"10\">Within the context of a restricted domain, many polysemous words have a strong preference for one word sense, so knowing the most probable word sense in a domain can strongly constrain the ambiguity.</S>\n    <S sid=\"18\" ssid=\"11\">We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically.</S>\n    <S sid=\"19\" ssid=\"12\">Our system uses a text corpus and a small set of seed words for a category to identify other words that also belong to the category.</S>\n    <S sid=\"20\" ssid=\"13\">The algorithm uses simple statistics and a bootstrapping mechanism to generate a ranked list of potential category words.</S>\n    <S sid=\"21\" ssid=\"14\">A human then reviews the top words and selects the best ones for the dictionary.</S>\n    <S sid=\"22\" ssid=\"15\">Our approach is geared toward fast semantic lexicon construction: given a handful of seed words for a category and a representative text corpus, one can build a semantic lexicon for a category in just a few minutes.</S>\n    <S sid=\"23\" ssid=\"16\">In the first section, we describe the statistical bootstrapping algorithm for identifying candidate category words and ranking them.</S>\n    <S sid=\"24\" ssid=\"17\">Next, we describe experimental results for five categories.</S>\n    <S sid=\"25\" ssid=\"18\">Finally, we discuss our experiences with additional categories and seed word lists, and summarize our results.</S>\n  </SECTION>\n  <SECTION title=\"2 Generating a Semantic Lexicon\" number=\"2\">\n    <S sid=\"26\" ssid=\"1\">Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish).</S>\n    <S sid=\"27\" ssid=\"2\">Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category.</S>\n    <S sid=\"28\" ssid=\"3\">Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques.</S>\n    <S sid=\"29\" ssid=\"4\">We begin with a small set of seed words for a category.</S>\n    <S sid=\"30\" ssid=\"5\">We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well.</S>\n    <S sid=\"31\" ssid=\"6\">As an example, the seed word lists used in our experiments are shown below.</S>\n    <S sid=\"32\" ssid=\"7\">Energy: fuel gas gasoline oil power Financial: bank banking currency dollar money Military: army commander infantry soldier troop Vehicle: airplane car jeep plane truck Weapon: bomb dynamite explosives gun rifle The input to our system is a text corpus and an initial set of seed words for each category.</S>\n    <S sid=\"33\" ssid=\"8\">Ideally, the text corpus should contain many references to the category.</S>\n    <S sid=\"34\" ssid=\"9\">Our approach is designed for domainspecific text processing, so the text corpus should be a representative sample of texts for the domain and the categories should be semantic classes associated with the domain.</S>\n    <S sid=\"35\" ssid=\"10\">Given a text corpus and an initial seed word list for a category C, the algorithm for building a semantic lexicon is as follows: The context windows do not cut across sentence boundaries.</S>\n    <S sid=\"36\" ssid=\"11\">Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992).</S>\n    <S sid=\"37\" ssid=\"12\">We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category.</S>\n    <S sid=\"38\" ssid=\"13\">Note that this is not exactly a conditional probability because a single word occurrence can belong to more than one context window.</S>\n    <S sid=\"39\" ssid=\"14\">For example, consider the sentence: I bought an AK-47 gun and an M-16 rifle.</S>\n    <S sid=\"40\" ssid=\"15\">The word M-16 would be in the context windows for both gun and rifle even though there was just one occurrence of it in the sentence.</S>\n    <S sid=\"41\" ssid=\"16\">Consequently, the category score for a word can be greater than 1.</S>\n    <S sid=\"42\" ssid=\"17\">4.</S>\n    <S sid=\"43\" ssid=\"18\">Next, we remove stopwords, numbers, and any words with a corpus frequency &lt; 5.</S>\n    <S sid=\"44\" ssid=\"19\">We used a stopword list containing about 30 general nouns, mostly pronouns (e.g., I, he, she, they) and determiners (e.g., this, that, those).</S>\n    <S sid=\"45\" ssid=\"20\">The stopwords and numbers are not specific to any category and are common across many domains, so we felt it was safe to remove them.</S>\n    <S sid=\"46\" ssid=\"21\">The remaining nouns are sorted by category score and ranked so that the nouns most strongly associated with the category appear at the top.</S>\n    <S sid=\"47\" ssid=\"22\">5.</S>\n    <S sid=\"48\" ssid=\"23\">The top five nouns that are not already seed words are added to the seed word list dynamically.</S>\n    <S sid=\"49\" ssid=\"24\">We then go back to Step 1 and repeat the process.</S>\n    <S sid=\"50\" ssid=\"25\">This bootstrapping mechanism dynamically grows the seed word list so that each iteration produces a larger category context.</S>\n    <S sid=\"51\" ssid=\"26\">In our experiments, the top five nouns were added automatically without any human intervention, but this sometimes allows non-category words to dilute the growing seed word list.</S>\n    <S sid=\"52\" ssid=\"27\">A few inappropriate words are not likely to have much impact, but many inappropriate words or a few highly frequent words can weaken the feedback process.</S>\n    <S sid=\"53\" ssid=\"28\">One could have a person verify that each word belongs to the target category before adding it to the seed word list, but this would require human interaction at each iteration of the feedback cycle.</S>\n    <S sid=\"54\" ssid=\"29\">We decided to see how well the technique could work without this additional human interaction, but the potential benefits of human feedback still need to be investigated.</S>\n    <S sid=\"55\" ssid=\"30\">After several iterations, the seed word list typically contains many relevant category words.</S>\n    <S sid=\"56\" ssid=\"31\">But more importantly, the ranked list contains many additional category words, especially near the top.</S>\n    <S sid=\"57\" ssid=\"32\">The number of iterations can make a big difference in the quality of the ranked list.</S>\n    <S sid=\"58\" ssid=\"33\">Since new seed words are generated dynamically without manual review, the quality of the ranked list can deteriorate rapidly when too many non-category words become seed words.</S>\n    <S sid=\"59\" ssid=\"34\">In our experiments, we found that about eight iterations usually worked well.</S>\n    <S sid=\"60\" ssid=\"35\">The output of the system is the ranked list of nouns after the final iteration.</S>\n    <S sid=\"61\" ssid=\"36\">The seed word list is thrown away.</S>\n    <S sid=\"62\" ssid=\"37\">Note that the original seed words were already known to be category members, and the new seed words are already in the ranked list because that is how they were selected.2 Finally, a user must review the ranked list and identify the words that are true category members.</S>\n    <S sid=\"63\" ssid=\"38\">How one defines a &amp;quot;true&amp;quot; category member is subjective and may depend on the specific application, so we leave this exercise to a person.</S>\n    <S sid=\"64\" ssid=\"39\">Typically, the words near the top of the ranked list are highly associated with the category but the density of category words decreases as one proceeds down the list.</S>\n    <S sid=\"65\" ssid=\"40\">The user may scan down the list until a sufficient number of category words is found, or as long as time permits.</S>\n    <S sid=\"66\" ssid=\"41\">The words selected by the user are added to a permanent semantic lexicon with the appropriate category label.</S>\n    <S sid=\"67\" ssid=\"42\">Our goal is to allow a user to build a semantic lexicon for one or more categories using only a small set of known category members as seed words and a text corpus.</S>\n    <S sid=\"68\" ssid=\"43\">The output is a ranked list of potential category words that a user can review to create a semantic lexicon quickly.</S>\n    <S sid=\"69\" ssid=\"44\">The success of this approach depends on the quality of the ranked list, especially the density of category members near the top.</S>\n    <S sid=\"70\" ssid=\"45\">In the next section, we describe experiments to evaluate our system.</S>\n    <S sid=\"71\" ssid=\"46\">2It is possible that a word may be near the top of the ranked list during one iteration (and subsequently become a seed word) but become buried at the bottom of the ranked list during later iterations.</S>\n    <S sid=\"72\" ssid=\"47\">However, we have not observed this to be a problem so far.</S>\n  </SECTION>\n  <SECTION title=\"3 Experimental Results\" number=\"3\">\n    <S sid=\"73\" ssid=\"1\">We performed experiments with five categories to evaluate the effectiveness and generality of our approach: energy, financial, military, vehicles, and weapons.</S>\n    <S sid=\"74\" ssid=\"2\">The MUC-4 development corpus (1700 texts) was used as the text corpus (MUC-4 Proceedings, 1992).</S>\n    <S sid=\"75\" ssid=\"3\">We chose these five categories because they represented relatively different semantic classes, they were prevalent in the MUC-4 corpus, and they seemed to be useful categories.</S>\n    <S sid=\"76\" ssid=\"4\">For each category, we began with the seed word lists shown in Figure 1.</S>\n    <S sid=\"77\" ssid=\"5\">We ran the bootstrapping algorithm for eight iterations, adding five new words to the seed word list after each cycle.</S>\n    <S sid=\"78\" ssid=\"6\">After the final iteration, we had ranked lists of potential category words for each of the five categories.</S>\n    <S sid=\"79\" ssid=\"7\">The top 45 words3 from each ranked list are shown in Figure 2.</S>\n    <S sid=\"80\" ssid=\"8\">While the ranked lists are far from perfect, one can see that there are many category members near the top of each list.</S>\n    <S sid=\"81\" ssid=\"9\">It is also apparent that a few additional heuristics could be used to remove many of the extraneous words.</S>\n    <S sid=\"82\" ssid=\"10\">For example, our number processor failed to remove numbers with commas (e.g., 2,000).</S>\n    <S sid=\"83\" ssid=\"11\">And the military category contains several ordinal numbers (e.g., 10th 3rd 1st) that could be easily identified and removed.</S>\n    <S sid=\"84\" ssid=\"12\">But the key question is whether the ranked list contains many true category members.</S>\n    <S sid=\"85\" ssid=\"13\">Since this is a subjective question, we set up an experiment involving human judges.</S>\n    <S sid=\"86\" ssid=\"14\">For each category, we selected the top 200 words from its ranked list and presented them to a user.</S>\n    <S sid=\"87\" ssid=\"15\">We presented the words in random order so that the user had no idea how our system had ranked the words.</S>\n    <S sid=\"88\" ssid=\"16\">This was done to minimize contextual effects (e.g., seeing five category members in a row might make someone more inclined to judge the next word as relevant).</S>\n    <S sid=\"89\" ssid=\"17\">Each category was judged by two people independently.4 The judges were asked to rate each word on a scale from 1 to 5 indicating how strongly it was associated with the category.</S>\n    <S sid=\"90\" ssid=\"18\">Since category judgements can be highly subjective, we gave them guidelines to help establish uniform criteria.</S>\n    <S sid=\"91\" ssid=\"19\">The instructions that were given to the judges are shown in Figure 3.</S>\n    <S sid=\"92\" ssid=\"20\">We asked the judges to rate the words on a scale from 1 to 5 because different degrees of category membership might be acceptable for different applications.</S>\n    <S sid=\"93\" ssid=\"21\">Some applications might require strict cat3 Note that some of these words are not nouns, such as boarded and U.S.-made.</S>\n    <S sid=\"94\" ssid=\"22\">Our parser tags unknown words as nouns, so sometimes unknown words are mistakenly selected for context windows.</S>\n    <S sid=\"95\" ssid=\"23\">'The judges were members of our research group but not the authors. aLimon-Covenas refers to an oil pipeline. aLa_Aurora refers to an airport.</S>\n    <S sid=\"96\" ssid=\"24\">CRITERIA: On a scale of 0 to 5, rate each word's strength of association with the given category using the following criteria.</S>\n    <S sid=\"97\" ssid=\"25\">We'll use the category ANIMAL as an example.</S>\n    <S sid=\"98\" ssid=\"26\">5: CORE MEMBER OF THE CATEGORY: If a word is clearly a member of the category, then it deserves a 5.</S>\n    <S sid=\"99\" ssid=\"27\">For example, dogs and sparrows are members of the ANIMAL category.</S>\n  </SECTION>\n  <SECTION title=\"4: SUBPART OF MEMBER OF THE CATEGORY:\" number=\"4\">\n    <S sid=\"100\" ssid=\"1\">If a word refers to a part of something that is a member of the category, then it deserves a 4.</S>\n    <S sid=\"101\" ssid=\"2\">For example, feathers and tails are parts of ANIMALS.</S>\n  </SECTION>\n  <SECTION title=\"3: STRONGLY ASSOCIATED WITH THE CATEGORY:\" number=\"5\">\n    <S sid=\"102\" ssid=\"1\">If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.</S>\n    <S sid=\"103\" ssid=\"2\">For example, zoos and nests are strongly associated with ANIMALS.</S>\n  </SECTION>\n  <SECTION title=\"2: WEAKLY ASSOCIATED WITH THE CATEGORY:\" number=\"6\">\n    <S sid=\"104\" ssid=\"1\">If a word refers to something that can be associated with members of the category, but is also associated with many other types of things, then it deserves a 2.</S>\n    <S sid=\"105\" ssid=\"2\">For example, bowls and parks are weakly associated with ANIMALS.</S>\n    <S sid=\"106\" ssid=\"3\">1: NO ASSOCIATION WITH THE CATEGORY: If a word has virtually no association with the category, then it deserves a 1.</S>\n    <S sid=\"107\" ssid=\"4\">For example, tables and moons have virtually no association with ANIMALS.</S>\n    <S sid=\"108\" ssid=\"5\">0: UNKNOWN WORD: If you do not know what a word means, then it should be labeled with a 0.</S>\n    <S sid=\"109\" ssid=\"6\">IMPORTANT!</S>\n    <S sid=\"110\" ssid=\"7\">Many words have several distinct meanings.</S>\n    <S sid=\"111\" ssid=\"8\">For example, the word &amp;quot;horse&amp;quot; can refer to an animal, a piece of gymnastics equipment, or it can mean to fool around (e.g., &amp;quot;Don't horse around!&amp;quot;).</S>\n    <S sid=\"112\" ssid=\"9\">If a word has ANY meaning associated with the given category, then only consider that meaning when assigning numbers.</S>\n    <S sid=\"113\" ssid=\"10\">For example, the word &amp;quot;horse&amp;quot; would be a 5 because one of its meanings refers to an ANIMAL. egory membership, for example only words like gun, rifle, and bomb should be labeled as weapons.</S>\n    <S sid=\"114\" ssid=\"11\">But from a practical perspective, subparts of category members might also be acceptable.</S>\n    <S sid=\"115\" ssid=\"12\">For example, if a cartridge or trigger is mentioned in the context of an event, then one can infer that a gun was used.</S>\n    <S sid=\"116\" ssid=\"13\">And for some applications, any word that is strongly associated with a category might be useful to include in the semantic lexicon.</S>\n    <S sid=\"117\" ssid=\"14\">For example, words like ammunition or bullets are highly suggestive of a weapon.</S>\n    <S sid=\"118\" ssid=\"15\">In the UMass/MUC-4 information extraction system (Lehnert et al., 1992), the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions.</S>\n    <S sid=\"119\" ssid=\"16\">The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category.</S>\n    <S sid=\"120\" ssid=\"17\">Since the instructions allowed the users to assign a zero to a word if they did not know what it meant, we manually removed the zeros and assigned ratings that we thought were appropriate.</S>\n    <S sid=\"121\" ssid=\"18\">We considered ignoring the zeros, but some of the categories would have been severely impacted.</S>\n    <S sid=\"122\" ssid=\"19\">For example, many of the legitimate weapons (e.g., M-16 and AR-15) were not known to the judges.</S>\n    <S sid=\"123\" ssid=\"20\">Fortunately, most of the unknown words were proper nouns with relatively unambiguous semantics, so we do not believe that this process compromised the integrity of the experiment.</S>\n    <S sid=\"124\" ssid=\"21\">Finally, we graphed the results from the human judges.</S>\n    <S sid=\"125\" ssid=\"22\">We counted the number of words judged as 5's by either judge, the number of words judged as 5's or 4's by either judge, the number of words judged as 5's, 4's, or 3's by either judge, and the number of words judged as either 5's, 4's, 3's, or 2's.</S>\n    <S sid=\"126\" ssid=\"23\">We plotted the results after each 20 words, stepping down the ranked list, to see whether the words near the top of the list were more highly associated with the category than words farther down.</S>\n    <S sid=\"127\" ssid=\"24\">We also wanted to see whether the number of category words leveled off or whether it continued to grow.</S>\n    <S sid=\"128\" ssid=\"25\">The results from this experiment are shown in Figures 4-8.</S>\n    <S sid=\"129\" ssid=\"26\">With the exception of the Energy category, we were able to find 25-45 words that were judged as 4's or 5's for each category.</S>\n    <S sid=\"130\" ssid=\"27\">This was our strictest test because only true category members (or subparts of true category members) earned this rating.</S>\n    <S sid=\"131\" ssid=\"28\">Although this might not seem like a lot of category words, 25-45 words is enough to produce a reasonable core semantic lexicon.</S>\n    <S sid=\"132\" ssid=\"29\">For example, the words judged as 5's for each category are shown in Figure 9.</S>\n    <S sid=\"133\" ssid=\"30\">Figure 9 illustrates an important benefit of the corpus-based approach.</S>\n    <S sid=\"134\" ssid=\"31\">By sifting through a large text corpus, the algorithm can find many relevant category words that a user would probably not enter in a semantic lexicon on their own.</S>\n    <S sid=\"135\" ssid=\"32\">For example, suppose a user wanted to build a dictionary of Vehicle words.</S>\n    <S sid=\"136\" ssid=\"33\">Most people would probably define words such as car, truck, plane, and automobile.</S>\n    <S sid=\"137\" ssid=\"34\">But it is doubtful that most people would think of words like gunships, fighter, carrier, and ambulances.</S>\n    <S sid=\"138\" ssid=\"35\">The corpus-based algorithm is especially good at identifying words that are common in the text corpus even though they might not be commonly used in general.</S>\n    <S sid=\"139\" ssid=\"36\">As another example, specific types of weapons (e.g., M-16, AR-15, M-60, or M-79) might not even be known to most users, but they are abundant in the MUC-4 corpus.</S>\n    <S sid=\"140\" ssid=\"37\">If we consider all the words rated as 3's, 4's, or 5's, then we were able to find about 50-65 words for every category except Energy.</S>\n    <S sid=\"141\" ssid=\"38\">Many of these words would be useful in a semantic dictionary for the category.</S>\n    <S sid=\"142\" ssid=\"39\">For example, some of the words rated as 3's for the Vehicle category include: flight, flights, aviation, pilot, airport, and highways.</S>\n    <S sid=\"143\" ssid=\"40\">Most of the words rated as 2's are not specific to the target category, but some of them might be useful for certain tasks.</S>\n    <S sid=\"144\" ssid=\"41\">For example, some words judged as 2's for the Energy category are: spill, pole, tower, and fields.</S>\n    <S sid=\"145\" ssid=\"42\">These words may appear in many different contexts, but in texts about Energy topics these words are likely to be relevant and probably should be defined in the dictionary.</S>\n    <S sid=\"146\" ssid=\"43\">Therefore we expect that a user would likely keep some of these words in the semantic lexicon but would probably be very selective.</S>\n    <S sid=\"147\" ssid=\"44\">Finally, the graphs show that most of the acquisition curves displayed positive slopes even at the end of the 200 words.</S>\n    <S sid=\"148\" ssid=\"45\">This implies that more category words would likely have been found if the users had reviewed more than 200 words.</S>\n    <S sid=\"149\" ssid=\"46\">The one exception, again, was the Energy category, which we will discuss in the next section.</S>\n    <S sid=\"150\" ssid=\"47\">The size of the ranked lists ranged from 442 for the financial category to 919 for the military category, so it would be interesting to know how many category members would have been found if we had given the entire lists to our judges.</S>\n  </SECTION>\n  <SECTION title=\"4 Selecting Categories and Seed Words\" number=\"7\">\n    <S sid=\"151\" ssid=\"1\">When we first began this work, we were unsure about what types of categories would be amenable to this approach.</S>\n    <S sid=\"152\" ssid=\"2\">So we experimented with a number of different categories.</S>\n    <S sid=\"153\" ssid=\"3\">Fortunately, most of them worked fairly well, but some of them did not.</S>\n    <S sid=\"154\" ssid=\"4\">We do not claim to understand exactly what types of categories will work well and which ones will not, but our early experiences did shed some light on the strengths and weaknesses of this approach.</S>\n    <S sid=\"155\" ssid=\"5\">In addition to the previous five categories, we also experimented with categories for Location, Commercial, and Person.</S>\n    <S sid=\"156\" ssid=\"6\">The Location category performed very well using seed words such as city, town, and province.</S>\n    <S sid=\"157\" ssid=\"7\">We didn't formally evaluate this category because most of the category words were proper nouns and we did not expect that our judges would know what they were.</S>\n    <S sid=\"158\" ssid=\"8\">But it is worth noting that this category achieved good results, presumably because location names often cluster together in appositives, conjunctions, and nominal compounds.</S>\n    <S sid=\"159\" ssid=\"9\">For the Commercial category, we chose seed words such as store, shop, and market.</S>\n    <S sid=\"160\" ssid=\"10\">Only a few new commercial words were identified, such as hotel and restaurant.</S>\n    <S sid=\"161\" ssid=\"11\">In retrospect, we realized that there were probably few words in the MUC-4 corpus that referred to commercial establishments.</S>\n    <S sid=\"162\" ssid=\"12\">(The MUC-4 corpus mainly contains reports of terrorist and military events.)</S>\n    <S sid=\"163\" ssid=\"13\">The relatively poor performance of the Energy category was probably due to the same problem.</S>\n    <S sid=\"164\" ssid=\"14\">If a category is not well-represented in the corpus then it is doomed because inappropriate words become seed words in the early iterations and quickly derail the feedback loop.</S>\n    <S sid=\"165\" ssid=\"15\">The Person category produced mixed results.</S>\n    <S sid=\"166\" ssid=\"16\">Some good category words were found, such as rebel, advisers, criminal, and citizen.</S>\n    <S sid=\"167\" ssid=\"17\">But many of the words referred to organizations (e.g., FMLN), groups (e.g., forces), and actions (e.g., attacks).</S>\n    <S sid=\"168\" ssid=\"18\">Some of these words seemed reasonable, but it was hard to draw a line between specific references to people and concepts like organizations and groups that may or may not consist entirely of people.</S>\n    <S sid=\"169\" ssid=\"19\">The large proportion of action words also diluted the list.</S>\n    <S sid=\"170\" ssid=\"20\">More experiments are needed to better understand whether this category is inherently difficult or whether a more carefully chosen set of seed words would improve performance.</S>\n    <S sid=\"171\" ssid=\"21\">More experiments are also needed to evaluate different seed word lists.</S>\n    <S sid=\"172\" ssid=\"22\">The algorithm is clearly sensitive to the initial seed words, but the degree of sensitivity is unknown.</S>\n    <S sid=\"173\" ssid=\"23\">For the five categories reported in this paper, we arbitrarily chose a few words that were central members of the category.</S>\n    <S sid=\"174\" ssid=\"24\">Our initial seed words worked well enough that we did not experiment with them very much.</S>\n    <S sid=\"175\" ssid=\"25\">But we did perform a few experiments varying the number of seed words.</S>\n    <S sid=\"176\" ssid=\"26\">In general, we found that additional seed words tend to improve performance, but the results were not substantially different using five seed words or using ten.</S>\n    <S sid=\"177\" ssid=\"27\">Of course, there is also a law of diminishing returns: using a seed word list containing 60 category words is almost like creating a semantic lexicon for the category by hand!</S>\n  </SECTION>\n  <SECTION title=\"5 Conclusions\" number=\"8\">\n    <S sid=\"178\" ssid=\"1\">Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used.</S>\n    <S sid=\"179\" ssid=\"2\">But there is no question that semantic knowledge is essential for many problems in natural language processing.</S>\n    <S sid=\"180\" ssid=\"3\">Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically.</S>\n    <S sid=\"181\" ssid=\"4\">Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994)).</S>\n    <S sid=\"182\" ssid=\"5\">Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993)).</S>\n    <S sid=\"183\" ssid=\"6\">Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.</S>\n    <S sid=\"184\" ssid=\"7\">To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge.</S>\n    <S sid=\"185\" ssid=\"8\">The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing.</S>\n    <S sid=\"186\" ssid=\"9\">Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)).</S>\n    <S sid=\"187\" ssid=\"10\">Our corpus-based approach is designed to support fast semantic lexicon construction.</S>\n    <S sid=\"188\" ssid=\"11\">A user only needs to supply a representative text corpus and a small set of seed words for each target category.</S>\n    <S sid=\"189\" ssid=\"12\">Our experiments suggest that a core semantic lexicon can be built for each category with only 10-15 minutes of human interaction.</S>\n    <S sid=\"190\" ssid=\"13\">While more work needs to be done to refine this procedure and characterize the types of categories it can handle, we believe that this is a promising approach for corpus-based semantic knowledge acquisition.</S>\n  </SECTION>\n  <SECTION title=\"6 Acknowledgments\" number=\"9\">\n    <S sid=\"191\" ssid=\"1\">This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee.</S>\n    <S sid=\"192\" ssid=\"2\">We would like to thank David Bean, Jeff Lorenzen, and Kiri Wagstaff for their help in judging our category lists.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "ed324d9a3c35296183853f41afc9b909",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "A Corpus-Based Approach For Building Semantic Lexicons"
                }
            }
        },
        {
            "index": 17,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.",
                    "filename": "Improving_Machine_Translation_Performance_By_Exploiting_Non-Parallel_Corpora",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J05-4003/Documents_xml/J05-4003.xml",
                    "title": "Improving Machine Translation Performance By Exploiting Non-Parallel Corpora"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a novel method for discovering parallel sentences in comparable, non-parallel corpora. The method is based on training a maximum entropy classifier that can reliably determine whether or not a pair of sentences are translations of each other. The study utilizes large Chinese, Arabic, and English non-parallel newspaper corpora and evaluates the quality of the extracted data by demonstrating that it improves the performance of a state-of-the-art statistical machine translation system. Additionally, the paper shows that language pairs with limited parallel data can benefit significantly from the method. The method is robust, efficient, and has the potential to address the lack of parallel corpora in the development of statistical machine translation systems.",
                        "This paper presents a method for discovering parallel sentences in comparable non-parallel corpora. The authors introduce a novel approach for extracting parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. They evaluate the quality of the extracted data and demonstrate its impact on improving the performance of a machine translation system. The paper also discusses the robustness of the parallel sentence selection approach and the efficiency of the computational processes involved. The authors emphasize the usefulness of the extracted data, particularly in scenarios where only limited amounts of parallel data are available. The method is shown to be effective in increasing the amount of training data for machine translation systems, regardless of the domain of interest.",
                        "This paper presents a novel method for discovering parallel sentences in comparable, non-parallel corpora. The method involves training a maximum entropy classifier to reliably determine whether two sentences are translations of each other. The approach is utilized to extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. The paper demonstrates that the extracted data improves the performance of a state-of-the-art statistical machine translation system. Additionally, the authors show that a good-quality machine translation system can be built from scratch by starting with a very small parallel corpus and exploiting a large non-parallel corpus. The paper highlights the potential benefits of this method for language pairs with limited available resources. The paper is supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276.",
                        "This academic paper presents a method for discovering parallel sentences in comparable, non-parallel corpora in order to improve statistical machine translation (SMT) performance. The approach uses a maximum entropy classifier to judge whether two sentences are translations of each other, without relying on context. The method is applied to large Chinese, Arabic, and English corpora and is found to improve the performance of machine translation systems, especially in situations where limited parallel data is available. The paper demonstrates that the extracted data is of good quality and its impact on machine translation performance is comparable to that of human-translated data of similar size and domain. The study highlights the robustness and efficiency of the method, and the potential for application to different language pairs and domains."
                    ],
                    "filename": "improving_machine_translation_pe-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 123 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Improving Machine Translation Performance By Exploiting Non-Parallel Corpora</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.</S>\n    <S sid=\"8\" ssid=\"2\">We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.</S>\n    <S sid=\"9\" ssid=\"3\">Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.</S>\n    <S sid=\"10\" ssid=\"4\">We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.</S>\n    <S sid=\"11\" ssid=\"5\">We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.</S>\n    <S sid=\"12\" ssid=\"6\">Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"13\" ssid=\"1\">Parallel texts—texts that are translations of each other—are an important resource in many NLP applications.</S>\n    <S sid=\"14\" ssid=\"2\">They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001).</S>\n    <S sid=\"15\" ssid=\"3\">Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register.</S>\n    <S sid=\"16\" ssid=\"4\">There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations).</S>\n    <S sid=\"17\" ssid=\"5\">This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles).</S>\n    <S sid=\"18\" ssid=\"6\">One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora.</S>\n    <S sid=\"19\" ssid=\"7\">Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information.</S>\n    <S sid=\"20\" ssid=\"8\">Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc.</S>\n    <S sid=\"21\" ssid=\"9\">Such texts are widely available on the Web for many language pairs and domains.</S>\n    <S sid=\"22\" ssid=\"10\">They often contain many sentence pairs that are fairly good translations of each other.</S>\n    <S sid=\"23\" ssid=\"11\">The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora.</S>\n    <S sid=\"24\" ssid=\"12\">However, identifying good translations in comparable corpora is hard.</S>\n    <S sid=\"25\" ssid=\"13\">Even texts that convey the same information will exhibit great differences at the sentence level.</S>\n    <S sid=\"26\" ssid=\"14\">Consider the two newspaper articles in Figure 1.</S>\n    <S sid=\"27\" ssid=\"15\">They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang.</S>\n    <S sid=\"28\" ssid=\"16\">The lines in the figure connect sentence pairs that are approximate translations of each other.</S>\n    <S sid=\"29\" ssid=\"17\">Discovering these links automatically is clearly non-trivial.</S>\n    <S sid=\"30\" ssid=\"18\">Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document.</S>\n    <S sid=\"31\" ssid=\"19\">Thus, they perform poorly on comparable, non-parallel texts.</S>\n    <S sid=\"32\" ssid=\"20\">What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context.</S>\n    <S sid=\"33\" ssid=\"21\">This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).</S>\n    <S sid=\"34\" ssid=\"22\">We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.</S>\n    <S sid=\"35\" ssid=\"23\">Using this classifier, we extract parallel sentences from very large comparable corpora of newspaper articles.</S>\n    <S sid=\"36\" ssid=\"24\">We demonstrate the quality of our A pair of comparable texts. extracted sentences by showing that adding them to the training data of an SMT system improves the system’s performance.</S>\n    <S sid=\"37\" ssid=\"25\">We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data.</S>\n    <S sid=\"38\" ssid=\"26\">Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.</S>\n    <S sid=\"39\" ssid=\"27\">In such a situation, the test data is in-domain, and the training data is out-of-domain.</S>\n    <S sid=\"40\" ssid=\"28\">The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn’t really help the system to produce good translations.</S>\n    <S sid=\"41\" ssid=\"29\">What is needed is additional in-domain training data.</S>\n    <S sid=\"42\" ssid=\"30\">Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system.</S>\n    <S sid=\"43\" ssid=\"31\">We work in the context of Arabic-English and Chinese-English statistical machine translation systems.</S>\n    <S sid=\"44\" ssid=\"32\">Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles.</S>\n    <S sid=\"45\" ssid=\"33\">In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium:1 In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles.</S>\n    <S sid=\"46\" ssid=\"34\">Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings.</S>\n    <S sid=\"47\" ssid=\"35\">We are interested in the situation that exists when we need to translate news data but only have UN data available for training.</S>\n    <S sid=\"48\" ssid=\"36\">The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set.</S>\n    <S sid=\"49\" ssid=\"37\">The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution.</S>\n    <S sid=\"50\" ssid=\"38\">We can train baseline systems on UN parallel data (using the data from the first bullet in the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet).</S>\n    <S sid=\"51\" ssid=\"39\">In the next section, we give a high-level overview of our parallel sentence extraction system.</S>\n    <S sid=\"52\" ssid=\"40\">In Section 3, we describe in detail the core of the system, the parallel sentence classifier.</S>\n    <S sid=\"53\" ssid=\"41\">In Section 4, we discuss several data extraction experiments.</S>\n    <S sid=\"54\" ssid=\"42\">In Section 5, we evaluate the extracted data by showing that adding it to out-of-domain parallel data improves the in-domain performance of an out-of-domain MT system, and in Section 6, we show that in certain cases, even larger improvements can be obtained by using bootstrapping.</S>\n    <S sid=\"55\" ssid=\"43\">In Section 7, we present examples of sentence pairs extracted by our method and discuss some of its weaknesses.</S>\n    <S sid=\"56\" ssid=\"44\">Before concluding, we discuss related work.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"57\" ssid=\"1\">The general architecture of our extraction system is presented in Figure 2.</S>\n    <S sid=\"58\" ssid=\"2\">Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1).</S>\n    <S sid=\"59\" ssid=\"3\">From each such pair, we generate all possible sentence pairs and pass them through a simple word-overlapbased filter (Section 2.2), thus obtaining candidate sentence pairs.</S>\n    <S sid=\"60\" ssid=\"4\">The candidates are presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the sentences in each pair are mutual translations of each other.</S>\n    <S sid=\"61\" ssid=\"5\">The resources required by the system are minimal: a bilingual dictionary and a small amount of parallel data (used for training the ME classifier).</S>\n    <S sid=\"62\" ssid=\"6\">The dictionaries used in our experiments are learned automatically from (out-of-domain) parallel corpora;2 thus, the only resource used by our system consists of parallel sentences.</S>\n    <S sid=\"63\" ssid=\"7\">2 If such a resource is unavailable, other dictionaries can be used.</S>\n    <S sid=\"64\" ssid=\"8\">Our comparable corpus consists of two large, non-parallel, news corpora, one in English and the other in the foreign language of interest (in our case, Chinese or Arabic).</S>\n    <S sid=\"65\" ssid=\"9\">The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one.</S>\n    <S sid=\"66\" ssid=\"10\">This step of the process emphasizes recall rather than precision.</S>\n    <S sid=\"67\" ssid=\"11\">For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents.</S>\n    <S sid=\"68\" ssid=\"12\">The subsequent components of the system are robust enough to filter out the extra noise introduced by the selection of additional (possibly bad) English documents.</S>\n    <S sid=\"69\" ssid=\"13\">We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan 2001).</S>\n    <S sid=\"70\" ssid=\"14\">We first index all the English documents into a database.</S>\n    <S sid=\"71\" ssid=\"15\">For each foreign document, we take the top five translations of each of its words (according to our probabilistic dictionary) and create an English language query.</S>\n    <S sid=\"72\" ssid=\"16\">The translation probabilities are only used to choose the word translations; they do not appear in the query.</S>\n    <S sid=\"73\" ssid=\"17\">We use the query to run TF-IDF retrieval against the database, take the top 20 English documents returned by Lemur, and pair each of them with the foreign query document.</S>\n    <S sid=\"74\" ssid=\"18\">This document matching procedure is both slow (it looks at all possible document pairs, so it is quadratic in the number of documents) and imprecise (due to noise in the dictionary, the query will contain many wrong words).</S>\n    <S sid=\"75\" ssid=\"19\">We attempt to fix these problems by using the following heuristic: we consider it likely that articles with similar content have publication dates that are close to each other.</S>\n    <S sid=\"76\" ssid=\"20\">Thus, each query is actually run only against English documents published within a window of five days around the publication date of the foreign query document; we retrieve the best 20 of these documents.</S>\n    <S sid=\"77\" ssid=\"21\">Each query is thus run against fewer documents, so it becomes faster and has a better chance of getting the right documents at the top.</S>\n    <S sid=\"78\" ssid=\"22\">Our experiments have shown that the final performance of the system does not depend too much on the size of the window (for example, doubling the size to 10 days made no difference).</S>\n    <S sid=\"79\" ssid=\"23\">However, having no window at all leads to a decrease in the overall performance of the system.</S>\n    <S sid=\"80\" ssid=\"24\">From each foreign document and set of associated English documents, we take all possible sentence pairs and pass them through a word-overlap filter.</S>\n    <S sid=\"81\" ssid=\"25\">The filter verifies that the ratio of the lengths of the two sentences is no greater than two.</S>\n    <S sid=\"82\" ssid=\"26\">It then checks that at least half the words in each sentence have a translation in the other sentence, according to the dictionary.</S>\n    <S sid=\"83\" ssid=\"27\">Pairs that do not fulfill these two conditions are discarded.</S>\n    <S sid=\"84\" ssid=\"28\">The others are passed on to the parallel sentence selection stage.</S>\n    <S sid=\"85\" ssid=\"29\">This step removes most of the noise (i.e., pairs of non-parallel sentences) introduced by our recall-oriented document selection procedure.</S>\n    <S sid=\"86\" ssid=\"30\">It also removes good pairs that fail to pass the filter because the dictionary does not contain the necessary entries; but those pairs could not have been handled reliably anyway, so the overall effect of the filter is to improve the precision and robustness of the system.</S>\n    <S sid=\"87\" ssid=\"31\">However, the filter also accepts many wrong pairs, because the word-overlap condition is weak; for instance, stopwords almost always have a translation on the other side, so if a few of the content For each candidate sentence pair, we need a reliable way of deciding whether the two sentences in the pair are mutual translations.</S>\n    <S sid=\"88\" ssid=\"32\">This is achieved by a Maximum Entropy (ME) classifier (described at length in Section 3), which is the core component of our system.</S>\n    <S sid=\"89\" ssid=\"33\">Those pairs that are classified as being translations of each other constitute the output of the system.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"90\" ssid=\"1\">In the Maximum Entropy (ME) statistical modeling framework, we impose constraints on the model of our data by defining a set of feature functions.</S>\n    <S sid=\"91\" ssid=\"2\">These feature functions emphasize properties of the data that we believe to be useful for the modeling task.</S>\n    <S sid=\"92\" ssid=\"3\">For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel.</S>\n    <S sid=\"93\" ssid=\"4\">We therefore define a feature function f (sp), whose value is the word overlap of the sentences in sp.</S>\n    <S sid=\"94\" ssid=\"5\">According to the ME principle, the optimal parametric form of the model of our data, taking into account the constraints imposed by the feature functions, is a log linear combination of these functions.</S>\n    <S sid=\"95\" ssid=\"6\">Thus, for our classification problem, we have: where ci is the class (c0=”parallel”, c1=”not parallel”), Z(sp) is a normalization factor, and fij are the feature functions (indexed both by class and by feature).</S>\n    <S sid=\"96\" ssid=\"7\">The resulting model has free parameters λj, the feature weights.</S>\n    <S sid=\"97\" ssid=\"8\">The parameter values that maximize the likelihood of a given training corpus can be computed using various optimization algorithms (see [Malouf 2002] for a comparison of such algorithms).</S>\n    <S sid=\"98\" ssid=\"9\">For our particular classification problem, we need to find feature functions that distinguish between parallel and non-parallel sentence pairs.</S>\n    <S sid=\"99\" ssid=\"10\">For this purpose, we compute and exploit word-level alignments between the sentences in each pair.</S>\n    <S sid=\"100\" ssid=\"11\">A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other.</S>\n    <S sid=\"101\" ssid=\"12\">Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990).</S>\n    <S sid=\"102\" ssid=\"13\">Since then, they were found useful in many other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question answering [Echihabi and Marcu 2003]).</S>\n    <S sid=\"103\" ssid=\"14\">Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus.</S>\n    <S sid=\"104\" ssid=\"15\">Each figure contains two alignments.</S>\n    <S sid=\"105\" ssid=\"16\">The one on the left is a correct alignment, produced by a human, while the one on the right Alignments between two parallel sentences. was computed automatically.</S>\n    <S sid=\"106\" ssid=\"17\">As can be seen from the gloss next to the Arabic words, the sentences in Figure 3 are parallel while the sentences in Figure 4 are not.</S>\n    <S sid=\"107\" ssid=\"18\">In a correct alignment between two non-parallel sentences, most words would have no translation equivalents; in contrast, in an alignment between parallel sentences, most words would be aligned.</S>\n    <S sid=\"108\" ssid=\"19\">Automatically computed alignments, however, may have incorrect connections; for example, on the right side of Figure 3, the Arabic word issue is connected to the comma; and in Figure 4, the Arabic word at is connected to the English phrase its case to the.</S>\n    <S sid=\"109\" ssid=\"20\">Such errors are due to noisy dictionary entries and to Alignments between two non-parallel sentences. shortcomings of the model used to generate the alignments.</S>\n    <S sid=\"110\" ssid=\"21\">Thus, merely looking at the number of unconnected words, while helpful, is not discriminative enough.</S>\n    <S sid=\"111\" ssid=\"22\">Still, automatically produced alignments have certain additional characteristics that can be exploited.</S>\n    <S sid=\"112\" ssid=\"23\">We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.</S>\n    <S sid=\"113\" ssid=\"24\">The presence, in an automatically computed alignment between a pair of sentences, of words of high fertility (such as the Arabic word at in Figure 4) is indicative of non-parallelism.</S>\n    <S sid=\"114\" ssid=\"25\">Most likely, these connections were produced because of a lack of better alternatives.</S>\n    <S sid=\"115\" ssid=\"26\">Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring.</S>\n    <S sid=\"116\" ssid=\"27\">Such a span may contain a few words without any connection (a small percentage of the length of the span), but no word with a connection outside the span.</S>\n    <S sid=\"117\" ssid=\"28\">Examples of such spans can be seen in Figure 3: the English strings after saudi mediation failed or to the international court ofjustice together with their Arabic counterparts.</S>\n    <S sid=\"118\" ssid=\"29\">Long contiguous connected spans are indicative of parallelism, since they suggest that the two sentences have long phrases in common.</S>\n    <S sid=\"119\" ssid=\"30\">And, in contrast, long substrings whose words are all unconnected are indicative of non-parallelism.</S>\n    <S sid=\"120\" ssid=\"31\">To summarize, our classifier uses the following features, defined over two sentences and an automatically computed alignment between them.</S>\n    <S sid=\"121\" ssid=\"32\">General features (independent of the word alignment): In order to compute word alignments we need a simple and efficient model.</S>\n    <S sid=\"122\" ssid=\"33\">We want to align a large number of sentences, with many out-of-vocabulary words, in reasonable time.</S>\n    <S sid=\"123\" ssid=\"34\">We also want a model with as few parameters as possible—preferably only wordfor-word translation probabilities.</S>\n    <S sid=\"124\" ssid=\"35\">One such model is the IBM Model 1 (Brown et al. 1993).</S>\n    <S sid=\"125\" ssid=\"36\">According to this model, given foreign sentence (fj1&lt;=j&lt;=m), English sentence (ei1&lt;=i&lt;=l), and translation probabilities t(fj|ei), the best alignment f → e is obtained by linking each foreign word fj to its most likely English translation argmaxeit(fj|ei).</S>\n    <S sid=\"126\" ssid=\"37\">Thus, each foreign word is aligned to exactly one English word (or to a special NULL token).</S>\n    <S sid=\"127\" ssid=\"38\">Due to its simplicity, this model has several shortcomings, some more structural than others (see Moore [2004] for a discussion).</S>\n    <S sid=\"128\" ssid=\"39\">Thus, we use a version that is augmented with two simple heuristics that attempt to alleviate some of these shortcomings.</S>\n    <S sid=\"129\" ssid=\"40\">One possible improvement concerns English words that appear more than once in a sentence.</S>\n    <S sid=\"130\" ssid=\"41\">According to the model, a foreign word that prefers to be aligned with such an English word could be equally well aligned with any instance of that word.</S>\n    <S sid=\"131\" ssid=\"42\">In such situations, instead of arbitrarily choosing the first instance or a random instance, we attempt to make a ”smarter” decision.</S>\n    <S sid=\"132\" ssid=\"43\">First, we create links only for those English words that appear exactly once; next, for words that appear more than once, we choose which instance to link with so that we minimize the number of crossings with already existing links.</S>\n    <S sid=\"133\" ssid=\"44\">The second heuristic attempts to improve the choice of the most likely English translation of a foreign word.</S>\n    <S sid=\"134\" ssid=\"45\">Our translation probabilities are automatically learned from parallel data, and we learn values for both t(fj|ei) and t(ei|fj).</S>\n    <S sid=\"135\" ssid=\"46\">We can therefore decide that the most likely English translation of fj is argmaxei{t(fj|ei),t(ei|fj)}.</S>\n    <S sid=\"136\" ssid=\"47\">Using both sets of probabilities is likely to help us make a better-informed decision.</S>\n    <S sid=\"137\" ssid=\"48\">Using this alignment strategy, we follow (Och and Ney 2003) and compute one alignment for each translation direction (f - 4e and e -4 f), and then combine them.</S>\n    <S sid=\"138\" ssid=\"49\">Och and Ney present three combination methods: intersection, union, and refined (a form of intersection expanded with certain additional neighboring links).</S>\n    <S sid=\"139\" ssid=\"50\">Thus, for each sentence pair, we compute five alignments (two modified-IBMModel-1 plus three combinations) and then extract one set of general features and five sets of alignment features (as described in the previous section).</S>\n    <S sid=\"140\" ssid=\"51\">We create training instances for our classifier from a small parallel corpus.</S>\n    <S sid=\"141\" ssid=\"52\">The simplest way to obtain classifier training data from a parallel corpus is to generate all possible sentence pairs from the corpus (the Cartesian product).</S>\n    <S sid=\"142\" ssid=\"53\">This generates 5,0002 training instances, out of which 5,000 are positive (i.e., belong to class ”parallel”) and the rest are negative.</S>\n    <S sid=\"143\" ssid=\"54\">One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones.</S>\n    <S sid=\"144\" ssid=\"55\">Classifiers trained on such data do not achieve good performance; they generally tend to predict the majority class, i.e., classify most sentences as non-parallel (which has indeed been the case in our experiments).</S>\n    <S sid=\"145\" ssid=\"56\">Our solution to this is to downsample, i.e., eliminate a number of (randomly selected) negative instances.</S>\n    <S sid=\"146\" ssid=\"57\">Another problem is that the large majority of sentence pairs in the Cartesian product have low word overlap (i.e., few words that are translations of each other).</S>\n    <S sid=\"147\" ssid=\"58\">As explained in Section 2 (and shown in Figure 2), when extracting data from a comparable corpus, we only apply the classifier on the output of the word-overlap filter.</S>\n    <S sid=\"148\" ssid=\"59\">Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.</S>\n    <S sid=\"149\" ssid=\"60\">We therefore use for training only those pairs from the Cartesian product that are accepted by the word-overlap filter.</S>\n    <S sid=\"150\" ssid=\"61\">This has the additional advantage that, since all these pairs have many words in common, the classifier learns to make distinctions that cannot be made based on word overlap alone.</S>\n    <S sid=\"151\" ssid=\"62\">To summarize, we prepare our classifier training set in the following manner: starting from a parallel corpus of about 5,000 sentence pairs, we generate all the sentence pairs in the Cartesian product; we discard the pairs that do not fulfill the conditions of the word-overlap filter; if the resulting set is imbalanced, i.e., the ratio of non-parallel to parallel pairs is greater than five, we balance it by removing randomly chosen nonparallel pairs.</S>\n    <S sid=\"152\" ssid=\"63\">We then compute word alignments and extract feature values.</S>\n    <S sid=\"153\" ssid=\"64\">Using the training set, we compute values for the classifier feature weights using the YASMET4 implementation of the GIS algorithm (Darroch and Ratcliff 1974).</S>\n    <S sid=\"154\" ssid=\"65\">Since we are dealing with few parameters and have sufficiently many training instances, using more advanced training algorithms is unlikely to bring significant improvements.</S>\n    <S sid=\"155\" ssid=\"66\">We test the performance of the classifier by generating test instances from a different parallel corpus (also around 5,000 sentence pairs) and checking how many of these instances are correctly classified.</S>\n    <S sid=\"156\" ssid=\"67\">We prepare the test set by creating the Cartesian product of the sentences in the test parallel corpus and applying the word-overlap filter (we do not perform any balancing).</S>\n    <S sid=\"157\" ssid=\"68\">Although we apply the filter, we still conceptually classify all pairs from the Cartesian product in a two-stage classification process: all pairs discarded by the filter are classified as ”non-parallel,” and for the rest, we obtain predictions from the classifier.</S>\n    <S sid=\"158\" ssid=\"69\">Since this is how we apply the system on truly unseen data, this is the process in whose performance we are interested.</S>\n    <S sid=\"159\" ssid=\"70\">We measure the performance of the classification process by computing precision and recall.</S>\n    <S sid=\"160\" ssid=\"71\">Precision is the ratio of sentence pairs correctly judged as parallel to the total number of pairs judged as parallel by the classifier.</S>\n    <S sid=\"161\" ssid=\"72\">Recall is the ratio of sentence pairs correctly identified as parallel by the classifier to the total number of truly parallel pairs—i.e., the number of pairs in the parallel corpus used to generate the test instances.</S>\n    <S sid=\"162\" ssid=\"73\">Both numbers are expressed as percentages.</S>\n    <S sid=\"163\" ssid=\"74\">More formally: let classified parallel be the total number of sentence pairs from our test set that the classifier judged as parallel, classified well be the number of pairs that the classifier correctly judged as parallel, and true parallel be the total number of parallel pairs in the test set.</S>\n    <S sid=\"164\" ssid=\"75\">Then: classified parallel true parallel There are two factors that influence a classifier’s performance: dictionary coverage and similarity between the domains of the training and test instances.</S>\n    <S sid=\"165\" ssid=\"76\">We performed evaluation experiments to account for both these factors.</S>\n    <S sid=\"166\" ssid=\"77\">All our dictionaries are automatically learned from parallel data; thus, we can create dictionaries of various coverage by learning them from parallel corpora of different sizes.</S>\n    <S sid=\"167\" ssid=\"78\">We use five dictionaries, learned from five initial out-of-domain parallel corpora, whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English side.</S>\n    <S sid=\"168\" ssid=\"79\">Since we want to use the classifier to extract sentence pairs from our in-domain comparable corpus, we test it on instances generated from an in-domain parallel corpus.</S>\n    <S sid=\"169\" ssid=\"80\">In order to measure the effect of the domain difference, we use two training sets: one generated from an in-domain parallel corpus and another one from an out-ofdomain parallel corpus.</S>\n    <S sid=\"170\" ssid=\"81\">In summary, for each language pair, we use the following corpora: Precision and recall of the Arabic-English classifiers.</S>\n    <S sid=\"171\" ssid=\"82\">From each initial, out-of-domain corpus, we learn a dictionary.</S>\n    <S sid=\"172\" ssid=\"83\">We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances.</S>\n    <S sid=\"173\" ssid=\"84\">We train two classifiers (one on each training set) and evaluate both of them on the test set.</S>\n    <S sid=\"174\" ssid=\"85\">The parallel corpora used for generating training and test instances have around 5k sentence pairs each (approximately 150k English tokens), and generate around 10k training instances (for each training set) and 8k test instances.</S>\n    <S sid=\"175\" ssid=\"86\">Precision and recall of the Chinese-English classifiers.</S>\n    <S sid=\"176\" ssid=\"87\">Figures 5 and 6 show the recall and precision of our classifiers, for both ArabicEnglish and Chinese-English.</S>\n    <S sid=\"177\" ssid=\"88\">The results show that the precision of our classification process is robust with respect to dictionary coverage and training domain.</S>\n    <S sid=\"178\" ssid=\"89\">Even when starting from a very small initial parallel corpus, we can build a high-precision classifier.</S>\n    <S sid=\"179\" ssid=\"90\">Having a good dictionary and training data from the right domain does help though, mainly with respect to recall.</S>\n    <S sid=\"180\" ssid=\"91\">The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments.</S>\n    <S sid=\"181\" ssid=\"92\">The low recall results are partly due to the word-overlap filter (the first stage of the classification process), which discards many parallel pairs.</S>\n    <S sid=\"182\" ssid=\"93\">If we don’t apply the filter before the classifier, the recall results increase by about 20% (with no loss in precision).</S>\n    <S sid=\"183\" ssid=\"94\">However, the filter plays a very important role in keeping the extraction pipeline robust and efficient (as shown in Figure 7, the filter discards 99% of the candidate pairs), so this loss of recall is a price worth paying.</S>\n    <S sid=\"184\" ssid=\"95\">Classifier evaluations using different subsets of features show that most of the classifier performance comes from the general features together with the alignment features concerning the percentage and number of words that have no connection.</S>\n    <S sid=\"185\" ssid=\"96\">However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no The amounts of data processed by our system during extraction from the Chinese-English comparable corpus. longer be accounted for only by counting the linked words; thus, the other features should become more important.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"186\" ssid=\"1\">The comparable corpora that we use for parallel sentence extraction are collections of news stories published by the Agence France Presse and Xinhua News agencies.</S>\n    <S sid=\"187\" ssid=\"2\">They are parts of the Arabic, English, and Chinese Gigaword corpora which are available from the Linguistic Data Consortium.</S>\n    <S sid=\"188\" ssid=\"3\">From these collections, for each language pair, we create an in-domain comparable corpus by putting together articles coming from the same agency and the same time period.</S>\n    <S sid=\"189\" ssid=\"4\">Table 1 presents in detail the sources and sizes of the resulting comparable corpora.</S>\n    <S sid=\"190\" ssid=\"5\">The remainder of the section presents the various data sets that we extracted automatically from these corpora, under various experimental conditions.</S>\n    <S sid=\"191\" ssid=\"6\">In the experiments described in Section 3.4, we started out with five out-of-domain initial parallel corpora of various sizes and obtained five dictionaries and five out-ofdomain trained classifiers (per language pair).</S>\n    <S sid=\"192\" ssid=\"7\">We now plug in each of these classifiers (and their associated dictionaries) in our extraction system (Section 2) and apply it to our comparable corpora.</S>\n    <S sid=\"193\" ssid=\"8\">We thus obtain five Arabic-English and five Chinese-English extracted corpora.</S>\n    <S sid=\"194\" ssid=\"9\">Note that in each of these experiments the only resource used by our system is the initial, out-of-domain parallel corpus.</S>\n    <S sid=\"195\" ssid=\"10\">Thus, the experiments fit in the framework of interest described in Section 1, which assumes the availability of (limited amounts of) out-of-domain training data and (large amounts of) in-domain comparable data.</S>\n    <S sid=\"196\" ssid=\"11\">Table 2 shows the sizes of the extracted corpora for each initial corpus size, for both Chinese-English and Arabic-English.</S>\n    <S sid=\"197\" ssid=\"12\">As can be seen, when the initial parallel corpus is very small, the amount of extracted data is also quite small.</S>\n    <S sid=\"198\" ssid=\"13\">This is due to the low coverage of the dictionary learned from that corpus.</S>\n    <S sid=\"199\" ssid=\"14\">Our candidate pair selection step (Section 2.2) discards pairs with too many unknown (or unrelated) words, according to the dictionary; thus, only few sentences fulfill the word-overlap condition of our filter.</S>\n    <S sid=\"200\" ssid=\"15\">As mentioned in Section 1, our goal is to use the extracted data as additional MT training data and obtain better translation performance on a given in-domain MT test set.</S>\n    <S sid=\"201\" ssid=\"16\">A simple way of estimating the usefulness of the data for this purpose is to measure its coverage of the test set, i.e., the percentage of running n-grams from the test corpus that are also in our corpus.</S>\n    <S sid=\"202\" ssid=\"17\">Tables 3 and 4 present the coverage of our extracted corpora.</S>\n    <S sid=\"203\" ssid=\"18\">For each initial corpus size, the first column shows the coverage of that initial corpus, and the second column shows the coverage of the initial corpus plus the extracted corpus.</S>\n    <S sid=\"204\" ssid=\"19\">Each cell contains four numbers that represent the coverage with respect to unigrams, bigrams, trigrams, and 4-grams.</S>\n    <S sid=\"205\" ssid=\"20\">The numbers show that unigram coverage depends only on the size of the corpus (and not on the domain), but for longer n-grams, our in-domain extracted data brings significant improvements in coverage.</S>\n    <S sid=\"206\" ssid=\"21\">The extraction experiments from the previous section are controlled experiments in which we only use limited amounts of parallel data for our extraction system.</S>\n    <S sid=\"207\" ssid=\"22\">In this section, we describe experiments in which the goal is to assess the applicability of our method to data that we mined from the Web.</S>\n    <S sid=\"208\" ssid=\"23\">We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently.</S>\n    <S sid=\"209\" ssid=\"24\">In order to get as many articles as possible, we used the web site’s search engine to get lists of articles and their URLs, and then crawled those lists.</S>\n    <S sid=\"210\" ssid=\"25\">We used the AgentBuilder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling.</S>\n    <S sid=\"211\" ssid=\"26\">The tool can be programmed to automatically initiate searches with different parameters and to identify and extract the desired article URLs (as well as other information such as dates and titles) from the result pages.</S>\n    <S sid=\"212\" ssid=\"27\">Table 5 shows the sources, time periods, and size of the datasets that we downloaded.</S>\n    <S sid=\"213\" ssid=\"28\">For the extraction experiments, we used dictionaries of high coverage, learned from all our available parallel training data.</S>\n    <S sid=\"214\" ssid=\"29\">The sizes of these training corpora, measured in number of English tokens, are as follows: We applied our extraction method on both the LDC-released Gigaword corpora and the Web-downloaded comparable corpora.</S>\n    <S sid=\"215\" ssid=\"30\">For each language pair, we used the highest precision classifier from those presented in Section 3.4.</S>\n    <S sid=\"216\" ssid=\"31\">In order to obtain data of higher quality, we didn’t use all the sentences classified as parallel, but only those for which the probability computed by our classifier was higher than 0.70.</S>\n    <S sid=\"217\" ssid=\"32\">Table 6 shows the amounts of extracted data, measured in number of English tokens.</S>\n    <S sid=\"218\" ssid=\"33\">For ArabicEnglish, we were able to extract from the Gigaword corpora much more data than in our previous experiments (see Table 2), clearly due to the better dictionary.</S>\n    <S sid=\"219\" ssid=\"34\">For ChineseEnglish, there was no increase in the size of extracted data (although the amount from Table 6 is smaller than that from Table 2, it counts only sentence pairs extracted with confidence higher than 0.70).</S>\n    <S sid=\"220\" ssid=\"35\">In the previous section, we measured, for our training corpora, their coverage of the test set (Tables 3 and 4).</S>\n    <S sid=\"221\" ssid=\"36\">We repeated the measurements for the training data from Table 6 and obtained very similar results: using the additional extracted data improves coverage, especially for longer n-grams.</S>\n    <S sid=\"222\" ssid=\"37\">To give the reader an idea of the amount of data that is funneled through our system, we show in Figure 7 the sizes of the data processed by each of the system’s components during extraction from the Gigaword and Web-based Chinese-English comparable corpora.</S>\n    <S sid=\"223\" ssid=\"38\">We use a dictionary learned from a parallel corpus on 190M English tokens and a classifier trained on instances generated from a parallel corpus of 220k English tokens.</S>\n    <S sid=\"224\" ssid=\"39\">We start with a comparable corpus consisting of 500k Chinese articles and 600k English articles.</S>\n    <S sid=\"225\" ssid=\"40\">The article selection step (Section 2.1) outputs 7.5M similar article pairs; from each article pair we generate all possible sentence pairs and obtain 2,400M pairs.</S>\n    <S sid=\"226\" ssid=\"41\">Of these, less than 1% (17M) pass the candidate selection stage (Section 2.2) and are presented to the ME classifier.</S>\n    <S sid=\"227\" ssid=\"42\">The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7).</S>\n    <S sid=\"228\" ssid=\"43\">The figure also presents, in the lower part, the parameters that control the filtering at each stage. the particular sentence pair to be parallel; the higher the value, the higher the classifier’s confidence.</S>\n    <S sid=\"229\" ssid=\"44\">Thus, in order to obtain higher precision, we can choose to define as parallel only those pairs for which the classifier probability is above a certain threshold.</S>\n    <S sid=\"230\" ssid=\"45\">In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7.</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"231\" ssid=\"1\">Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system.</S>\n    <S sid=\"232\" ssid=\"2\">Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance.</S>\n    <S sid=\"233\" ssid=\"3\">We first evaluate the extracted corpora presented in Section 4.1.</S>\n    <S sid=\"234\" ssid=\"4\">The extraction system used to obtain each of those corpora made use of a certain initial out-of-domain parallel corpus.</S>\n    <S sid=\"235\" ssid=\"5\">We train a Baseline MT system on that initial corpus.</S>\n    <S sid=\"236\" ssid=\"6\">We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus.</S>\n    <S sid=\"237\" ssid=\"7\">In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data.</S>\n    <S sid=\"238\" ssid=\"8\">For each initial corpus, we use the same amount of human-translated data as there is extracted data (see Table 2).</S>\n    <S sid=\"239\" ssid=\"9\">Thus, for each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline, PlusExtracted, and UpperBound.</S>\n    <S sid=\"240\" ssid=\"10\">All our MT systems were trained using a variant of the alignment template model described in (Och 2003).</S>\n    <S sid=\"241\" ssid=\"11\">Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system.</S>\n    <S sid=\"242\" ssid=\"12\">This ensured that any differences in performance are caused only by differences in the training data.</S>\n    <S sid=\"243\" ssid=\"13\">The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations.</S>\n    <S sid=\"244\" ssid=\"14\">Figures 8 and 9 show the BLEU scores obtained by our MT systems.</S>\n    <S sid=\"245\" ssid=\"15\">The 95% confidence intervals of the scores computed by bootstrap resampling (Koehn 2004) are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for Chinese-English.</S>\n    <S sid=\"246\" ssid=\"16\">As the results show, the automatically extracted additional training data yields significant improvements in performance over most initial training corpora for both language pairs.</S>\n    <S sid=\"247\" ssid=\"17\">At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data.</S>\n    <S sid=\"248\" ssid=\"18\">And, as can be expected, the impact of the extracted data decreases as the size of the initial corpus increases.</S>\n    <S sid=\"249\" ssid=\"19\">In order to check that the classifier really does something important, we performed a few experiments without it.</S>\n    <S sid=\"250\" ssid=\"20\">After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus.</S>\n    <S sid=\"251\" ssid=\"21\">The resulting BLEU scores were practically the same as the baseline; thus, our classifier does indeed help to discover higher-quality parallel data.</S>\n    <S sid=\"252\" ssid=\"22\">We also measured the MT performance impact of the extracted corpora described in Section 4.2.</S>\n    <S sid=\"253\" ssid=\"23\">We trained a Baseline MT system on all our available (in-domain and MT performance improvements for Arabic-English. out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the extracted in-domain data.</S>\n    <S sid=\"254\" ssid=\"24\">Clearly, we have access to no UpperBound system in this case.</S>\n    <S sid=\"255\" ssid=\"25\">The results are presented in the first two rows of Table 7.</S>\n    <S sid=\"256\" ssid=\"26\">Adding the extracted corpus lowers the score for the Arabic-English system and improves the score for the Chinese-English one; however, none of the differences are statistically significant.</S>\n    <S sid=\"257\" ssid=\"27\">Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact.</S>\n    <S sid=\"258\" ssid=\"28\">In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data.</S>\n    <S sid=\"259\" ssid=\"29\">The BLEU scores obtained by the systems we trained on them are presented in the third row of Table 7.</S>\n    <S sid=\"260\" ssid=\"30\">For comparison purposes, the last line of the table shows the scores of systems trained on 10M English tokens of outof-domain data.</S>\n    <S sid=\"261\" ssid=\"31\">As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size.</S>\n    <S sid=\"262\" ssid=\"32\">It’s true that this is not a fair comparison, since the extracted corpora were obtained using all our available parallel data.</S>\n    <S sid=\"263\" ssid=\"33\">The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"264\" ssid=\"1\">As can be seen from Table 2, the amount of data we can extract from our comparable corpora is adversely affected by poor dictionary coverage.</S>\n    <S sid=\"265\" ssid=\"2\">Thus, if we start with very little parallel data, we do not make good use of the comparable corpora.</S>\n    <S sid=\"266\" ssid=\"3\">One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again.</S>\n    <S sid=\"267\" ssid=\"4\">Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004).</S>\n    <S sid=\"268\" ssid=\"5\">We performed bootstrapping iterations starting from two very small corpora: 100k English tokens and 1M English tokens, respectively.</S>\n    <S sid=\"269\" ssid=\"6\">After each iteration, we trained MT performance improvements for Chinese-English.</S>\n    <S sid=\"270\" ssid=\"7\">(and evaluated) an MT system on the initial data plus the data extracted in that iteration.</S>\n    <S sid=\"271\" ssid=\"8\">We did not use any of the data extracted in previous iterations since it is mostly a subset of that extracted in the current iteration.</S>\n    <S sid=\"272\" ssid=\"9\">We iterated until there were no further improvements in MT performance on our development data.</S>\n    <S sid=\"273\" ssid=\"10\">Figures 10 and 11 show the sizes of the data extracted at each iteration, for both initial corpus sizes.</S>\n    <S sid=\"274\" ssid=\"11\">Iteration 0 is the one that uses the dictionary learned from the initial corpus.</S>\n    <S sid=\"275\" ssid=\"12\">Starting with 100k words of parallel data, we eventually collect 20M words of in-domain Arabic-English data and 90M words of in-domain Chinese-English data.</S>\n    <S sid=\"276\" ssid=\"13\">Figures 12 and 13 show the BLEU scores of these MT systems.</S>\n    <S sid=\"277\" ssid=\"14\">For comparison purposes, we also plotted on each graph the performance of our best MT system for that language pair, trained on all our available parallel data (Table 7).</S>\n    <S sid=\"278\" ssid=\"15\">As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores.</S>\n    <S sid=\"279\" ssid=\"16\">Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7–10 BLEU points of systems trained on parallel corpora of more than 100M English tokens.</S>\n    <S sid=\"280\" ssid=\"17\">This shows that using our method, a good-quality MT system can be built from very little parallel data and a large amount of comparable, non-parallel data.</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"281\" ssid=\"1\">We conclude the description of our method by presenting a few sentence pairs extracted by our system.</S>\n    <S sid=\"282\" ssid=\"2\">We chose the examples by looking for cases when a given foreign sentence was judged parallel to several different English sentences.</S>\n    <S sid=\"283\" ssid=\"3\">Figures 14 and 15 show the foreign sentence in Arabic and Chinese, respectively, followed by a human-produced translation in bold italic font, followed by the automatically extracted matching English sentences in normal font.</S>\n    <S sid=\"284\" ssid=\"4\">The sentences are picked from the data sets presented in Section 4.2.</S>\n    <S sid=\"285\" ssid=\"5\">The examples reveal the two main types of errors that our system makes.</S>\n    <S sid=\"286\" ssid=\"6\">The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7.</S>\n    <S sid=\"287\" ssid=\"7\">The second concerns pairs in which the two sentences convey different amounts of information.</S>\n    <S sid=\"288\" ssid=\"8\">In such pairs, one of the sentences contains a transSizes of the Chinese-English corpora extracted using bootstrapping, in millions of English tokens.</S>\n    <S sid=\"289\" ssid=\"9\">BLEU scores of the Arabic-English MT systems using bootstrapping. lation of the other, plus additional (often quite long) phrases (Figure 15, examples 1 and 5).</S>\n    <S sid=\"290\" ssid=\"10\">These errors are caused by the noise present in the automatically learned dictionaries and by the use of a weak word alignment model for extracting the classifier BLEU scores of the Chinese-English MT systems using bootstrapping. features.</S>\n    <S sid=\"291\" ssid=\"11\">In an automatically learned dictionary, many words (especially the frequent, non-content ones) will have a lot of spurious translations.</S>\n    <S sid=\"292\" ssid=\"12\">The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words.</S>\n    <S sid=\"293\" ssid=\"13\">Alignments computed using this model and a noisy, automatically learned, dictionary will contain many incorrect links.</S>\n    <S sid=\"294\" ssid=\"14\">Thus, if two sentences share several content words, these incorrect links together with the correct links between the common content words will yield an alignment good enough to make the classifier judge the sentence pair as parallel.</S>\n    <S sid=\"295\" ssid=\"15\">The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6.</S>\n    <S sid=\"296\" ssid=\"16\">The sentences in that example are tables of soccer team statistics.</S>\n    <S sid=\"297\" ssid=\"17\">They are judged parallel because corresponding digits align to each other, and according to our dictionary, the Arabic word for “Mexico” can be translated as any of the country names listed in the example.</S>\n    <S sid=\"298\" ssid=\"18\">These examples also show that the problem of finding only true translation pairs is hard.</S>\n    <S sid=\"299\" ssid=\"19\">Two sentences may share many content words and yet express different meanings (see Figure 14, example 1).</S>\n    <S sid=\"300\" ssid=\"20\">However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance.</S>\n  </SECTION>\n  <SECTION title=\"8.\" number=\"9\">\n    <S sid=\"301\" ssid=\"1\">While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004).</S>\n    <S sid=\"302\" ssid=\"2\">We are aware of only three previous efforts aimed at discovering parallel sentences.</S>\n    <S sid=\"303\" ssid=\"3\">Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus.</S>\n    <S sid=\"304\" ssid=\"4\">Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus.</S>\n    <S sid=\"305\" ssid=\"5\">Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stories from different sources and time periods.</S>\n    <S sid=\"306\" ssid=\"6\">The first two systems extend algorithms designed to perform sentence alignment of parallel texts.</S>\n    <S sid=\"307\" ssid=\"7\">They start by attempting to identify similar article pairs from the two corpora.</S>\n    <S sid=\"308\" ssid=\"8\">Then they treat each of those pairs as parallel texts and align their sentences by defining a sentence pair similarity score and use dynamic programming to find the least-cost alignment over the whole document pair.</S>\n    <S sid=\"309\" ssid=\"9\">In the article pair selection stage, the researchers try to identify, for an article in one language, the best matching article in the other language.</S>\n    <S sid=\"310\" ssid=\"10\">Zhao and Vogel (2002) measure article similarity by defining a generative model in which an English story generates a Chinese story with a given probability.</S>\n    <S sid=\"311\" ssid=\"11\">Utiyama et al. (2003) use the BM25 (Robertson and Walker 1994) similarity measure.</S>\n    <S sid=\"312\" ssid=\"12\">The two works also differ in the way they define the sentence similarity score.</S>\n    <S sid=\"313\" ssid=\"13\">Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model.</S>\n    <S sid=\"314\" ssid=\"14\">Utiyama et al. (2003) define a score based on word overlap (i.e., number of word pairs from the two sentences that are translations of each other), which also includes the similarity score of the article pair from which the sentence pair originates.</S>\n    <S sid=\"315\" ssid=\"15\">The performance of these approaches depends heavily on the ability to reliably find similar document pairs.</S>\n    <S sid=\"316\" ssid=\"16\">Moreover, comparable article pairs, even those similar in content, may exhibit great differences at the sentence level (reorderings, additions, etc).</S>\n    <S sid=\"317\" ssid=\"17\">Therefore, they pose hard problems for the dynamic programming alignment approach.</S>\n    <S sid=\"318\" ssid=\"18\">In contrast, our method is more robust.</S>\n    <S sid=\"319\" ssid=\"19\">The document pair selection part plays a minor role; it only acts as a filter.</S>\n    <S sid=\"320\" ssid=\"20\">We do not attempt to find the best-matching English document for each foreign one, but rather a set of similar documents.</S>\n    <S sid=\"321\" ssid=\"21\">And, most importantly, we are able to reliably judge each sentence pair in isolation, without need for context.</S>\n    <S sid=\"322\" ssid=\"22\">On the other hand, the dynamic programming approach enables discovery of many-to-one sentence alignments, whereas our method is limited to finding one-toone alignments.</S>\n    <S sid=\"323\" ssid=\"23\">The approach of Fung and Cheung (2004) is a simpler version of ours.</S>\n    <S sid=\"324\" ssid=\"24\">They match each foreign document with a set of English documents, using a threshold on their cosine similarity.</S>\n    <S sid=\"325\" ssid=\"25\">Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel.</S>\n    <S sid=\"326\" ssid=\"26\">Using the set of extracted sentences, they learn a new dictionary, try to extend their set of matching document pairs (by looking for other documents that contain these sentences), and iterate.</S>\n    <S sid=\"327\" ssid=\"27\">The evaluation methodologies of these previous approaches are less direct than ours.</S>\n    <S sid=\"328\" ssid=\"28\">Utiyama et al. (2003) evaluate their sentence pairs manually; they estimate that about 90% of the sentence pairs in their final corpus are parallel.</S>\n    <S sid=\"329\" ssid=\"29\">Fung and Cheung (2004) also perform a manual evaluation of the extracted sentences and estimate their precision to be 65.7% after bootstrapping.</S>\n    <S sid=\"330\" ssid=\"30\">In addition, they also estimate the quality of a lexicon automatically learned from those sentences.</S>\n    <S sid=\"331\" ssid=\"31\">Zhao and Vogel (2002) go one step further and show that the sentences extracted with their method improve the accuracy of automatically computed word alignments, to an F-score of 52.56% over a baseline of 46.46%.</S>\n    <S sid=\"332\" ssid=\"32\">In a subsequent publication, Vogel (2003) evaluates these sentences in the context of an MT system and shows that they bring improvement under special circumstances (i.e., a language model constructed from reference translations) designed to reduce the noise introduced by the automatically extracted corpus.</S>\n    <S sid=\"333\" ssid=\"33\">We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing.</S>\n    <S sid=\"334\" ssid=\"34\">Moreover, we show that our approach works even when only a limited amount of initial parallel data (i.e., a low-coverage dictionary) is available.</S>\n    <S sid=\"335\" ssid=\"35\">The problem of aligning sentences in comparable corpora was also addressed for monolingual texts.</S>\n    <S sid=\"336\" ssid=\"36\">Barzilay and Elhadad (2003) present a method of aligning sentences in two comparable English corpora for the purpose of building a training set of text-totext rewriting examples.</S>\n    <S sid=\"337\" ssid=\"37\">Monolingual parallel sentence detection presents a particular challenge: there are many sentence pairs that have low lexical overlap but are nevertheless parallel.</S>\n    <S sid=\"338\" ssid=\"38\">Therefore pairs cannot be judged in isolation, and context becomes an important factor.</S>\n    <S sid=\"339\" ssid=\"39\">Barzilay and Elhadad (2003) make use of contextual information by detecting the topical structure of the articles in the two corpora and aligning them at paragraph level based on the topic assigned to each paragraph.</S>\n    <S sid=\"340\" ssid=\"40\">Afterwards, they proceed and align sentences within paragraph pairs using dynamic programming.</S>\n    <S sid=\"341\" ssid=\"41\">Their results show that both the induced topical structure and the paragraph alignment improve the precision of their extraction method.</S>\n    <S sid=\"342\" ssid=\"42\">A line of research that is both complementary and related to ours is that of Resnik and Smith (2003).</S>\n    <S sid=\"343\" ssid=\"43\">Their STRAND Web-mining system has a purpose that is similar to ours: to identify translational pairs.</S>\n    <S sid=\"344\" ssid=\"44\">However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.</S>\n    <S sid=\"345\" ssid=\"45\">Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs.</S>\n    <S sid=\"346\" ssid=\"46\">Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method.</S>\n  </SECTION>\n  <SECTION title=\"9.\" number=\"10\">\n    <S sid=\"347\" ssid=\"1\">The most important feature of our parallel sentence selection approach is its robustness.</S>\n    <S sid=\"348\" ssid=\"2\">Comparable corpora are inherently noisy environments, where even similar content may be expressed in very different ways.</S>\n    <S sid=\"349\" ssid=\"3\">Moreover, out-of-domain corpora introduce additional difficulties related to limited dictionary coverage.</S>\n    <S sid=\"350\" ssid=\"4\">Therefore, the ability to reliably judge sentence pairs in isolation is crucial.</S>\n    <S sid=\"351\" ssid=\"5\">Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms.</S>\n    <S sid=\"352\" ssid=\"6\">The computational processes involved in our system are quite modest.</S>\n    <S sid=\"353\" ssid=\"7\">All the operations necessary for the classification of a sentence pair (filter, word alignment computation, and feature extraction) can be implemented efficiently and scaled up to very large amounts of data.</S>\n    <S sid=\"354\" ssid=\"8\">The task can be easily parallelized for increased speed.</S>\n    <S sid=\"355\" ssid=\"9\">For example, extracting data from 600k English documents and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors.</S>\n    <S sid=\"356\" ssid=\"10\">The data that we extract is useful.</S>\n    <S sid=\"357\" ssid=\"11\">Its impact on MT performance is comparable to that of human-translated data of similar size and domain.</S>\n    <S sid=\"358\" ssid=\"12\">Thus, although we have focused our experiments on the particular scenario where there is little in-domain training data available, we believe that our method can be useful for increasing the amount of training data, regardless of the domain of interest.</S>\n    <S sid=\"359\" ssid=\"13\">As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available.</S>\n    <S sid=\"360\" ssid=\"14\">By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system.</S>\n    <S sid=\"361\" ssid=\"15\">We suspect our approach can be used on comparable corpora coming from any domain.</S>\n    <S sid=\"362\" ssid=\"16\">The only domain-dependent element of the system is the date window parameter of the article selection stage (Figure 7); for other domains, this can be replaced with a more appropriate indication of where the parallel sentences are likely to be found.</S>\n    <S sid=\"363\" ssid=\"17\">For example, if the domain were that of technical manuals, one would cluster printer manuals and aircraft manuals separately.</S>\n    <S sid=\"364\" ssid=\"18\">It is important to note that our work assumes that the comparable corpus does contain parallel sentences (which is the case for our data).</S>\n    <S sid=\"365\" ssid=\"19\">Whether this is true for comparable corpora from other domains is an empirical question outside the scope of this article; however, both our results and those of Resnik and Smith (2003) strongly indicate that good data is available on the Web.</S>\n    <S sid=\"366\" ssid=\"20\">Lack of parallel corpora is a major bottleneck in the development of SMT systems for most language pairs.</S>\n    <S sid=\"367\" ssid=\"21\">The method presented in this paper is a step towards the important goal of automatic acquisition of such corpora.</S>\n    <S sid=\"368\" ssid=\"22\">Comparable texts are available on the Web in large quantities for many language pairs and domains.</S>\n    <S sid=\"369\" ssid=\"23\">In this article, we have shown how they can be efficiently mined for parallel sentences.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"11\">\n    <S sid=\"370\" ssid=\"1\">This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276.</S>\n    <S sid=\"371\" ssid=\"2\">The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc).</S>\n    <S sid=\"372\" ssid=\"3\">We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments.</S>\n    <S sid=\"373\" ssid=\"4\">Any remaining errors are of course our own.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "bd5d2ad5547cf25c8a707f177e6597cc",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Improving Machine Translation Performance By Exploiting Non-Parallel Corpora"
                }
            }
        },
        {
            "index": 18,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach.",
                    "filename": "CogNIAC_High_Precision_Coreference_With_Limited_Knowledge_And_Linguistic_Resources",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W97-1306/Documents_xml/W97-1306.xml",
                    "title": "CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents CogNIAC, a pronoun resolution engine designed to resolve pronouns with limited knowledge and linguistic resources. CogNIAC operates on the assumption that certain types of anaphora do not require general purpose reasoning but only specific types of information, such as gender, number, and parse trees. The system focuses on high precision resolution and avoids guesswork in ambiguous contexts. The paper evaluates CogNIAC's performance and discusses its potential applications in information retrieval, language learning, and information extraction. While the system demonstrates good precision and reasonable recall, it also highlights the need for better world knowledge sources to improve recall. The study concludes by discussing the future developments and challenges in improving CogNIAC's capabilities for resolving various types of coreference, including split antecedents and event references.",
                        "This paper presents CogNIAC, a pronoun resolution engine designed to resolve pronouns with limited world knowledge and linguistic resources. CogNIAC uses a set of high precision rules which mimic strategies used by humans when resolving pronouns, such as the use of syntax to constrain reflexive pronouns' antecedents. The system's performance in experiments demonstrates that it can achieve respectable recall with very high precision for some kinds of pronominal resolution, even with minimal world knowledge. The paper discusses the potential applications of CogNIAC, including its use in information retrieval and its role in checking the coherence of pronoun use in text for language learners. While the system's recall limitations are due to insufficient filtering mechanisms on candidate antecedents, the paper suggests that better world knowledge sources could improve the system's recall. Overall, the paper highlights the potential of CogNIAC as a high precision coreference resolution system in various domains.",
                        "This paper presents CogNIAC, a pronoun resolution engine designed to resolve pronominal anaphora with limited knowledge and linguistic resources. CogNIAC employs a set of high precision rules to mimic human strategies in resolving pronouns, based on syntactic constraints and regularities of English usage. The system is evaluated against the Naive Algorithm and shows comparable performance in resolving pronouns. The high precision core of CogNIAC demonstrates strong potential across varied domains, although there are limitations and challenges, such as ambiguity noticing and processing split antecedents, that require further exploration. The paper also discusses possible applications of high precision coreference, such as in information retrieval and information extraction, and explores the future development of CogNIAC, including the processing of split antecedents and event reference. Overall, CogNIAC represents a promising approach to pronominal resolution in variable knowledge contexts.",
                        "This paper presents CogNIAC, a pronoun resolution engine designed around the assumption that a sub-class of anaphora does not require general purpose reasoning. The system uses limited knowledge sources and linguistic resources to resolve pronouns with high precision, even in ambiguous contexts. CogNIAC consists of a set of high precision rules that are applied in a left-to-right manner to resolve pronouns. The paper also discusses the performance of CogNIAC in comparison to the Naive Algorithm and its potential applications in information retrieval, language learning, and information extraction. The system's limitations and future directions for improvement are also explored. Overall, CogNIAC demonstrates respectable recall and precision for certain kinds of pronominal resolution, highlighting the potential for its use in various domains."
                    ],
                    "filename": "cogniac_high_precision_coreferen-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 157 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources</S>\n  <SECTION title=\"1 Introduction:\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Pronoun resolution is one of the 'classic' computational linguistics problems.</S>\n    <S sid=\"8\" ssid=\"2\">It is also widely considered to be inherently an A.I. complete' task-meaning that resolution of pronouns requires full world knowledge and inference.</S>\n    <S sid=\"9\" ssid=\"3\">CogNIAC is a pronoun resolution engine designed around the assumption that there is a sub-class of anaphora that does not require general purpose reasoning.</S>\n    <S sid=\"10\" ssid=\"4\">The kinds of information CogNIAC does require includes: sentence detection, part-of-speech tagging, simple noun phrase recognition, basic semantic category information like, gender, number, and in one configuration, partial parse trees.</S>\n    <S sid=\"11\" ssid=\"5\">What distinguishes CogNIAC from algorithms that use similar sorts of information is that it will not resolve a pronoun in circumstances of ambiguity.</S>\n    <S sid=\"12\" ssid=\"6\">Crucially, ambiguity is a function of how much knowledge an understander has.</S>\n    <S sid=\"13\" ssid=\"7\">Since CogNIAC does not have as rich a representation of world knowledge as humans, it finds much more ambiguity in texts than humans do.</S>\n    <S sid=\"14\" ssid=\"8\">2 A path to high precision pronominal resolution-- avoid guesswork in ambiguous contexts: It is probably safe to say that few referring pronouns are conveyed without the speaker/writer having an antecedent in mind.</S>\n    <S sid=\"15\" ssid=\"9\">Ambiguity occurs when the perceiver cannot recover from the context what conveyer has in mind.</S>\n    <S sid=\"16\" ssid=\"10\">I have found myself uttering pronouns which the hearer has no chance of recovering the antecedent to because they are not attending to the same part of the external environment, &amp;quot;He sure looks familiar&amp;quot;, or in text I am so focused on the context of what I am writing that use a pronoun to refer to a highly salient concept for me, but the antecedent may completely evade a reader without my familiarity with the topic.</S>\n    <S sid=\"17\" ssid=\"11\">Of course it is possible to explicitly leave the reader hanging as in, &amp;quot;Earl and Dave were working together when suddenly he fell into the threshing machine.&amp;quot; Humans, unlike most coreference algorithms, notice such cases of ambiguity and can then ask for clarification or at least grumble about how we cannot climb into the writers head to figure out what they meant.</S>\n    <S sid=\"18\" ssid=\"12\">But in that grumble we have articulated the essence of the problem--we don't have sufficient knowledge to satisfy ourselves that an antecedent has been found.</S>\n    <S sid=\"19\" ssid=\"13\">Pronoun resolution systems have extremely limited knowledge sources, they cannot access a fraction of human common sense knowledge.</S>\n    <S sid=\"20\" ssid=\"14\">To appreciate this consider the following text with grammatical tags replacing words with pronouns and names left in place: The city council VERBGROUP the women NP CC they VB NN Mariana VBD PP Sarah TO VB herself PP DT MD NN Without lexical knowledge a human attempting to resolve the pronouns is in much the knowledge impoverished position of the typical coreference algorithm.</S>\n    <S sid=\"21\" ssid=\"15\">It is no surprise that texts with so little information provided in them tend to be more ambiguous than the texts in fleshed out form.</S>\n    <S sid=\"22\" ssid=\"16\">The conclusion to draw from this example is that the limiting factor in CogNIAC is knowledge sources, not an artificial restriction on domains or kinds of coreference.</S>\n    <S sid=\"23\" ssid=\"17\">This point will be resumed in the discussion section when what the consequences of fuller knowledge sources would be on CogNIAC.</S>\n    <S sid=\"24\" ssid=\"18\">For noun phrase anaphora, gathering semantically possible antecedents amounts to running all the noun phrases in a text through various databases for number and gender, and perhaps then a classifier that determines whether a noun phrase is a company, person or place'.</S>\n    <S sid=\"25\" ssid=\"19\">This set of candidate antecedents rarely has more than 5 members when some reasonable locality constraints are adhered to, and this set almost always contains the actual antecedent.</S>\n    <S sid=\"26\" ssid=\"20\">The remainder of the coreference resolution process amounts to picking the right entity from this set.</S>\n    <S sid=\"27\" ssid=\"21\">For the kinds of data considered here (narratives and newspaper articles) there is a rarely a need for general world knowledge in assembling the initial set of possible antecedents for pronouns.</S>\n    <S sid=\"28\" ssid=\"22\">This does not address the issue of inferred antecedents, event reference, discourse deixis and many other sorts of referring phenomenon which clearly require the use of world knowledge but are beyond the scope of this work.</S>\n    <S sid=\"29\" ssid=\"23\">As it happens, recognizing the possible antecedents of these pronouns is within the capabilities of current knowledge sources.</S>\n    <S sid=\"30\" ssid=\"24\">Better knowledge sources could be used to reduce the space of possible antecedents.</S>\n    <S sid=\"31\" ssid=\"25\">For example the well known [Winograd 19721 alternation: The city council refused to give the women a permit because they {feared/advocated} violence.</S>\n    <S sid=\"32\" ssid=\"26\">There are two semantically possible antecedents to they: The city council, and the women.</S>\n    <S sid=\"33\" ssid=\"27\">The problem is picking the correct one.</S>\n    <S sid=\"34\" ssid=\"28\">Dependent on verb choice, they strongly prefers one antecedent to the other.</S>\n    <S sid=\"35\" ssid=\"29\">Capturing this generalization requires a sophisticated theory of verb meaning as relates to pronoun resolution.</S>\n    <S sid=\"36\" ssid=\"30\">Speaking anecdotally, these kinds of resolutions happen quite often in text.</S>\n    <S sid=\"37\" ssid=\"31\">CogNIAC recognizes knowledge intensive coreference and does not attempt to resolve such instances.</S>\n    <S sid=\"38\" ssid=\"32\">I The named entity task at MUC-6 used a similar classification task and the best system performance was 96% precision/97% recall.</S>\n    <S sid=\"39\" ssid=\"33\">Fortunately not all instances of pronominal anaphora require world knowledge for successful resolution.</S>\n    <S sid=\"40\" ssid=\"34\">In lieu of full world knowledge, CogNIAC uses regularities of English usage in an attempt to mimic strategies used by humans when resolving pronouns.</S>\n    <S sid=\"41\" ssid=\"35\">For example, the syntax of a sentence highly constrains a reflexive pronoun's antecedent.</S>\n    <S sid=\"42\" ssid=\"36\">Also if there is just one possible antecedent in entire the prior discourse, then that entity is nearly always the correct antecedent.</S>\n    <S sid=\"43\" ssid=\"37\">CogNIAC consists of a set of such observations implemented in Perl.</S>\n    <S sid=\"44\" ssid=\"38\">CogNIAC has been used with a range of linguistic resources, ranging from scenarios where almost no linguistic processing of the text is done at all to partial parse trees being provided.</S>\n    <S sid=\"45\" ssid=\"39\">At the very least, there must be sufficient linguistic resources to recognize pronouns in the text and the space of candidate antecedents must be identified.</S>\n    <S sid=\"46\" ssid=\"40\">For the first experiment the text has been part of speech tagged and basal noun phrases have been identified with 11' (i.e. noun phrases that have no nested noun phrases) as shown below: [ Mariana/NNP ] motioned/VBD for/IN [ Sarah/NNP] to/TO seatNB [herself/PRP ] on/IN [ a/DT twoseater/NN lounge/NN ] In addition, finite clauses were identified (by hand for experiment 1) and various regular expressions are used to identify subjects, objects and what verbs take as arguments for the purposes of coreference restrictions.</S>\n    <S sid=\"47\" ssid=\"41\">With this level of linguistic annotation, nearly all the parts of CogNIAC can be used to resolve pronouns.</S>\n    <S sid=\"48\" ssid=\"42\">The core rules of CogNIAC are given below, with their performance on training data provided (200 pronouns of narrative text).</S>\n    <S sid=\"49\" ssid=\"43\">In addition, examples where the rules successfully apply have been provided for most of the rules with the relevant anaphors and antecedents in boldface.</S>\n    <S sid=\"50\" ssid=\"44\">The term 'possible antecedents' refers to the set of entities from the discourse that are compatible with an anaphor's gender, number and coreference restrictions (i.e. non-reflexive pronouns cannot corefer with the other arguments of its verb/preposition etc.)</S>\n    <S sid=\"51\" ssid=\"45\">.</S>\n    <S sid=\"52\" ssid=\"46\">Mariana motioned for Sarah to seat herself on a two-seater lounge. sentence, then pick i as the antecedent: 114 correct, and 2 incorrect.</S>\n    <S sid=\"53\" ssid=\"47\">Rupert Murdock's News Corp. confirmed his interest in buying back the ailing New York Post.</S>\n    <S sid=\"54\" ssid=\"48\">But analysts said that if he winds up bidding for the paper,.... possessive pronoun and there is a single exact string match i of the possessive in the prior sentence, then pick i as the antecedent: 4 correct, and 1 incorrect.</S>\n    <S sid=\"55\" ssid=\"49\">After he was dry, Joe carefully laid out the damp towel in front of his locker.</S>\n    <S sid=\"56\" ssid=\"50\">Travis went over to his locker, took out a towel and started to dry off.</S>\n    <S sid=\"57\" ssid=\"51\">5) Unique Current Sentence: If there is a single possible antecedent in the read-in portion of the current sentence, then pick i as the antecedent: 21 correct, and 1 incorrect.</S>\n    <S sid=\"58\" ssid=\"52\">Like a large bear, he sat motionlessly in the lounge in one of the faded armchairs, watching Constantin.</S>\n    <S sid=\"59\" ssid=\"53\">After a week Constantin tired of reading the old novels in the bottom shelf of the bookcase-somewhere among the gray well thumbed pages he had hoped to find a message from one of his predecessors</S>\n  </SECTION>\n  <SECTION title=\"6) Unique Subject/ Subject Pronoun:\" number=\"2\">\n    <S sid=\"60\" ssid=\"1\">If the subject of the prior sentence contains a single possible antecedent i, and the anaphor is the subject of the current sentence, then pick i as the antecedent: 11 correct, and 0 incorrect.</S>\n    <S sid=\"61\" ssid=\"2\">Besides, if he provoked Malek, uncertainties were introduced, of which there were already far too many.</S>\n    <S sid=\"62\" ssid=\"3\">He noticed the supervisor enter the lounge ...</S>\n    <S sid=\"63\" ssid=\"4\">The method of resolving pronouns within CogNIAC works as follows: Pronouns are resolved left-to-right in the text.</S>\n    <S sid=\"64\" ssid=\"5\">For each pronoun, the rules are applied in the presented order.</S>\n    <S sid=\"65\" ssid=\"6\">For a given rule, if an antecedent is found, then the appropriate annotations are made to the text and no more rules are tried for that pronoun, otherwise the next rule is tried.</S>\n    <S sid=\"66\" ssid=\"7\">If no rules resolve the pronoun, then it is left unresolved.</S>\n    <S sid=\"67\" ssid=\"8\">These rules are individually are high precision rules, and collectively they add up to reasonable recall.</S>\n    <S sid=\"68\" ssid=\"9\">The precision is 97% (121/125) and the recall is 60% (121/201) for 198 pronouns of training data.</S>\n  </SECTION>\n  <SECTION title=\"3 Evaluation:\" number=\"3\">\n    <S sid=\"69\" ssid=\"1\">The Naive Algorithm [Hobbs 1976] works by specifying a total order on noun phrases in the prior discourse and comparing each noun phrase against the selectional restrictions (i.e. gender, number) of the anaphor, and taking the antecedent to be the first one to satisfy them.</S>\n    <S sid=\"70\" ssid=\"2\">The specification of the ordering constitutes a traversal order of the syntax tree of the anaphors clause and from there to embedding clauses and prior clauses.</S>\n    <S sid=\"71\" ssid=\"3\">The Winograd sentences, with either verb, would yield the following ordering of possible antecedents: The city council &gt; the women The algorithm would resolve they to The city council.</S>\n    <S sid=\"72\" ssid=\"4\">This is incorrect on one choice of verb, but the algorithm does not integrate the verb information into the salience ranking.</S>\n    <S sid=\"73\" ssid=\"5\">In comparison, none of the six rules of CogNIAC would resolve the pronoun.</S>\n    <S sid=\"74\" ssid=\"6\">Rules have been tried that resolved a subject pronoun of a nested clause with the subject of the dominating clause, but no configuration has been found that yielded sufficient precision2.</S>\n    <S sid=\"75\" ssid=\"7\">Consequently, they is not resolved'.</S>\n    <S sid=\"76\" ssid=\"8\">The naive algorithm has some interesting properties.</S>\n    <S sid=\"77\" ssid=\"9\">First it models relative salience as relative depth in a search space.</S>\n    <S sid=\"78\" ssid=\"10\">For two candidate antecedents a and b, if a is encountered before b in the search space, then a is more salient than b.</S>\n    <S sid=\"79\" ssid=\"11\">Second, the relative saliency of all candidate antecedents is totally ordered, that is, for any two candidate antecedents a and b , a is more salient than b xor b is more salient than a.</S>\n    <S sid=\"80\" ssid=\"12\">2 In experiment 2, discussed below, the rule 'subject same clause' would resolve they to the city council, but it was added to the MUC-6 system without testing, and has shown itself to not be a high precision rule.</S>\n    <S sid=\"81\" ssid=\"13\">CogNIAC shares several features of the Naive Algorithm: circumstances of many possible antecedents, and will not resolve pronouns in such cases.</S>\n    <S sid=\"82\" ssid=\"14\">The Naive Algorithm has no means of noting ambiguity and will resolve a pronoun as long as there is at least one possible antecedent.</S>\n    <S sid=\"83\" ssid=\"15\">Perhaps the most convincing reason to endorse partially ordered salience rankings is that salience distinctions fade as the discourse moves on.</S>\n    <S sid=\"84\" ssid=\"16\">Earl was working with Ted the other day.</S>\n    <S sid=\"85\" ssid=\"17\">He fell into the threshing machine.</S>\n    <S sid=\"86\" ssid=\"18\">Earl was working with Ted the other day.</S>\n    <S sid=\"87\" ssid=\"19\">All of the sudden, the cows started making a ruckus.</S>\n    <S sid=\"88\" ssid=\"20\">The noise was unbelievable.</S>\n    <S sid=\"89\" ssid=\"21\">He fell into the threshing machine.</S>\n    <S sid=\"90\" ssid=\"22\">In the first example 'He' takes `Earl' as antecedent, which is what rule 6, Unique Subject/Subject Pronoun, would resolve the pronoun to.</S>\n    <S sid=\"91\" ssid=\"23\">However in the second example, the use of `He' is ambiguous--a distinction that existed before is now gone.</S>\n    <S sid=\"92\" ssid=\"24\">The Naive Algorithm would still maintain a salience distinction between 'Earl' and `Ted', where CogNIAC has no rule that makes a salience distinction between subject and object of a sentence which has two intervening sentences.</S>\n    <S sid=\"93\" ssid=\"25\">The closest rule would be Unique in Discourse, rule 1, which does not yield a unique antecedent.</S>\n    <S sid=\"94\" ssid=\"26\">CogNIAC has been evaluated in two different contexts.</S>\n    <S sid=\"95\" ssid=\"27\">The goal of the first experiment was to establish relative performance of CogNIAC to Hobbs' Naive Algorithm--a convenient benchmark that allows indirect comparison to other algorithms.</S>\n    <S sid=\"96\" ssid=\"28\">The second experiment reports results on Wall Street Journal data.</S>\n    <S sid=\"97\" ssid=\"29\">The chosen domain for comparison with Hobbs' Naive Algorithm was narrative texts about two persons of the same gender told from a third person perspective.</S>\n    <S sid=\"98\" ssid=\"30\">The motivation for this data was that we wanted to maximize the ambiguity of resolving pronouns.</S>\n    <S sid=\"99\" ssid=\"31\">Only singular third person pronouns were considered.</S>\n    <S sid=\"100\" ssid=\"32\">The text was pre-processed with a part-of-speech tagger over which basal noun phrases were delimited and finite clauses and their relative nesting were identified by machine.</S>\n    <S sid=\"101\" ssid=\"33\">This pre-processing was subjected to hand correction in order to make comparison with Hobbs as fair as possible since that was an entirely hand executed algorithm, but CogNIAC was otherwise machine run and scored.</S>\n    <S sid=\"102\" ssid=\"34\">Errors were not chained, i.e. in left-to-right processing of the text, earlier mistakes were corrected before processing the next noun phrase.</S>\n    <S sid=\"103\" ssid=\"35\">Since the Naive Algorithm resolves all pronouns, two lower precision rules were added to rules 1-6) for comparisons sake.</S>\n    <S sid=\"104\" ssid=\"36\">The rules are: The last two rules are lower precision than the first six, but perform well enough to merit their inclusion in a 'resolve all pronouns' configuration.</S>\n    <S sid=\"105\" ssid=\"37\">Rule 7 performed reasonably well with 77% precision in training (10/13 correct for 201 pronouns), and rule 8 performed with 65% precision in training (44/63 correct).</S>\n    <S sid=\"106\" ssid=\"38\">The first six rules each had a precision of greater than 90% for the training data with the exception of rule 4 which had a precision of 80% for 5 resolutions.</S>\n    <S sid=\"107\" ssid=\"39\">The summary performance of the Naive Algorithm and CogNIAC (including all 8 rules) for the first 100 or so pronouns in three narrative texts are: Results for 298 third person pronouns in text about two same gender people.</S>\n    <S sid=\"108\" ssid=\"40\">Since both the Naive Algorithm and the resolve all pronouns configuration of CogNIAC are required to resolve all pronouns, precision and recall figures are not appropriate.</S>\n    <S sid=\"109\" ssid=\"41\">Instead % correct figures are given.</S>\n    <S sid=\"110\" ssid=\"42\">The high precision version of CogNIAC is reported with recall (number correct/number of instances of coreference) and precision (number correct/number of guesses) measures.</S>\n    <S sid=\"111\" ssid=\"43\">The conclusion to draw from these results is: if forced to commit to all anaphors, CogNIAC performs comparably to the Naive Algorithm.</S>\n    <S sid=\"112\" ssid=\"44\">Lappin and Leass 3 Rule 7 is based on the primitives of Centering Theory (Grosz, Joshi and Weinstein '86).</S>\n    <S sid=\"113\" ssid=\"45\">The Cb of an utterance is the highest ranked NP (Ranking being: Subject &gt; All other NPs) from the prior finite clause realized anaphorically in the current finite clause.</S>\n    <S sid=\"114\" ssid=\"46\">Please see Baldwin '95 for a full discussion of the details of the rule.</S>\n    <S sid=\"115\" ssid=\"47\">1994 correctly resolved 86% of 360 pronouns in computer manuals.</S>\n    <S sid=\"116\" ssid=\"48\">Lapin and Leass run Hobbs' algorithm on the their data and the Naive Algorithm is correct 82% of the time--4% worse.</S>\n    <S sid=\"117\" ssid=\"49\">This allows indirect comparison with CogNIAC, with the suggestive conclusion that the resolve all pronouns configuration of CogNIAC, like the Naive Algorithm, is at least in the ballpark of more modern approaches&amp; The breakdown of the individual rules is as follows: Performance of individual rules in Experiment 1.</S>\n    <S sid=\"118\" ssid=\"50\">Note the high precision of rules 1 - 6).</S>\n    <S sid=\"119\" ssid=\"51\">Recall = #correct/#actual, Precision = #correct/#guessed Far more interesting to consider is the performance of the high precision rules 1 through 6.</S>\n    <S sid=\"120\" ssid=\"52\">The first four rules perform quite well at 96% precision (148/154) and 50% recall (148/298).</S>\n    <S sid=\"121\" ssid=\"53\">Adding in rules 5 and 6 resolves a total of 190 pronouns correctly, with only 16 mistakes, a precision of 92% and recall of 64%.</S>\n    <S sid=\"122\" ssid=\"54\">This contrasts strongly with the resolve-all-pronouns results of 78%.</S>\n    <S sid=\"123\" ssid=\"55\">The last two rules, 7 and 8 performed quite badly on the test data.</S>\n    <S sid=\"124\" ssid=\"56\">Despite their poor performance, CogNIAC still remained comparable to the Naive Algorithm.</S>\n    <S sid=\"125\" ssid=\"57\">3.2.2 Experiment 2-- All pronouns in MUC-6 evaluation: CogNIAC was used as the pronoun component in the University Pennsylvania's coreference entry5 in the MUC-6 evaluation.</S>\n    <S sid=\"126\" ssid=\"58\">Pronominal anaphora constitutes 17% of coreference annotations in the evaluation data used.</S>\n    <S sid=\"127\" ssid=\"59\">The remaining instances of anaphora included common noun anaphora and coreferent instances of proper nouns.</S>\n    <S sid=\"128\" ssid=\"60\">As a result being part of a larger system, changes were made to CogNIAC to make it fit in better with the other components of the overall system in addition to adding rules that were specialized for the new kinds of pronominal anaphora.</S>\n    <S sid=\"129\" ssid=\"61\">These changes include: 4 This is not to say that RAP was not an advancement of the state of the art.</S>\n    <S sid=\"130\" ssid=\"62\">A significant aspect of that research is that both RAP and the Naive Algorithm were machine executed--the Naive Algorithm was not machine executed in either the Hobbs 76 paper or in the evaluation in this work.</S>\n    <S sid=\"131\" ssid=\"63\">A total of thirty articles were used in the formal evaluation, of which I chose the first fifteen for closer analysis.</S>\n    <S sid=\"132\" ssid=\"64\">The remaining fifteen were retained for future evaluations.</S>\n    <S sid=\"133\" ssid=\"65\">The performance of CogNIAC was as follows: The precision (73%) is quite a bit worse than that encountered in the narrative.</S>\n    <S sid=\"134\" ssid=\"66\">The performance of the individual rules was quite different from the narrative texts, as shown in the table below: The results for CogNIAC for all pronouns in the first 15 articles of the MUC-6 evaluation.</S>\n    <S sid=\"135\" ssid=\"67\">Upon closer examination approximately 75% of the errors were due to factors outside the scope of the CogNIAC pronominal resolution component.</S>\n    <S sid=\"136\" ssid=\"68\">Software problems accounted for 20% of the incorrect cases, another 30% were due to semantic errors like misclassification of a noun phrase into person or company, singular/plural etc.</S>\n    <S sid=\"137\" ssid=\"69\">The remaining errors were due to incorrect noun phrase identification, failure to recognize pleonastic-it or other cases where there is no instance of an antecedent.</S>\n    <S sid=\"138\" ssid=\"70\">However, 25% of the errors were due directly to the rules of CogNIAC being plain wrong.</S>\n  </SECTION>\n  <SECTION title=\"4 Discussion:\" number=\"4\">\n    <S sid=\"139\" ssid=\"1\">CogNIAC is both an engineering effort and a different approach to information processing in variable knowledge contexts.</S>\n    <S sid=\"140\" ssid=\"2\">Each point is addressed in turn.</S>\n    <S sid=\"141\" ssid=\"3\">A question raised by a reviewer asked whether there was any use for high precision coreference given that it is not resolving as much coreference as other methods.</S>\n    <S sid=\"142\" ssid=\"4\">In the first experiment, the high precision version of CogNIAC correctly resolved 62% of the pronouns as compared to the resolve all pronouns version which resolved 79% of them--a 27% loss of overall recall.</S>\n    <S sid=\"143\" ssid=\"5\">The answer to this question quite naturally depends on the application coreference is being used in.</S>\n    <S sid=\"144\" ssid=\"6\">Some examples follow.</S>\n    <S sid=\"145\" ssid=\"7\">Information retrieval is characterized as a process by which a query is used to retrieve relevant documents from a text database.</S>\n    <S sid=\"146\" ssid=\"8\">Queries are typically natural language based or Boolean expressions.</S>\n    <S sid=\"147\" ssid=\"9\">Documents are retrieved and ranked for relevance using various string matching techniques with query terms in a document and the highest scoring documents are presented to the user first.</S>\n    <S sid=\"148\" ssid=\"10\">The role that coreference resolution might play in information retrieval is that retrieval algorithms that a) count the number of matches to a query term in a document, or b) count the proximity of matches to query terms, would benefit by noticing alternative realizations of the terms like 'he' in place 'George Bush'.</S>\n    <S sid=\"149\" ssid=\"11\">In such an application, high precision coreference would be more useful than high recall coreference if the information retrieval engine was returning too many irrelevant documents but getting a reasonable number of relevant documents.</S>\n    <S sid=\"150\" ssid=\"12\">The coreference would only help the scores of presumably relevant documents, but at the expense of missing some relevant documents.</S>\n    <S sid=\"151\" ssid=\"13\">A higher recall, lower precision algorithm would potentially add more irrelevant documents.</S>\n    <S sid=\"152\" ssid=\"14\">A direct application of the &amp;quot;ambiguity noticing&amp;quot; ability of CogNIAC is in checking the coherence of pronoun use in text for children and English as a second language learners.</S>\n    <S sid=\"153\" ssid=\"15\">Ambiguous pronoun use is a substantial problem for beginning writers and language learners.</S>\n    <S sid=\"154\" ssid=\"16\">CogNIAC could scan texts as they are being written and evaluate whether there was sufficient syntactic support from the context to resolve the pronoun--if not, then the user could be notified of a potentially ambiguous use.</S>\n    <S sid=\"155\" ssid=\"17\">It is not clear that CogNIAC's current levels of performance could support such an application, but it is a promising application.</S>\n    <S sid=\"156\" ssid=\"18\">Information extraction amounts to filling in template like data structures from free text.</S>\n    <S sid=\"157\" ssid=\"19\">Typically the patterns which are used to fill the templates are hand built.</S>\n    <S sid=\"158\" ssid=\"20\">The latest MUC-6 evaluation involved management changes at companies.</S>\n    <S sid=\"159\" ssid=\"21\">A major problem in information extraction is the fact that the desired information can be spread over many sentences in the text and coreference resolution is essential to relate relevant sentences to the correct individuals, companies etc.</S>\n    <S sid=\"160\" ssid=\"22\">The MUC-6 coreference task was developed with the idea that it would aid information extraction technologies.</S>\n    <S sid=\"161\" ssid=\"23\">The consequences for an incorrectly resolved pronoun can be devastating to the final template filling task--one runs the risk of conflating information about one individual with another.</S>\n    <S sid=\"162\" ssid=\"24\">High precision coreference appears to be a natural candidate for such applications.</S>\n    <S sid=\"163\" ssid=\"25\">CogNIAC effectively circumscribes those cases where coreference can be done with high confidence and those cases that require greater world knowledge, but how might CogNIAC be a part of a more knowledge rich coreference application?</S>\n    <S sid=\"164\" ssid=\"26\">CogNIAC as a set of seven or so high precision rules would act as an effective filter on what a more knowledge rich application would have to resolve.</S>\n    <S sid=\"165\" ssid=\"27\">But the essential component behind CogNIAC is not the rules themselves, but the control structure of behind its coreference resolution algorithm.</S>\n    <S sid=\"166\" ssid=\"28\">This control structure could control general inference techniques as well.</S>\n    <S sid=\"167\" ssid=\"29\">An interesting way to look at CogNIAC is as a search procedure.</S>\n    <S sid=\"168\" ssid=\"30\">The Naive Algorithm can be over simplified as depth first search over parse trees.</S>\n    <S sid=\"169\" ssid=\"31\">Depth first search is also a perfectly reasonable control structure for an inference engine-- as it is with PROLOG.</S>\n    <S sid=\"170\" ssid=\"32\">The search structure of CogNIAC could be characterized as parallel iterative deepening with solutions being accepted only if a unique solution is found to the depth of the parallel search.</S>\n    <S sid=\"171\" ssid=\"33\">But there is not enough room in this paper to explore the general properties of CogNIAC's search and evaluation strategy.</S>\n    <S sid=\"172\" ssid=\"34\">Another angle on CogNIAC's role with more robust knowledge sources is to note that the recall limitations of CogNIAC for the class of pronouns/data considered are due to insufficient filtering mechanisms on candidate antecedents.</S>\n    <S sid=\"173\" ssid=\"35\">There is not a need to expand the space of candidate antecedents with additional knowledge, but rather eliminate semantically plausible antecedents with constraints from verb knowledge and other sources of constraints currently not available to the system.</S>\n    <S sid=\"174\" ssid=\"36\">However, there are classes of coreference that require strong knowledge representation to assemble the initial set of candidate antecedents.</S>\n    <S sid=\"175\" ssid=\"37\">This includes the realm of inferred definites &amp;quot;I went to the house and opened the door&amp;quot; and synonymy between definite common nouns as in &amp;quot;the tax' and 'the levy.</S>\n    <S sid=\"176\" ssid=\"38\">Hobbs 1976 ultimately rejects the Naive Algorithm as a stand-alone solution to the pronoun resolution problem.</S>\n    <S sid=\"177\" ssid=\"39\">In that rejection he states: The naive algorithm does not work.</S>\n    <S sid=\"178\" ssid=\"40\">Anyone can think of examples where it fails.</S>\n    <S sid=\"179\" ssid=\"41\">In these cases it not only fails; it gives no indication that it has failed and offers no help in finding the real antecedent.</S>\n    <S sid=\"180\" ssid=\"42\">Hobbs then articulates a vision of what the appropriate technology is, which entails inference over an encoding of world knowledge.</S>\n    <S sid=\"181\" ssid=\"43\">But is world knowledge inherent in resolving all pronouns as Hobbs skepticism seems to convey?</S>\n    <S sid=\"182\" ssid=\"44\">It has not been clear up to this point whether any anaphora can be resolved with high confidence given that there are clear examples which can only be resolved with sophisticated world knowledge, e.g. the Winograd city council sentences.</S>\n    <S sid=\"183\" ssid=\"45\">But the results from the first and second experiments demonstrate that it is possible to have respectable recall with very high precision (greater than 90%) for some kinds of pronominal resolution.</S>\n    <S sid=\"184\" ssid=\"46\">However, good performance does not necessarily falsify Hobbs' skepticism.</S>\n    <S sid=\"185\" ssid=\"47\">The high precision component of CogNIAC still makes mistakes, 8-9% error for the first experiment--it is harder to evaluate the second experiment.</S>\n    <S sid=\"186\" ssid=\"48\">If it were the case that integration of world knowledge would have prevented those errors, then Hobbs' skepticism still holds since CogNIAC has only minimized the role of world knowledge, not eliminated it.</S>\n    <S sid=\"187\" ssid=\"49\">In looking at the mistakes made in the second experiment, there were no examples that appeared to be beyond the scope of further improving the syntactic rules or expanding the basic categorization of noun phrases into person, company or place.</S>\n    <S sid=\"188\" ssid=\"50\">For the data considered so far, there does appear to be a class of anaphors that can be reliably recognized and resolved with non-knowledge intensive techniques.</S>\n    <S sid=\"189\" ssid=\"51\">Whether this holds in general remains an open question, but it is a central design assumption behind the system.</S>\n    <S sid=\"190\" ssid=\"52\">A more satisfying answer to Hobbs' skepticism is contained in the earlier suggestive conjecture that world knowledge facilitates anaphora by eliminating ambiguity.</S>\n    <S sid=\"191\" ssid=\"53\">This claim can be advanced to say that world knowledge comes into play in those cases of anaphora that do not fall under the purview of rules 1 through 7 and their refinements.</S>\n    <S sid=\"192\" ssid=\"54\">If this is correct, then the introduction of better world knowledge sources will help in the recall of the system rather than the precision.</S>\n    <S sid=\"193\" ssid=\"55\">Ultimately, the utility of CogNIAC is a function of how it performs.</S>\n    <S sid=\"194\" ssid=\"56\">The high precision rules of CogNIAC performed very well, greater than 90% precision with good recall for the first experiment.</S>\n    <S sid=\"195\" ssid=\"57\">In the second experiment, components other than the rules of CogNIAC began to degrade the performance of the system unduly.</S>\n    <S sid=\"196\" ssid=\"58\">But there is promise in the high precision core of CogNIAC across varied domains.</S>\n  </SECTION>\n  <SECTION title=\"5 The future of CogNIAC:\" number=\"5\">\n    <S sid=\"197\" ssid=\"1\">CogNIAC is currently the common noun and pronoun resolution component of the University of Pennsylvania's coreference resolution software and general NLP software (Camp).</S>\n    <S sid=\"198\" ssid=\"2\">This paper does not address the common noun coreference aspects of the system but there are some interesting parallels with pronominal coreference.</S>\n    <S sid=\"199\" ssid=\"3\">Some changes planned include the following sorts of coreference: The processing of split antecedents, John called Mary.</S>\n    <S sid=\"200\" ssid=\"4\">They went to a movie.</S>\n    <S sid=\"201\" ssid=\"5\">This class of coreference is quite challenging because the plural anaphor 'they' must be able to collect a set of antecedents from the prior discourse--but how far should it look back, and once it has found two antecedents, should it continue to look for more?</S>\n    <S sid=\"202\" ssid=\"6\">Event reference is a class of coreference that will also prove to be quite challenging.</S>\n    <S sid=\"203\" ssid=\"7\">For example: The computer won the match.</S>\n    <S sid=\"204\" ssid=\"8\">It was a great triumph.</S>\n    <S sid=\"205\" ssid=\"9\">The antecedent to 'It' could be any of 'The computer', 'the match' or the event of winning.</S>\n    <S sid=\"206\" ssid=\"10\">The space of ambiguity will certainly grow substantially when events are considered as candidate antecedents.</S>\n    <S sid=\"207\" ssid=\"11\">Currently the system uses no verb semantics to try and constrain possible coreference.</S>\n    <S sid=\"208\" ssid=\"12\">While the Winograd sentences are too difficult for current robust lexical semantic systems, simpler generalizations about what can fill an argument are possible, consider: The price of aluminum rose today due to large purchases by ALCOA Inc.</S>\n    <S sid=\"209\" ssid=\"13\">It claimed that it was not trying to corner the market.</S>\n    <S sid=\"210\" ssid=\"14\">Since 'It' is an argument to 'claimed' , a verb that requires that its subject be animate, we can eliminate 'The price of aluminum' and 'today' from consideration, leaving `ALCOA Inc.' as the sole singular antecedent from the prior sentence.</S>\n    <S sid=\"211\" ssid=\"15\">Work has been done along these lines by Dagan '90.</S>\n  </SECTION>\n  <SECTION title=\"6 Acknowledgments:\" number=\"6\">\n    <S sid=\"212\" ssid=\"1\">I would like to thank my advisors Ellen Prince and Aravind Joshi for their support.</S>\n    <S sid=\"213\" ssid=\"2\">Also the comments of two anonymous reviewers proved quite helpful.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "cef819662caa94e6318f6fa09e0e594f",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources"
                }
            }
        },
        {
            "index": 19,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.",
                    "filename": "Unsupervised_Multilingual_Learning_for_Morphological_Segmentation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P08-1084/Documents_xml/P08-1084.xml",
                    "title": "Unsupervised Multilingual Learning for Morphological Segmentation"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores the benefits of unsupervised multilingual learning for morphological segmentation. The model presented in the paper induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases, aiming to capitalize on structural regularities across languages. The paper demonstrates that cross-lingual patterns can be successfully exploited for unsupervised morphological segmentation, with gains observed for all language pairs. The results suggest that closely related languages may provide more benefit than distant pairs when explicit representations of shared language structure are considered. The authors conclude by highlighting the potential application of similar multilingual models to other unsupervised analysis tasks and the need for further investigation into the role of language relatedness in such models.",
                        "This paper explores the potential for unsupervised multilingual learning to improve morphological segmentation across languages. The study investigates the effectiveness of cross-lingual patterns and joint analysis for related and unrelated languages. The proposed model, based on a hierarchical Bayesian framework with Dirichlet Process priors, capitalizes on structural regularities across languages for mutual benefit. Results from experiments using a multilingual corpus of short parallel phrases from Semitic languages and English show that the model reduces relative error by up to 24%. The paper concludes that cross-lingual patterns can indeed be exploited successfully for unsupervised morphological segmentation and suggests future work to apply similar multilingual models to other unsupervised analysis tasks.",
                        "This paper explores the use of unsupervised multilingual learning for morphological segmentation. The study investigates the benefits of exploiting cross-lingual patterns to improve unsupervised analysis and whether joint analysis provides more or less benefit when the languages belong to the same family. The model proposed in the paper uses a hierarchical Bayesian model to simultaneously model probabilistic dependencies across languages and morpheme distributions within each language. Results from the experiments show that cross-lingual patterns can be successfully exploited for unsupervised morphological segmentation, with related language pairs benefitting the most when shared language structure is explicitly represented. The paper concludes by highlighting the potential for applying similar multilingual models to other unsupervised analysis tasks and further investigating the role of language relatedness.",
                        "This paper explores unsupervised multilingual learning for morphological segmentation, investigating the benefits of joint analysis for languages from the same family or distant languages. The study proposes a model that can exploit cross-lingual patterns and align morphemes across languages, demonstrating the potential of exploiting structural commonalities across different languages for morphological analysis. The model is based on a hierarchical Bayesian framework and is tested on a multilingual corpus of short parallel phrases from Semitic languages and English translations. The results indicate that the model can successfully exploit cross-lingual patterns, reducing relative error by as much as 24%. The paper concludes that multilingual learning can improve unsupervised analysis, especially for closely related languages, and suggests applying similar models to other unsupervised analysis tasks."
                    ],
                    "filename": "unsupervised_multilingual_learni-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 124 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Unsupervised Multilingual Learning for Morphological Segmentation</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995).</S>\n    <S sid=\"8\" ssid=\"2\">The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals.</S>\n    <S sid=\"9\" ssid=\"3\">The connection between languages should be a powerful source of information for automatic linguistic analysis as well.</S>\n    <S sid=\"10\" ssid=\"4\">In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning?</S>\n    <S sid=\"11\" ssid=\"5\">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>\n    <S sid=\"12\" ssid=\"6\">We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning).</S>\n    <S sid=\"13\" ssid=\"7\">For example, the English word misunderstanding would be segmented into mis understand - ing.</S>\n    <S sid=\"14\" ssid=\"8\">This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004).</S>\n    <S sid=\"15\" ssid=\"9\">The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows:</S>\n  </SECTION>\n  <SECTION title=\"Arabic: Hebrew: Aramaic:\" number=\"2\">\n    <S sid=\"16\" ssid=\"1\">This example illustrates the potential benefits of unsupervised multilingual learning.</S>\n    <S sid=\"17\" ssid=\"2\">The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land.</S>\n    <S sid=\"18\" ssid=\"3\">They also use an identical suffix (-y) to represent the first person possessive pronoun (my).</S>\n    <S sid=\"19\" ssid=\"4\">These similarities in form should guide the model by constraining the space of joint segmentations.</S>\n    <S sid=\"20\" ssid=\"5\">The corresponding English phrase lacks this resemblance to its Semitic counterparts.</S>\n    <S sid=\"21\" ssid=\"6\">However, in this as in many cases, no segmentation is required for English as all the morphemes are expressed as individual words.</S>\n    <S sid=\"22\" ssid=\"7\">For this reason, English should provide a strong source of disambiguation for highly inflected languages, such as Arabic and Hebrew.</S>\n    <S sid=\"23\" ssid=\"8\">In general, we pose the following question.</S>\n    <S sid=\"24\" ssid=\"9\">In which scenario will multilingual learning be most effective?</S>\n    <S sid=\"25\" ssid=\"10\">Will it be for related languages, which share a common core of linguistic features, or for distant languages, whose linguistic divergence can provide strong sources of disambiguation?</S>\n    <S sid=\"26\" ssid=\"11\">As a first step towards answering this question, we propose a model which can take advantage of both similarities and differences across languages.</S>\n    <S sid=\"27\" ssid=\"12\">This joint bilingual model identifies optimal morphemes for two languages and at the same time finds compact multilingual representations.</S>\n    <S sid=\"28\" ssid=\"13\">For each language in the pair, the model favors segmentations which yield high frequency morphemes.</S>\n    <S sid=\"29\" ssid=\"14\">Moreover, bilingual morpheme pairs which consistently share a common semantic or syntactic function are treated as abstract morphemes, generated by a single language-independent process.</S>\n    <S sid=\"30\" ssid=\"15\">These abstract morphemes are induced automatically by the model from recurring bilingual patterns.</S>\n    <S sid=\"31\" ssid=\"16\">For example, in the case above, the tuple (in, fy, b-, b-) would constitute one of three abstract morphemes in the phrase.</S>\n    <S sid=\"32\" ssid=\"17\">When a morpheme occurs in one language without a direct counterpart in the other language, our model can explain away the stray morpheme as arising through a language-specific process.</S>\n    <S sid=\"33\" ssid=\"18\">To achieve this effect in a probabilistic framework, we formulate a hierarchical Bayesian model with Dirichlet Process priors.</S>\n    <S sid=\"34\" ssid=\"19\">This framework allows us to define priors over the infinite set of possible morphemes in each language.</S>\n    <S sid=\"35\" ssid=\"20\">In addition, we define a prior over abstract morphemes.</S>\n    <S sid=\"36\" ssid=\"21\">This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood.</S>\n    <S sid=\"37\" ssid=\"22\">The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages.</S>\n    <S sid=\"38\" ssid=\"23\">We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations.</S>\n    <S sid=\"39\" ssid=\"24\">The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977).</S>\n    <S sid=\"40\" ssid=\"25\">Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation.</S>\n    <S sid=\"41\" ssid=\"26\">When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%.</S>\n    <S sid=\"42\" ssid=\"27\">Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning.</S>\n    <S sid=\"43\" ssid=\"28\">However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"3\">\n    <S sid=\"44\" ssid=\"1\">Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006).</S>\n    <S sid=\"45\" ssid=\"2\">Most of this research assumes that one language has annotations for the task of interest.</S>\n    <S sid=\"46\" ssid=\"3\">Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language.</S>\n    <S sid=\"47\" ssid=\"4\">In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer.</S>\n    <S sid=\"48\" ssid=\"5\">An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006).</S>\n    <S sid=\"49\" ssid=\"6\">While their approach does not require a parallel corpus it does assume the availability of annotations in one language.</S>\n    <S sid=\"50\" ssid=\"7\">Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language.</S>\n    <S sid=\"51\" ssid=\"8\">The key assumption here is that certain distributional properties are invariant across languages from the same language families.</S>\n    <S sid=\"52\" ssid=\"9\">An example of such a property is the distribution of part-of-speech bigrams.</S>\n    <S sid=\"53\" ssid=\"10\">Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version.</S>\n    <S sid=\"54\" ssid=\"11\">The approach presented here differs from previous work in two significant ways.</S>\n    <S sid=\"55\" ssid=\"12\">First, we do not assume supervised data in any of the languages.</S>\n    <S sid=\"56\" ssid=\"13\">Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time.</S>\n    <S sid=\"57\" ssid=\"14\">This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language.</S>\n    <S sid=\"58\" ssid=\"15\">Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007).</S>\n    <S sid=\"59\" ssid=\"16\">Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution.</S>\n    <S sid=\"60\" ssid=\"17\">The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings.</S>\n    <S sid=\"61\" ssid=\"18\">Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007).</S>\n    <S sid=\"62\" ssid=\"19\">In these approaches, models with short description length are preferred.</S>\n    <S sid=\"63\" ssid=\"20\">Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon.</S>\n    <S sid=\"64\" ssid=\"21\">A locally optimal segmentation is identified using a task-specific greedy search.</S>\n    <S sid=\"65\" ssid=\"22\">In contrast to previous approaches, our model induces morphological segmentation for multiple related languages simultaneously.</S>\n    <S sid=\"66\" ssid=\"23\">By representing morphemes abstractly through the simultaneous alignment and segmentation of data in two languages, our algorithm capitalizes on deep connections between morpheme usage across different languages.</S>\n  </SECTION>\n  <SECTION title=\"3 Multilingual Morphological Segmentation\" number=\"4\">\n    <S sid=\"67\" ssid=\"1\">The underlying assumption of our work is that structural commonality across different languages is a powerful source of information for morphological analysis.</S>\n    <S sid=\"68\" ssid=\"2\">In this section, we provide several examples that motivate this assumption.</S>\n    <S sid=\"69\" ssid=\"3\">The main benefit of joint multilingual analysis is that morphological structure ambiguous in one language is sometimes explicitly marked in another language.</S>\n    <S sid=\"70\" ssid=\"4\">For example, in Hebrew, the preposition meaning “in”, b-, is always prefixed to its nominal argument.</S>\n    <S sid=\"71\" ssid=\"5\">On the other hand, in Arabic, the most common corresponding particle is fy, which appears as a separate word.</S>\n    <S sid=\"72\" ssid=\"6\">By modeling crosslingual morpheme alignments while simultaneously segmenting, the model effectively propagates information between languages and in this case would be encouraged to segment the Hebrew prefix b-.</S>\n    <S sid=\"73\" ssid=\"7\">Cognates are another important means of disambiguation in the multilingual setting.</S>\n    <S sid=\"74\" ssid=\"8\">Consider translations of the phrase “...and they wrote it...”: In both languages, the triliteral root ktb is used to express the act of writing.</S>\n    <S sid=\"75\" ssid=\"9\">By considering the two phrases simultaneously, the model can be encouraged to split off the respective Hebrew and Arabic prefixes w- and f- in order to properly align the cognate root ktb.</S>\n    <S sid=\"76\" ssid=\"10\">In the following section, we describe a model that can model both generic cross-lingual patterns (fy and b-), as well as cognates between related languages (ktb for Hebrew and Arabic).</S>\n  </SECTION>\n  <SECTION title=\"4 Model\" number=\"5\">\n    <S sid=\"77\" ssid=\"1\">Overview In order to simultaneously model probabilistic dependencies across languages as well as morpheme distributions within each language, we employ a hierarchical Bayesian model.2 Our segmentation model is based on the notion that stable recurring string patterns within words are indicative of morphemes.</S>\n    <S sid=\"78\" ssid=\"2\">In addition to learning independent morpheme patterns for each language, the model will prefer, when possible, to join together frequently occurring bilingual morpheme pairs into single abstract morphemes.</S>\n    <S sid=\"79\" ssid=\"3\">The model is fully unsupervised and is driven by a preference for stable and high frequency cross-lingual morpheme patterns.</S>\n    <S sid=\"80\" ssid=\"4\">In addition the model can incorporate character-to-character phonetic correspondences between alphabets as prior information, thus allowing the implicit modeling of cognates.</S>\n    <S sid=\"81\" ssid=\"5\">Our aim is to induce a model which concentrates probability on highly frequent patterns while still allowing for the possibility of those previously unseen.</S>\n    <S sid=\"82\" ssid=\"6\">Dirichlet processes are particularly suitable for such conditions.</S>\n    <S sid=\"83\" ssid=\"7\">In this framework, we can encode prior knowledge over the infinite sets of possible morpheme strings as well as abstract morphemes.</S>\n    <S sid=\"84\" ssid=\"8\">Distributions drawn from a Dirichlet process nevertheless produce sparse representations with most probability mass concentrated on a small number of observed and predicted patterns.</S>\n    <S sid=\"85\" ssid=\"9\">Our model utilizes a Dirichlet process prior for each language, as well as for the cross-lingual links (abstract morphemes).</S>\n    <S sid=\"86\" ssid=\"10\">Thus, a distribution over morphemes and morpheme alignments is first drawn from the set of Dirichlet processes and then produces the observed data.</S>\n    <S sid=\"87\" ssid=\"11\">In practice, we never deal with such distributions directly, but rather integrate over them during Gibbs sampling.</S>\n    <S sid=\"88\" ssid=\"12\">In the next section we describe our model’s “generative story” for producing the data we observe.</S>\n    <S sid=\"89\" ssid=\"13\">We formalize our model in the context of two languages £ and F. However, the formulation can be extended to accommodate evidence from multiple languages as well.</S>\n    <S sid=\"90\" ssid=\"14\">We provide an example of parallel phrase generation in Figure 1.</S>\n    <S sid=\"91\" ssid=\"15\">High-level Generative Story We have a parallel corpus of several thousand short phrases in the two languages £ and F. Our model provides a generative story explaining how these parallel phrases were probabilistically created.</S>\n    <S sid=\"92\" ssid=\"16\">The core of the model consists of three components: a distribution A over bilingual morpheme pairs (abstract morphemes), a distribution E over stray morphemes in language £ occurring without a counterpart in language F, and a similar distribution F for stray morphemes in language F. As usual for hierarchical Bayesian models, the generative story begins by drawing the model parameters themselves – in our case the three distributions A, E, and F. These three distributions are drawn from three separate Dirichlet processes, each with appropriately defined base distributions.</S>\n    <S sid=\"93\" ssid=\"17\">The Dirichlet processes ensure that the resulting distributions concentrate their probability mass on a small number of morphemes while holding out reasonable probability for unseen possibilities.</S>\n    <S sid=\"94\" ssid=\"18\">Once A, E, and F have been drawn, we model our parallel corpus of short phrases as a series of independent draws from a phrase-pair generation model.</S>\n    <S sid=\"95\" ssid=\"19\">For each new phrase-pair, the model first chooses the number and type of morphemes to be generated.</S>\n    <S sid=\"96\" ssid=\"20\">In particular, it must choose how many unaligned stray morphemes from language £, unaligned stray morphemes from language F, and abstract morphemes are to compose the parallel phrases.</S>\n    <S sid=\"97\" ssid=\"21\">These three numbers, respectively denoted as m, n, and k, are drawn from a Poisson distribution.</S>\n    <S sid=\"98\" ssid=\"22\">This step is illustrated in Figure 1 part (a).</S>\n    <S sid=\"99\" ssid=\"23\">The model then proceeds to independently draw m language £ morphemes from distribution E, n language-F morphemes from distribution F, and k abstract morphemes from distribution A.</S>\n    <S sid=\"100\" ssid=\"24\">This step is illustrated in part (b) of Figure 1.</S>\n    <S sid=\"101\" ssid=\"25\">The m + k resulting language-£ morphemes are then ordered and fused to form a phrase in language £, and likewise for the n + k resulting languageF morphemes.</S>\n    <S sid=\"102\" ssid=\"26\">The ordering and fusing decisions are modeled as draws from a uniform distribution over the set of all possible orderings and fusings for sizes m, n, and k. These final steps are illustrated in parts (c)-(d) of Figure 1.</S>\n    <S sid=\"103\" ssid=\"27\">Now we describe the model more formally.</S>\n    <S sid=\"104\" ssid=\"28\">Stray Morpheme Distributions Sometimes a morpheme occurs in a phrase in one language without a corresponding foreign language morpheme in the parallel phrase.</S>\n    <S sid=\"105\" ssid=\"29\">We call these “stray morphemes,” and we employ language-specific morpheme distributions to model their generation.</S>\n    <S sid=\"106\" ssid=\"30\">For each language, we draw a distribution over all possible morphemes (finite-length strings composed of characters in the appropriate alphabet) from a Dirichlet process with concentration parameter α and base distribution Pe or Pf respectively: The base distributions Pe and Pf can encode prior knowledge about the properties of morphemes in each of the two languages, such as length and character n-grams.</S>\n    <S sid=\"107\" ssid=\"31\">For simplicity, we use a geometric distribution over the length of the string with a final end-morpheme character.</S>\n    <S sid=\"108\" ssid=\"32\">The distributions E and F which result from the respective Dirichlet processes place most of their probability mass on a small number of morphemes with the degree of concentration As before, the resulting distribution A will give non-zero probability to all abstract morphemes (e, f).</S>\n    <S sid=\"109\" ssid=\"33\">The base distribution acts as a prior on such pairs.</S>\n    <S sid=\"110\" ssid=\"34\">To define we can simply use a mixture of geometric distributions in the lengths of the component morphemes.</S>\n    <S sid=\"111\" ssid=\"35\">However, if the languages £ and are related and the regular phonetic correspondences between the letter in the two alphabets are known, then we can use to assign higher likelihood to potential cognates.</S>\n    <S sid=\"112\" ssid=\"36\">In particular we define the prior f) to be the probabilistic string-edit distance (Ristad and Yianilos, 1998) between and the known phonetic correspondences to parameterize the string-edit model.</S>\n    <S sid=\"113\" ssid=\"37\">In particular, insertion and deletion probabilities are held constant for all characters, and substitution probabilities are determined based on the known sound correspondences.</S>\n    <S sid=\"114\" ssid=\"38\">We report results for both the simple geometric prior as well as the string-edit prior.</S>\n    <S sid=\"115\" ssid=\"39\">Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution.</S>\n    <S sid=\"116\" ssid=\"40\">These three integers represent the number and type of the morphemes d the number of coupled bilingual morpheme pairs, respectively. controlled by the prior Nevertheless, some nonzero probability is reserved for every possible string.</S>\n    <S sid=\"117\" ssid=\"41\">We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006).</S>\n    <S sid=\"118\" ssid=\"42\">Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes.</S>\n    <S sid=\"119\" ssid=\"43\">This model assigns probabilities to all pairs of morphemes is, all pairs of finite strings from the respective alphabets (e, f).</S>\n    <S sid=\"120\" ssid=\"44\">Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g.</S>\n    <S sid=\"121\" ssid=\"45\">(fy, b-) for in Arabic and Hebrew).</S>\n    <S sid=\"122\" ssid=\"46\">These morpheme pairs can thus be viewed as representing abstract morphemes.</S>\n    <S sid=\"123\" ssid=\"47\">As with the stray morpheme models, we wish to define a distribution which concentrates probability mass on a small number of highly co-occurring morpheme pairs while still holding out some probability for all other pairs.</S>\n    <S sid=\"124\" ssid=\"48\">We define this abstract morpheme model A as a draw from an that compose the parallel phrase, giving the number of stray morphemes in each language £ and an m, n, k ti Given these values, we now draw the appropriate number of stray an d abstract morphemes from the corresponding distributions: e1, ..., em ∼ E f1, ..., fn ∼ F (ei, fi), ..., (ek, f�k) ∼ A The sets of morphemes drawn for each language are then ordered: ˜e1, ..., ˜em+k ∼ ORDER|e1, ..., em, ei, ..., ek ˜f1, ..., ˜fn+k ∼ ORDER|f1, ..., fn, f1, ..., fk Finally the ordered morphemes are fused into the words that form the parallel phrases: To keep the model as simple as possible, we employ uniform distributions over the sets of orderings and fusings.</S>\n    <S sid=\"125\" ssid=\"49\">In other words, given a set of r morphemes (for each language), we define the distribution over permutations of the morphemes to simply be ORDER(·|r) = 1��.</S>\n    <S sid=\"126\" ssid=\"50\">Then, given a fixed morpheme order, we consider fusing each adjacent morpheme into a single word.</S>\n    <S sid=\"127\" ssid=\"51\">Again, we simply model the distribution over the r − 1 fusing decisions uniformly as FUSE(·|r) = 1 2''−1 .</S>\n    <S sid=\"128\" ssid=\"52\">Implicit Alignments Note that nowhere do we explicitly assign probabilities to morpheme alignments between parallel phrases.</S>\n    <S sid=\"129\" ssid=\"53\">However, our model allows morphemes to be generated in precisely one of two ways: as a lone stray morpheme or as part of a bilingual abstract morpheme pair.</S>\n    <S sid=\"130\" ssid=\"54\">Thus, our model implicitly assumes that each morpheme is either unaligned, or aligned to exactly one morpheme in the opposing language.</S>\n    <S sid=\"131\" ssid=\"55\">If we are given a parallel phrase with already segmented morphemes we can easily induce the distribution over alignments implied by our model.</S>\n    <S sid=\"132\" ssid=\"56\">As we will describe in the next section, drawing from these induced alignment distributions plays a crucial role in our inference procedure.</S>\n    <S sid=\"133\" ssid=\"57\">Inference Given our corpus of short parallel bilingual phrases, we wish to make segmentation decisions which yield a set of morphemes with high joint probability.</S>\n    <S sid=\"134\" ssid=\"58\">To assess the probability of a potential morpheme set, we need to marginalize over all possible alignments (i.e. possible abstract morpheme pairings and stray morpheme assignments).</S>\n    <S sid=\"135\" ssid=\"59\">We also need to marginalize over all possible draws of the distributions A, E, and F from their respective Dirichlet process priors.</S>\n    <S sid=\"136\" ssid=\"60\">We achieve these aims by performing Gibbs sampling.</S>\n    <S sid=\"137\" ssid=\"61\">Sampling We follow (Neal, 1998) in the derivation of our blocked and collapsed Gibbs sampler.</S>\n    <S sid=\"138\" ssid=\"62\">Gibbs sampling starts by initializing all random variables to arbitrary starting values.</S>\n    <S sid=\"139\" ssid=\"63\">At each iteration, the sampler selects a random variable Xi, and draws a new value for Xi from the conditional distribution of Xi given the current value of the other variables: P(Xi|X_i).</S>\n    <S sid=\"140\" ssid=\"64\">The stationary distribution of variables derived through this procedure is guaranteed to converge to the true joint distribution of the random variables.</S>\n    <S sid=\"141\" ssid=\"65\">However, if some variables can be jointly sampled, then it may be beneficial to perform block sampling of these variables to speed convergence.</S>\n    <S sid=\"142\" ssid=\"66\">In addition, if a random variable is not of direct interest, we can avoid sampling it directly by marginalizing it out, yielding a collapsed sampler.</S>\n    <S sid=\"143\" ssid=\"67\">We utilize variable blocking by jointly sampling multiple segmentation and alignment decisions.</S>\n    <S sid=\"144\" ssid=\"68\">We also collapse our Gibbs sampler in the standard way, by using predictive posteriors marginalized over all possible draws from the Dirichlet processes (resulting in Chinese Restaurant Processes).</S>\n    <S sid=\"145\" ssid=\"69\">Resampling For each bilingual phrase, we resample each word in the phrase in turn.</S>\n    <S sid=\"146\" ssid=\"70\">For word w in language E, we consider at once all possible segmentations, and for each segmentation all possible alignments.</S>\n    <S sid=\"147\" ssid=\"71\">We keep fixed the previously sampled segmentation decisions for all other words in the phrase as well as sampled alignments involving morphemes in other words.</S>\n    <S sid=\"148\" ssid=\"72\">We are thus considering at once: all possible segmentations of w along with all possible alignments involving morphemes in w with some subset of previously sampled languageF morphemes.3 The sampling formulas are easily derived as products of the relevant Chinese Restaurant Processes (with a minor adjustment to take into account the number of stray and abstract morphemes resulting from each decision).</S>\n    <S sid=\"149\" ssid=\"73\">See (Neal, 1998) for general formulas for Gibbs sampling from distributions with Dirichlet process priors.</S>\n    <S sid=\"150\" ssid=\"74\">All results reported are averaged over five runs using simulated annealing.</S>\n  </SECTION>\n  <SECTION title=\"5 Experimental Set-Up\" number=\"6\">\n    <S sid=\"151\" ssid=\"1\">Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots.</S>\n    <S sid=\"152\" ssid=\"2\">We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation.</S>\n    <S sid=\"153\" ssid=\"3\">Dataset As a source of parallel data, we use the Hebrew Bible and translations.</S>\n    <S sid=\"154\" ssid=\"4\">For the Hebrew version, we use an edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006).</S>\n    <S sid=\"155\" ssid=\"5\">This Bible edition is augmented by gold standard morphological analysis (including segmentation) performed by biblical scholars.</S>\n    <S sid=\"156\" ssid=\"6\">For the Arabic, Aramaic, and English versions, fied by augmenting the model with a pair of “morphemeidentity” variables deterministically drawn from each abstract morpheme.</S>\n    <S sid=\"157\" ssid=\"7\">Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism. we use the Van Dyke Arabic translation,4 Targum Onkelos,5 and the Revised Standard Version (Nelson, 1952), respectively.</S>\n    <S sid=\"158\" ssid=\"8\">We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data (Habash and Rambow, 2005).</S>\n    <S sid=\"159\" ssid=\"9\">The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included.</S>\n    <S sid=\"160\" ssid=\"10\">We don’t have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic.</S>\n    <S sid=\"161\" ssid=\"11\">To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph.</S>\n    <S sid=\"162\" ssid=\"12\">This process allows us to group together phrases like fy s.bah. in Arabic and bbqr in Hebrew while being reasonably certain that all the relevant morphemes are contained in the short extracted phrases.</S>\n    <S sid=\"163\" ssid=\"13\">The number of words in such phrases ranges from one to four words in the Semitic languages and up to six words in English.</S>\n    <S sid=\"164\" ssid=\"14\">Before performing any experiments, a manual inspection of the generated parallel phrases revealed that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment.</S>\n    <S sid=\"165\" ssid=\"15\">Therefore, we eliminated all parallel phrases that occur fewer than five times.</S>\n    <S sid=\"166\" ssid=\"16\">As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English.</S>\n    <S sid=\"167\" ssid=\"17\">The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic.</S>\n    <S sid=\"168\" ssid=\"18\">For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of (Thackston, 1999).</S>\n    <S sid=\"169\" ssid=\"19\">All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero.</S>\n    <S sid=\"170\" ssid=\"20\">Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score.</S>\n    <S sid=\"171\" ssid=\"21\">This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points.</S>\n    <S sid=\"172\" ssid=\"22\">The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase.</S>\n    <S sid=\"173\" ssid=\"23\">During testing, we do not allow the models to consider any multilingual evidence.</S>\n    <S sid=\"174\" ssid=\"24\">This restriction allows us to simulate future performance on purely monolingual data.</S>\n    <S sid=\"175\" ssid=\"25\">Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart.</S>\n    <S sid=\"176\" ssid=\"26\">However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR (Creutz and Lagus, 2007), a state-of-the-art unsupervised system for morphological segmentation.</S>\n    <S sid=\"177\" ssid=\"27\">While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English.</S>\n    <S sid=\"178\" ssid=\"28\">The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task.</S>\n    <S sid=\"179\" ssid=\"29\">We use the publicly available implementation of this system.</S>\n    <S sid=\"180\" ssid=\"30\">To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decisions based on a coin weighted with the true segmentation frequency.</S>\n  </SECTION>\n  <SECTION title=\"6 Results\" number=\"7\">\n    <S sid=\"181\" ssid=\"1\">Table 1 shows the performance of the various automatic segmentation methods.</S>\n    <S sid=\"182\" ssid=\"2\">The first three rows provide baselines, as mentioned in the previous section.</S>\n    <S sid=\"183\" ssid=\"3\">Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don’t use character-tocharacter phonetic correspondences to capture cognate information.</S>\n    <S sid=\"184\" ssid=\"4\">We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL.</S>\n    <S sid=\"185\" ssid=\"5\">We notice that in general, adding English – which has comparatively little morphological ambiguity – is about as useful as adding a more closely related Semitic language.</S>\n    <S sid=\"186\" ssid=\"6\">However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusions and Future Work\" number=\"8\">\n    <S sid=\"187\" ssid=\"1\">We started out by posing two questions: (i) Can we exploit cross-lingual patterns to improve unsupervised analysis?</S>\n    <S sid=\"188\" ssid=\"2\">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>\n    <S sid=\"189\" ssid=\"3\">The model and results presented in this paper answer the first question in the affirmative, at least for the task of morphological segmentation.</S>\n    <S sid=\"190\" ssid=\"4\">We also provided some evidence that considering closely related languages may be more beneficial than distant pairs if the model is able to explicitly represent shared language structure (the characterto-character phonetic correspondences in our case).</S>\n    <S sid=\"191\" ssid=\"5\">In the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models.</S>\n    <S sid=\"192\" ssid=\"6\">7</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "7903b8f167b56c622a27fa8561ca1d4c",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Unsupervised Multilingual Learning for Morphological Segmentation"
                }
            }
        },
        {
            "index": 20,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Morphology induction is a subproblem of important tasks like automatic learning of machine-readable dictionaries and grammar induction. Previous morphology induction approaches have relied solely on statistics of hypothesized stems and affixes to choose which affixes to consider legitimate. Relying on stemand-affix statistics rather than semantic knowledge leads to a number of problems, such as the inappropriate use of valid affixes (&quot;ally&quot; stemming to &quot;all&quot;). We introduce a semantic-based algorithm for learning morphology which only proposes affixes when the stem and stem-plusaffix are sufficiently similar semantically. We implement our approach using Latent Semantic Analysis and show that our semantics-only approach provides morphology induction results that rival a current state-of-the-art system.",
                    "filename": "Knowledge-Free_Induction_Of_Morphology_Using_Latent_Semantic_Analysis",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W00-0712/Documents_xml/W00-0712.xml",
                    "title": "Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis"
                },
                "llm": {
                    "descriptions": [
                        "This paper describes a knowledge-free approach to inducing morphology using Latent Semantic Analysis (LSA). The authors propose an algorithm that automatically extracts potential affixes from untagged corpora, identifies word pairs sharing the same stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs. The results show that LSA-based semantics alone can provide results that rival a state-of-the-art system based on stem-and-affix statistics. The authors compare their algorithm to Goldsmith's Linguistica and conclude that semantics and LSA can play a key part in knowledge-free morphology induction, with the potential to complement frequency-based approaches. Further work is being done to combine these two approaches.",
                        "This paper presents a knowledge-free approach to automatic morphology induction using Latent Semantic Analysis (LSA). The authors demonstrate how LSA can be used to automatically induce morphological relationships between words and compare the results to a state-of-the-art system based on stem-and-affix statistics. The approach focuses on inflectional languages and considers the impact of semantics, showing that semantics alone can provide results that rival frequency-based approaches. The paper also discusses the evaluation of the algorithm using a gold standard database and compares the results to a previous system. Overall, the findings suggest that semantics and LSA can play a key part in knowledge-free morphology induction, with potential for combining semantics-based and frequency-based approaches.",
                        "This paper explores the use of Latent Semantic Analysis (LSA) to automatically induce morphological relationships between words without human intervention. The authors propose a knowledge-free algorithm that focuses on inflectional languages and specifically on English, comparing their results to Goldsmith's Linguistica. By incorporating semantic vectors and LSA-based semantics, the algorithm identifies valid morphological relations and provides results that rival state-of-the-art systems. The paper concludes that semantics and LSA can play a key part in knowledge-free morphology induction, with the potential to complement frequency-based approaches. The results suggest that semantics alone can work as well as frequency-based approaches, and the authors are currently examining how to combine these two approaches.",
                        "This paper explores the use of Latent Semantic Analysis (LSA) in the knowledge-free induction of morphology. The authors propose an algorithm that automatically induces morphological relationships between words using semantic information obtained from LSA. The algorithm is compared to Goldsmith's Linguistica using CELEX's suffixes as a gold standard. Results show that semantics and LSA can play a key part in knowledge-free morphology induction, working at least as well as frequency-based approaches. The authors believe that semantics-based and frequency-based approaches play complementary roles, and they are examining how to combine these two approaches in future work.",
                        "This paper presents a knowledge-free approach to inducing morphology using Latent Semantic Analysis (LSA). The authors highlight the development of computational morphological analyzers and the time-consuming process of manually building them. They propose an algorithm that automatically induces morphological relationships between words using LSA-based semantics, which they demonstrate can rival state-of-the-art systems based on stem-and-affix statistics. The paper outlines their approach, compares it to previous work, and presents the results of their algorithm's performance compared to Goldsmith's Linguistica. The results suggest that semantics and LSA can play a key part in knowledge-free morphology induction, with potential complementary roles to frequency-based approaches.",
                        "This paper presents a method for knowledge-free induction of morphology using Latent Semantic Analysis (LSA). The authors demonstrate how to automatically induce morphological relationships between words without human intervention. The approach focuses on inflectional languages and incorporates semantics to identify valid morphological relations. The paper compares the results of this approach to a state-of-the-art system based on stem-and-affix statistics and shows that the LSA-based approach provides comparable results. The findings suggest that semantics and LSA can play a key part in knowledge-free morphology induction, with the potential for both semantics-based and frequency-based approaches to play complementary roles. Future work will focus on combining these two approaches.",
                        "This paper presents a knowledge-free approach to inducing morphology using Latent Semantic Analysis (LSA). The authors propose an algorithm that automatically extracts potential affixes from untagged corpora, identifies potential morphological variants, and uses LSA to judge semantic relatedness between word pairs. The results show that incorporating LSA-based semantics into the morphology-induction process can provide results that rival a state-of-the-art system based on stem-and-affix statistics. The paper compares the algorithm to Goldsmith's Linguistica and demonstrates that semantics alone can work at least as well as a frequency-based approach. The authors believe that semantics-based and frequency-based approaches play complementary roles and are exploring how to combine these two approaches.",
                        "This paper presents a knowledge-free approach to inducing morphology using latent semantic analysis (LSA). The authors show how to automatically induce morphological relationships between words by incorporating word semantics. They compare their algorithm to Goldsmith's Linguistica and demonstrate that semantics and LSA can play a key part in knowledge-free morphology induction, with semantics alone working at least as well as frequency-based approaches. The paper concludes by discussing the complementary roles of semantics-based and frequency-based approaches and ongoing work on combining these two approaches.",
                        "This paper discusses the development of an algorithm for knowledge-free induction of morphology using latent semantic analysis (LSA). The paper highlights the need for automatic morphological analysis without human intervention and focuses on inflectional languages, particularly English. The approach involves identifying candidate affixes, computing semantic vectors for words, and using LSA to judge semantic relatedness between word pairs. The results show that semantics and LSA can be effective in knowledge-free morphology induction, performing at least as well as frequency-based approaches. The paper concludes by emphasizing the complementary roles of semantics-based and frequency-based approaches, with ongoing work to combine the two.",
                        "This paper discusses the use of Latent Semantic Analysis (LSA) for knowledge-free induction of morphology. The authors propose a method for automatically inducing morphological relationships between words using LSA, showing that incorporating LSA-based semantics alone can provide results comparable to state-of-the-art systems based on stem-and-affix statistics. The paper details the algorithm's approach, compares it to existing methods, and presents results demonstrating the effectiveness of the proposed approach. The findings suggest that semantics and LSA can significantly contribute to knowledge-free morphology induction, with potential for combining semantic and frequency-based approaches."
                    ],
                    "filename": "knowledgefree_induction_of_morph-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 109 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Computational morphological analyzers have existed in various languages for years and it has been said that &amp;quot;the quest for an efficient method for the analysis and generation of word-forms is no longer an academic research topic&amp;quot; (Karlsson and Karttunen, 1997).</S>\n    <S sid=\"7\" ssid=\"2\">However, development of these analyzers typically begins with human intervention requiring time spans from days to weeks.</S>\n    <S sid=\"8\" ssid=\"3\">If it were possible to build such analyzers automatically without human knowledge, significant development time could be saved.</S>\n    <S sid=\"9\" ssid=\"4\">On a larger scale, consider the task of inducing machine-readable dictionaries (MRDs) using no human-provided information (&amp;quot;knowledge-free&amp;quot;).</S>\n    <S sid=\"10\" ssid=\"5\">In building an MRD, &amp;quot;simply expanding the dictionary to encompass every word one is ever likely to encounter.. .fails to take advantage of regularities&amp;quot; (Sproat, 1992, p. xiii).</S>\n    <S sid=\"11\" ssid=\"6\">Hence, automatic morphological analysis is also critical for selecting appropriate and non-redundant MRD headwords.</S>\n    <S sid=\"12\" ssid=\"7\">For the reasons expressed above, we are interested in knowledge-free morphology induction.</S>\n    <S sid=\"13\" ssid=\"8\">Thus, in this paper, we show how to automatically induce morphological relationships between words.</S>\n    <S sid=\"14\" ssid=\"9\">Previous morphology induction approaches (Goldsmith, 1997, 2000; Mean, 1998; Gaussier, 1999) have focused on inflectional languages and have used statistics of hypothesized stems and affixes to choose which affixes to consider legitimate.</S>\n    <S sid=\"15\" ssid=\"10\">Several problems can arise using only stem-and-affix statistics: (1) valid affixes may be applied inappropriately (&amp;quot;ally&amp;quot; stemming to &amp;quot;all&amp;quot;), (2) morphological ambiguity may arise (&amp;quot;rating&amp;quot; conflating with &amp;quot;rat&amp;quot; instead of &amp;quot;rate&amp;quot;), and (3) non-productive affixes may get accidentally pruned (the relationship between &amp;quot;dirty&amp;quot; and &amp;quot;dirt&amp;quot; may be lost).1 Some of these problems could be resolved if one could incorporate word semantics.</S>\n    <S sid=\"16\" ssid=\"11\">For instance, &amp;quot;all&amp;quot; is not semantically similar to &amp;quot;ally,&amp;quot; so with knowledge of semantics, an algorithm could avoid conflating these two words.</S>\n    <S sid=\"17\" ssid=\"12\">To maintain the &amp;quot;knowledge-free&amp;quot; paradigm, such semantics would need to be automatically induced.</S>\n    <S sid=\"18\" ssid=\"13\">Latent Semantic Analysis (LSA) (Deerwester, et al., 1990); Landauer, et at., 1998) is a technique which automatically identifies semantic information from a corpus.</S>\n    <S sid=\"19\" ssid=\"14\">We here show that incorporating LSA-based semantics alone into the morphology-induction process can provide results that rival a state-ofthe-art system based on stem-and-affix statistics (Goldsmith's Linguistica). lError examples are from Goldsmith's Linguistica Our algorithm automatically extracts potential affixes from an untagged corpus, identifies word pairs sharing the same proposed stem but having different affixes, and uses LSA to judge semantic relatedness between word pairs.</S>\n    <S sid=\"20\" ssid=\"15\">This process serves to identify valid morphological relations.</S>\n    <S sid=\"21\" ssid=\"16\">Though our algorithm could be applied to any inflectional language, we here restrict it to English in order to perform evaluations against the human-labeled CELEX database (Baayen, et al., 1993).</S>\n  </SECTION>\n  <SECTION title=\"2 Previous work\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">Existing induction algorithms all focus on identifying prefixes, suffixes, and word stems in inflectional languages (avoiding infixes and other language types like concatenative or agglutinative languages (Sproat, 1992)).</S>\n    <S sid=\"23\" ssid=\"2\">They also observe high frequency occurrences of some word endings or beginnings, perform statistics thereon, and propose that some of these appendages are valid morphemes.</S>\n    <S sid=\"24\" ssid=\"3\">However, these algorithms differ in specifics.</S>\n    <S sid=\"25\" ssid=\"4\">DeJean (1998) uses an approach derived from Harris (1951) where word-splitting occurs if the number of distinct letters that follows a given sequence of characters surpasses a threshold.</S>\n    <S sid=\"26\" ssid=\"5\">He uses these hypothesized affixes to resegment words and thereby identify additional affixes that were initially overlooked.</S>\n    <S sid=\"27\" ssid=\"6\">His overall goal is different from ours: he primarily seeks an affix inventory.</S>\n    <S sid=\"28\" ssid=\"7\">Goldsmith (1997) tries cutting each word in exactly one place based on probability and lengths of hypothesized stems and affixes.</S>\n    <S sid=\"29\" ssid=\"8\">He applies the EM algorithm to eliminate inappropriate parses.</S>\n    <S sid=\"30\" ssid=\"9\">He collects the possible suffixes for each stem calling these a signature which aid in determining word classes.</S>\n    <S sid=\"31\" ssid=\"10\">Goldsmith (2000) later incorporates minimum description length to identify stemming characteristics that most compress the data, but his algorithm otherwise remains similar in nature.</S>\n    <S sid=\"32\" ssid=\"11\">Goldsmith's algorithm is practically knowledge-free, though he incorporates capitalization removal and some word segmentation.</S>\n    <S sid=\"33\" ssid=\"12\">Gaussier (1999) begins with an inflectional lexicon and seeks to find derivational morphology.</S>\n    <S sid=\"34\" ssid=\"13\">The words and parts of speech from his inflectional lexicon serve for building relational families of words and identifying sets of word pairs and suffixes therefrom.</S>\n    <S sid=\"35\" ssid=\"14\">Gaussier splits words based on p-similarity — words that agree in exactly the first p characters.</S>\n    <S sid=\"36\" ssid=\"15\">He also builds a probabilistic model which indicates that the probability of two words being morphological variants is based upon the probability of their respective changes in orthography and morphosynt act ics .</S>\n  </SECTION>\n  <SECTION title=\"3 Current approach\" number=\"3\">\n    <S sid=\"37\" ssid=\"1\">Our algorithm also focuses on inflectional languages.</S>\n    <S sid=\"38\" ssid=\"2\">However, with the exception of word segmentation, we provide it no human information and we consider only the impact of semantics.</S>\n    <S sid=\"39\" ssid=\"3\">Our approach (see Figure 1) can be decomposed into four components: (1) initially selecting candidate affixes, (2) identifying affixes which are potential morphological variants of each other, (3) computing semantic vectors for words possessing these candidate affixes, and (4) selecting as valid morphological variants those words with similar semantic vectors.</S>\n    <S sid=\"40\" ssid=\"4\">To select candidate affixes, we, like Gaussier, identify p-similar words.</S>\n    <S sid=\"41\" ssid=\"5\">We insert words into a trie (Figure 2) and extract potential affixes by observing those places in the trie where branching occurs.</S>\n    <S sid=\"42\" ssid=\"6\">Figure 2's hypothesized suffixes are NULL, &amp;quot;s,&amp;quot; &amp;quot;ed,&amp;quot; &amp;quot;es,&amp;quot; &amp;quot;ing,&amp;quot; &amp;quot;e,&amp;quot; and &amp;quot;eful.&amp;quot; We retain only the K most-frequent candidate affixes for subsequent processing.</S>\n    <S sid=\"43\" ssid=\"7\">The value for K needs to be large enough to account for the number of expected regular affixes in any given language as well as some of the more frequent irregular affixes.</S>\n    <S sid=\"44\" ssid=\"8\">We arbitrarily chose K to be 200 in our system.</S>\n    <S sid=\"45\" ssid=\"9\">(It should also be mentioned that we can identify potential prefixes by inserting words into the trie in reversed order.</S>\n    <S sid=\"46\" ssid=\"10\">This prefix mode can additionally serve for identifying capitalization.)</S>\n    <S sid=\"47\" ssid=\"11\">Stage 3 Stage 4 ind wo-r\\ pairs that are possible morphoWe next identify pairs of candidate affixes that descend from a common ancestor node in the trie.</S>\n    <S sid=\"48\" ssid=\"12\">For example, (&amp;quot;s&amp;quot;, NULL) constitutes such a pair from Figure 2.</S>\n    <S sid=\"49\" ssid=\"13\">We call these pairs rules.</S>\n    <S sid=\"50\" ssid=\"14\">Two words sharing the same root and the same affix rule, such as &amp;quot;cars&amp;quot; and &amp;quot;car,&amp;quot; form what we call a pair of potential morphological variants (PPMVs).</S>\n    <S sid=\"51\" ssid=\"15\">We define the ruleset of a given rule to be the set of all PPM Vs that have that rule in common.</S>\n    <S sid=\"52\" ssid=\"16\">For instance, from Figure 2, the ruleset for (&amp;quot;s&amp;quot;, NULL) would be the pairs &amp;quot;cars/car&amp;quot; and &amp;quot;cares/care.&amp;quot; Our algorithm establishes a list which identifies the rulesets for every hypothesized rule extracted from the data and then it must proceed to determine which rulesets or PPM Vs describe true morphological relationships.</S>\n    <S sid=\"53\" ssid=\"17\">Deerwester, et al. (1990) showed that it is possible to find significant semantic relationships between words and documents in a corpus with virtually no human intervention (with the possible exception of a human-built stop word list).</S>\n    <S sid=\"54\" ssid=\"18\">This is typically done by applying singular value decomposition (SVD) to a matrix, M, where each entry M(i,j) contains the frequency of word i as seen in document j of the corpus.</S>\n    <S sid=\"55\" ssid=\"19\">This methodology is referred to as Latent Semantic Analysis (LSA) and is well-described in the literature (Landauer, et al., 1998; Manning and Schiitze, 1999).</S>\n    <S sid=\"56\" ssid=\"20\">SVDs seek to decompose a matrix A into the product of three matrices U, D, and VT where U and VT are orthogonal matrices and D is a diagonal matrix containing the singular values (squared eigenvalues) of A.</S>\n    <S sid=\"57\" ssid=\"21\">Since SVD's can be performed which identify singular values by descending order of size (Berry, et al., 1993), LSA truncates after finding the k largest singular values.</S>\n    <S sid=\"58\" ssid=\"22\">This corresponds to projecting the vector representation of each word into a k-dimensional subspace whose axes form k (latent) semantic directions.</S>\n    <S sid=\"59\" ssid=\"23\">These projections are precisely the rows of the matrix product UkDk.</S>\n    <S sid=\"60\" ssid=\"24\">A typical k is 300, which is the value we used.</S>\n    <S sid=\"61\" ssid=\"25\">However, we have altered the algorithm somewhat to fit our needs.</S>\n    <S sid=\"62\" ssid=\"26\">First, to stay as close to the knowledge-free scenario as possible, we neither apply a stopword list nor remove capitalization.</S>\n    <S sid=\"63\" ssid=\"27\">Secondly, since SVDs are more designed to work on normally-distributed data (Manning and Schiitze, 1999, p. 565), we operate on Zscores rather than counts.</S>\n    <S sid=\"64\" ssid=\"28\">Lastly, instead of generating a term-document matrix, we build a term-term matrix.</S>\n    <S sid=\"65\" ssid=\"29\">Schiitze (1993) achieved excellent performance at classifying words into quasi-partof-speech classes by building and performing an SVD on an Nx4N term-term matrix, M(i,Np+j).</S>\n    <S sid=\"66\" ssid=\"30\">The indices i and j represent the top N highest frequency words.</S>\n    <S sid=\"67\" ssid=\"31\">The p values range from 0 to 3 representing whether the word indexed by j is positionally offset from the word indexed by i by -2, -1, +1, or +2, respectively.</S>\n    <S sid=\"68\" ssid=\"32\">For example, if &amp;quot;the&amp;quot; and &amp;quot;people&amp;quot; were respectively the 1st and 100th highest frequency words, then upon seeing the phrase &amp;quot;the people,&amp;quot; Schiitze's approach would increment the counts of M(1,2N+100) and M(100,N+1).</S>\n    <S sid=\"69\" ssid=\"33\">We used Schfitze's general framework but tailored it to identify local semantic information.</S>\n    <S sid=\"70\" ssid=\"34\">We built an Nx2N matrix and our p values correspond to those words whose offsets from word i are in the intervals [-50,-1] and [1,50], respectively.</S>\n    <S sid=\"71\" ssid=\"35\">We also reserve the Nth position as a catch-all position to account for all words that are not in the top (N-1).</S>\n    <S sid=\"72\" ssid=\"36\">An important issue to resolve is how large should N be.</S>\n    <S sid=\"73\" ssid=\"37\">We would like to be able to incorporate semantics for an arbitrarily large number of words and LSA quickly becomes impractical on large sets.</S>\n    <S sid=\"74\" ssid=\"38\">Fortunately, it is possible to build a matrix with a smaller value of N (say, 2500), perform an SVD thereon, and then fold in remaining terms (Manning and Schaze, 1999, p. 563).</S>\n    <S sid=\"75\" ssid=\"39\">Since the U and V matrices of an SVD are orthogonal matrices, then UUT=VVT=I.</S>\n    <S sid=\"76\" ssid=\"40\">This implies that AV=UD.</S>\n    <S sid=\"77\" ssid=\"41\">This means that for a new word, w, one can build a vector a which identifies how w relates to the top N words according to the p different conditions described above.</S>\n    <S sid=\"78\" ssid=\"42\">For example, if w were one of the top N words, then a would simply represent w's particular row from the A matrix.</S>\n    <S sid=\"79\" ssid=\"43\">The product aw= avk is the projection of 6T into the k-dimensional latent semantic space.</S>\n    <S sid=\"80\" ssid=\"44\">By storing an index to the words of the corpus as well as a sorted list of these words, one can efficiently build a set of semantic vectors which includes each word of interest.</S>\n    <S sid=\"81\" ssid=\"45\">Morphologically-related words frequently share similar semantics, so we want to see how well semantic vectors of PPMVs correlate.</S>\n    <S sid=\"82\" ssid=\"46\">If we know how PPMVs correlate in comparison to other word pairs from their same rulesets, we can actually determine the semantic-based probability that the variants are legitimate.</S>\n    <S sid=\"83\" ssid=\"47\">In this section, we identify a measure for correlating PPMVs and illustrate how ruleset-based statistics help identify legitimate PPMVs.</S>\n    <S sid=\"84\" ssid=\"48\">The cosine of the angle between two vectors v1 and v2 is given by, We want to determine the correlation between each of the words of every PPMV.</S>\n    <S sid=\"85\" ssid=\"49\">We use what we call a normalized cosine score (NCS) as a correlation.</S>\n    <S sid=\"86\" ssid=\"50\">To obtain a NCS, we first calculate the cosine between each semantic vector, nw, and the semantic vectors from 200 randomly chosen words.</S>\n    <S sid=\"87\" ssid=\"51\">By this means we obtain w's correlation mean (p,w) and standard deviation (cru,).</S>\n    <S sid=\"88\" ssid=\"52\">If v is one of w's variants, then we define the NCS between nw and Itv to be By considering NCSs for all word pairs coupled under a particular rule, we can determine semantic-based probabilities that indicate which PPMVs are legitimate.</S>\n    <S sid=\"89\" ssid=\"53\">We expect random NCSs to be normally-distributed according to .A.r(0,1).</S>\n    <S sid=\"90\" ssid=\"54\">Given that a particular ruleset contains nR PPMVs, we can therefore approximate the number (nT), mean (AT) and standard deviation (o-T) of true correlations.</S>\n    <S sid=\"91\" ssid=\"55\">If we define (I, z(t, a) to be iy e—( .x)2dx, then we can compute the probability that the particular correlation is legitimate: It is possible that a rule can be hypothesized at the trie stage that is true under only certain conditions.</S>\n    <S sid=\"92\" ssid=\"56\">A prime example of such a rule is (&amp;quot;es&amp;quot;, NULL).</S>\n    <S sid=\"93\" ssid=\"57\">Observe from Table 1 that the word &amp;quot;cares&amp;quot; poorly correlates with &amp;quot;car.&amp;quot; Yet, it is true that &amp;quot;-es&amp;quot; is a valid suffix for the words &amp;quot;flashes,&amp;quot; &amp;quot;catches,&amp;quot; &amp;quot;kisses,&amp;quot; and many other words where the &amp;quot;-es&amp;quot; is preceded by a voiceless sibilant.</S>\n    <S sid=\"94\" ssid=\"58\">Hence, there is merit to considering subrules that arise while performing analysis on a particular rule.</S>\n    <S sid=\"95\" ssid=\"59\">For instance, while evaluating the (&amp;quot;es&amp;quot;, NULL) rule, it is desirable to also consider potential subrules such as (&amp;quot;ches&amp;quot;, &amp;quot;ch&amp;quot;) and (&amp;quot;tes&amp;quot;, &amp;quot;t&amp;quot;).</S>\n    <S sid=\"96\" ssid=\"60\">One might expect that the average NCS for the (&amp;quot;ches&amp;quot;, &amp;quot;ch&amp;quot;) subrule might be higher than the overall rule (&amp;quot;es&amp;quot;, NULL) whereas the opposite will likely be true for (&amp;quot;tes&amp;quot;, &amp;quot;t&amp;quot;).</S>\n    <S sid=\"97\" ssid=\"61\">Table 2 confirms this.</S>\n  </SECTION>\n  <SECTION title=\"4 Results\" number=\"4\">\n    <S sid=\"98\" ssid=\"1\">We compare our algorithm to Goldsmith's Linguistica (2000) by using CELEX's (Baayen, et al., 1993) suffixes as a gold standard.</S>\n    <S sid=\"99\" ssid=\"2\">CELEX is a hand-tagged, morphologicallyanalyzed database of English words.</S>\n    <S sid=\"100\" ssid=\"3\">CELEX has limited coverage of the words from our data set (where our data consists of over eight million words from random subcollections of TREC data (Voorhees, et a1,1997/8)), so we only considered words with frequencies of 10 or more.</S>\n    <S sid=\"101\" ssid=\"4\">Morphological relationships can be represented graphically as directed graphs (see Figure 3, where three separate graphs are depicted).</S>\n    <S sid=\"102\" ssid=\"5\">Developing a scoring algorithm to compare directed graphs is likely to be prone to disagreements.</S>\n    <S sid=\"103\" ssid=\"6\">Therefore, we score only the vertex sets of directed graphs.</S>\n    <S sid=\"104\" ssid=\"7\">We will refer to these vertex sets as conflation sets.</S>\n    <S sid=\"105\" ssid=\"8\">For example, concern's conflation set contains itself as well as &amp;quot;concerned,&amp;quot; &amp;quot;concerns,&amp;quot; and &amp;quot;concerning&amp;quot; (or, in shorthand notation, the set is fa,b,c,d1).</S>\n    <S sid=\"106\" ssid=\"9\">To evaluate an algorithm, we sum the number of correct (C), inserted (I) , and deleted (D) words it predicts for each hypothesized conflation set.</S>\n    <S sid=\"107\" ssid=\"10\">If Xu, represents word w's conflation set according to the algorithm, and if Yw represents its CELEX-based conflation set, then However, in making these computations, we disregard any CELEX words that are not in the algorithm's data set and vice versa.</S>\n    <S sid=\"108\" ssid=\"11\">For example, suppose two algorithms were being compared on a data set where all the words from Figure 3 were available except &amp;quot;concerting&amp;quot; and &amp;quot;concertos.&amp;quot; Suppose further that one algorithm proposed that { a,b,c,d,e,f,g,i} formed a single conflation set whereas the other algorithm proposed the three sets { a,b,c,d},{e,g,i}, and {f}.</S>\n    <S sid=\"109\" ssid=\"12\">Then Table 3 illustrates how the two algorithms would be scored.</S>\n    <S sid=\"110\" ssid=\"13\">To explain Table 3, consider algorithm one's entries for 'a.'</S>\n    <S sid=\"111\" ssid=\"14\">Algorithm one had proposed that Xa=fa,b,c,d,e,f,g,il when in reality, = Ya={ a,b,c,d}.</S>\n    <S sid=\"112\" ssid=\"15\">Since IX,, n Ya I = 4 and IYak4, then CA=4/ 4.</S>\n    <S sid=\"113\" ssid=\"16\">The remaining values of the table can be computed accordingly.</S>\n    <S sid=\"114\" ssid=\"17\">Using the values from Table 3, we can also compute precision, recall, and F-Score.</S>\n    <S sid=\"115\" ssid=\"18\">Precision is defined to be C/(C+/), recall is C/(C+D), and F-Score is the product of precision and recall divided by the average of the two.</S>\n    <S sid=\"116\" ssid=\"19\">For the first algorithm, the precision, recall, and F-Score would have respectively been 1/3, 1, and 1/2.</S>\n    <S sid=\"117\" ssid=\"20\">In the second algorithm, these numbers would have been 5/7, 5/6, and 10/13.</S>\n    <S sid=\"118\" ssid=\"21\">Table 4 uses the above scoring mechanism to compare between Linguistica and our system (at various probability thresholds).</S>\n    <S sid=\"119\" ssid=\"22\">Note that since Linguistica removes capitalization, it will have a different total word count than our system.</S>\n  </SECTION>\n  <SECTION title=\"5 Conclusions\" number=\"5\">\n    <S sid=\"120\" ssid=\"1\">These results suggest that semantics and LSA can play a key part in knowledge-free morphology induction.</S>\n    <S sid=\"121\" ssid=\"2\">Semantics alone worked at least as well as Goldsmith's frequency-based approach.</S>\n    <S sid=\"122\" ssid=\"3\">Yet we believe that semantics-based and frequency-based approaches play complementary roles.</S>\n    <S sid=\"123\" ssid=\"4\">In current work, we are examining how to combine these two approaches.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "510326b62f1ad0118dd94208263c132e",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Knowledge-Free Induction Of Morphology Using Latent Semantic Analysis"
                }
            }
        },
        {
            "index": 21,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the",
                    "filename": "Indirect-HMM-based_Hypothesis_Alignment_for_Combining_Outputs_from_Machine_Translation_Systems",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D08-1011/Documents_xml/D08-1011.xml",
                    "title": "Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems"
                },
                "llm": {
                    "descriptions": [
                        "This paper proposes an indirect hidden Markov model (IHMM) for monolingual hypothesis alignment in confusion-network-based machine translation (MT) system combination. The IHMM method addresses the challenges of synonym matching and word ordering in hypothesis alignment by using a similarity model for synonym matching and a distortion model for word ordering. The paper presents experimental results showing that the IHMM-based method outperforms the TER-based method on the NIST MT08 Chinese-to-English test set. Furthermore, the paper demonstrates that the IHMM-based system combination method can scale up to combining more systems and produce superior output. Overall, the IHMM-based method provides a significant improvement in machine translation system combination.",
                        "This paper presents an Indirect-HMM-based method for hypothesis alignment in confusion-network-based machine translation system combination. The proposed method addresses the challenges of aligning synonymous words and handling word re-ordering in hypothesis alignment. Unlike traditional HMMs, the parameters of the IHMM are estimated indirectly from various sources, without requiring a large amount of training data. Experimental results show that the IHMM-based method outperforms the TER-based method, and can scale up to combining more systems, producing a better output. The paper also evaluates the effect of semantic and surface similarity models, as well as the distance-based distortion model. The results demonstrate the effectiveness of the IHMM-based method in improving machine translation system combination.",
                        "This academic paper proposes an indirect hidden Markov model (IHMM) for monolingual hypothesis alignment in confusion-network-based machine translation system combination. The IHMM method uses a similarity model for synonym matching and a distortion model for word ordering. The similarity model incorporates both semantic and surface word similarity, while the distortion model models the first-order dependency of word ordering. Experimental results show that the IHMM-based method outperforms the TER-based method, producing a superior output with a case sensitive BLEU score of 34.82. The paper also demonstrates the scalability of the IHMM-based system combination method, achieving a result that is 3.9 BLEU points better than the best official submission of MT08.",
                        "This paper presents an Indirect-HMM-based method for hypothesis alignment in confusion-network-based machine translation system combination. The proposed method uses a similarity model for synonym matching and a distortion model for word ordering. Unlike previous methods, the similarity model explicitly incorporates both semantic and surface word similarity, which is critical for monolingual word alignment. The experimental results demonstrate that the IHMM-based hypothesis alignment method outperforms the TER-based method, leading to superior system combination results on the NIST MT08 C2E test set. Furthermore, the system combination method scales up to combining more systems and produces a better output, achieving a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission of MT08."
                    ],
                    "filename": "indirecthmmbased_hypothesis_alig-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 109 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">System combination has been applied successfully to various machine translation tasks.</S>\n    <S sid=\"7\" ssid=\"2\">Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007).</S>\n    <S sid=\"8\" ssid=\"3\">A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores.</S>\n    <S sid=\"9\" ssid=\"4\">The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by using posterior probability estimates, or by using a combination of these measures and other features.</S>\n    <S sid=\"10\" ssid=\"5\">Constructing a confusion network requires choosing one of the hypotheses as the backbone (also called “skeleton” in the literature), and other hypotheses are aligned to it at the word level.</S>\n    <S sid=\"11\" ssid=\"6\">High quality hypothesis alignment is crucial to the performance of the resulting system combination.</S>\n    <S sid=\"12\" ssid=\"7\">However, there are two challenging issues that make MT hypothesis alignment difficult.</S>\n    <S sid=\"13\" ssid=\"8\">First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.</S>\n    <S sid=\"14\" ssid=\"9\">Second, correct translations may have different word orderings in different hypotheses and these words need to be properly reordered in hypothesis alignment.</S>\n    <S sid=\"15\" ssid=\"10\">In this paper, we propose an indirect hidden Markov model (IHMM) for MT hypothesis alignment.</S>\n    <S sid=\"16\" ssid=\"11\">The HMM provides a way to model both synonym matching and word ordering.</S>\n    <S sid=\"17\" ssid=\"12\">Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distancebased distortion penalty, without using large amount of training data.</S>\n    <S sid=\"18\" ssid=\"13\">Our combined SMT system using the proposed method gave the best result on the Chinese-to-English test in the constrained training track of the 2008 NIST Open MT Evaluation (MT08).</S>\n  </SECTION>\n  <SECTION title=\"2 Confusion-network-based MT system combination\" number=\"2\">\n    <S sid=\"19\" ssid=\"1\">The current state-of-the-art is confusion-networkbased MT system combination as described by Rosti and colleagues (Rosti et al., 2007a, Rosti et al., 2007b).</S>\n    <S sid=\"20\" ssid=\"2\">The major steps are illustrated in Figure 1.</S>\n    <S sid=\"21\" ssid=\"3\">In Fig.</S>\n    <S sid=\"22\" ssid=\"4\">1 (a), hypotheses from different MT systems are first collected.</S>\n    <S sid=\"23\" ssid=\"5\">Then in Fig.</S>\n    <S sid=\"24\" ssid=\"6\">1 (b), one of the hypotheses is selected as the backbone for hypothesis alignment.</S>\n    <S sid=\"25\" ssid=\"7\">This is usually done by a sentence-level minimum Bayes risk (MBR) method which selects a hypothesis that has the minimum average distance compared to all hypotheses.</S>\n    <S sid=\"26\" ssid=\"8\">The backbone determines the word order of the combined output.</S>\n    <S sid=\"27\" ssid=\"9\">Then as illustrated in Fig.</S>\n    <S sid=\"28\" ssid=\"10\">1 (c), all other hypotheses are aligned to the backbone.</S>\n    <S sid=\"29\" ssid=\"11\">Note that in Fig.</S>\n    <S sid=\"30\" ssid=\"12\">1 (c) the symbol ε denotes a null word, which is inserted by the alignment normalization algorithm described in section 3.4.</S>\n    <S sid=\"31\" ssid=\"13\">Fig.</S>\n    <S sid=\"32\" ssid=\"14\">1 (c) also illustrates the handling of synonym alignment (e.g., aligning “car” to “sedan”), and word re-ordering of the hypothesis.</S>\n    <S sid=\"33\" ssid=\"15\">Then in Fig.</S>\n    <S sid=\"34\" ssid=\"16\">1 (d), a confusion network is constructed based on the aligned hypotheses, which consists of a sequence of sets in which each word is aligned to a list of alternative words (including null) in the same set.</S>\n    <S sid=\"35\" ssid=\"17\">Then, a set of global and local features are used to decode the confusion network.</S>\n  </SECTION>\n  <SECTION title=\"3 Indirect-HMM-based Hypothesis Alignment\" number=\"3\">\n    <S sid=\"36\" ssid=\"1\">In confusion-network-based system combination for SMT, a major difficulty is aligning hypotheses to the backbone.</S>\n    <S sid=\"37\" ssid=\"2\">One possible statistical model for word alignment is the HMM, which has been widely used for bilingual word alignment (Vogel et al., 1996, Och and Ney, 2003).</S>\n    <S sid=\"38\" ssid=\"3\">In this paper, we propose an indirect-HMM method for monolingual hypothesis alignment.</S>\n    <S sid=\"39\" ssid=\"4\">Let denote the backbone, �' _ ( ,...,dj) a hypothesis to be aligned to e; , and the alignment that specifies the position of the backbone word aligned to each hypothesis word.</S>\n    <S sid=\"40\" ssid=\"5\">We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence.</S>\n    <S sid=\"41\" ssid=\"6\">We use a first-order HMM, assuming that the emission probability depends only on the backbone word, and the transition probability p(aj I aj_,,I) depends only on the position of the last state and the length of the backbone.</S>\n    <S sid=\"42\" ssid=\"7\">Treating the alignment as hidden variable, the conditional probability that the hypothesis is generated by the backbone is given by As in HMM-based bilingual word alignment (Och and Ney, 2003), we also associate a null with each backbone word to allow generating hypothesis words that do not align to any backbone word.</S>\n    <S sid=\"43\" ssid=\"8\">In HMM-based hypothesis alignment, emission probabilities model the similarity between a backbone word and a hypothesis word, and will be referred to as the similarity model.</S>\n    <S sid=\"44\" ssid=\"9\">The transition probabilities model word reordering, and will be called the distortion model.</S>\n    <S sid=\"45\" ssid=\"10\">The similarity model, which specifies the emission probabilities of the HMM, models the similarity between a backbone word and a hypothesis word.</S>\n    <S sid=\"46\" ssid=\"11\">Since both words are in the same language, the similarity model can be derived based on both semantic similarity and surface similarity, and the overall similarity model is a linear interpolation of the two: where and reflect the semantic and surface similarity between and e; , respectively, and α is the interpolation factor.</S>\n    <S sid=\"47\" ssid=\"12\">Since the semantic similarity between two target words is source-dependent, the semantic similarity model is derived by using the source word sequence as a hidden layer: where is the source sentence.</S>\n    <S sid=\"48\" ssid=\"13\">Moreover, in order to handle the case that two target words are synonyms but neither of them has counter-part in the source sentence, a null is introduced on the source side, which is represented by f0.</S>\n    <S sid=\"49\" ssid=\"14\">The last step in (3) assumes that first ei generates all source words including null.</S>\n    <S sid=\"50\" ssid=\"15\">Then ej’ is generated by all source words including null.</S>\n    <S sid=\"51\" ssid=\"16\">In the common SMT scenario where a large amount of bilingual parallel data is available, we can estimate the translation probabilities from a source word to a target word and vice versa via conventional bilingual word alignment.</S>\n    <S sid=\"52\" ssid=\"17\">Then both p(fk I e;) and in (3) can be derived: where is the translation model from the source-to-target word alignment model, and p(fk I e;) , which enforces the sum-to-1 constraint over all words in the source sentence, takes the following form, where A2s (fk I e;) is the translation model from the target-to-source word alignment model.</S>\n    <S sid=\"53\" ssid=\"18\">In our method, A2s (null I e;) for all target words is simply a constant pnull, whose value is optimized on held-out data 1.</S>\n    <S sid=\"54\" ssid=\"19\">The surface similarity model can be estimated in several ways.</S>\n    <S sid=\"55\" ssid=\"20\">A very simple model could be based on exact match: the surface similarity model, per,,.</S>\n    <S sid=\"56\" ssid=\"21\">(elj I , would take the value 1.0 if e’= e, and 0 otherwise 2 .</S>\n    <S sid=\"57\" ssid=\"22\">However, a smoothed surface similarity model is used in our method.</S>\n    <S sid=\"58\" ssid=\"23\">If the target language uses alphabetic orthography, as English does, we treat words as letter sequences and the similarity measure can be the length of the longest matched prefix (LMP) or the length of the longest common subsequence (LCS) between them.</S>\n    <S sid=\"59\" ssid=\"24\">Then, this raw similarity measure is transformed to a surface similarity score between 0 and 1 through an exponential mapping, where is computed as and is the raw similarity measure of ej’ ei, which is the length of the LMP or LCS of ej’ and ei. and p is a smoothing factor that characterizes the mapping, Thus as p approaches infinity, backs off to the exact match model.</S>\n    <S sid=\"60\" ssid=\"25\">We found the smoothed similarity model of (4) yields slightly better results than the exact match model.</S>\n    <S sid=\"61\" ssid=\"26\">Both LMP- and LCS- based methods achieve similar performance but the computation of LMP is faster.</S>\n    <S sid=\"62\" ssid=\"27\">Therefore, we only report results of the LMP-based smoothed similarity model.</S>\n    <S sid=\"63\" ssid=\"28\">The distortion model, which specifies the transition probabilities of the HMM, models the first-order dependencies of word ordering.</S>\n    <S sid=\"64\" ssid=\"29\">In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities Following Och and Ney (2003), we use a fixed value p0 for the probability of jumping to a null state, which can be optimized on held-out data, and the overall distortion model becomes As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets.</S>\n    <S sid=\"65\" ssid=\"30\">In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6).</S>\n    <S sid=\"66\" ssid=\"31\">The probability mass for transitions with jump distance larger than 6 and less than -4 is uniformly divided.</S>\n    <S sid=\"67\" ssid=\"32\">By doing this, only a handful of c(d) parameters need to be estimated.</S>\n    <S sid=\"68\" ssid=\"33\">Although it is possible to estimate them using the EM algorithm on a small development set, we found that a particularly simple model, described below, works surprisingly well in our experiments.</S>\n    <S sid=\"69\" ssid=\"34\">Since both the backbone and the hypothesis are in the same language, It seems intuitive that the distortion model should favor monotonic alignment and only allow non-monotonic alignment with a certain penalty.</S>\n    <S sid=\"70\" ssid=\"35\">This leads us to use a distortion model of the following form, where K is a tuning factor optimized on held-out data.</S>\n    <S sid=\"71\" ssid=\"36\">As shown in Fig.</S>\n    <S sid=\"72\" ssid=\"37\">2, the value of distortion score peaks at d=1, i.e., the monotonic alignment, and decays for non-monotonic alignments depending on how far it diverges from the monotonic alignment.</S>\n    <S sid=\"73\" ssid=\"38\">Given an HMM, the Viterbi alignment algorithm can be applied to find the best alignment between the backbone and the hypothesis, However, the alignment produced by the algorithm cannot be used directly to build a confusion network.</S>\n    <S sid=\"74\" ssid=\"39\">There are two reasons for this.</S>\n    <S sid=\"75\" ssid=\"40\">First, the alignment produced may contain 1-N mappings between the backbone and the hypothesis whereas 1-1 mappings are required in order to build a confusion network.</S>\n    <S sid=\"76\" ssid=\"41\">Second, if hypothesis words are aligned to a null in the backbone or vice versa, we need to insert actual nulls into the right places in the hypothesis and the backbone, respectively.</S>\n    <S sid=\"77\" ssid=\"42\">Therefore, we need to normalize the alignment produced by Viterbi search.</S>\n    <S sid=\"78\" ssid=\"43\">First, whenever more than one hypothesis words are aligned to one backbone word, we keep the link which gives the highest occupation probability computed via the forward-backward algorithm.</S>\n    <S sid=\"79\" ssid=\"44\">The other hypothesis words originally aligned to the backbone word will be aligned to the null associated with that backbone word.</S>\n    <S sid=\"80\" ssid=\"45\">Second, for the hypothesis words that are aligned to a particular null on the backbone side, a set of nulls are inserted around that backbone word associated with the null such that no links cross each other.</S>\n    <S sid=\"81\" ssid=\"46\">As illustrated in Fig.</S>\n    <S sid=\"82\" ssid=\"47\">3 (a), if a hypothesis word e2’ is aligned to the backbone word e2, a null is inserted in front of the backbone word e2 linked to the hypothesis word e1’ that comes before e2’.</S>\n    <S sid=\"83\" ssid=\"48\">Nulls are also inserted for other hypothesis words such as e3’ and e4’ after the backbone word e2.</S>\n    <S sid=\"84\" ssid=\"49\">If there is no hypothesis word aligned to that backbone word, all nulls are inserted after that backbone word .3 For a backbone word that is aligned to no hypothesis word, a null is inserted on the hypothesis side, right after the hypothesis word which is aligned to the immediately preceding backbone word.</S>\n    <S sid=\"85\" ssid=\"50\">An example is shown in Fig.</S>\n    <S sid=\"86\" ssid=\"51\">3 (b).</S>\n  </SECTION>\n  <SECTION title=\"4 Related work\" number=\"4\">\n    <S sid=\"87\" ssid=\"1\">The two main hypothesis alignment methods for system combination in the previous literature are GIZA++ and TER-based methods.</S>\n    <S sid=\"88\" ssid=\"2\">Matusov et al. (2006) proposed using GIZA++ to align words between different MT hypotheses, where all hypotheses of the test corpus are collected to create hypothesis pairs for GIZA++ training.</S>\n    <S sid=\"89\" ssid=\"3\">This approach uses the conventional HMM model bootstrapped from IBM Model-1 as implemented in GIZA++, and heuristically combines results from aligning in both directions.</S>\n    <S sid=\"90\" ssid=\"4\">System combination based on this approach gives an improvement over the best single system.</S>\n    <S sid=\"91\" ssid=\"5\">However, the number of hypothesis pairs for training is limited by the size of the test corpus.</S>\n    <S sid=\"92\" ssid=\"6\">Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples.</S>\n    <S sid=\"93\" ssid=\"7\">Therefore, GIZA++ training on such a data set may be unreliable.</S>\n    <S sid=\"94\" ssid=\"8\">Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al.</S>\n    <S sid=\"95\" ssid=\"9\">(2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment.</S>\n    <S sid=\"96\" ssid=\"10\">TER (Snover et al., 2006) measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis.</S>\n    <S sid=\"97\" ssid=\"11\">The best alignment is the one that gives the minimum number of translation edits.</S>\n    <S sid=\"98\" ssid=\"12\">TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007).</S>\n    <S sid=\"99\" ssid=\"13\">However, when searching for the optimal alignment, the TER-based method uses a strict surface hard match for counting edits.</S>\n    <S sid=\"100\" ssid=\"14\">Therefore, it is not able to handle synonym matching well.</S>\n    <S sid=\"101\" ssid=\"15\">Moreover, although TER-based alignment allows phrase shifts to accommodate the non-monotonic word ordering, all non-monotonic shifts are penalized equally no matter how short or how long the move is, and this penalty is set to be the same as that for substitution, deletion, and insertion edits.</S>\n    <S sid=\"102\" ssid=\"16\">Therefore, its modeling of non-monotonic word ordering is very coarse-grained.</S>\n    <S sid=\"103\" ssid=\"17\">In contrast to the GIZA++-based method, our IHMM-based method has a similarity model estimated using bilingual word alignment HMMs that are trained on a large amount of bi-text data.</S>\n    <S sid=\"104\" ssid=\"18\">Moreover, the surface similarity information is explicitly incorporated in our model, while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006).</S>\n    <S sid=\"105\" ssid=\"19\">On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model.</S>\n    <S sid=\"106\" ssid=\"20\">There have been other hypothesis alignment methods.</S>\n    <S sid=\"107\" ssid=\"21\">Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al.</S>\n    <S sid=\"108\" ssid=\"22\">(2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005).</S>\n  </SECTION>\n  <SECTION title=\"5 Evaluation\" number=\"5\">\n    <S sid=\"109\" ssid=\"1\">In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008).</S>\n    <S sid=\"110\" ssid=\"2\">We compare to the TER-based method used by Rosti et al. (2007).</S>\n    <S sid=\"111\" ssid=\"3\">In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections.</S>\n    <S sid=\"112\" ssid=\"4\">In our implementation, the backbone is selected with MBR.</S>\n    <S sid=\"113\" ssid=\"5\">Only the top hypothesis from each single system is considered as a backbone.</S>\n    <S sid=\"114\" ssid=\"6\">A uniform posteriori probability is assigned to all hypotheses.</S>\n    <S sid=\"115\" ssid=\"7\">TER is used as loss function in the MBR computation.</S>\n    <S sid=\"116\" ssid=\"8\">Similar to (Rosti et al., 2007), each word in the confusion network is associated with a word posterior probability.</S>\n    <S sid=\"117\" ssid=\"9\">Given a system S, each of its hypotheses is assigned with a rank-based score of 1/(1+r)η, where r is the rank of the hypothesis, and η is a rank smoothing parameter.</S>\n    <S sid=\"118\" ssid=\"10\">The system specific rank-based score of a word w for a given system S is the sum of all the rank-based scores of the hypotheses in system S that contain the word w at the given position (after hypothesis alignment).</S>\n    <S sid=\"119\" ssid=\"11\">This score is then normalized by the sum of the scores of all the alternative words at the same position and from the same system S to generate the system specific word posterior.</S>\n    <S sid=\"120\" ssid=\"12\">Then, the total word posterior of w over all systems is a sum of these system specific posteriors weighted by system weights.</S>\n    <S sid=\"121\" ssid=\"13\">Beside the word posteriors, we use language model scores and a word count as features for confusion network decoding.</S>\n    <S sid=\"122\" ssid=\"14\">Therefore, for an M-way system combination that uses N LMs, a total of M+N+1 decoding parameters, including M-1 system weights, one rank smoothing factor, N language model weights, and one weight for the word count feature, are optimized using Powell’s method (Brent, 1973) to maximize BLEU score on a development set4 .</S>\n    <S sid=\"123\" ssid=\"15\">Two language models are used in our experiments.</S>\n    <S sid=\"124\" ssid=\"16\">One is a trigram model estimated from the English side of the parallel training data, and the other is a 5-gram model trained on the English GigaWord corpus from LDC using the MSRLM toolkit (Nguyen et al, 2007).</S>\n    <S sid=\"125\" ssid=\"17\">4 The parameters of IHMM are not tuned by maximum-BLEU training.</S>\n    <S sid=\"126\" ssid=\"18\">In order to reduce the fluctuation of BLEU scores caused by the inconsistent translation output length, an unsupervised length adaptation method has been devised.</S>\n    <S sid=\"127\" ssid=\"19\">We compute an expected length ratio between the MT output and the source sentences on the development set after maximumBLEU training.</S>\n    <S sid=\"128\" ssid=\"20\">Then during test, we adapt the length of the translation output by adjusting the weight of the word count feature such that the expected output/source length ratio is met.</S>\n    <S sid=\"129\" ssid=\"21\">In our experiments, we apply length adaptation to the system combination output at the level of the whole test corpus.</S>\n    <S sid=\"130\" ssid=\"22\">The development (dev) set used for system combination parameter training contains 1002 sentences sampled from the previous NIST MT Chinese-to-English test sets: 35% from MT04, 55% from MT05, and 10% from MT06-newswire.</S>\n    <S sid=\"131\" ssid=\"23\">The test set is the MT08 Chinese-to-English “current” test set, which includes 1357 sentences from both newswire and web-data genres.</S>\n    <S sid=\"132\" ssid=\"24\">Both dev and test sets have four references per sentence.</S>\n    <S sid=\"133\" ssid=\"25\">As inputs to the system combination, 10-best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems.</S>\n    <S sid=\"134\" ssid=\"26\">All outputs on the MT08 test set were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008).</S>\n    <S sid=\"135\" ssid=\"27\">However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead.</S>\n    <S sid=\"136\" ssid=\"28\">In our main experiments, outputs from a total of eight single MT systems were combined.</S>\n    <S sid=\"137\" ssid=\"29\">As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al.</S>\n    <S sid=\"138\" ssid=\"30\">(2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b).</S>\n    <S sid=\"139\" ssid=\"31\">All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.</S>\n    <S sid=\"140\" ssid=\"32\">These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data.</S>\n    <S sid=\"141\" ssid=\"33\">The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition of MT08.</S>\n    <S sid=\"142\" ssid=\"34\">In the IHMM-based method, the smoothing factor for surface similarity model is set to ρ = 3, the interpolation factor of the overall similarity model is set to α = 0.3, and the controlling factor of the distance-based distortion parameters is set to K=2.</S>\n    <S sid=\"143\" ssid=\"35\">These settings are optimized on the dev set.</S>\n    <S sid=\"144\" ssid=\"36\">Individual system results and system combination results using both IHMM and TER alignment, on both the dev and test sets, are presented in Table 1.</S>\n    <S sid=\"145\" ssid=\"37\">The TER-based hypothesis alignment tool used in our experiments is the publicly available TER Java program, TERCOM (Snover et al., 2006).</S>\n    <S sid=\"146\" ssid=\"38\">Default settings of TERCOM are used in the following experiments.</S>\n    <S sid=\"147\" ssid=\"39\">On the dev set, the case insensitive BLEU score of the IHMM-based 8-way system combination output is about 5.8 points higher than that of the best single system.</S>\n    <S sid=\"148\" ssid=\"40\">Compared to the TER-based method, the IHMM-based method is about 1.5 BLEU points better.</S>\n    <S sid=\"149\" ssid=\"41\">On the MT08 test set, the IHMM-based system combination gave a case sensitive BLEU score of 30.89%.</S>\n    <S sid=\"150\" ssid=\"42\">It outperformed the best single system by 4.7 BLEU points and the TER-based system combination by 1.0 BLEU points.</S>\n    <S sid=\"151\" ssid=\"43\">Note that the best single system on the dev set and the test set are different.</S>\n    <S sid=\"152\" ssid=\"44\">The different single systems are optimized on different tuning sets, so this discrepancy between dev set and test set results is presumably due to differing degrees of mismatch between the dev and test sets and the various tuning sets.</S>\n    <S sid=\"153\" ssid=\"45\">In order to evaluate how well our method performs when we combine more systems, we collected MT outputs on MT08 from seven additional single systems as summarized in Table 2.</S>\n    <S sid=\"154\" ssid=\"46\">These systems belong to two groups.</S>\n    <S sid=\"155\" ssid=\"47\">Sys-9 to Sys-12 are in the first group.</S>\n    <S sid=\"156\" ssid=\"48\">They are syntaxaugmented hierarchical systems similar to those described by Shen et al. (2008) using different Chinese word segmentation and language models.</S>\n    <S sid=\"157\" ssid=\"49\">The second group has Sys-13 to Sys-15.</S>\n    <S sid=\"158\" ssid=\"50\">Sys-13 is a phrasal system proposed by Koehn et al. (2003), Sys-14 is a hierarchical system proposed by Chiang (2007), and Sys-15 is a syntax-based system proposed by Galley et al.</S>\n    <S sid=\"159\" ssid=\"51\">(2006).</S>\n    <S sid=\"160\" ssid=\"52\">All seven systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.</S>\n    <S sid=\"161\" ssid=\"53\">We collected 10-best MT outputs only on the MT08 test set from these seven extra systems.</S>\n    <S sid=\"162\" ssid=\"54\">No MT outputs on our dev set are available from them at present.</S>\n    <S sid=\"163\" ssid=\"55\">Therefore, we directly adopt system combination parameters trained for the previous 8way system combination, except the system weights, which are re-set by the following heuristics: First, the total system weight mass 1.0 is evenly divided among the three groups of single systems: {Sys-1~8}, {Sys-9~12}, and {Sys13~15}.</S>\n    <S sid=\"164\" ssid=\"56\">Each group receives a total system weight mass of 1/3.</S>\n    <S sid=\"165\" ssid=\"57\">Then the weight mass is further divided in each group: in the first group, the original weights of systems 1~8 are multiplied by 1/3; in the second and third groups, the weight mass is evenly distributed within the group, i.e., 1/12 for each system in group 2, and 1/9 for each system in group 35.</S>\n    <S sid=\"166\" ssid=\"58\">Length adaptation is applied to control the final output length, where the same expected length ratio of the previous 8-way system combination is adopted.</S>\n    <S sid=\"167\" ssid=\"59\">The results of the 15-way system combination are presented in Table 3.</S>\n    <S sid=\"168\" ssid=\"60\">It shows that the IHMMbased method is still about 1 BLEU point better than the TER-based method.</S>\n    <S sid=\"169\" ssid=\"61\">Moreover, combining 15 single systems gives an output that has a NIST BLEU score of 34.82%, which is 3.9 points better than the best submission to the NIST MT08 constrained training track (NIST, 2008).</S>\n    <S sid=\"170\" ssid=\"62\">To our knowledge, this is the best result reported on this task.</S>\n    <S sid=\"171\" ssid=\"63\">In this section, we evaluate the effect of the semantic similarity model and the surface similarity model by varying the interpolation weight α of (2).</S>\n    <S sid=\"172\" ssid=\"64\">The results on both the dev and test sets are reported in Table 4.</S>\n    <S sid=\"173\" ssid=\"65\">In one extreme case, α = 1, the overall similarity model is based only on semantic similarity.</S>\n    <S sid=\"174\" ssid=\"66\">This gives a case insensitive BLEU score of 41.70% and a case sensitive BLEU score of 28.92% on the dev and test set, respectively.</S>\n    <S sid=\"175\" ssid=\"67\">The accuracy is significantly improved to 43.62% on the dev set and 30.89% on test set when α = 0.3.</S>\n    <S sid=\"176\" ssid=\"68\">In another extreme case, α = 0, in which only the surface similarity model is used for the overall similarity model, the performance degrades by about 0.2 point.</S>\n    <S sid=\"177\" ssid=\"69\">Therefore, the surface similarity information seems more important for monolingual hypothesis alignment, but both sub-models are useful.</S>\n    <S sid=\"178\" ssid=\"70\">We investigate the effect of the distance-based distortion model by varying the controlling factor K in (6).</S>\n    <S sid=\"179\" ssid=\"71\">For example, setting K=1.0 gives a lineardecay distortion model, and setting K=2.0 gives a quadratic smoothed distance-based distortion model.</S>\n    <S sid=\"180\" ssid=\"72\">As shown in Table 5, the optimal result can be achieved using a properly smoothed distancebased distortion model.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion\" number=\"6\">\n    <S sid=\"181\" ssid=\"1\">Synonym matching and word ordering are two central issues for hypothesis alignment in confusion-network-based MT system combination.</S>\n    <S sid=\"182\" ssid=\"2\">In this paper, an IHMM-based method is proposed for hypothesis alignment.</S>\n    <S sid=\"183\" ssid=\"3\">It uses a similarity model for synonym matching and a distortion model for word ordering.</S>\n    <S sid=\"184\" ssid=\"4\">In contrast to previous methods, the similarity model explicitly incorporates both semantic and surface word similarity, which is critical to monolingual word alignment, and a smoothed distance-based distortion model is used to model the first-order dependency of word ordering, which is shown to be better than simpler approaches.</S>\n    <S sid=\"185\" ssid=\"5\">Our experimental results show that the IHMMbased hypothesis alignment method gave superior results on the NIST MT08 C2E test set compared to the TER-based method.</S>\n    <S sid=\"186\" ssid=\"6\">Moreover, we show that our system combination method can scale up to combining more systems and produce a better output that has a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission of MT08.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgement\" number=\"7\">\n    <S sid=\"187\" ssid=\"1\">The authors are grateful to Chris Quirk, Arul Menezes, Kristina Toutanova, William Dolan, Mu Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, Ming Zhou, George Foster, Roland Kuhn, Jing Zheng, Wen Wang, Necip Fazil Ayan, Dimitra Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin Knight, Jens-Soenke Voeckler, Spyros Matsoukas, and Antti-Veikko Rosti for assistance with the MT systems and/or for the valuable suggestions and discussions.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "2864426014ac3b60fc75fb4e8b4c4475",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems"
                }
            }
        },
        {
            "index": 22,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper proposes a Hidden Markov Model (HMM) and an HMM-based chunk tagger, from which a named entity (NE) recognition (NER) system is built to recognize and classify names, times and numerical quantities. Through the HMM, our system is able to apply and integrate four types of internal and external evidences: 1) simple deterministic internal feature of the words, such as capitalization and digitalization; 2) internal semantic feature of important triggers; 3) internal gazetteer feature; 4) external macro context feature. In this way, the NER problem can be resolved effectively. Evaluation of our system on MUC-6 and MUC-7 English NE tasks achieves F-measures of 96.6% and 94.1% respectively. It shows that the performance is significantly better than reported by any other machine-learning system. Moreover, the performance is even consistently better than those based on handcrafted rules.",
                    "filename": "Named_Entity_Recognition_Using_An_HMM-Based_Chunk_Tagger",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P02-1060/Documents_xml/P02-1060.xml",
                    "title": "Named Entity Recognition Using An HMM-Based Chunk Tagger"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a Named Entity Recognition (NER) system based on a Hidden Markov Model (HMM) chunk tagger. NER is crucial for extracting specific information from documents, and this system aims to address ambiguity, robustness, and portability issues in NER. The paper discusses the use of machine-learning approaches, particularly HMM, for NER, and presents the system's performance in comparison to other methods. The HMM-based chunk tagger effectively integrates different sub-features to capture internal and external evidence for NER. Experimental results demonstrate that this NER system outperforms published machine-learning and rule-based systems, achieving near-human performance. The paper provides detailed insights into the HMM-based chunk tagger, word feature determination, back-off modeling, and experimental results, highlighting the potential for further system improvements.",
                        "This paper introduces a Named Entity Recognition (NER) system using a Hidden Markov Model (HMM) based chunk tagger. NER is crucial for information extraction, and the paper discusses the challenges and importance of this task. The paper presents a detailed description of the HMM-based chunk tagger, word feature determination, back-off modeling, and experimental results. The results show that the proposed system outperforms other machine-learning and rule-based systems, achieving near-human performance. The system effectively integrates various sub-features to capture internal and external evidence for NER. The paper concludes by highlighting the potential for further improvements in the system. This work contributes to advancing the field of NER and demonstrates the effectiveness of the HMM-based approach in addressing the challenges of NER.",
                        "This paper presents a Named Entity Recognition (NER) system using a Hidden Markov Model (HMM) based chunk tagger. NER is a crucial task in information extraction, aiming to classify words in a document into predefined categories. The paper discusses the challenges of NER, the limitations of rule-based approaches, and the benefits of machine-learning systems. It introduces the HMM-based chunk tagger, explaining its generative model and the word features used to capture internal and external evidence. The paper also describes back-off modeling to handle sparseness issues and presents experimental results, demonstrating the high performance of the proposed NER system. Overall, the paper provides a comprehensive overview of the HMM-based approach to NER and its effectiveness in capturing different types of evidence for accurate entity recognition.",
                        "This paper introduces a Named Entity Recognition (NER) system using a Hidden Markov Model (HMM) based chunk tagger. NER is crucial for information extraction, and the paper outlines the significance and challenges of the task. The paper discusses the use of machine learning approaches, particularly HMM, for NER and presents a detailed description of the HMM-based chunk tagger, word feature determination, back-off modeling, and experimental results. The experimental results show that the proposed NER system outperforms both machine-learning and rule-based systems, achieving near-human performance. The paper concludes by highlighting the potential for further improvements in the system. Overall, this paper provides a comprehensive overview of NER using an HMM-based chunk tagger and demonstrates its effectiveness in NER tasks."
                    ],
                    "filename": "named_entity_recognition_using_a-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 136 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Named Entity Recognition Using An HMM-Based Chunk Tagger</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Named Entity (NE) Recognition (NER) is to classify every word in a document into some predefined categories and &amp;quot;none-of-the-above&amp;quot;.</S>\n    <S sid=\"8\" ssid=\"2\">In the taxonomy of computational linguistics tasks, it falls under the domain of &amp;quot;information extraction&amp;quot;, which extracts specific kinds of information from documents as opposed to the more general task of &amp;quot;document management&amp;quot; which seeks to extract all of the information found in a document.</S>\n    <S sid=\"9\" ssid=\"3\">Since entity names form the main content of a document, NER is a very important step toward more intelligent information extraction and management.</S>\n    <S sid=\"10\" ssid=\"4\">The atomic elements of information extraction -- indeed, of language as a whole -- could be considered as the &amp;quot;who&amp;quot;, &amp;quot;where&amp;quot; and &amp;quot;how much&amp;quot; in a sentence.</S>\n    <S sid=\"11\" ssid=\"5\">NER performs what is known as surface parsing, delimiting sequences of tokens that answer these important questions.</S>\n    <S sid=\"12\" ssid=\"6\">NER can also be used as the first step in a chain of processors: a next level of processing could relate two or more NEs, or perhaps even give semantics to that relationship using a verb.</S>\n    <S sid=\"13\" ssid=\"7\">In this way, further processing could discover the &amp;quot;what&amp;quot; and &amp;quot;how&amp;quot; of a sentence or body of text.</S>\n    <S sid=\"14\" ssid=\"8\">While NER is relatively simple and it is fairly easy to build a system with reasonable performance, there are still a large number of ambiguous cases that make it difficult to attain human performance.</S>\n    <S sid=\"15\" ssid=\"9\">There has been a considerable amount of work on NER problem, which aims to address many of these ambiguity, robustness and portability issues.</S>\n    <S sid=\"16\" ssid=\"10\">During last decade, NER has drawn more and more attention from the NE tasks [Chinchor95a] [Chinchor98a] in MUCs [MUC6] [MUC7], where person names, location names, organization names, dates, times, percentages and money amounts are to be delimited in text using SGML mark-ups.</S>\n    <S sid=\"17\" ssid=\"11\">Previous approaches have typically used manually constructed finite state patterns, which attempt to match against a sequence of words in much the same way as a general regular expression matcher.</S>\n    <S sid=\"18\" ssid=\"12\">Typical systems are Univ. of Sheffield's LaSIE-II [Humphreys+98], ISOQuest's NetOwl [Aone+98] [Krupha+98] and Univ. of Edinburgh's LTG [Mikheev+98] [Mikheev+99] for English NER.</S>\n    <S sid=\"19\" ssid=\"13\">These systems are mainly rule-based.</S>\n    <S sid=\"20\" ssid=\"14\">However, rule-based approaches lack the ability of coping with the problems of robustness and portability.</S>\n    <S sid=\"21\" ssid=\"15\">Each new source of text requires significant tweaking of rules to maintain optimal performance and the maintenance costs could be quite steep.</S>\n    <S sid=\"22\" ssid=\"16\">The current trend in NER is to use the machine-learning approach, which is more attractive in that it is trainable and adaptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one.</S>\n    <S sid=\"23\" ssid=\"17\">The representative machine-learning approaches used in NER are HMM (BBN's IdentiFinder in [Miller+98] [Bikel+99] and KRDL's system [Yu+98] for Chinese NER.</S>\n    <S sid=\"24\" ssid=\"18\">), Maximum Entropy (New York Univ.</S>\n    <S sid=\"25\" ssid=\"19\">'s MEME in [Borthwick+98] [Borthwich99]) and Decision Tree (New York Univ.</S>\n    <S sid=\"26\" ssid=\"20\">'s system in [Sekine98] and SRA's system in [Bennett+97]).</S>\n    <S sid=\"27\" ssid=\"21\">Besides, a variant of Eric Brill's transformation-based rules [Brill95] has been applied to the problem [Aberdeen+95].</S>\n    <S sid=\"28\" ssid=\"22\">Among these approaches, the evaluation performance of HMM is higher than those of others.</S>\n    <S sid=\"29\" ssid=\"23\">The main reason may be due to its better ability of capturing the locality of phenomena, which indicates names in text.</S>\n    <S sid=\"30\" ssid=\"24\">Moreover, HMM seems more and more used in NE recognition because of the efficiency of the Viterbi algorithm [Viterbi67] used in decoding the NE-class state sequence.</S>\n    <S sid=\"31\" ssid=\"25\">However, the performance of a machine-learning system is always poorer than that of a rule-based one by about 2% [Chinchor95b] [Chinchor98b].</S>\n    <S sid=\"32\" ssid=\"26\">This may be because current machine-learning approaches capture important evidence behind NER problem much less effectively than human experts who handcraft the rules, although machine-learning approaches always provide important statistical information that is not available to human experts.</S>\n    <S sid=\"33\" ssid=\"27\">As defined in [McDonald96], there are two kinds of evidences that can be used in NER to solve the ambiguity, robustness and portability problems described above.</S>\n    <S sid=\"34\" ssid=\"28\">The first is the internal evidence found within the word and/or word string itself while the second is the external evidence gathered from its context.</S>\n    <S sid=\"35\" ssid=\"29\">In order to effectively apply and integrate internal and external evidences, we present a NER system using a HMM.</S>\n    <S sid=\"36\" ssid=\"30\">The approach behind our NER system is based on the HMM-based chunk tagger in text chunking, which was ranked the best individual system [Zhou+00a] [Zhou+00b] in CoNLL'2000 [Tjong+00].</S>\n    <S sid=\"37\" ssid=\"31\">Here, a NE is regarded as a chunk, named &amp;quot;NE-Chunk&amp;quot;.</S>\n    <S sid=\"38\" ssid=\"32\">To date, our system has been successfully trained and applied in English NER.</S>\n    <S sid=\"39\" ssid=\"33\">To our knowledge, our system outperforms any published machine-learning systems.</S>\n    <S sid=\"40\" ssid=\"34\">Moreover, our system even outperforms any published rule-based systems.</S>\n    <S sid=\"41\" ssid=\"35\">The layout of this paper is as follows.</S>\n    <S sid=\"42\" ssid=\"36\">Section 2 gives a description of the HMM and its application in NER: HMM-based chunk tagger.</S>\n    <S sid=\"43\" ssid=\"37\">Section 3 explains the word feature used to capture both the internal and external evidences.</S>\n    <S sid=\"44\" ssid=\"38\">Section 4 describes the back-off schemes used to tackle the sparseness problem.</S>\n    <S sid=\"45\" ssid=\"39\">Section 5 gives the experimental results of our system.</S>\n    <S sid=\"46\" ssid=\"40\">Section 6 contains our remarks and possible extensions of the proposed work.</S>\n  </SECTION>\n  <SECTION title=\"2 HMM-based Chunk Tagger\" number=\"2\">\n    <S sid=\"47\" ssid=\"1\">Given a token sequence G1n = g1g2 g , the goal The second item in (2-1) is the mutual information between T1n and n simplify the computation of this item, we assume mutual information independence: The basic premise of this model is to consider the raw text, encountered when decoding, as though it had passed through a noisy channel, where it had been originally marked with NE tags.</S>\n    <S sid=\"48\" ssid=\"2\">The job of our generative model is to directly generate the original NE tags from the output words of the noisy channel.</S>\n    <S sid=\"49\" ssid=\"3\">It is obvious that our generative model is reverse to the generative model of traditional HMM1, as used in BBN's IdentiFinder, which models the original process that generates the NE-class annotated words from the original NE tags.</S>\n    <S sid=\"50\" ssid=\"4\">Another difference is that our model assumes mutual information independence (2-2) while traditional HMM assumes conditional probability independence (I-1).</S>\n    <S sid=\"51\" ssid=\"5\">Assumption (2-2) is much looser than assumption (I-1) because assumption (I-1) has the same effect with the sum of assumptions (2-2) and (I-3)2.</S>\n    <S sid=\"52\" ssid=\"6\">In this way, our model can apply more context information to determine the tag of current token.</S>\n    <S sid=\"53\" ssid=\"7\">From equation (2-4), we can see that: We will not discuss both the first and second items further in this paper.</S>\n    <S sid=\"54\" ssid=\"8\">This paper will focus on difference between our tagger and other traditional HMM-based taggers, as used in BBN's IdentiFinder.</S>\n    <S sid=\"55\" ssid=\"9\">Ideally, it can be estimated by using the forward-backward algorithm [Rabiner89] recursively for the 1st-order [Rabiner89] or 2nd -order HMMs [Watson+92].</S>\n    <S sid=\"56\" ssid=\"10\">However, an alternative back-off modeling approach is applied instead in this paper (more details in section 4). arg max log (  |) Then we assume conditional probability word sequence and F1n = f 1 f2 ... fn is the word-feature sequence.</S>\n    <S sid=\"57\" ssid=\"11\">In the meantime, NE-chunk tag ti is structural and consists of three parts: Obviously, there exist some constraints between ti −1 and ti on the boundary and entity categories, as shown in Table 1, where &amp;quot;valid&amp;quot; / &amp;quot;invalid&amp;quot; means the tag sequence ti−1ti is valid / invalid while &amp;quot;valid on&amp;quot; means ti−1ti is valid with an additional condition ECi −1 = ECi .</S>\n    <S sid=\"58\" ssid=\"12\">Such constraints have been used in Viterbi decoding algorithm to ensure valid NE chunking.</S>\n  </SECTION>\n  <SECTION title=\"3 Determining Word Feature\" number=\"3\">\n    <S sid=\"59\" ssid=\"1\">As stated above, token is denoted as ordered pairs of word-feature and word itself: gi =&lt; fi , wi &gt; .</S>\n    <S sid=\"60\" ssid=\"2\">Here, the word-feature is a simple deterministic computation performed on the word and/or word string with appropriate consideration of context as looked up in the lexicon or added to the context.</S>\n    <S sid=\"61\" ssid=\"3\">In our model, each word-feature consists of several sub-features, which can be classified into internal sub-features and external sub-features.</S>\n    <S sid=\"62\" ssid=\"4\">The internal sub-features are found within the word and/or word string itself to capture internal evidence while external sub-features are derived within the context to capture external evidence.</S>\n    <S sid=\"63\" ssid=\"5\">Our model captures three types of internal sub-features: 1) 1 f : simple deterministic internal feature of the words, such as capitalization and digitalization; 2) f 2: internal semantic feature of important triggers; 3) f 3: internal gazetteer feature. f is the basic sub-feature exploited in this model, as shown in Table 2 with the descending order of priority.</S>\n    <S sid=\"64\" ssid=\"6\">For example, in the case of non-disjoint feature classes such as ContainsDigitAndAlpha and ContainsDigitAndDash, the former will take precedence.</S>\n    <S sid=\"65\" ssid=\"7\">The first eleven features arise from the need to distinguish and annotate monetary amounts, percentages, times and dates.</S>\n    <S sid=\"66\" ssid=\"8\">The rest of the features distinguish types of capitalization and all other words such as punctuation marks.</S>\n    <S sid=\"67\" ssid=\"9\">In particular, the FirstWord feature arises from the fact that if a word is capitalized and is the first word of the sentence, we have no good information as to why it is capitalized (but note that AllCaps and CapPeriod are computed before FirstWord, and take precedence.)</S>\n    <S sid=\"68\" ssid=\"10\">This sub-feature is language dependent.</S>\n    <S sid=\"69\" ssid=\"11\">Fortunately, the feature computation is an extremely small part of the implementation.</S>\n    <S sid=\"70\" ssid=\"12\">This kind of internal sub-feature has been widely used in machine-learning systems, such as BBN's IdendiFinder and New York Univ.</S>\n    <S sid=\"71\" ssid=\"13\">'s MENE.</S>\n    <S sid=\"72\" ssid=\"14\">The rationale behind this sub-feature is clear: a) capitalization gives good evidence of NEs in Roman languages; b) Numeric symbols can automatically be grouped into categories.</S>\n    <S sid=\"73\" ssid=\"15\">2) 2 f is the semantic classification of important triggers, as seen in Table 3, and is unique to our system.</S>\n    <S sid=\"74\" ssid=\"16\">It is based on the intuitions that important triggers are useful for NER and can be classified according to their semantics.</S>\n    <S sid=\"75\" ssid=\"17\">This sub-feature applies to both single word and multiple words.</S>\n    <S sid=\"76\" ssid=\"18\">This set of triggers is collected semi-automatically from the NEs and their local context of the training data. determined by finding a match in the gazetteer of the corresponding NE type where n (in Table 4) represents the word number in the matched word string.</S>\n    <S sid=\"77\" ssid=\"19\">In stead of collecting gazetteer lists from training data, we collect a list of 20 public holidays in several countries, a list of 5,000 locations from websites such as GeoHive3, a list of 10,000 organization names from websites such as Yahoo4 and a list of 10,000 famous people from websites such as Scope Systems5.</S>\n    <S sid=\"78\" ssid=\"20\">Gazetters have been widely used in NER systems to improve performance.</S>\n    <S sid=\"79\" ssid=\"21\">For external evidence, only one external macro context feature 4 f , as shown in Table 5, is captured in our model.</S>\n    <S sid=\"80\" ssid=\"22\">4 f is about whether and how the encountered NE candidate is occurred in the list of NEs already recognized from the document, as shown in Table 5 (n is the word number in the matched NE from the recognized NE list and m is the matched word number between the word string and the matched NE with the corresponding NE type.).</S>\n    <S sid=\"81\" ssid=\"23\">This sub-feature is unique to our system.</S>\n    <S sid=\"82\" ssid=\"24\">The intuition behind this is the phenomena of name alias.</S>\n    <S sid=\"83\" ssid=\"25\">During decoding, the NEs already recognized from the document are stored in a list.</S>\n    <S sid=\"84\" ssid=\"26\">When the system encounters a NE candidate, a name alias algorithm is invoked to dynamically determine its relationship with the NEs in the recognized list.</S>\n    <S sid=\"85\" ssid=\"27\">Initially, we also consider part-of-speech (POS) sub-feature.</S>\n    <S sid=\"86\" ssid=\"28\">However, the experimental result is disappointing that incorporation of POS even decreases the performance by 2%.</S>\n    <S sid=\"87\" ssid=\"29\">This may be because capitalization information of a word is submerged in the muddy of several POS tags and the performance of POS tagging is not satisfactory, especially for unknown capitalized words (since many of NEs include unknown capitalized words.).</S>\n    <S sid=\"88\" ssid=\"30\">Therefore, POS is discarded.</S>\n  </SECTION>\n  <SECTION title=\"4 Back-off Modeling\" number=\"4\">\n    <S sid=\"89\" ssid=\"1\">Given the model in section 2 and word feature in section 3, the main problem is how to sufficient training data for every event whose conditional probability we wish to calculate.</S>\n    <S sid=\"90\" ssid=\"2\">Unfortunately, there is rarely enough training data to compute accurate probabilities when decoding on new data, especially considering the complex word feature described above.</S>\n    <S sid=\"91\" ssid=\"3\">In order to resolve the sparseness problem, two levels of back-off modeling are applied to approximate ( / 1 ) 1) First level back-off scheme is based on different contexts of word features and words themselves, and n descending order of fi −2 fi −1 fi wi , fi w ifi+1fi+2 , fi−1fiwi , fiwifi+1 , f i − 1 wi− 1 f i , fifi+1wi+1 , fi−2fi−1 f i , f i f i +1 f i+2 , fi wi , fi −2fi −1fi , fifi +1 and fi .</S>\n    <S sid=\"92\" ssid=\"4\">2) The second level back-off scheme is based on different combinations of the four sub-features described in section 3, and fk is approximated in the descending order of 12 3 4 f k f k f k fk , 1 3 fk fk , fk fk , 1 2 1 4fkfk and 1 fk .</S>\n  </SECTION>\n  <SECTION title=\"5 Experimental Results\" number=\"5\">\n    <S sid=\"93\" ssid=\"1\">In this section, we will report the experimental results of our system for English NER on MUC-6 and MUC-7 NE shared tasks, as shown in Table 6, and then for the impact of training data size on performance using MUC-7 training data.</S>\n    <S sid=\"94\" ssid=\"2\">For each experiment, we have the MUC dry-run data as the held-out development data and the MUC formal test data as the held-out test data.</S>\n    <S sid=\"95\" ssid=\"3\">For both MUC-6 and MUC-7 NE tasks, Table 7 shows the performance of our system using MUC evaluation while Figure 1 gives the comparisons of our system with others.</S>\n    <S sid=\"96\" ssid=\"4\">Here, the precision (P) measures the number of correct NEs in the answer file over the total number of NEs in the answer file and the recall (R) measures the number of correct NEs in the answer file over the total number of NEs in the key file while F-measure is the weighted harmonic mean of precision and recall: with β2 =1.</S>\n    <S sid=\"97\" ssid=\"5\">It shows that the performance is significantly better than reported by any other machine-learning system.</S>\n    <S sid=\"98\" ssid=\"6\">Moreover, the performance is consistently better than those based on handcrafted rules.</S>\n    <S sid=\"99\" ssid=\"7\">With any learning technique, one important question is how much training data is required to achieve acceptable performance.</S>\n    <S sid=\"100\" ssid=\"8\">More generally how does the performance vary as the training data size changes?</S>\n    <S sid=\"101\" ssid=\"9\">The result is shown in Figure 2 for MUC-7 NE task.</S>\n    <S sid=\"102\" ssid=\"10\">It shows that 200KB of training data would have given the performance of 90% while reducing to 100KB would have had a significant decrease in the performance.</S>\n    <S sid=\"103\" ssid=\"11\">It also shows that our system still has some room for performance improvement.</S>\n    <S sid=\"104\" ssid=\"12\">This may be because of the complex word feature and the corresponding sparseness problem existing in our system.</S>\n    <S sid=\"105\" ssid=\"13\">Another important question is about the effect of different sub-features.</S>\n    <S sid=\"106\" ssid=\"14\">Table 8 answers the question on MUC-7 NE task: 1) Applying only 1 f gives our system the performance of 77.6%.</S>\n    <S sid=\"107\" ssid=\"15\">2) 2 f is very useful for NER and increases the performance further by 10% to 87.4%.</S>\n    <S sid=\"108\" ssid=\"16\">3) 4 f is impressive too with another 5.5% performance improvement.</S>\n    <S sid=\"109\" ssid=\"17\">4) However, 3 f contributes only further 1.2% to the performance.</S>\n    <S sid=\"110\" ssid=\"18\">This may be because information included in 3 f has already been captured by 2 f and f4 .</S>\n    <S sid=\"111\" ssid=\"19\">Actually, the experiments show that the contribution of 3 f comes from where there is no explicit indicator information in/around the NE and there is no reference to other NEs in the macro context of the document.</S>\n    <S sid=\"112\" ssid=\"20\">The NEs contributed by 3 f are always well-known ones, e.g.</S>\n    <S sid=\"113\" ssid=\"21\">Microsoft, IBM and Bach (a composer), which are introduced in texts without much helpful context.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion\" number=\"6\">\n    <S sid=\"114\" ssid=\"1\">This paper proposes a HMM in that a new generative model, based on the mutual information independence assumption (2-3) instead of the conditional probability independence assumption (I-1) after Bayes' rule, is applied.</S>\n    <S sid=\"115\" ssid=\"2\">Moreover, it shows that the HMM-based chunk tagger can effectively apply and integrate four different kinds of sub-features, ranging from internal word information to semantic information to NE gazetteers to macro context of the document, to capture internal and external evidences for NER problem.</S>\n    <S sid=\"116\" ssid=\"3\">It also shows that our NER system can reach &amp;quot;near human performance&amp;quot;.</S>\n    <S sid=\"117\" ssid=\"4\">To our knowledge, our NER system outperforms any published machine-learning system and any published rule-based system.</S>\n    <S sid=\"118\" ssid=\"5\">While the experimental results have been impressive, there is still much that can be done potentially to improve the performance.</S>\n    <S sid=\"119\" ssid=\"6\">In the near feature, we would like to incorporate the following into our system:</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "88aa421a0130f02a92f72a55ca085b76",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Named Entity Recognition Using An HMM-Based Chunk Tagger"
                }
            }
        },
        {
            "index": 23,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We how to solve this dilemma with sama simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",
                    "filename": "Incorporating_Non-Local_Information_Into_Information_Extraction_Systems_By_Gibbs_Sampling",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P05-1045/Documents_xml/P05-1045.xml",
                    "title": "Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling"
                },
                "llm": {
                    "descriptions": [
                        "This paper discusses the incorporation of non-local information into information extraction systems using Gibbs sampling. The authors address the limitations of existing statistical models in natural language processing, which represent only local structure, by introducing approximate inference algorithms. They demonstrate the use of Gibbs sampling to add non-local dependencies to sequence models for information extraction, resulting in significant improvements on two established IE tasks. The paper also explores the application of non-local structure models to various tasks, such as named entity recognition and information extraction. The authors present experimental results comparing the impact of adding non-local models with Gibbs sampling to their baseline CRF implementation, showing promising increases in accuracy. Overall, the paper provides a novel approach to incorporating non-local information into information extraction systems.",
                        "This paper presents a method for incorporating non-local information into information extraction systems using Gibbs sampling. Most statistical models used in natural language processing only represent local structure, which is a key limitation for tasks that require non-local structure. The paper introduces a method to relax the exact inference requirement and use approximate inference algorithms, such as Gibbs sampling, to enable tractable inference in models with non-local structure. The authors demonstrate the effectiveness of this technique by incorporating non-local dependencies into sequence models for information extraction tasks. The results show significant improvements on two established information extraction tasks, demonstrating that the proposed method can successfully impose long-distance constraints. The paper concludes by discussing the generalizability of the model to other statistical models and tasks.",
                        "This academic paper discusses the incorporation of non-local information into information extraction systems using Gibbs sampling. The authors identify the limitations of current statistical models in natural language processing, which only represent local structure. To address this, the paper proposes the use of Gibbs sampling to relax the requirement of exact inference, allowing for the incorporation of non-local dependencies into sequence models for information extraction. The paper presents results from experiments on established datasets, demonstrating the effectiveness of adding non-local models with Gibbs sampling to the baseline CRF implementation. The results show increased accuracy in named entity recognition tasks. The paper emphasizes the computational cost of the proposed method but highlights its stability and potential for broader applications. The work is supported by the Advanced Research and Development Activity (ARDA)’s Advanced Question Answering for Intelligence (AQUAINT) Program.",
                        "This paper discusses the incorporation of non-local information into information extraction systems using Gibbs sampling. Most statistical models in natural language processing are limited to representing local structure, which can be a key constraint in many tasks. The paper introduces Gibbs sampling as an algorithm to relax the requirement of exact inference, allowing tractable computation in models with non-local structure. The authors demonstrate the effectiveness of this technique by adding non-local dependencies to sequence models for information extraction, using it to improve performance on established tasks. The paper also outlines the evaluation of the technique on two datasets and compares the results with a baseline CRF implementation. The study concludes that the proposed method can effectively impose various long-distance constraints and has the potential for application in other statistical models and tasks."
                    ],
                    "filename": "incorporating_nonlocal_informati-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 132 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Most statistical models currently used in natural language processing represent only local structure.</S>\n    <S sid=\"7\" ssid=\"2\">Although this constraint is critical in enabling tractable model inference, it is a key limitation in many tasks, since natural language contains a great deal of nonlocal structure.</S>\n    <S sid=\"8\" ssid=\"3\">A general method for solving this problem is to relax the requirement of exact inference, substituting approximate inference algorithms instead, thereby permitting tractable inference in models with non-local structure.</S>\n    <S sid=\"9\" ssid=\"4\">One such algorithm is Gibbs sampling, a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model, including sequence models and probabilistic context free grammars (Geman and Geman, 1984).</S>\n    <S sid=\"10\" ssid=\"5\">Although Gibbs sampling is widely used elsewhere, there has been extremely little use of it in natural language processing.1 Here, we use it to add non-local dependencies to sequence models for information extraction.</S>\n    <S sid=\"11\" ssid=\"6\">Statistical hidden state sequence models, such as Hidden Markov Models (HMMs) (Leek, 1997; Freitag and McCallum, 1999), Conditional Markov Models (CMMs) (Borthwick, 1999), and Conditional Random Fields (CRFs) (Lafferty et al., 2001) are a prominent recent approach to information extraction tasks.</S>\n    <S sid=\"12\" ssid=\"7\">These models all encode the Markov property: decisions about the state at a particular position in the sequence can depend only on a small local window.</S>\n    <S sid=\"13\" ssid=\"8\">It is this property which allows tractable computation: the Viterbi, Forward Backward, and Clique Calibration algorithms all become intractable without it.</S>\n    <S sid=\"14\" ssid=\"9\">However, information extraction tasks can benefit from modeling non-local structure.</S>\n    <S sid=\"15\" ssid=\"10\">As an example, several authors (see Section 8) mention the value of enforcing label consistency in named entity recognition (NER) tasks.</S>\n    <S sid=\"16\" ssid=\"11\">In the example given in Figure 1, the second occurrence of the token Tanjug is mislabeled by our CRF-based statistical NER system, because by looking only at local evidence it is unclear whether it is a person or organization.</S>\n    <S sid=\"17\" ssid=\"12\">The first occurrence of Tanjug provides ample evidence that it is an organization, however, and by enforcing label consistency the system should be able to get it right.</S>\n    <S sid=\"18\" ssid=\"13\">We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure, and demonstrate that this technique yields significant improvements on two established IE tasks.</S>\n    <S sid=\"19\" ssid=\"14\">In hidden state sequence models such as HMMs, CMMs, and CRFs, it is standard to use the Viterbi algorithm, a dynamic programming algorithm, to infer the most likely hidden state sequence given the input and the model (see, e.g., Rabiner (1989)).</S>\n    <S sid=\"20\" ssid=\"15\">Although this is the only tractable method for exact computation, there are other methods for computing an approximate solution.</S>\n    <S sid=\"21\" ssid=\"16\">Monte Carlo methods are a simple and effective class of methods for approximate inference based on sampling.</S>\n    <S sid=\"22\" ssid=\"17\">Imagine we have a hidden state sequence model which defines a probability distribution over state sequences conditioned on any given input.</S>\n    <S sid=\"23\" ssid=\"18\">With such a model M we should be able to compute the conditional probability PM(s|o) of any state sequence s = {s0, ... , sN} given some observed input sequence o = {o0, ... , oN}.</S>\n    <S sid=\"24\" ssid=\"19\">One can then sample sequences from the conditional distribution defined by the model.</S>\n    <S sid=\"25\" ssid=\"20\">These samples are likely to be in high probability areas, increasing our chances of finding the maximum.</S>\n    <S sid=\"26\" ssid=\"21\">The challenge is how to sample sequences efficiently from the conditional distribution defined by the model.</S>\n    <S sid=\"27\" ssid=\"22\">Gibbs sampling provides a clever solution (Geman and Geman, 1984).</S>\n    <S sid=\"28\" ssid=\"23\">Gibbs sampling defines a Markov chain in the space of possible variable assignments (in this case, hidden state sequences) such that the stationary distribution of the Markov chain is the joint distribution over the variables.</S>\n    <S sid=\"29\" ssid=\"24\">Thus it is called a Markov Chain Monte Carlo (MCMC) method; see Andrieu et al. (2003) for a good MCMC tutorial.</S>\n    <S sid=\"30\" ssid=\"25\">In practical terms, this means that we can walk the Markov chain, occasionally outputting samples, and that these samples are guaranteed to be drawn from the target distribution.</S>\n    <S sid=\"31\" ssid=\"26\">Furthermore, the chain is defined in very simple terms: from each state sequence we can only transition to a state sequence obtained by changing the state at any one position i, and the distribution over these possible transitions is just where s−i is all states except si.</S>\n    <S sid=\"32\" ssid=\"27\">In other words, the transition probability of the Markov chain is the conditional distribution of the label at the position given the rest of the sequence.</S>\n    <S sid=\"33\" ssid=\"28\">This quantity is easy to compute in any Markov sequence model, including HMMs, CMMs, and CRFs.</S>\n    <S sid=\"34\" ssid=\"29\">One easy way to walk the Markov chain is to loop through the positions i from 1 to N, and for each one, to resample the hidden state at that position from the distribution given in Equation 1.</S>\n    <S sid=\"35\" ssid=\"30\">By outputting complete sequences at regular intervals (such as after resampling all N positions), we can sample sequences from the conditional distribution defined by the model.</S>\n    <S sid=\"36\" ssid=\"31\">This is still a gravely inefficient process, however.</S>\n    <S sid=\"37\" ssid=\"32\">Random sampling may be a good way to estimate the shape of a probability distribution, but it is not an efficient way to do what we want: find the maximum.</S>\n    <S sid=\"38\" ssid=\"33\">However, we cannot just transition greedily to higher probability sequences at each step, because the space is extremely non-convex.</S>\n    <S sid=\"39\" ssid=\"34\">We can, however, borrow a technique from the study of non-convex optimization and use simulated annealing (Kirkpatrick et al., 1983).</S>\n    <S sid=\"40\" ssid=\"35\">Geman and Geman (1984) show that it is easy to modify a Gibbs Markov chain to do annealing; at time t we replace the distribution in (1) with where c = {c0, ... , cT} defines a cooling schedule.</S>\n    <S sid=\"41\" ssid=\"36\">At each step, we raise each value in the conditional distribution to an exponent and renormalize before sampling from it.</S>\n    <S sid=\"42\" ssid=\"37\">Note that when c = 1 the distribution is unchanged, and as c → 0 the distribution becomes sharper, and when c = 0 the distribution places all of its mass on the maximal outcome, having the effect that the Markov chain always climbs uphill.</S>\n    <S sid=\"43\" ssid=\"38\">Thus if we gradually decrease c from 1 to 0, the Markov chain increasingly tends to go uphill.</S>\n    <S sid=\"44\" ssid=\"39\">This annealing technique has been shown to be an effective technique for stochastic optimization (Laarhoven and Arts, 1987).</S>\n    <S sid=\"45\" ssid=\"40\">To verify the effectiveness of Gibbs sampling and simulated annealing as an inference technique for hidden state sequence models, we compare Gibbs and Viterbi inference methods for a basic CRF, without the addition of any non-local model.</S>\n    <S sid=\"46\" ssid=\"41\">The results, given in Table 1, show that if the Gibbs sampler is run long enough, its accuracy is the same as a Viterbi decoder.</S>\n  </SECTION>\n  <SECTION title=\"3 A Conditional Random Field Model\" number=\"2\">\n    <S sid=\"47\" ssid=\"1\">Our basic CRF model follows that of Lafferty et al. (2001).</S>\n    <S sid=\"48\" ssid=\"2\">We choose a CRF because it represents the state of the art in sequence modeling, allowing both discriminative training and the bi-directional flow of probabilistic information across the sequence.</S>\n    <S sid=\"49\" ssid=\"3\">A CRF is a conditional sequence model which represents the probability of a hidden state sequence given some observations.</S>\n    <S sid=\"50\" ssid=\"4\">In order to facilitate obtaining the conditional probabilities we need for Gibbs sampling, we generalize the CRF model in a way that is consistent with the Markov Network literature (see Cowell et al. (1999)): we create a linear chain of cliques, where each clique, c, represents the probabilistic relationship between an adjacent pair of states2 using a clique potential φc, which is just a table containing a value for each possible state assignment.</S>\n    <S sid=\"51\" ssid=\"5\">The table is not a true probability distribution, as it only accounts for local interactions within the clique.</S>\n    <S sid=\"52\" ssid=\"6\">The clique potentials themselves are defined in terms of exponential models conditioned on features of the observation sequence, and must be instantiated for each new observation sequence.</S>\n    <S sid=\"53\" ssid=\"7\">The sequence of potentials in the clique chain then defines the probability of a state sequence (given the observation sequence) as where φi(si−1, si) is the element of the clique potential at position i corresponding to states si−1 and si.3 Although a full treatment of CRF training is beyond the scope of this paper (our technique assumes the model is already trained), we list the features used by our CRF for the two tasks we address in Table 2.</S>\n    <S sid=\"54\" ssid=\"8\">During training, we regularized our exponential models with a quadratic prior and used the quasi-Newton method for parameter optimization.</S>\n    <S sid=\"55\" ssid=\"9\">As is customary, we used the Viterbi algorithm to infer the most likely state sequence in a CRF.</S>\n    <S sid=\"56\" ssid=\"10\">The clique potentials of the CRF, instantiated for some observation sequence, can be used to easily compute the conditional distribution over states at a position given in Equation 1.</S>\n    <S sid=\"57\" ssid=\"11\">Recall that at position i we want to condition on the states in the rest of the sequence.</S>\n    <S sid=\"58\" ssid=\"12\">The state at this position can be influenced by any other state that it shares a clique with; in particular, when the clique size is 2, there are 2 such cliques.</S>\n    <S sid=\"59\" ssid=\"13\">In this case the Markov blanket of the state (the minimal set of states that renders a state conditionally independent of all other states) consists of the two neighboring states and the observation sequence, all of which are observed.</S>\n    <S sid=\"60\" ssid=\"14\">The conditional distribution at position i can then be computed simply as where the factor tables F in the clique chain are already conditioned on the observation sequence.</S>\n  </SECTION>\n  <SECTION title=\"4 Datasets and Evaluation\" number=\"3\">\n    <S sid=\"61\" ssid=\"1\">We test the effectiveness of our technique on two established datasets: the CoNLL 2003 English named entity recognition dataset, and the CMU Seminar Announcements information extraction dataset.</S>\n    <S sid=\"62\" ssid=\"2\">This dataset was created for the shared task of the Seventh Conference on Computational Natural Language Learning (CoNLL),4 which concerned named entity recognition.</S>\n    <S sid=\"63\" ssid=\"3\">The English data is a collection of Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC).</S>\n    <S sid=\"64\" ssid=\"4\">The data is separated into a training set, a development set (testa), and a test set (testb).</S>\n    <S sid=\"65\" ssid=\"5\">The training set contains 945 documents, and approximately 203,000 tokens.</S>\n    <S sid=\"66\" ssid=\"6\">The development set has 216 documents and approximately 51,000 tokens, and the test set has 231 documents and approximately 46,000 tokens.</S>\n    <S sid=\"67\" ssid=\"7\">We evaluate performance on this task in the manner dictated by the competition so that results can be properly compared.</S>\n    <S sid=\"68\" ssid=\"8\">Precision and recall are evaluated on a per-entity basis (and combined into an F1 score).</S>\n    <S sid=\"69\" ssid=\"9\">There is no partial credit; an incorrect entity boundary is penalized as both a false positive and as a false negative.</S>\n    <S sid=\"70\" ssid=\"10\">This dataset was developed as part of Dayne Freitag’s dissertation research Freitag (1998).5 It consists of 485 emails containing seminar announcements at Carnegie Mellon University.</S>\n    <S sid=\"71\" ssid=\"11\">It is annotated for four fields: speaker, location, start time, and end time.</S>\n    <S sid=\"72\" ssid=\"12\">Sutton and McCallum (2004) used 5-fold cross validation when evaluating on this dataset, so we obtained and used their data splits, so that results can be properly compared.</S>\n    <S sid=\"73\" ssid=\"13\">Because the entire dataset is used for testing, there is no development set.</S>\n    <S sid=\"74\" ssid=\"14\">We also used their evaluation metric, which is slightly different from the method for CoNLL data.</S>\n    <S sid=\"75\" ssid=\"15\">Instead of evaluating precision and recall on a per-entity basis, they are evaluated on a per-token basis.</S>\n    <S sid=\"76\" ssid=\"16\">Then, to calculate the overall F1 score, the F1 scores for each class are averaged.</S>\n  </SECTION>\n  <SECTION title=\"5 Models of Non-local Structure\" number=\"4\">\n    <S sid=\"77\" ssid=\"1\">Our models of non-local structure are themselves just sequence models, defining a probability distribution over all possible state sequences.</S>\n    <S sid=\"78\" ssid=\"2\">It is possible to flexibly model various forms of constraints in a way that is sensitive to the linguistic structure of the data (e.g., one can go beyond imposing just exact identity conditions).</S>\n    <S sid=\"79\" ssid=\"3\">One could imagine many ways of defining such models; for simplicity we use the form where the product is over a set of violation types A, and for each violation type A we specify a penalty parameter θλ.</S>\n    <S sid=\"80\" ssid=\"4\">The exponent #(A, s, o) is the count of the number of times that the violation A occurs in the state sequence s with respect to the observation sequence o.</S>\n    <S sid=\"81\" ssid=\"5\">This has the effect of assigning sequences with more violations a lower probability.</S>\n    <S sid=\"82\" ssid=\"6\">The particular violation types are defined specifically for each task, and are described in the following two sections.</S>\n    <S sid=\"83\" ssid=\"7\">This model, as defined above, is not normalized, and clearly it would be expensive to do so.</S>\n    <S sid=\"84\" ssid=\"8\">This doesn’t matter, however, because we only use the model for Gibbs sampling, and so only need to compute the conditional distribution at a single position i (as defined in Equation 1).</S>\n    <S sid=\"85\" ssid=\"9\">One (inefficient) way to compute this quantity is to enumerate all possible sequences differing only at position i, compute the score assigned to each by the model, and renormalize.</S>\n    <S sid=\"86\" ssid=\"10\">Although it seems expensive, this computation can be made very efficient with a straightforward memoization technique: at all times we maintain data structures representing the relationship between entity labels and token sequences, from which we can quickly compute counts of different types of violations.</S>\n    <S sid=\"87\" ssid=\"11\">Label consistency structure derives from the fact that within a particular document, different occurrences of a particular token sequence are unlikely to be labeled as different entity types.</S>\n    <S sid=\"88\" ssid=\"12\">Although any one occurrence may be ambiguous, it is unlikely that all instances are unclear when taken together.</S>\n    <S sid=\"89\" ssid=\"13\">The CoNLL training data empirically supports the strength of the label consistency constraint.</S>\n    <S sid=\"90\" ssid=\"14\">Table 3 shows the counts of entity labels for each pair of identical token sequences within a document, where both are labeled as an entity.</S>\n    <S sid=\"91\" ssid=\"15\">Note that inconsistent labelings are very rare.6 In addition, we also want to model subsequence constraints: having seen Geoff Woods earlier in a document as a person is a good indicator that a subsequent occurrence of Woods should also be labeled as a person.</S>\n    <S sid=\"92\" ssid=\"16\">However, if we examine all cases of the labelings of other occurrences of subsequences of a labeled entity, we find that the consistency constraint does not hold nearly so strictly in this case.</S>\n    <S sid=\"93\" ssid=\"17\">As an example, one document contains references to both The China Daily, a newspaper, and China, the country.</S>\n    <S sid=\"94\" ssid=\"18\">Counts of subsequence labelings within a document are listed in Table 4.</S>\n    <S sid=\"95\" ssid=\"19\">Note that there are many offdiagonal entries: the China Daily case is the most common, occurring 328 times in the dataset.</S>\n    <S sid=\"96\" ssid=\"20\">The penalties used in the long distance constraint model for CoNLL are the Empirical Bayes estimates taken directly from the data (Tables 3 and 4), except that we change counts of 0 to be 1, so that the distribution remains positive.</S>\n    <S sid=\"97\" ssid=\"21\">So the estimate of a PER also being an ORG is 5 3151; there were 5 instance of an entity being labeled as both, PER appeared 3150 times in the data, and we add 1 to this for smoothing, because PER-MISC never occured.</S>\n    <S sid=\"98\" ssid=\"22\">However, when we have a phrase labeled differently in two different places, continuing with the PER-ORG example, it is unclear if we should penalize it as PER that is also an ORG or an ORG that is also a PER.</S>\n    <S sid=\"99\" ssid=\"23\">To deal with this, we multiply the square roots of each estimate together to form the penalty term.</S>\n    <S sid=\"100\" ssid=\"24\">The penalty term is then multiplied in a number of times equal to the length of the offending entity; this is meant to “encourage” the entity to shrink.7 For example, say we have a document with three entities, Rotor Volgograd twice, once labeled as PER and once as ORG, and Rotor, labeled as an ORG.</S>\n    <S sid=\"101\" ssid=\"25\">The likelihood of a PER also being an ORG is 5 3151, and of an ORG also Due to the lack of a development set, our consistency model for the CMU Seminar Announcements is much simpler than the CoNLL model, the numbers where selected due to our intuitions, and we did not spend much time hand optimizing the model.</S>\n    <S sid=\"102\" ssid=\"26\">Specifically, we had three constraints.</S>\n    <S sid=\"103\" ssid=\"27\">The first is that all entities labeled as start time are normalized, and are penalized if they are inconsistent.</S>\n    <S sid=\"104\" ssid=\"28\">The second is a corresponding constraint for end times.</S>\n    <S sid=\"105\" ssid=\"29\">The last constraint attempts to consistently label the speakers.</S>\n    <S sid=\"106\" ssid=\"30\">If a phrase is labeled as a speaker, we assume that the last word is the speaker’s last name, and we penalize for each occurrance of that word which is not also labeled speaker.</S>\n    <S sid=\"107\" ssid=\"31\">For the start and end times the penalty is multiplied in based on how many words are in the entity.</S>\n    <S sid=\"108\" ssid=\"32\">For the speaker, the penalty is only multiplied in once.</S>\n    <S sid=\"109\" ssid=\"33\">We used a hand selected penalty of exp −4.0.</S>\n  </SECTION>\n  <SECTION title=\"6 Combining Sequence Models\" number=\"5\">\n    <S sid=\"110\" ssid=\"1\">In the previous section we defined two models of non-local structure.</S>\n    <S sid=\"111\" ssid=\"2\">Now we would like to incorporate them into the local model (in our case, the trained CRF), and use Gibbs sampling to find the most likely state sequence.</S>\n    <S sid=\"112\" ssid=\"3\">Because both the trained CRF and the non-local models are themselves sequence models, we simply combine the two models into a factored sequence model of the following form where M is the local CRF model, L is the new nonlocal model, and F is the factored model.8 In this form, the probability again looks difficult to compute (because of the normalizing factor, a sum over all hidden state sequences of length N).</S>\n    <S sid=\"113\" ssid=\"4\">However, since we are only using the model for Gibbs sampling, we never need to compute the distribution explicitly.</S>\n    <S sid=\"114\" ssid=\"5\">Instead, we need only the conditional probability of each position in the sequence, which can be computed as</S>\n  </SECTION>\n  <SECTION title=\"7 Results and Discussion\" number=\"6\">\n    <S sid=\"115\" ssid=\"1\">In our experiments we compare the impact of adding the non-local models with Gibbs sampling to our baseline CRF implementation.</S>\n    <S sid=\"116\" ssid=\"2\">In the CoNLL named entity recognition task, the non-local models increase the F1 accuracy by about 1.3%.</S>\n    <S sid=\"117\" ssid=\"3\">Although such gains may appear modest, note that they are achieved relative to a near state-of-the-art NER system: the winner of the CoNLL English task reported an F1 score of 88.76.</S>\n    <S sid=\"118\" ssid=\"4\">In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task.</S>\n    <S sid=\"119\" ssid=\"5\">Our performance is similar on the CMU Seminar Announcements dataset.</S>\n    <S sid=\"120\" ssid=\"6\">We show the per-field F1 results that were reported by Sutton and McCallum (2004) for comparison, and note that we are again achieving gains against a more competitive baseline system.</S>\n    <S sid=\"121\" ssid=\"7\">For all experiments involving Gibbs sampling, we used a linear cooling schedule.</S>\n    <S sid=\"122\" ssid=\"8\">For the CoNLL dataset we collected 200 samples per trial, and for the CMU Seminar Announcements we collected 100 samples.</S>\n    <S sid=\"123\" ssid=\"9\">We report the average of all trials, and in all cases we outperform the baseline with greater than 95% confidence, using the standard t-test.</S>\n    <S sid=\"124\" ssid=\"10\">The trials had low standard deviations - 0.083% and 0.007% and high minimun F-scores - 86.72%, and 92.28% - for the CoNLL and CMU Seminar Announcements respectively, demonstrating the stability of our method.</S>\n    <S sid=\"125\" ssid=\"11\">The biggest drawback to our model is the computational cost.</S>\n    <S sid=\"126\" ssid=\"12\">Taking 100 samples dramatically increases test time.</S>\n    <S sid=\"127\" ssid=\"13\">Averaged over 3 runs on both Viterbi and Gibbs, CoNLL testing time increased from 55 to 1738 seconds, and CMU Seminar Announcements testing time increases from 189 to 6436 seconds.</S>\n  </SECTION>\n  <SECTION title=\"8 Related Work\" number=\"7\">\n    <S sid=\"128\" ssid=\"1\">Several authors have successfully incorporated a label consistency constraint into probabilistic sequence model named entity recognition systems.</S>\n    <S sid=\"129\" ssid=\"2\">Mikheev et al. (1999) and Finkel et al.</S>\n    <S sid=\"130\" ssid=\"3\">(2004) incorporate label consistency information by using adhoc multi-stage labeling procedures that are effective but special-purpose.</S>\n    <S sid=\"131\" ssid=\"4\">Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document.</S>\n    <S sid=\"132\" ssid=\"5\">Note that this violates the Markov property, but is achieved by slightly relaxing the requirement of exact inference.</S>\n    <S sid=\"133\" ssid=\"6\">Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences.</S>\n    <S sid=\"134\" ssid=\"7\">This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest.</S>\n    <S sid=\"135\" ssid=\"8\">Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.</S>\n    <S sid=\"136\" ssid=\"9\">This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these “global” features relative to the local ones.</S>\n    <S sid=\"137\" ssid=\"10\">However, this approach cannot easily be extended to incorporate other types of non-local structure.</S>\n    <S sid=\"138\" ssid=\"11\">The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes.</S>\n    <S sid=\"139\" ssid=\"12\">Unfortunately, in the RMN model, the dependencies must be defined in the model structure before doing any inference, and so the authors use crude heuristic part-of-speech patterns, and then add dependencies between these text spans using clique templates.</S>\n    <S sid=\"140\" ssid=\"13\">This generates a extremely large number of overlapping candidate entities, which then necessitates additional templates to enforce the constraint that text subsequences cannot both be different entities, something that is more naturally modeled by a CRF.</S>\n    <S sid=\"141\" ssid=\"14\">Another disadvantage of this approach is that it uses loopy beliefpropagation and a voted perceptron for approximate learning and inference – ill-founded and inherently unstable algorithms which are noted by the authors to have caused convergence problems.</S>\n    <S sid=\"142\" ssid=\"15\">In the skip-chain CRFs model, the decision of which nodes to connect is also made heuristically, and because the authors focus on named entity recognition, they chose to connect all pairs of identical capitalized words.</S>\n    <S sid=\"143\" ssid=\"16\">They also utilize loopy belief propagation for approximate learning and inference.</S>\n    <S sid=\"144\" ssid=\"17\">While the technique we propose is similar mathematically and in spirit to the above approaches, it differs in some important ways.</S>\n    <S sid=\"145\" ssid=\"18\">Our model is implemented by adding additional constraints into the model at inference time, and does not require the preprocessing step necessary in the two previously mentioned works.</S>\n    <S sid=\"146\" ssid=\"19\">This allows for a broader class of long-distance dependencies, because we do not need to make any initial assumptions about which nodes should be connected, and is helpful when you wish to model relationships between nodes which are the same class, but may not be similar in any other way.</S>\n    <S sid=\"147\" ssid=\"20\">For instance, in the CMU Seminar Announcements dataset, we can normalize all entities labeled as a start time and penalize the model if multiple, nonconsistent times are labeled.</S>\n    <S sid=\"148\" ssid=\"21\">This type of constraint cannot be modeled in an RMN or a skip-CRF, because it requires the knowledge that both entities are given the same class label.</S>\n    <S sid=\"149\" ssid=\"22\">We also allow dependencies between multi-word phrases, and not just single words.</S>\n    <S sid=\"150\" ssid=\"23\">Additionally, our model can be applied on top of a pre-existing trained sequence model.</S>\n    <S sid=\"151\" ssid=\"24\">As such, our method does not require complex training procedures, and can instead leverage all of the established methods for training high accuracy sequence models.</S>\n    <S sid=\"152\" ssid=\"25\">It can indeed be used in conjunction with any statistical hidden state sequence model: HMMs, CMMs, CRFs, or even heuristic models.</S>\n    <S sid=\"153\" ssid=\"26\">Third, our technique employs Gibbs sampling for approximate inference, a simple and probabilistically well-founded algorithm.</S>\n    <S sid=\"154\" ssid=\"27\">As a consequence of these differences, our approach is easier to understand, implement, and adapt to new applications.</S>\n  </SECTION>\n  <SECTION title=\"9 Conclusions\" number=\"8\">\n    <S sid=\"155\" ssid=\"1\">We have shown that a constraint model can be effectively combined with an existing sequence model in a factored architecture to successfully impose various sorts of long distance constraints.</S>\n    <S sid=\"156\" ssid=\"2\">Our model generalizes naturally to other statistical models and other tasks.</S>\n    <S sid=\"157\" ssid=\"3\">In particular, it could in the future be applied to statistical parsing.</S>\n    <S sid=\"158\" ssid=\"4\">Statistical context free grammars provide another example of statistical models which are restricted to limiting local structure, and which could benefit from modeling nonlocal structure.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgements\" number=\"9\">\n    <S sid=\"159\" ssid=\"1\">This work was supported in part by the Advanced Researchand Development Activity (ARDA)’s Advanced Question Answeringfor Intelligence (AQUAINT) Program.</S>\n    <S sid=\"160\" ssid=\"2\">Additionally, we would like to that our reviewers for their helpful comments.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "71c62a1343e1bcfbdfcd8b11493e84f2",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling"
                }
            }
        },
        {
            "index": 24,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Syntactic reordering approaches are an effective method for handling word-order differences between source and target lan guages in statistical machine translation(SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntac tic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al, 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. Wealso conducted a series of experiments to an alyze the accuracy and impact of different types of reordering rules.",
                    "filename": "Chinese_Syntactic_Reordering_for_Statistical_Machine_Translation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D07-1077/Documents_xml/D07-1077.xml",
                    "title": "Chinese Syntactic Reordering for Statistical Machine Translation"
                },
                "llm": {
                    "descriptions": [
                        "This paper describes a syntactic reordering approach for translation from Chinese to English within the context of statistical machine translation (SMT) systems. The authors outline the process of applying reordering rules to transform Chinese sentences into a word order closer to that of English. The paper discusses the syntactic reordering rules used, including verb phrases, noun phrases, and clause phrases in Chinese. The study reports results showing that the reordering rules give an improvement in translation accuracy, as well as the precision of the reordering rules. The authors also compare their approach to related work and discuss the limitations of existing systems in handling syntactic reordering. Overall, the paper provides a valuable contribution to the field of statistical machine translation from Chinese to English through syntactic reordering.",
                        "This paper describes a syntactic reordering approach for translation from Chinese to English within the context of statistical machine translation (SMT) systems. The reordering rules are based on systematic differences between Chinese and English word order and are used as a preprocessor for both training and test sentences. The paper reports an improvement in translation accuracy using these reordering rules, with experimental results showing the precision of the rules. The study finds that Chinese parsers, despite their low accuracy, can still be useful in SMT. The paper also discusses related work on preprocessing approaches in SMT systems and provides detailed descriptions of the Chinese syntactic reordering rules, including verb phrases, noun phrases, and clauses. The results demonstrate the effectiveness of the reordering rules in enhancing translation accuracy.",
                        "This academic paper discusses a syntactic reordering approach for translation from Chinese to English. The authors describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order. These rules are used as a preprocessor for both training and test sentences, transforming Chinese sentences to be closer to English. The paper reports results showing an improvement in accuracy from 28.52 to 30.86 BLEU score using the MOSES phrase-based SMT system. The authors also discuss the impact of each type of reordering rule on translation accuracy. Additionally, the paper presents the Chinese syntactic reordering rules for verb phrases, noun phrases, and clause phrases. The research fills a gap in the existing literature, as previous work on this topic has focused on translation from other languages to English.",
                        "This paper describes a syntactic reordering approach for Chinese to English translation in statistical machine translation systems. The study focuses on identifying systematic differences in word order between the two languages and developing reordering rules to address these differences. The paper presents a set of syntactic reordering rules that exploit differences in word order between Chinese and English. The rules target verb phrases, noun phrases, and clausal modifiers in Chinese sentences. The study also reports results on the effectiveness of the reordering rules, showing an improvement in translation accuracy. The paper contributes to the field by addressing the specific challenges of Chinese to English translation, which has not been extensively researched previously. The findings demonstrate that using Chinese parses in statistical machine translation systems can be useful despite the complexities of Chinese syntax."
                    ],
                    "filename": "chinese_syntactic_reordering_for-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 137 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n\t<S sid=\"0\">Chinese Syntactic Reordering for Statistical Machine Translation</S><SECTION title=\"Introduction\" number=\"1\">\n\t\t\t<S sid=\"8\" ssid=\"8\">Syntactic reordering approaches are an effective method for handling systematic differences in word order between source and target languages within the context of statistical machine translation (SMT) systems (Xia and McCord, 2004; Collins et al, 2005).</S>\n\t\t\t<S sid=\"9\" ssid=\"9\">In reordering approaches, sentences in the source language are first parsed, for example using a Treebank-trained parser.</S>\n\t\t\t<S sid=\"10\" ssid=\"10\">A series of transformations is then applied to the resulting parse tree, with the goal of transforming the source language sentence into a word order that is closer to that of the targetlanguage.</S>\n\t\t\t<S sid=\"11\" ssid=\"11\">The reordering process is used to prepro cess both the training and test data used within an existing SMT system.</S>\n\t\t\t<S sid=\"12\" ssid=\"12\">Reordering approaches have given significant improvements in performance fortranslation from French to English (Xia and Mc Cord, 2004) and from German to English (Collins et al, 2005).This paper describes a syntactic reordering approach for translation from Chinese to English.</S>\n\t\t\t<S sid=\"13\" ssid=\"13\">Figure 1 gives an example illustrating some of the dif ferences in word order between the two languages.</S>\n\t\t\t<S sid=\"14\" ssid=\"14\">The example shows a Chinese sentence whose literal translation in English is: this is French delegation at WinterOlympics on achieve DEC best accom plishment and where a natural translation would be this is the best accomplishment that the French delegation achieved at the Winter Olympics As exemplified by this sentence, Chinese differsfrom English in several important respects: for ex ample, relative clauses appear before the noun being modified; prepositional phrases often appear before the head they modify; and so on.</S>\n\t\t\t<S sid=\"15\" ssid=\"15\">It can be seen that some significant reordering of the input is requiredto produce a good English translation.</S>\n\t\t\t<S sid=\"16\" ssid=\"16\">For this ex ample, application of reordering rules leads to a newChinese string whose word-by-word English para phrase is: 737 Before syntactic reordering After syntactic reordering IP NP PN ?(this) VP VC(is) NP CP IP NP NR {I(French) NN ?L?(delegation) VP PP P 3(at) LCP NP NN ?G (Winter) NR $?</S>\n\t\t\t<S sid=\"17\" ssid=\"17\">(Olympics) LC ?(on) VP-A VV (achieve) DEC (DEC) ADJP JJ ?(best) NPB NN ?1(accomplishment) IP NP PN ?(this) VP VC(is) NP ADJP JJ ?(best) NPB NN ?1(accomplishment) CP DEC (DEC) IP NP NR {I(French) NN ?L?(delegation) VP VP-A VV (achieve) PP P 3(at) LCP LC ?(on) NP NN ?G (Winter) NR $?</S>\n\t\t\t<S sid=\"18\" ssid=\"18\">(Olympics) Figure 1: Original (left) and reordered (right) parse trees for the Chinese sentence ???{I?L?3 ?G $????1,?</S>\n\t\t\t<S sid=\"19\" ssid=\"19\">which translates into ?This is the best accomplishment that the French delegation achieved at the Winter Olympics?</S>\n\t\t\t<S sid=\"20\" ssid=\"20\">in English.</S>\n\t\t\t<S sid=\"21\" ssid=\"21\">this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations?for example, it is simpleto move the entire relative clause ?French delegation at Winter Olympics on achieve DEC? to a posi tion that is after the noun phrase it modifies, namely ?best accomplishment.?</S>\n\t\t\t<S sid=\"22\" ssid=\"22\">Phrase-based systems arequite limited in their ability to perform transformations of this type.</S>\n\t\t\t<S sid=\"23\" ssid=\"23\">More recently developed hier archical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al, 2006)) may be betterequipped to deal with reordering of this type; how ever, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax.In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order.</S>\n\t\t\t<S sid=\"24\" ssid=\"24\">The result ing system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English.</S>\n\t\t\t<S sid=\"25\" ssid=\"25\">We report results forthe method on the NIST 2006 evaluation data, us ing the MOSES phrase-based SMT system (Koehnet al, 2007).</S>\n\t\t\t<S sid=\"26\" ssid=\"26\">The reordering rules give an improve ment in accuracy from 28.52 to 30.86 BLEU score.</S>\n\t\t\t<S sid=\"27\" ssid=\"27\">A concern for methods that make use of Chinese parsers is that these parsers are typically of relativelylow accuracy, particularly given that Chinese re quires a word-segmentation step that is not required in languages such as English.</S>\n\t\t\t<S sid=\"28\" ssid=\"28\">Our results show that Chinese parses are useful in SMT in spite of this problem.</S>\n\t\t\t<S sid=\"29\" ssid=\"29\">We report results showing the precisionof the reordering rules?essentially testing how of ten the Chinese sentences are correctly reordered?</S>\n\t\t\t<S sid=\"30\" ssid=\"30\">to give more insight into this issue.</S>\n\t\t\t<S sid=\"31\" ssid=\"31\">We also report experiments which assess the impact of each type of reordering rule on translation accuracy.</S>\n\t</SECTION>\n\t<SECTION title=\"Related Work. \" number=\"2\">\n\t\t\t<S sid=\"32\" ssid=\"1\">A number of researchers (Brown et al, 1992; Bergeret al, 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al, 2005) have described ap proaches that preprocess the source language input in SMT systems.</S>\n\t\t\t<S sid=\"33\" ssid=\"2\">We are not, however, aware of work on this topic for translation from Chinese to English.</S>\n\t\t\t<S sid=\"34\" ssid=\"3\">Brown et al (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order.</S>\n\t\t\t<S sid=\"35\" ssid=\"4\">Berger et al (1996) describe an approach for French that reorders phrases of the form NOUN1 deNOUN2.</S>\n\t\t\t<S sid=\"36\" ssid=\"5\">Xia and McCord (2004) describe an approach for French, where reordering rules that operate on context-free rule productions are acquired au 738tomatically.</S>\n\t\t\t<S sid=\"37\" ssid=\"6\">Niessen and Ney (2004) describe an ap proach for translation from German to English that combines verbs with associated particles, and alsoreorders questions.</S>\n\t\t\t<S sid=\"38\" ssid=\"7\">Collins et al (2005) also describe an approach for German, concentrating on re ordering German clauses, which have quite different word order from clauses in English.</S>\n\t\t\t<S sid=\"39\" ssid=\"8\">Our approach is most similar to that of Collins et al (2005).</S>\n\t\t\t<S sid=\"40\" ssid=\"9\">Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding).</S>\n\t\t\t<S sid=\"41\" ssid=\"10\">TheMOSES phrase-based system that we use has a rel atively simple reordering model which has a fixed penalty for reordering moves in the decoder.</S>\n\t\t\t<S sid=\"42\" ssid=\"11\">Moresophisticated models include reordering parameters that are sensitive to lexical information (Till mann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005).</S>\n\t\t\t<S sid=\"43\" ssid=\"12\">The model of Chiang (2005) employsa synchronous context-free grammar to allow hierarchical approaches to reordering.</S>\n\t\t\t<S sid=\"44\" ssid=\"13\">The syntax based models of Yamada and Knight (2001) andMarcu et al (2006) build a full parse tree in the target language, again effectively allowing hierarchi cal reordering based on synchronous grammars.</S>\n\t\t\t<S sid=\"45\" ssid=\"14\">Itis worth noting that none of these approaches to re ordering make use of explicit syntactic information in the source language?for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al (2006) make use of a parser in the target language, i.e., English).</S>\n\t\t\t<S sid=\"46\" ssid=\"15\">Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al, 2005; Liu et al,2006; Huang et al, 2006).</S>\n\t\t\t<S sid=\"47\" ssid=\"16\">In contrast to the prepro cessing approach, they attempt to incorporate syntax directly into the decoding stage.</S>\n\t</SECTION>\n\t<SECTION title=\"Chinese Syntactic Reordering Rules. \" number=\"3\">\n\t\t\t<S sid=\"48\" ssid=\"1\">We used the Penn Chinese Treebank guidelines (Xueet al, 2005) in searching for a suitable set of reordering rules.</S>\n\t\t\t<S sid=\"49\" ssid=\"2\">We examined all phrase types in the Treebank; potentially phrases of any type could be can didates for reordering rules.</S>\n\t\t\t<S sid=\"50\" ssid=\"3\">Table 1 provides a list of Treebank phrase tags for easy reference.</S>\n\t\t\t<S sid=\"51\" ssid=\"4\">We ruled out several phrase types as not requiring reordering ADJP adjective phrase ADVP adverbial phrase headed by AD (adverb) CLP classifier phrase CP clause headed by C (complementizer) DNP phrase formed by ?XP+DEG?</S>\n\t\t\t<S sid=\"52\" ssid=\"5\">DP determiner phrase DVP phrase formed by ?XP+DEV?</S>\n\t\t\t<S sid=\"53\" ssid=\"6\">FRAG fragment IP simple clause headed by I (INFL) LCP phrase formed by ?XP+LC?</S>\n\t\t\t<S sid=\"54\" ssid=\"7\">LST list marker NP noun phrase PP preposition phrase PRN parenthetical QP quantifier phrase UCP unidentical coordination phrase VP verb phrase Table 1: Penn Chinese Treebank phrase tags.</S>\n\t\t\t<S sid=\"55\" ssid=\"8\">rules.</S>\n\t\t\t<S sid=\"56\" ssid=\"9\">For example, Chinese ADJPs, ADVPs, DPs,QPs, and PPs all have similar internal word order ing to their English counterparts.</S>\n\t\t\t<S sid=\"57\" ssid=\"10\">Also similar are a group of special structures such as LST, FRAG, and PRN.</S>\n\t\t\t<S sid=\"58\" ssid=\"11\">We identified three categories that we consideredto be the most prominent candidates for reorder ing.</S>\n\t\t\t<S sid=\"59\" ssid=\"12\">These phrases include VPs (verb phrases), NPs (noun phrases), and LCPs (localizer phrases, which frequently map to prepositional phrases in English).</S>\n\t\t\t<S sid=\"60\" ssid=\"13\">In the following, we discuss each of the three main categories in more detail.</S>\n\t\t\t<S sid=\"61\" ssid=\"14\">3.1 Verb Phrases.</S>\n\t\t\t<S sid=\"62\" ssid=\"15\">In Chinese, verb phrase modifiers typically occur in pre-verbal position.</S>\n\t\t\t<S sid=\"63\" ssid=\"16\">VP modifiers can be ADVPs, temporal and spatial NPs, QP, PPs, CPs, IPs, DVPs, and LCPs.</S>\n\t\t\t<S sid=\"64\" ssid=\"17\">The ADVPs are simple adverbs, which can occur both preverbal and postverbal in an English verb phrase, so we do not attempt to move them.</S>\n\t\t\t<S sid=\"65\" ssid=\"18\">Similarly, the CP, IP, and DVP modifiers are typically adverbial phrases, which do not have afixed position in English verb phrases.</S>\n\t\t\t<S sid=\"66\" ssid=\"19\">In the follow ing, we only consider cases involving PPs, LCPs, temporal and spatial NPs, and QPs.</S>\n\t\t\t<S sid=\"67\" ssid=\"20\">PPs and LCPs Figure 2 shows an example verb phrase with a PP modifier, which translates literally 739 VP PP P 3(at) NP-A NPB NN ??(Eastern) NN ??(Division) VP-A VV ?(rank) QP OD 1?(10th) Figure 2: Example VP with PP modifier.</S>\n\t\t\t<S sid=\"68\" ssid=\"21\">The phrase translates into ?ranks 10th in the Eastern Division.?</S>\n\t\t\t<S sid=\"69\" ssid=\"22\">VP NP NPB NT U(same day) NT ??(morning) VP-A VV uL(issue) NP-A NPB NN (?(statement) Figure 3: Example VP with temporal NP modifier.</S>\n\t\t\t<S sid=\"70\" ssid=\"23\">The phrase translates into ?issued a statement that morning.?</S>\n\t\t\t<S sid=\"71\" ssid=\"24\">into ?at Eastern Division rank 10th.?</S>\n\t\t\t<S sid=\"72\" ssid=\"25\">Recognizingthat PPs in English verb phrases almost always occur after the verb, we use a simple VP(PP:VP) re ordering rule which states that a PP in a parent VP needs to be repositioned after the sibling VP.</S>\n\t\t\t<S sid=\"73\" ssid=\"26\">LCPs are similar to PPs and typically map to prepositional phrases in English.</S>\n\t\t\t<S sid=\"74\" ssid=\"27\">Thus they are handled similarly to PPs, i.e., LCPs in a parent VP are repositioned after the sibling VP.</S>\n\t\t\t<S sid=\"75\" ssid=\"28\">NPs Figure 3 gives an example of a verb phrasewith a temporal NP modifier, which literally trans lates into ?same day morning issue statement.?</S>\n\t\t\t<S sid=\"76\" ssid=\"29\">InEnglish, temporal phrases such as these almost al ways occur after the head verb.</S>\n\t\t\t<S sid=\"77\" ssid=\"30\">Conveniently, the Chinese Treebank uses the part of speech (POS) tag NT for temporal nouns.</S>\n\t\t\t<S sid=\"78\" ssid=\"31\">Thus, we use a rule whichstates that a preverbal NP will be repositioned af ter the sibling VP if there is at least one NT in the NP subtree.</S>\n\t\t\t<S sid=\"79\" ssid=\"32\">A similar rule might apply to locative NPS; however, there is no special POS tag in the Treebank marking locations,1 so we do not have a syntax-based reordering rule to handle locative NPs.QPs QP modifiers in verb phrases often corre spond to time-related concepts such as duration and frequency.</S>\n\t\t\t<S sid=\"80\" ssid=\"33\">Figure 4 shows an example verb phrase with a QP modifier, literally translating into ?many time injured.?</S>\n\t\t\t<S sid=\"81\" ssid=\"34\">Since temporal phrases almost alwaysoccur after the verb in English verb phrases, we han 1One can argue that NR (proper nouns) in that context are likely to be places.</S>\n\t\t\t<S sid=\"82\" ssid=\"35\">However, there also exist many exceptions, and so we decided not to exploit the NR tag.</S>\n\t\t\t<S sid=\"83\" ssid=\"36\">VP QP CD ?(many) CLP M g(time) VP-A VV ??(injured) Figure 4: Example VP with QP modifier.</S>\n\t\t\t<S sid=\"84\" ssid=\"37\">The phrase translates into ?injured many times.?</S>\n\t\t\t<S sid=\"85\" ssid=\"38\">NP-A DNP PP P ?(to) NP-A NPB NR 9n??(Zimbabwe) DEG (DEG) NPB NN ?L(financial) NN ?(aid)Figure 5: An example Chinese NP with a DNP mod ifier headed by a PP.</S>\n\t\t\t<S sid=\"86\" ssid=\"39\">The phrase translates into ?the financial aid to Zimbabwe?</S>\n\t\t\t<S sid=\"87\" ssid=\"40\">in English.</S>\n\t\t\t<S sid=\"88\" ssid=\"41\">dle such cases by a simple rule which states that theQP in a parent VP will be repositioned after the sib ling VP.</S>\n\t\t\t<S sid=\"89\" ssid=\"42\">3.2 Noun Phrases.</S>\n\t\t\t<S sid=\"90\" ssid=\"43\">Noun phrases in Chinese can take several types of modifiers: for example, phrases of type QP, DP, ADJP, NP, DNP, and CP.</S>\n\t\t\t<S sid=\"91\" ssid=\"44\">The placement of QP, DP, and ADJP modifiers is somewhat similar to English in that these phrases typically occur before the noun they modify.</S>\n\t\t\t<S sid=\"92\" ssid=\"45\">The case of NP modifiers in NPs is very limited in the Chinese Treebank, since most noun-noun sequences form compounds in a single NP.</S>\n\t\t\t<S sid=\"93\" ssid=\"46\">Hence we only developed reordering rules to handle DNP and clausal (CP) modifiers.</S>\n\t\t\t<S sid=\"94\" ssid=\"47\">DNPs DNPs are formed by ?XP+DEG,?</S>\n\t\t\t<S sid=\"95\" ssid=\"48\">where XP can be a phrase of the type ADJP, QP, PP, LCP, or NP.</S>\n\t\t\t<S sid=\"96\" ssid=\"49\">When the XP is an ADJP or a QP, no reordering is needed because the word order is the same as that of English.When the XP is a PP or an LCP, the DNP essentially corresponds to a prepositional phrase in En glish, which almost always appears after the noun being modified.</S>\n\t\t\t<S sid=\"97\" ssid=\"50\">Figure 5 shows an example where the XP in the DNP is a PP.</S>\n\t\t\t<S sid=\"98\" ssid=\"51\">The reordering rule to handle these two cases states that, if a parent NP has a child DNP which in turn has a child PP or LCP, then the DNP is repositioned after the last sibling NP.</S>\n\t\t\t<S sid=\"99\" ssid=\"52\">Figure 6 shows an example noun phrase for which the XP in the DNP is NP.</S>\n\t\t\t<S sid=\"100\" ssid=\"53\">On the surface, the Chinese?NP1 DEG NP2?</S>\n\t\t\t<S sid=\"101\" ssid=\"54\">sequence is analogous to the En glish possessive structure of ?NP1?s NP2?</S>\n\t\t\t<S sid=\"102\" ssid=\"55\">and does 740 NP-A DNP NP DP DT T(this) CLP M ?(measure word) NPB NN E?(technique) DEG (DEG) NPB NN ??(mastery) Figure 6: An example Chinese NP phrase with a DNP modifier headed by a NP.</S>\n\t\t\t<S sid=\"103\" ssid=\"56\">The phrase translates into ?the mastery of this technique?</S>\n\t\t\t<S sid=\"104\" ssid=\"57\">in English.</S>\n\t\t\t<S sid=\"105\" ssid=\"58\">not require reordering, for example, ??(Sue) (?s)*l(friend)?</S>\n\t\t\t<S sid=\"106\" ssid=\"59\">in Chinese and ?Sue?s friend?</S>\n\t\t\t<S sid=\"107\" ssid=\"60\">in En glish.</S>\n\t\t\t<S sid=\"108\" ssid=\"61\">However, the Chinese possessive structure?NP1 DEG NP2?</S>\n\t\t\t<S sid=\"109\" ssid=\"62\">can express more sophisticated re lationships which are inappropriate for the ?NP1?sNP2?</S>\n\t\t\t<S sid=\"110\" ssid=\"63\">expression.</S>\n\t\t\t<S sid=\"111\" ssid=\"64\">For example, the phrase in Fig ure 6 can only be translated into ?the mastery of this technique,?</S>\n\t\t\t<S sid=\"112\" ssid=\"65\">but not ?this technique?s mastery.?We decide to reorder DNPs of the ?NP+DEG?</S>\n\t\t\t<S sid=\"113\" ssid=\"66\">for mat, because they often can only map to the ?NP2 of NP1?</S>\n\t\t\t<S sid=\"114\" ssid=\"67\">expression in English.</S>\n\t\t\t<S sid=\"115\" ssid=\"68\">Additionally, the ?NP2 of NP1?</S>\n\t\t\t<S sid=\"116\" ssid=\"69\">expression is more general and can replace ?NP1?s NP2?</S>\n\t\t\t<S sid=\"117\" ssid=\"70\">in many cases.</S>\n\t\t\t<S sid=\"118\" ssid=\"71\">One exception is when the NP is a pronoun (PN), e.g., ?(he) (?s) ? i(name),?</S>\n\t\t\t<S sid=\"119\" ssid=\"72\">in which case the DNP acts simply like a possessive pronoun.</S>\n\t\t\t<S sid=\"120\" ssid=\"73\">Our reordering rule thus states that, if a parent NP has a child DNPwhich in turn hasa child NP that is not a PN, then the DNP is reposi tioned after the last sibling NP.CPs Relative clauses correspond to the CP cate gory in the Treebank.</S>\n\t\t\t<S sid=\"121\" ssid=\"74\">Figure 7 shows an examplenoun phrase with two nested CP modifiers.</S>\n\t\t\t<S sid=\"122\" ssid=\"75\">As illus trated in the figure, relative clauses in Chinese also occur before the noun they modify, which makes the word order of this sentence quite different from that of the English translation.</S>\n\t\t\t<S sid=\"123\" ssid=\"76\">Such distortions in the word reordering will be quite difficult for the word or phrase-based alignment model to capture.</S>\n\t\t\t<S sid=\"124\" ssid=\"77\">However, with the application of a reordering ruleto reposition the child CP after its sibling NP un der a parent NP, and the PP VP reordering rule for VP introduced previously, the sentence can be easily transformed into ?French delegation participate 8th handicap people Winter Olympics hold at US Salt Lake City,?</S>\n\t\t\t<S sid=\"125\" ssid=\"78\">a sentence whose word order is much closer to that of English.</S>\n\t\t\t<S sid=\"126\" ssid=\"79\">CP is typically formed by ?IP+DEC?, in which DEC?s only function is to mark the IP as a relative NP CP IP VP VV ?\\ (participate) NP CP IP VP PP P 3 (at) NP NR {I(US) NR ??</S>\n\t\t\t<S sid=\"127\" ssid=\"80\">(Salt Lake City) VP VV ?1 (hold) DEC  (DEC) QP OD 1l (8th) CLP M 3 (measure word) NPB NN ?; (handicap people) NR ? ?</S>\n\t\t\t<S sid=\"128\" ssid=\"81\">(Winter Olympics) DEC  (DEC) NPB NR {I (French) NPB NN ?L?</S>\n\t\t\t<S sid=\"129\" ssid=\"82\">(delegation)Figure 7: An example with two nested CP modifiers.</S>\n\t\t\t<S sid=\"130\" ssid=\"83\">The phrase translates into ?the French delega tion participating in the 8th Special Winter Olympics held in Salt Lake City US.?</S>\n\t\t\t<S sid=\"131\" ssid=\"84\">LCP IP NP-A NPB NN ?(accident) VP VV u)(happen) LC </S></SECTION></PAPER>",
                    "generation_prompt_uid": "486953b40577b0a31c57d57ed10b6968",
                    "item_type": "paper",
                    "llm_engine": "gpt-3.5-turbo-1106",
                    "origin": "LLM",
                    "title": "Chinese Syntactic Reordering for Statistical Machine Translation"
                }
            }
        }
    ]
}