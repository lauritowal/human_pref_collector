{
    "timestamp": "20250511124241",
    "username": "test",
    "email": "test@fsdfds.de",
    "model": "gpt4",
    "category": "paper",
    "totalLLMChoices": 14,
    "totalHumanChoices": 11,
    "totalNoPreference": 0,
    "userChoices": [
        {
            "index": 0,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "A Bloom filter (BF) is a randomised data structure for set membership queries. Its space requirements are significantly below lossless information-theoretic lower bounds but it produces false positives with some quantifiable probability. Here we explore the use of BFs for language modelling in statistical machine translation. show how a BF containing can enable us to use much larger corpora and higher-order models complementing a con- LM within an SMT system. We also consider (i) how to include approximate frequency information efficiently within a BF and (ii) how to reduce the error rate of these models by first checking for sub-sequences in candidate grams. Our solutions in both cases retain the one-sided error guarantees of the BF while taking advantage of the Zipf-like distribution of word frequencies to reduce the space requirements.",
                    "filename": "Randomised_Language_Modelling_for_Statistical_Machine_Translation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P07-1065/Documents_xml/P07-1065.xml",
                    "title": "Randomised Language Modelling for Statistical Machine Translation"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores the use of Bloom filters (BF) for creating space-efficient language models (LM) in Statistical Machine Translation (SMT). Standard n-gram models, which are critical for assigning probabilities to translation hypotheses, face challenges with large corpora due to exponential parameter growth. The authors propose a novel approach involving randomized representations with Bloom filters, particularly the log-frequency BF, which encodes frequency information efficiently and complements traditional n-gram models. The paper presents a sub-sequence filtering mechanism to reduce error rates and details experiments using higher-order n-grams and larger monolingual corpora. Results show that BF-based LMs can significantly improve translation quality with minimal space requirements. The study concludes that BFs can be a practical alternative to conventional data structures in computational linguistics, particularly in SMT.",
                        "This paper explores the use of Bloom filters as a space-efficient alternative for language modeling in statistical machine translation (SMT). Bloom filters, known for their compact representation of sets with a one-sided error rate, are adapted to encode n-gram statistics and corpus frequencies, facilitating the use of larger corpora and higher-order n-grams in SMT systems. The authors introduce the log-frequency Bloom filter, extending the standard Bloom filter to incorporate frequency information. They also propose sub-sequence filtering to reduce error rates by leveraging the frequency bounds of n-gram sub-sequences. Experiments show that these models improve translation performance with reduced memory requirements, suggesting that Bloom filters can be a practical substitute for conventional data structures in computational linguistics and SMT.",
                        "This paper explores the use of Bloom filters as a space-efficient alternative to traditional language models in statistical machine translation (SMT). The authors introduce a log-frequency Bloom filter that encodes frequency information and a sub-sequence filtering strategy to lower error rates. Experiments demonstrate that these models, when combined with standard n-gram models, allow for the incorporation of larger corpora and higher-order n-grams, improving translation quality with less memory. The work suggests Bloom filters as a viable and practical tool for language modeling in computational linguistics, potentially replacing standard n-gram models in SMT systems.",
                        "This paper explores the use of randomized language modeling through Bloom filters (BF) in statistical machine translation (SMT). It introduces the log-frequency Bloom filter, an innovative extension of the traditional BF, which utilizes the Zipf-like distribution of words in large corpora to encode frequency information efficiently. The study proposes sub-sequence filtering, a technique that further reduces error rates by using the frequency of sub-sequences to establish upper bounds on n-gram frequencies. Through various experiments, the paper demonstrates that these BF-based models, when used alongside or in place of conventional smoothed trigram models, can significantly enhance translation quality while using less memory. The findings suggest that the BF approach is a viable and space-efficient alternative for incorporating higher-order n-grams and extensive monolingual corpora into language models for SMT."
                    ],
                    "filename": "randomised_language_modelling_fo-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 131 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Randomised Language Modelling for Statistical Machine Translation</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Language modelling (LM) is a crucial component in statistical machine translation (SMT).</S>\n    <S sid=\"7\" ssid=\"2\">Standard ngram language models assign probabilities to translation hypotheses in the target language, typically as smoothed trigram models, e.g.</S>\n    <S sid=\"8\" ssid=\"3\">(Chiang, 2005).</S>\n    <S sid=\"9\" ssid=\"4\">Although it is well-known that higher-order LMs and models trained on additional monolingual corpora can yield better translation performance, the challenges in deploying large LMs are not trivial.</S>\n    <S sid=\"10\" ssid=\"5\">Increasing the order of an n-gram model can result in an exponential increase in the number of parameters; for corpora such as the English Gigaword corpus, for instance, there are 300 million distinct trigrams and over 1.2 billion 5-grams.</S>\n    <S sid=\"11\" ssid=\"6\">Since a LM may be queried millions of times per sentence, it should ideally reside locally in memory to avoid time-consuming remote or disk-based look-ups.</S>\n    <S sid=\"12\" ssid=\"7\">Against this background, we consider a radically different approach to language modelling: instead of explicitly storing all distinct n-grams, we store a randomised representation.</S>\n    <S sid=\"13\" ssid=\"8\">In particular, we show that the Bloom filter (Bloom (1970); BF), a simple space-efficient randomised data structure for representing sets, may be used to represent statistics from larger corpora and for higher-order n-grams to complement a conventional smoothed trigram model within an SMT decoder.</S>\n    <S sid=\"14\" ssid=\"9\">1 The space requirements of a Bloom filter are quite spectacular, falling significantly below informationtheoretic error-free lower bounds while query times are constant.</S>\n    <S sid=\"15\" ssid=\"10\">This efficiency, however, comes at the price of false positives: the filter may erroneously report that an item not in the set is a member.</S>\n    <S sid=\"16\" ssid=\"11\">False negatives, on the other hand, will never occur: the error is said to be one-sided.</S>\n    <S sid=\"17\" ssid=\"12\">In this paper, we show that a Bloom filter can be used effectively for language modelling within an SMT decoder and present the log-frequency Bloom filter, an extension of the standard Boolean BF that takes advantage of the Zipf-like distribution of corpus statistics to allow frequency information to be associated with n-grams in the filter in a spaceefficient manner.</S>\n    <S sid=\"18\" ssid=\"13\">We then propose a mechanism, sub-sequence filtering, for reducing the error rates of these models by using the fact that an n-gram’s frequency is bound from above by the frequency of its least frequent sub-sequence.</S>\n    <S sid=\"19\" ssid=\"14\">We present machine translation experiments using these models to represent information regarding higher-order n-grams and additional larger monolingual corpora in combination with conventional smoothed trigram models.</S>\n    <S sid=\"20\" ssid=\"15\">We also run experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.</S>\n    <S sid=\"21\" ssid=\"16\">Finally we provide some empirical analysis of the effectiveness of both the log frequency Bloom filter and sub-sequence filtering.</S>\n  </SECTION>\n  <SECTION title=\"2 The Bloom filter\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">In this section, we give a brief overview of the Bloom filter (BF); refer to Broder and Mitzenmacher (2005) for a more in detailed presentation.</S>\n    <S sid=\"23\" ssid=\"2\">A BF represents a set S = {x1, x2, ..., xn} with n elements drawn from a universe U of size N. The structure is attractive when N » n. The only significant storage used by a BF consists of a bit array of size m. This is initially set to hold zeroes.</S>\n    <S sid=\"24\" ssid=\"3\">To train the filter we hash each item in the set k times using distinct hash functions h1, h2, ..., hk.</S>\n    <S sid=\"25\" ssid=\"4\">Each function is assumed to be independent from each other and to map items in the universe to the range 1 to m uniformly at random.</S>\n    <S sid=\"26\" ssid=\"5\">The k bits indexed by the hash values for each item are set to 1; the item is then discarded.</S>\n    <S sid=\"27\" ssid=\"6\">Once a bit has been set to 1 it remains set for the lifetime of the filter.</S>\n    <S sid=\"28\" ssid=\"7\">Distinct items may not be hashed to k distinct locations in the filter; we ignore collisons.</S>\n    <S sid=\"29\" ssid=\"8\">Bits in the filter can, therefore, be shared by distinct items allowing significant space savings but introducing a non-zero probability of false positives at test time.</S>\n    <S sid=\"30\" ssid=\"9\">There is no way of directly retrieving or ennumerating the items stored in a BF.</S>\n    <S sid=\"31\" ssid=\"10\">At test time we wish to discover whether a given item was a member of the original set.</S>\n    <S sid=\"32\" ssid=\"11\">The filter is queried by hashing the test item using the same k hash functions.</S>\n    <S sid=\"33\" ssid=\"12\">If all bits referenced by the k hash values are 1 then we assume that the item was a member; if any of them are 0 then we know it was not.</S>\n    <S sid=\"34\" ssid=\"13\">True members are always correctly identified, but a false positive will occur if all k corresponding bits were set by other items during training and the item was not a member of the training set.</S>\n    <S sid=\"35\" ssid=\"14\">This is known as a one-sided error.</S>\n    <S sid=\"36\" ssid=\"15\">The probability of a false postive, f, is clearly the probability that none of k randomly selected bits in the filter are still 0 after training.</S>\n    <S sid=\"37\" ssid=\"16\">Letting p be the proportion of bits that are still zero after these n elements have been inserted, this gives, As n items have been entered in the filter by hashing each k times, the probability that a bit is still zero is, which is the expected value of p. Hence the false positive rate can be approximated as, f = (1 − p)k : (1 − p�)k : (1 − e−kn m)k .</S>\n    <S sid=\"38\" ssid=\"17\">By taking the derivative we find that the number of functions k* that minimizes f is, m k* = ln 2 · . n which leads to the intuitive result that exactly half the bits in the filter will be set to 1 when the optimal number of hash functions is chosen.</S>\n    <S sid=\"39\" ssid=\"18\">The fundmental difference between a Bloom filter’s space requirements and that of any lossless representation of a set is that the former does not depend on the size of the (exponential) universe N from which the set is drawn.</S>\n    <S sid=\"40\" ssid=\"19\">A lossless representation scheme (for example, a hash map, trie etc.) must depend on N since it assigns a distinct representation to each possible set drawn from the universe.</S>\n  </SECTION>\n  <SECTION title=\"3 Language modelling with Bloom filters\" number=\"3\">\n    <S sid=\"41\" ssid=\"1\">In our experiments we make use of both standard (i.e.</S>\n    <S sid=\"42\" ssid=\"2\">Boolean) BFs containing n-gram types drawn from a training corpus and a novel BF scheme, the log-frequency Bloom filter, that allows frequency information to be associated efficiently with items stored in the filter. return BF The efficiency of our scheme for storing n-gram statistics within a BF relies on the Zipf-like distribution of n-gram frequencies in natural language corpora: most events occur an extremely small number of times, while a small number are very frequent.</S>\n    <S sid=\"43\" ssid=\"3\">We quantise raw frequencies, c(x), using a logarithmic codebook as follows, The precision of this codebook decays exponentially with the raw counts and the scale is determined by the base of the logarithm b; we examine the effect of this parameter in experiments below.</S>\n    <S sid=\"44\" ssid=\"4\">Given the quantised count qc(x) for an n-gram x, the filter is trained by entering composite events consisting of the n-gram appended by an integer counter j that is incremented from 1 to qc(x) into the filter.</S>\n    <S sid=\"45\" ssid=\"5\">To retrieve the quantised count for an n-gram, it is first appended with a count of 1 and hashed under the k functions; if this tests positive, the count is incremented and the process repeated.</S>\n    <S sid=\"46\" ssid=\"6\">The procedure terminates as soon as any of the k hash functions hits a 0 and the previous count is reported.</S>\n    <S sid=\"47\" ssid=\"7\">The one-sided error of the BF and the training scheme ensure that the actual quantised count cannot be larger than this value.</S>\n    <S sid=\"48\" ssid=\"8\">As the counts are quantised logarithmically, the counter will be incremented only a small number of times.</S>\n    <S sid=\"49\" ssid=\"9\">The training and testing routines are given here as Algorithms 1 and 2 respectively.</S>\n    <S sid=\"50\" ssid=\"10\">Errors for the log-frequency BF scheme are onesided: frequencies will never be underestimated.</S>\n    <S sid=\"51\" ssid=\"11\">Algorithm 2 Test frequency BF Input: x, MAXQCOUNT, {h1, ...hk} and BF Output: Upper bound on qc(x) ∈ Strain for j = 1 to MAXQCOUNT do for i = 1 to k do The probability of overestimating an item’s frequency decays exponentially with the size of the overestimation error d (i.e. as fd for d &gt; 0) since each erroneous increment corresponds to a single false positive and d such independent events must occur together.</S>\n    <S sid=\"52\" ssid=\"12\">The error analysis in Section 2 focused on the false positive rate of a BF; if we deploy a BF within an SMT decoder, however, the actual error rate will also depend on the a priori membership probability of items presented to it.</S>\n    <S sid=\"53\" ssid=\"13\">The error rate Err is, This implies that, unlike a conventional lossless data structure, the model’s accuracy depends on other components in system and how it is queried.</S>\n    <S sid=\"54\" ssid=\"14\">We take advantage of the monotonicity of the ngram event space to place upper bounds on the frequency of an n-gram prior to testing for it in the filter and potentially truncate the outer loop in Algorithm 2 when we know that the test could only return postive in error.</S>\n    <S sid=\"55\" ssid=\"15\">Specifically, if we have stored lower-order ngrams in the filter, we can infer that an n-gram cannot present, if any of its sub-sequences test negative.</S>\n    <S sid=\"56\" ssid=\"16\">Since our scheme for storing frequencies can never underestimate an item’s frequency, this relation will generalise to frequencies: an n-gram’s frequency cannot be greater than the frequency of its least frequent sub-sequence as reported by the filter, We use this to reduce the effective error rate of BFLMs that we use in the experiments below.</S>\n    <S sid=\"57\" ssid=\"17\">A standard BF can implement a Boolean ‘language model’ test: have we seen some fragment of language before?</S>\n    <S sid=\"58\" ssid=\"18\">This does not use any frequency information.</S>\n    <S sid=\"59\" ssid=\"19\">The Boolean BF-LM is a standard BF containing all n-grams of a certain length in the training corpus, Strain.</S>\n    <S sid=\"60\" ssid=\"20\">It implements the following binary feature function in a log-linear decoder, Separate Boolean BF-LMs can be included for different order n and assigned distinct log-linear weights that are learned as part of a minimum error rate training procedure (see Section 4).</S>\n    <S sid=\"61\" ssid=\"21\">The log-frequency BF-LM implements a multinomial feature function in the decoder that returns the value associated with an n-gram by Algorithm 2.</S>\n    <S sid=\"62\" ssid=\"22\">Sub-sequence filtering can be performed by using the minimum value returned by lower-order models as an upper-bound on the higher-order models.</S>\n    <S sid=\"63\" ssid=\"23\">By boosting the score of hypotheses containing ngrams observed in the training corpus while remaining agnostic for unseen n-grams (with the exception of errors), these feature functions have more in common with maximum entropy models than conventionally smoothed n-gram models.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiments\" number=\"4\">\n    <S sid=\"64\" ssid=\"1\">We conducted a range of experiments to explore the effectiveness and the error-space trade-off of Bloom filters for language modelling in SMT.</S>\n    <S sid=\"65\" ssid=\"2\">The spaceefficiency of these models also allows us to investigate the impact of using much larger corpora and higher-order n-grams on translation quality.</S>\n    <S sid=\"66\" ssid=\"3\">While our main experiments use the Bloom filter models in conjunction with a conventional smoothed trigram model, we also present experiments with these models in isolation to highlight the impact of different order n-grams on the translation process.</S>\n    <S sid=\"67\" ssid=\"4\">Finally, we present some empirical analysis of both the logfrequency Bloom filter and the sub-sequence filtering technique which may be of independent interest.</S>\n    <S sid=\"68\" ssid=\"5\">All of our experiments use publically available resources.</S>\n    <S sid=\"69\" ssid=\"6\">We use the French-English section of the Europarl (EP) corpus for parallel data and language modelling (Koehn, 2003) and the English Gigaword Corpus (LDC2003T05; GW) for additional language modelling.</S>\n    <S sid=\"70\" ssid=\"7\">Decoding is carried-out using the Moses decoder (Koehn and Hoang, 2007).</S>\n    <S sid=\"71\" ssid=\"8\">We hold out 500 test sentences and 250 development sentences from the parallel text for evaluation purposes.</S>\n    <S sid=\"72\" ssid=\"9\">The feature functions in our models are optimised using minimum error rate training and evaluation is performed using the BLEU score.</S>\n    <S sid=\"73\" ssid=\"10\">Our baseline LM and other comparison models are conventional n-gram models smoothed using modified Kneser-Ney and built using the SRILM Toolkit (Stolcke, 2002); as is standard practice these models drop entries for n-grams of size 3 and above when the corresponding discounted count is less than 1.</S>\n    <S sid=\"74\" ssid=\"11\">The baseline language model, EP-KN-3, is a trigram model trained on the English portion of the parallel corpus.</S>\n    <S sid=\"75\" ssid=\"12\">For additional comparisons we also trained a smoothed 4-gram model on this Europarl data (EPKN-4) and a trigram model on the Agence France Press section of the Gigaword Corpus (AFP-KN-3).</S>\n    <S sid=\"76\" ssid=\"13\">Table 1 shows the amount of memory these models take up on disk and compressed using the gzip utility in parentheses as well as the number of distinct n-grams of each order.</S>\n    <S sid=\"77\" ssid=\"14\">We give the gzip compressed size as an optimistic lower bound on the size of any lossless representation of each model.2 2Note, in particular, that gzip compressed files do not support direct random access as required by our application.</S>\n    <S sid=\"78\" ssid=\"15\">To create Bloom filter LMs we gathered n-gram counts from both the Europarl (EP) and the whole of the Gigaword Corpus (GW).</S>\n    <S sid=\"79\" ssid=\"16\">Table 2 shows the numbers of distinct n-grams in these corpora.</S>\n    <S sid=\"80\" ssid=\"17\">Note that we use no pruning for these models and that the numbers of distinct n-grams is of the same order as that of the recently released Google Ngrams dataset (LDC2006T13).</S>\n    <S sid=\"81\" ssid=\"18\">In our experiments we create a range of models referred to by the corpus used (EP or GW), the order of the n-gram(s) entered into the filter (1 to 10), whether the model is Boolean (Bool-BF) or provides frequency information (FreqBF), whether or not sub-sequence filtering was used (FTR) and whether it was used in conjunction with the baseline trigram (+EP-KN-3).</S>\n    <S sid=\"82\" ssid=\"19\">Our first set of experiments examines the relationship between memory allocated to the BF and BLEU score.</S>\n    <S sid=\"83\" ssid=\"20\">We present results using the Boolean BFLM in isolation and then both the Boolean and logfrequency BF-LMS to add 4-grams to our baseline 3-gram model.Our second set of experiments adds 3-grams and 5-grams from the Gigaword Corpus to our baseline.</S>\n    <S sid=\"84\" ssid=\"21\">Here we constrast the Boolean BFLM with the log-frequency BF-LM with different quantisation bases (2 = fine-grained and 5 = coarsegrained).</S>\n    <S sid=\"85\" ssid=\"22\">We then evaluate the sub-sequence filtering approach to reducing the actual error rate of these models by adding both 3 and 4-grams from the Gigaword Corpus to the baseline.</S>\n    <S sid=\"86\" ssid=\"23\">Since the BF-LMs easily allow us to deploy very high-order n-gram models, we use them to evaluate the impact of different order n-grams on the translation process presenting results using the Boolean and log-frequency BF-LM in isolation for n-grams of order 1 to 10.</S>\n    <S sid=\"87\" ssid=\"24\">We analyse our log-frequency BF scheme in terms of the additional memory it requires and the error rate compared to a non-redundant scheme.</S>\n    <S sid=\"88\" ssid=\"25\">The nonredundant scheme involves entering just the exact quantised count for each n-gram and then searching over the range of possible counts at test time starting with the count with maximum a priori probability (i.e.</S>\n    <S sid=\"89\" ssid=\"26\">1) and incrementing until a count is found or the whole codebook has been searched (here the size is 16).</S>\n    <S sid=\"90\" ssid=\"27\">We also analyse the sub-sequence filtering scheme directly by creating a BF with only 3-grams and a BF containing both 2-grams and 3-grams and comparing their actual error rates when presented with 3-grams that are all known to be negatives.</S>\n  </SECTION>\n  <SECTION title=\"5 Results\" number=\"5\">\n    <S sid=\"91\" ssid=\"1\">Table 3 shows the results of the baseline (EP-KN3) and other conventional n-gram models trained on larger corpora (AFP-KN-3) and using higher-order dependencies (EP-KN-4).</S>\n    <S sid=\"92\" ssid=\"2\">The larger models improve somewhat on the baseline performance.</S>\n    <S sid=\"93\" ssid=\"3\">Figure 1 shows the relationship between space allocated to the BF models and BLEU score (left) and false positive rate (right) respectively.</S>\n    <S sid=\"94\" ssid=\"4\">These experiments do not include the baseline model.</S>\n    <S sid=\"95\" ssid=\"5\">We can see a clear correlation between memory / false positive rate and translation performance.</S>\n    <S sid=\"96\" ssid=\"6\">Adding 4-grams in the form of a Boolean BF or a log-frequency BF (see Figure 2) improves on the 3gram baseline with little additional memory (around 4MBs) while performing on a par with or above the Europarl 4-gram model with around 10MBs; this suggests that a lossy representation of the unpruned set of 4-grams contains more useful information than a lossless representation of the pruned set.3 As the false positive rate exceeds 0.20 the performance is severly degraded.</S>\n    <S sid=\"97\" ssid=\"7\">Adding 3-grams drawn from the whole of the Gigaword corpus rather than simply the Agence France Press section results in slightly improved performance with signficantly less memory than the AFP-KN-3 model (see Figure 3).</S>\n    <S sid=\"98\" ssid=\"8\">Figure 4 shows the results of adding 5-grams drawn from the Gigaword corpus to the baseline.</S>\n    <S sid=\"99\" ssid=\"9\">It also contrasts the Boolean BF and the log-frequency BF suggesting in this case that the log-frequency BF can provide useful information when the quantisation base is relatively fine-grained (base 2).</S>\n    <S sid=\"100\" ssid=\"10\">The Boolean BF and the base 5 (coarse-grained quantisation) log-frequency BF perform approximately the same.</S>\n    <S sid=\"101\" ssid=\"11\">The base 2 quantisation performs worse for smaller amounts of memory, possibly due to the larger set of events it is required to store.</S>\n    <S sid=\"102\" ssid=\"12\">Figure 5 shows sub-sequence filtering resulting in a small increase in performance when false positive rates are high (i.e. less memory is allocated).</S>\n    <S sid=\"103\" ssid=\"13\">We believe this to be the result of an increased a priori membership probability for n-grams presented to the filter under the sub-sequence filtering scheme.</S>\n    <S sid=\"104\" ssid=\"14\">Figure 6 shows that for this task the most useful n-gram sizes are between 3 and 6.</S>\n    <S sid=\"105\" ssid=\"15\">Figure 8 compares the memory requirements of the log-frequencey BF (base 2) and the Boolean BF for various order n-gram sets from the Gigaword Corpus with the same underlying false positive rate (0.125).</S>\n    <S sid=\"106\" ssid=\"16\">The additional space required by our scheme for storing frequency information is less than a factor of 2 compared to the standard BF.</S>\n    <S sid=\"107\" ssid=\"17\">Figure 7 shows the number and size of frequency estimation errors made by our log-frequency BF scheme and a non-redundant scheme that stores only the exact quantised count.</S>\n    <S sid=\"108\" ssid=\"18\">We presented 500K negatives to the filter and recorded the frequency of overestimation errors of each size.</S>\n    <S sid=\"109\" ssid=\"19\">As shown in Section 3.1, the probability of overestimating an item’s frequency under the log-frequency BF scheme decays exponentially in the size of this overestimation error.</S>\n    <S sid=\"110\" ssid=\"20\">Although the non-redundant scheme requires fewer items be stored in the filter and, therefore, has a lower underlying false positive rate (0.076 versus 0.</S>\n    <S sid=\"111\" ssid=\"21\">159), in practice it incurs a much higher error rate (0.717) with many large errors.</S>\n    <S sid=\"112\" ssid=\"22\">Figure 9 shows the impact of sub-sequence filtering on the actual error rate.</S>\n    <S sid=\"113\" ssid=\"23\">Although, the false positive rate for the BF containing 2-grams, in addition, to 3-grams (filtered) is higher than the false positive rate of the unfiltered BF containing only 3-grams, the actual error rate of the former is lower for models with less memory.</S>\n    <S sid=\"114\" ssid=\"24\">By testing for 2-grams prior to querying for the 3-grams, we can avoid performing some queries that may otherwise have incurred errors using the fact that a 3-gram cannot be present if one of its constituent 2-grams is absent.</S>\n  </SECTION>\n  <SECTION title=\"6 Related Work\" number=\"6\">\n    <S sid=\"115\" ssid=\"1\">We are not the first people to consider building very large scale LMs: Kumar et al. used a four-gram LM for re-ranking (Kumar et al., 2005) and in unpublished work, Google used substantially larger ngrams in their SMT system.</S>\n    <S sid=\"116\" ssid=\"2\">Deploying such LMs requires either a cluster of machines (and the overheads of remote procedure calls), per-sentence filtering (which again, is slow) and/or the use of some other lossy compression (Goodman and Gao, 2000).</S>\n    <S sid=\"117\" ssid=\"3\">Our approach can complement all these techniques.</S>\n    <S sid=\"118\" ssid=\"4\">Bloom filters have been widely used in database applications for reducing communications overheads and were recently applied to encode word frequencies in information retrieval (Linari and Weikum, 2006) using a method that resembles the non-redundant scheme described above.</S>\n    <S sid=\"119\" ssid=\"5\">Extensions of the BF to associate frequencies with items in the set have been proposed e.g., (Cormode and Muthukrishn, 2005); while these schemes are more general than ours, they incur greater space overheads for the distributions that we consider here.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusions\" number=\"7\">\n    <S sid=\"120\" ssid=\"1\">We have shown that Bloom Filters can form the basis for space-efficient language modelling in SMT.</S>\n    <S sid=\"121\" ssid=\"2\">Extending the standard BF structure to encode corpus frequency information and developing a strategy for reducing the error rates of these models by sub-sequence filtering, our models enable higherorder n-grams and larger monolingual corpora to be used more easily for language modelling in SMT.</S>\n    <S sid=\"122\" ssid=\"3\">In a companion paper (Talbot and Osborne, 2007) we have proposed a framework for deriving conventional smoothed n-gram models from the logfrequency BF scheme allowing us to do away entirely with the standard n-gram model in an SMT system.</S>\n    <S sid=\"123\" ssid=\"4\">We hope the present work will help establish the Bloom filter as a practical alternative to conventional associative data structures used in computational linguistics.</S>\n    <S sid=\"124\" ssid=\"5\">The framework presented here shows that with some consideration for its workings, the randomised nature of the Bloom filter need not be a significant impediment to is use in applications.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "911dc0c46b59e07081546f0fe3bcdde1",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Randomised Language Modelling for Statistical Machine Translation"
                }
            }
        },
        {
            "index": 1,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Best-first probabilistic chart parsing attempts to parse efficiently by working on edges that are judged &quot;best&quot; by some probabilistic figure of merit (FOM). Recent work has used probabilistic context-free grammars (PCFGs) to assign probabilities to constituents, and to use these probabilities as the starting point for the FOM. This paper extends this approach to using a probabilistic FOM to judge edges (incomplete constituents), thereby giving a much finergrained control over parsing effort. We show how this can be accomplished in a particularly simple way using the common idea of binarizing the PCFG. The results obtained are about a facof twenty improvement over the best results — that is, our parser achieves equivalent results using one twentieth the number of edges. Furthermore we show that this improvement is obtained with parsing precision and recall levels superior to those achieved by exhaustive parsing.",
                    "filename": "Edge-Based_Best-First_Chart_Parsing",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W98-1115/Documents_xml/W98-1115.xml",
                    "title": "Edge-Based Best-First Chart Parsing"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents an edge-based best-first chart parsing approach for context-free grammars, emphasizing the need for efficient parsing techniques for large grammars and long sentences. The authors build upon work by Caraballo and Charniak, adopting their figure of merit (FOM) and framework. A significant innovation is the use of the FOM to rank all edges, including incomplete ones, which dramatically reduces the number of edges required for parsing. The paper describes binarization techniques, experimental setups using the Penn Wall Street Journal text, and compares the results to previous research. The findings demonstrate that the proposed edge-based method significantly outperforms existing approaches in terms of the number of edges processed. The authors suggest that their FOM-based parser likely concentrates on plausible edges given their context, leading to more accurate parses. They conclude by highlighting the potential for applying these techniques to more complex probabilistic models and advocate for further research in this area.",
                        "This paper introduces an edge-based best-first chart parsing approach for efficiently parsing sentences using probabilistic context-free grammars (PCFGs). The method involves ranking both complete and incomplete edges with a figure of merit (FOM), constituting a significant enhancement over the constituent-based parsing employed by Caraballo and Charniak. The new parsing algorithm dramatically reduces the average number of edges required per sentence from thousands to hundreds or even tens, while maintaining or even improving accuracy compared to exhaustive parsing methods. Experiments on the Penn Wall Street Journal text showed that adjusting the normalization constant of the FOM can optimize the trade-off between parsing efficiency and accuracy. The results are highly promising, suggesting that edge-based best-first chart parsing could be the most efficient technique for parsing large treebanks with PCFGs. Further research into the application of this technique beyond PCFGs is recommended.",
                        "This study introduces an edge-based best-first chart parsing technique that offers significant improvements in the efficiency of parsing sentences using probabilistic context-free grammars (PCFGs). By adopting a figure of merit (FOM) to prioritize the processing of edges, including incomplete ones, the parser reduces the computational workload dramatically compared to traditional exhaustive parsing methods. The parser was tested on the Penn Wall Street Journal corpus, demonstrating that it requires orders of magnitude fewer edges to parse sentences, with only a minor trade-off in accuracy. The results indicate that by focusing on the most probable edges first, the parsing process is expedited, and even when the parser stops after finding the first parse, the precision and recall are better than those obtained by exhaustive methods. The paper suggests that the FOM used might be mimicking more complex parsing algorithms, accounting for the high performance. This edge-based approach is recommended for efficient PCFG parsing and could potentially be extended to other grammar frameworks.",
                        "This paper introduces an edge-based best-first chart parsing technique for context-free grammars, specifically addressing the challenge of parsing sentences from large grammars with long sentences. Utilizing a figure of merit (FOM) derived from probabilistic context-free grammar (PCFG) probabilities, the method ranks both complete and incomplete edges to efficiently parse sentences. The experiment conducted with a treebank grammar from the Penn Wall Street Journal text demonstrates significant reductions in the number of edges required for parsing—down to hundreds or tens—compared to thousands needed by previous methods. The results show the parser's ability to achieve high accuracy with minimal computational effort. The paper suggests that the success of the approach may be due to the parser's focus on plausible edges and the resemblance of the FOM to algorithms that maximize labeled bracket recall. The study concludes by recommending the edge-based best-first chart parsing technique for efficient PCFG parsing and suggesting its potential applicability with other probabilistic models."
                    ],
                    "filename": "edgebased_bestfirst_chart_parsin-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 141 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Edge-Based Best-First Chart Parsing</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Finding one (or all) parses for a sentence according to a context-free grammar requires search.</S>\n    <S sid=\"8\" ssid=\"2\">Fortunately, there are well known 0(n3) algorithms for parsing, where n is the length of the sentence.</S>\n    <S sid=\"9\" ssid=\"3\">Unfortunately, for large grammars (such as the PCFG induced from the Penn II WSJ corpus, which contains around 1.6 • 104 rules) and longish sentences (say, 40 words and punctuation), even 0(713) looks pretty bleak.</S>\n    <S sid=\"10\" ssid=\"4\">One well-known 0(n3) parsing method (Kay, 1980) is chart parsing.</S>\n    <S sid=\"11\" ssid=\"5\">In this approach one maintains an agenda of items remaining to be processed, one of which is processed during each iteration.</S>\n    <S sid=\"12\" ssid=\"6\">As each item is pulled off the agenda, it is added to the chart (unless it is already there, in which case it can be discarded) and used to extend and create additional items.</S>\n    <S sid=\"13\" ssid=\"7\">In &amp;quot;exhaustive&amp;quot; chart parsing one removes items from the agenda in some relatively simple way (last-in, first-out is common), and continues to do so until nothing remains.</S>\n    <S sid=\"14\" ssid=\"8\">A commonly discussed alternative is to remove the constituents from the agenda according to a figure of merit (FOM).</S>\n    <S sid=\"15\" ssid=\"9\">The idea is that the FOM selects &amp;quot;good&amp;quot; items to be processed, leaving the &amp;quot;bad&amp;quot; ones— the ones that are not, in fact, part of the correct parse— sitting on the agenda.</S>\n    <S sid=\"16\" ssid=\"10\">When one has a completed parse, or perhaps several possible parses, one simply stops parsing, leaving items remaining on the agenda.</S>\n    <S sid=\"17\" ssid=\"11\">The time that would have been spent processing these remaining items is time saved, and thus time earned.</S>\n    <S sid=\"18\" ssid=\"12\">In our work we have found that exhaustively parsing maximum-40-word sentences from the Penn II treebank requires an average of about 1.2 million edges per sentence.</S>\n    <S sid=\"19\" ssid=\"13\">Numbers like this suggest that any approach that offers the possibility of reducing the work load is well worth pursuing, a fact that has been noted by several researchers.</S>\n    <S sid=\"20\" ssid=\"14\">Early on, Kay (1980) suggested the use of the chart agenda for this purpose.</S>\n    <S sid=\"21\" ssid=\"15\">More recently, the statistical approach to language processing and the use of probabilistic context-free grammars (PCFGs) has suggested using the PCFG probabilities to create a FOM.</S>\n    <S sid=\"22\" ssid=\"16\">Bobrow (1990) and Chitrao and Grishman (1990) introduced best-first PCFG parsing, the approach taken here.</S>\n    <S sid=\"23\" ssid=\"17\">Subsequent work has suggested different FOMs built from PCFG probabilities (Miller and Fox.</S>\n    <S sid=\"24\" ssid=\"18\">1994: Kochman and Kupin.</S>\n    <S sid=\"25\" ssid=\"19\">1991: N1agerman and Marcus, 1991).</S>\n    <S sid=\"26\" ssid=\"20\">Probably the most extensive comparison of possible metrics for best-first PCFG parsing is that of Caraballo and Charniak (henceforth C&amp;C) (Forthcoming).</S>\n    <S sid=\"27\" ssid=\"21\">They consider a large number of FOMs, and view them as approximations of some &amp;quot;ideal&amp;quot; (but only computable after the fact) FOM.</S>\n    <S sid=\"28\" ssid=\"22\">Of these they recommend one as the best of the lot.</S>\n    <S sid=\"29\" ssid=\"23\">In this paper we basically adopt both their framework and their recommended FOM.</S>\n    <S sid=\"30\" ssid=\"24\">The next section describes their work in more detail, Besides C&amp;C the work that is most directly comparable to ours is that of Goodman (1997) and Ratnaparki (1997).</S>\n    <S sid=\"31\" ssid=\"25\">Goodman uses an FOM that is similar to that of C&amp;C but one that should, in general, be somewhat more accurate.</S>\n    <S sid=\"32\" ssid=\"26\">However, both Goodman's and Ratnaparki's work assumes that one is doing a beam search of some sort, rather than a best-first search, and their FOM are unfortunately tied to their frameworks and thus cannot be adopted here.</S>\n    <S sid=\"33\" ssid=\"27\">We briefly compare our results to theirs in Section 5.</S>\n    <S sid=\"34\" ssid=\"28\">As noted, our paper takes off from that of C&amp;C and uses the same FOM.</S>\n    <S sid=\"35\" ssid=\"29\">The major difference is simply that our parser uses the FOM to rank edges (including incomplete edges), rather than simply completed constituents, as was done by C&amp;C.</S>\n    <S sid=\"36\" ssid=\"30\">What is interesting about our approach is that such a seemingly simple change can produce rather dramatic results.</S>\n    <S sid=\"37\" ssid=\"31\">Rather than the thousands of edges required by C&amp;C, the parser presented here requires hundreds, or even, if one is willing to pay a small price in accuracy, tens.</S>\n  </SECTION>\n  <SECTION title=\"2 Constituent-Based Best-First\" number=\"2\">\n    <S sid=\"38\" ssid=\"1\">In the approach taken in C&amp;C, only completed edges, i.e., constituents, are entered into the agenda; incomplete edges are always processed as soon as they are constructed.</S>\n    <S sid=\"39\" ssid=\"2\">At each iteration the constituent with the highest figure of merit is removed from the agenda, added to the chart, and used to extend current partially completed constituents.</S>\n    <S sid=\"40\" ssid=\"3\">Thus we characterize their work as constituent-based best-first chart parsing.</S>\n    <S sid=\"41\" ssid=\"4\">C&amp;C take as an &amp;quot;ideal&amp;quot; FOM the quantity to,,).</S>\n    <S sid=\"42\" ssid=\"5\">Here NJ, is a constituent of type i (e.g., NP, VP, etc.) that spans the constituents from j up to but not including k, and tom are the n parts-of-speech (tags) of the sentence.</S>\n    <S sid=\"43\" ssid=\"6\">Note that C&amp;C simplify parsing by assuming that the input is a sequence of tags, not words.</S>\n    <S sid=\"44\" ssid=\"7\">We make the same assumption in this paper.</S>\n    <S sid=\"45\" ssid=\"8\">Thus taking p(N;rk I to,,) as an FOM says that one should work on the constituent that is most likely to be correct .given the tags of the sentence.</S>\n    <S sid=\"46\" ssid=\"9\">As p(N;,k to,n) can only be computed precisely after a full parse of the sentence, C&amp;C derive several approximations, in each case starting from the well known equation for p(Nlk I tom) in terms of the inside and outside probabilities, /3(1V.1,k) and where fi(Mjkj ) and a(N'k ) are defined as follows: (4) Informally, this can be obtained by approximating the outside probability a(N.;,k) in Equation 1 with a bitag estimate.</S>\n    <S sid=\"47\" ssid=\"10\">Of the five terms in Equation 4, two can be directly estimated from training data: the &amp;quot;boundary statistics&amp;quot; p(N.:,k I tj) (the probability of a constituent of type NIAstarting just after the tag tj) and p(tk I NIA) (the probability of tk appearing just after the end of a constituent of type N.4).</S>\n    <S sid=\"48\" ssid=\"11\">The tag sequence probabilitiy in the denominator is approximated using a bi-tag approximation: The basic algorithm then is quite simple.</S>\n    <S sid=\"49\" ssid=\"12\">One uses the standard chart-parsing algorithm, except at each iteration one takes from the agenda the constituent that maximizes the FOM described in Equation 4.</S>\n    <S sid=\"50\" ssid=\"13\">There are, however, two minor complexities that need to be noted.</S>\n    <S sid=\"51\" ssid=\"14\">The first relates to the inside probability 0(N:4).</S>\n    <S sid=\"52\" ssid=\"15\">C&amp;C approximate it with the sum of the probabilities of all the parses for Nlk found at that point in the parse.</S>\n    <S sid=\"53\" ssid=\"16\">This in turn requires a somewhat complicated scheme to avoid repeatedly re-evaluating Equation 4 whenever a new parse is found.</S>\n    <S sid=\"54\" ssid=\"17\">In this paper we adopt a slightly simpler method.</S>\n    <S sid=\"55\" ssid=\"18\">We approximate )3(Nk) by the most probable parse for Ni,k, rather than the sum of all the parses. j We justify this on the grounds that our parser eventually returns the most probable parse, so it seems reasonable to base our metric on its value.</S>\n    <S sid=\"56\" ssid=\"19\">This also simplifies updating 13(N3,k) when new parses are found for Nk.</S>\n    <S sid=\"57\" ssid=\"20\">Our algorithm compares the probability of the new parse to the best already found for Ni 4.</S>\n    <S sid=\"58\" ssid=\"21\">If the old one is higher, nothing need be done.</S>\n    <S sid=\"59\" ssid=\"22\">If the new one is higher, it is simply added to the agenda.</S>\n    <S sid=\"60\" ssid=\"23\">The second complexity has to do with the fact that in Equation 4 the probability of the tags t3,k are approximated using two different distributions, once in the numerator where we use the PCFG probabilities, and once in the denominator, where we use the bi-tag probabilities.</S>\n    <S sid=\"61\" ssid=\"24\">One fact noted by C&amp;C, but not discussed in their paper, is that typically the bitag model gives higher probabilities for a tag sequence than does the PCFG distribution.</S>\n    <S sid=\"62\" ssid=\"25\">For any single tag t3, the difference is not much, but as we use Equation 4 to compute our FOM for larger constituents, the numerator becomes smaller and smaller with respect to the denominator, effectively favoring smaller constituents.</S>\n    <S sid=\"63\" ssid=\"26\">To avoid this one needs to normalize the two distributions to produce more similar results.</S>\n    <S sid=\"64\" ssid=\"27\">We have empirically measured the normalization factor and found that the bi-tag distribution produces probabilities that are approximately 1.3 times those produced by the PCFG distribution, on a per-word basis.</S>\n    <S sid=\"65\" ssid=\"28\">We correct for this by making the PCFG probability of a known tag ri &gt; 1.</S>\n    <S sid=\"66\" ssid=\"29\">This has the effect of multiplying the inside probability /3(N.4) by rik-J.</S>\n    <S sid=\"67\" ssid=\"30\">In Section 4 we show how the behavior of our algorithm changes for is between 1.0 and 2.4.</S>\n  </SECTION>\n  <SECTION title=\"3 Chart parsing and binarization\" number=\"3\">\n    <S sid=\"68\" ssid=\"1\">Informally, our algorithm differs from the one presented in C&amp;C primarily in that we rank all edges, incomplete as well as complete, with respect to the FOM.</S>\n    <S sid=\"69\" ssid=\"2\">A straight-forward way to extend C&amp;C in this fashion is to transform the grammar so that all productions are either unary or binary.</S>\n    <S sid=\"70\" ssid=\"3\">Once this has been done there is no need for incomplete edges at all in bottomup parsing, and parsing can be performed using the CKY algorithm, suitably extended to handle unary productions.</S>\n    <S sid=\"71\" ssid=\"4\">One way to convert a PCFG into this form is left-factoring (Hoperoft and Ullman, 1979).</S>\n    <S sid=\"72\" ssid=\"5\">Left-factoring replaces each production A -4 : p, where p is the production probability and 101 = n&gt; 2, with the following set of binary productions: In these productions A is the ith element of /3 and `13i,j' is the subsequence A of 0, but treated as a 'new' single non-terminal in the left-factored grammar (the quote marks indicate that this subsequence is to be considered a single symbol).</S>\n    <S sid=\"73\" ssid=\"6\">For example, the production VP -4 V NP NP PP :0.7 left-factors to the following productions: VP --4 NP NP' PP :0.7 NP NP' NP' PP : 1.0 NP' -4 V NP :1.0 It is not difficult to show that the left-factored grammar defines the same probability distribution over strings as the original grammar, and to devise a tree transformation that maps each parse tree of the original grammar into a unique parse tree of the left-factored grammar of the same probability.</S>\n    <S sid=\"74\" ssid=\"7\">In fact, the assumption that all productions are at most binary is not extraordinary, since tabular parsers that construct complete parse forests in worst-case 0(n3) time explicitly or implicitly convert their grammars into binary branching form (Lang, 1974; Lang, 1991).</S>\n    <S sid=\"75\" ssid=\"8\">Sikkel and Nijholt (1997) describe in detail the close relationship between the CKY algorithm, the Earley algorithm and a bottom-up variant of the Earley algorithm.</S>\n    <S sid=\"76\" ssid=\"9\">The key observation is that the 'new' non-terminals `01,i' in a CKY parse using a left-factored grammar correspond to the set of non-empty incomplete edges A -4 01,z.</S>\n    <S sid=\"77\" ssid=\"10\">13z+1,n in the bottom-up variant of the Earley algorithm, where A -&gt; 131,, is a production of the original grammar.</S>\n    <S sid=\"78\" ssid=\"11\">Specifically, the fundamental rule of chart parsing (Kay, 1980), which combines an incomplete edge A --* a • BO with a complete edge B 7- to yield the edge A -+ a B • 0, corresponds to the left-factored productions `aB' --+ a B if /3 is non-empty or A 'a' B if i3 is empty.</S>\n    <S sid=\"79\" ssid=\"12\">Thus in general a single 'new' non-terminal in a CKY parse using the left-factored grammar abbreviates several incomplete edges in the Earley algorithm.</S>\n  </SECTION>\n  <SECTION title=\"4 The Experiment\" number=\"4\">\n    <S sid=\"80\" ssid=\"1\">For our experiment, we used a tree-bank grammar induced from sections 2-21 of the Penn Wall Street Journal text (Marcus et al., 1993), with section 22 reserved for testing.</S>\n    <S sid=\"81\" ssid=\"2\">All sentences of length greater than 40 were ignored for testing purposes as done in both C&amp;C and Goodman (1997).</S>\n    <S sid=\"82\" ssid=\"3\">We applied the binarization technique described above to the grammar.</S>\n    <S sid=\"83\" ssid=\"4\">We chose to measure the amount of work done by the parser in terms of the average number of edges popped off the agenda before finding a parse.</S>\n    <S sid=\"84\" ssid=\"5\">This method has the advantage of being platform independent, as well as providing a measure of &amp;quot;perfection&amp;quot;.</S>\n    <S sid=\"85\" ssid=\"6\">Here, perfection is the minimum number of edges we would need to pop off the agenda in order to create the correct parse.</S>\n    <S sid=\"86\" ssid=\"7\">For the binarized grammar, where each popped edge is a completed constituent, this number is simply the number of terminals plus nonterminals in the sentence— on average, 47.5.</S>\n    <S sid=\"87\" ssid=\"8\">Our algorithm includes some measures to reduce the number of items on the agenda, and thus (presumably) the number of popped edges.</S>\n    <S sid=\"88\" ssid=\"9\">Each time we add a constituent to the chart, we combine it with the constituents on either side of it, potentially creating several new edges.</S>\n    <S sid=\"89\" ssid=\"10\">For each of these new edges, we check to see if a matching constituent (i.e. a constituent with the same head, start, and end points) already exists in either the agenda or the chart.</S>\n    <S sid=\"90\" ssid=\"11\">If there is no match, we simply add the new edge to the agenda.</S>\n    <S sid=\"91\" ssid=\"12\">If there is a match but the old parse of Nild, is better than the new one, we discard the new parse.</S>\n    <S sid=\"92\" ssid=\"13\">Finally, if we have found a better parse of N.4, we add the new edge to the agenda, removing the old one if it has not already been popped.</S>\n    <S sid=\"93\" ssid=\"14\">We tested the parser on section section 22 of the WSJ text with various normalization constants .77, working on each sentence only until we reached the first full parse.</S>\n    <S sid=\"94\" ssid=\"15\">For each sentence we recorded the number of popped edges needed to reach the first parse, and the precision and recall of that parse.</S>\n    <S sid=\"95\" ssid=\"16\">The average number of popped edges to first parse as a function of q is shown in Figure 1, and the average precision and recall are shown in Figure 2.</S>\n    <S sid=\"96\" ssid=\"17\">The number of popped edges decreases as ij increases from 1.0 to 1.7, then begins to increase again.</S>\n    <S sid=\"97\" ssid=\"18\">See Section 5 for discussion of these results.</S>\n    <S sid=\"98\" ssid=\"19\">The precision and recall also decrease as 77 increases.</S>\n    <S sid=\"99\" ssid=\"20\">Note that, because we used a binarized grammer for parsing, the trees produced by the parser contain binarized labels rather than the labels in the treebank.</S>\n    <S sid=\"100\" ssid=\"21\">In order to calculate precision and recall. we &amp;quot;debinarized&amp;quot; the parser's output and then calculated the figures as usual.</S>\n    <S sid=\"101\" ssid=\"22\">These results suggest two further questions: Is the higher accuracy with lower 77 due in part to the higher number of edges popped?</S>\n    <S sid=\"102\" ssid=\"23\">If so, can we gain accuracy with higher i by letting the parser continue past the first parse (i.e. pop more edges)?</S>\n    <S sid=\"103\" ssid=\"24\">To answer these questions, we ran the parser again, this time allowing it to continue parsing until it had popped 20 times as many edges as needed to reach the first parse.</S>\n    <S sid=\"104\" ssid=\"25\">The results of this experiment are shown in Figure 3, where we plot (precision + recall)/2 (henceforth &amp;quot;accuracy&amp;quot;) as a function of edges.</S>\n    <S sid=\"105\" ssid=\"26\">Note that regardless of 7/ the accuracy of the parse increases given extra time, but that all of the increase is achieved with only 1.5 to 2 times as many edges as needed for the first parse.</S>\n    <S sid=\"106\" ssid=\"27\">For ij between 1.0 and 1.2, the highest accuracy is almost the same, about 75.2, but this value is reached with an average of slightly under 400 edges when 77 = 1.2, compared to about 650 when 7/ = 1.0.</S>\n  </SECTION>\n  <SECTION title=\"5 Results\" number=\"5\">\n    <S sid=\"107\" ssid=\"1\">To better understand the experimental results it first behooves us to compare them to those achieved previously.</S>\n    <S sid=\"108\" ssid=\"2\">Goodman's results (1997) are hard to compare against ours because his parser returns more than a singe best parse and because he measures processing time, not edges.</S>\n    <S sid=\"109\" ssid=\"3\">However he does give edges/second for one of his 2000 4000 6000 8000 10000 parsers and this plus his parsing times suggests that for him edges/sentence will measure in the tens of thousands — a far cry from our hundreds.</S>\n    <S sid=\"110\" ssid=\"4\">Ratnaparki's (1997) beam search parsing procedure produces higher accuracy results than our PCFG model, and achieves this with a beam width of 20.</S>\n    <S sid=\"111\" ssid=\"5\">Unfortunately his paper does not give statistics which can be directly compared with ours.</S>\n    <S sid=\"112\" ssid=\"6\">The work by C&amp;C is easier to compare.</S>\n    <S sid=\"113\" ssid=\"7\">In Figure 4 we reproduce C&amp;C's results on the percentage of sentences (length 18-26) parsed as a function of number of edges used.</S>\n    <S sid=\"114\" ssid=\"8\">We performed the same experiment, and our results are included there as well.</S>\n    <S sid=\"115\" ssid=\"9\">This figure makes dramatic the order of magnitude improvement provided by our new scheme, but it is not too easy to read numbers off of it.</S>\n    <S sid=\"116\" ssid=\"10\">Such numbers are provided in Table 1.</S>\n    <S sid=\"117\" ssid=\"11\">Our figures were obtained using ri = 1.2.</S>\n    <S sid=\"118\" ssid=\"12\">As can be seen, our parser requires about one twentieth the number of edges required by C&amp;C.</S>\n    <S sid=\"119\" ssid=\"13\">Indeed, the low average number of edges to first parse is probably the most striking thing about our results.</S>\n    <S sid=\"120\" ssid=\"14\">Even allowing for the fact that considerably more edges must be pushed than are popped, the total number of edges required to first parse is quite small.</S>\n    <S sid=\"121\" ssid=\"15\">Since the average number of edges required to construct just the (left-factored) test corpus trees is 47.5, our parsing system considers as few as 3 times as many edges as are required to actually produce the output tree.</S>\n    <S sid=\"122\" ssid=\"16\">Almost as interesting, if i is below 1.4, the precision and recall scores of the first parse are better than those obtained by running the parser to exhaustion, even though the probability of the first parses our algorithm returns cannot be higher than that found by the exhaustive version.</S>\n    <S sid=\"123\" ssid=\"17\">Furthermore, as seen in Figure 3, running our parser past the first parse by a small amount (150% of the edges required for the first parse) produces still more accurate parses.</S>\n    <S sid=\"124\" ssid=\"18\">At 150% of the minimum number of edges and I) = 1.2 the precision/recall figures are about 2% above those for the maximum likelihood parse.</S>\n    <S sid=\"125\" ssid=\"19\">We have two (possibly related) theories of these phenomona.</S>\n    <S sid=\"126\" ssid=\"20\">It may be that the FOM metric used to select constituents forces our parser to concentrate on edges which are plausible given their surrounding preterminals; information which is ignored by the exhaustive maximum likelihood parser.</S>\n    <S sid=\"127\" ssid=\"21\">Alternatively, it may be that because our FOM causes our parser to prefer edges with a high inside times (estimated) outside probability, it is in fact partially mimicking Goodman's (Goodman, 1996) 'Labelled Recall' parsing algorithm, which does not return the highest probability parse but attempts to maximize labelled bracket recall with the test set.</S>\n    <S sid=\"128\" ssid=\"22\">Finally, it is interesting to note that the minimum number of edges per parse is reached when 77 1.65, which is considerably larger than the theoretical estimate of 1.3 given earlier.</S>\n    <S sid=\"129\" ssid=\"23\">Notice that one effect of increasing r/ is to raise the FOM for longer constituents.</S>\n    <S sid=\"130\" ssid=\"24\">It may be that on average a partial parse is completed fastest if larger constituents receive more attention since they are more likely to lead quickly to a complete analysis, which would be one consequence of the larger than expected n. This last hypothesis is also consistent with the observation that average precision and recall sharply falls off when 77 is increased beyond its theoretically optimal value, since then the parser is presumably focusing on relatively larger constituents and ignoring other, strictly more plausible, smaller ones.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion\" number=\"6\">\n    <S sid=\"131\" ssid=\"1\">It is worth noting that while we have presented the use of edge-based best-first chart parsing in the service of a rather pure form of PCFG parsing, there is no particular reason to assume that the technique is so limited in its domain of applicability.</S>\n    <S sid=\"132\" ssid=\"2\">One can imagine the same techniques coupled with more informative probability distributions, such as lexicalized PCFGs (Charniak, 1997), or even grammars not based upon literal rules, but probability distributions that describe how rules are built up from smaller components (Magerman, 1995; Collins, 1997).</S>\n    <S sid=\"133\" ssid=\"3\">Clearly further research is warranted.</S>\n    <S sid=\"134\" ssid=\"4\">Be this as it may, the take-home lesson from this paper is simple: combining an edge-based agenda with the figure of merit from C&amp;C To the best of our knowledge this is currently the most effecient parsing technique for PCFG grammars induced from large tree-banks.</S>\n    <S sid=\"135\" ssid=\"5\">As such we strongly recommend this technique to others interested in PCFG parsing.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "d7c3aa39aa8762c70b07d82c3b4cc252",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Edge-Based Best-First Chart Parsing"
                }
            }
        },
        {
            "index": 2,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.",
                    "filename": "Learning_Subjective_Language",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J04-3002/Documents_xml/J04-3002.xml",
                    "title": "Learning Subjective Language"
                },
                "llm": {
                    "descriptions": [
                        "**Abstract:**\n\nThis paper presents an empirical study on learning subjective language from corpora with the aim of enhancing natural language processing (NLP) applications such as information extraction, text categorization, and summarization. Subjective language, which expresses opinions, evaluations, and speculations, plays a critical role in determining the stance and sentiment of text. To this end, the authors generate and test various subjectivity clues from corpora, including low-frequency words (unique words), collocations, and adjectives and verbs identified using distributional similarity. These features are evaluated on different datasets, exhibiting consistent performance improvements over baseline measures. The paper also introduces a novel method for contextual disambiguation of subjective expressions using a density feature, which assesses the likelihood of subjectivity based on the presence of other subjective elements in the surrounding context. The effectiveness of the learned features is demonstrated through opinion piece classification, achieving a 94% accuracy with a 28% reduction in error over the baseline. The study concludes with the potential of these subjective language clues to improve various NLP tasks and suggests future work directions, including the identification of objectivity clues and the classification of subjective expressions by type.",
                        "**Abstract:**\n\nThis paper presents an empirical study on learning subjective language from corpora. Subjective language, characterized by expressions of opinions, evaluations, and speculations, is crucial for various natural language processing (NLP) applications such as information extraction, text categorization, and opinion piece recognition. The authors introduce methods to automatically identify and test subjectivity clues and contextual features from large corpora, aiming to enhance system performance in recognizing subjective sentences and classifying opinionated documents.\n\nThe study generates subjectivity clues from low-frequency words, collocations, and words identified through distributional similarity. It evaluates these clues on both manually annotated expression-level data and document-level opinion piece annotations, finding consistent performance across different data sets and procedures. Furthermore, the paper explores the impact of context, showing that the density of subjectivity clues in proximity to a word greatly influences that word's subjectivity.\n\nThe practical utility of the learned knowledge is demonstrated through opinion piece classification, where the algorithm achieves significant accuracy improvements over baseline methods. The findings suggest that unique words, collocations with placeholders for unique words, and adjectives and verbs identified through distributional similarity are valuable predictors of subjectivity. Additionally, the authors conduct an annotation study to assess the subjectivity of sentences containing high-density features, which supports the effectiveness of their approach.\n\nThe paper concludes that knowledge of subjective language is beneficial for NLP applications and proposes future research directions, such as identifying objectivity clues and further exploring subjective expression types within discourse contexts.",
                        "**Abstract:**\n\nThis study addresses the task of learning subjective language from corpora, aiming to enhance natural language processing (NLP) applications such as information extraction, text categorization, and sentiment analysis. Subjectivity in language is used to express opinions, evaluations, and speculations. The research identifies clues to subjectivity, including unique words (hapax legomena), fixed and generalized collocations, and adjectives and verbs clustered by distributional similarity. These features are validated across different datasets, demonstrating consistency in their ability to distinguish between subjective and objective content. Furthermore, the study explores the influence of the density of subjectivity clues in surrounding text on the subjectivity of a given word, finding that high-density clues are more likely to be subjective. An annotation study confirms most high-density clues appear in subjective spans of text. The clues are utilized in a document-level classification task to distinguish opinion pieces from non-opinion texts, achieving a 28% reduction in error compared to a baseline. The results suggest that the features identified are not brittle and hold promise for use in NLP applications. Future work includes exploiting density features to improve text categorization and exploring objectivity clues and the classification of subjective expressions' types.",
                        "**Abstract:**\n\nThis work focuses on learning subjective language from corpora with applications in natural language processing (NLP). Subjective language, which encompasses opinions, evaluations, and speculations, is crucial for tasks such as information extraction, text categorization, opinion piece recognition, and genre detection. The study introduces methods for generating and testing subjectivity clues including unique low-frequency words, collocations, and adjectives and verbs identified through distributional similarity. These clues exhibit consistent performance across different data sets and, notably, the surrounding context's density of subjectivity clues significantly impacts word subjectivity. Additionally, the research presents an annotation study that validates the subjectivity of sentences with high-density features. To demonstrate the practical utility of the acquired knowledge, the clues are employed for opinion piece recognition, showing favorable results against baseline. This study's findings indicate that automatically identified features can be effective in recognizing subjective language and that density in context is a key factor in disambiguating potential subjective expressions."
                    ],
                    "filename": "learning_subjective_language-write_xml_paper_abstract",
                    "generation_prompt_nickname": "write_xml_paper_abstract",
                    "generation_prompt_text": "Read the following academic paper (in XML format) and create an abstract for it.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Learning Subjective Language</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"9\" ssid=\"1\">Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations.</S>\n    <S sid=\"10\" ssid=\"2\">There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization.</S>\n    <S sid=\"11\" ssid=\"3\">The goal of this work is learning subjective language from corpora.</S>\n    <S sid=\"12\" ssid=\"4\">Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity.</S>\n    <S sid=\"13\" ssid=\"5\">The features are also examined working together in concert.</S>\n    <S sid=\"14\" ssid=\"6\">The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets.</S>\n    <S sid=\"15\" ssid=\"7\">In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features.</S>\n    <S sid=\"16\" ssid=\"8\">Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"17\" ssid=\"1\">Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations (Banfield 1982; Wiebe 1994).</S>\n    <S sid=\"18\" ssid=\"2\">Many natural language processing (NLP) applications could benefit from being able to distinguish subjective language from language used to objectively present factual information.</S>\n    <S sid=\"19\" ssid=\"3\">Current extraction and retrieval technology focuses almost exclusively on the subject matter of documents.</S>\n    <S sid=\"20\" ssid=\"4\">However, additional aspects of a document influence its relevance, including evidential status and attitude (Kessler, Nunberg, Sch¨utze 1997).</S>\n    <S sid=\"21\" ssid=\"5\">Information extraction systems should be able to distinguish between factual information (which should be extracted) and nonfactual information (which should be discarded or labeled as uncertain).</S>\n    <S sid=\"22\" ssid=\"6\">Question-answering systems should distinguish between factual and speculative answers.</S>\n    <S sid=\"23\" ssid=\"7\">Multi-perspective question answering aims to present multiple answers to the user based upon speculation or opinions derived from different sources (Carbonell 1979; Wiebe et al. 2003).</S>\n    <S sid=\"24\" ssid=\"8\">Multidocument summarization systems should summarize different opinions and perspectives.</S>\n    <S sid=\"25\" ssid=\"9\">Automatic subjectivity analysis would also be useful to perform flame recognition (Spertus 1997; Kaufer 2000), e-mail classification (Aone, Ramos-Santacruze, and Niehaus 2000), intellectual attribution in text (Teufel and Moens 2000), recognition of speaker role in radio broadcasts (Barzialy et al. 2000), review mining (Terveen et al.</S>\n    <S sid=\"26\" ssid=\"10\">1997), review classification (Turney 2002; Pang, Lee, and Vaithyanathan 2002), style in generation (Hovy 1987), and clustering documents by ideological point of view (Sack 1995).</S>\n    <S sid=\"27\" ssid=\"11\">In general, nearly any information-seeking system could benefit from knowledge of how opinionated a text is and whether or not the writer purports to objectively present factual material.</S>\n    <S sid=\"28\" ssid=\"12\">To perform automatic subjectivity analysis, good clues must be found.</S>\n    <S sid=\"29\" ssid=\"13\">A huge variety of words and phrases have subjective usages, and while some manually developed resources exist, such as dictionaries of affective language (General-Inquirer 2000; Heise 2000) and subjective features in general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]), there is no comprehensive dictionary of subjective language.</S>\n    <S sid=\"30\" ssid=\"14\">In addition, many expressions with subjective usages have objective usages as well, so a dictionary alone would not suffice.</S>\n    <S sid=\"31\" ssid=\"15\">An NLP system must disambiguate these expressions in context.</S>\n    <S sid=\"32\" ssid=\"16\">The goal of our work is learning subjective language from corpora.</S>\n    <S sid=\"33\" ssid=\"17\">In this article, we generate and test subjectivity clues and contextual features and use the knowledge we gain to recognize subjective sentences and opinionated documents.</S>\n    <S sid=\"34\" ssid=\"18\">Two kinds of data are available to us: a relatively small amount of data manually annotated at the expression level (i.e., labels on individual words and phrases) of Wall Street Journal and newsgroup data and a large amount of data with existing documentlevel annotations from the Wall Street Journal (opinion pieces, such as editorials and reviews, versus nonopinion pieces).</S>\n    <S sid=\"35\" ssid=\"19\">Both are used as training data to identify clues of subjectivity.</S>\n    <S sid=\"36\" ssid=\"20\">In addition, we cross-validate the results between the two types of annotation: The clues learned from the expression-level data are evaluated against the document-level annotations, and those learned using the document-level annotations are evaluated against the expression-level annotations.</S>\n    <S sid=\"37\" ssid=\"21\">There were a number of motivations behind our decision to use document-level annotations, in addition to our manual annotations, to identify and evaluate clues of subjectivity.</S>\n    <S sid=\"38\" ssid=\"22\">The document-level annotations were not produced according to our annotation scheme and were not produced for the purpose of training and evaluating an NLP system.</S>\n    <S sid=\"39\" ssid=\"23\">Thus, they are an external influence from outside the laboratory.</S>\n    <S sid=\"40\" ssid=\"24\">In addition, there are a great number of these data, enabling us to evaluate the results on a larger scale, using multiple large test sets.</S>\n    <S sid=\"41\" ssid=\"25\">This and cross-training between the two types of annotations allows us to assess consistency in performance of the various identification procedures.</S>\n    <S sid=\"42\" ssid=\"26\">Good performance in cross-validation experiments between different types of annotations is evidence that the results are not brittle.</S>\n    <S sid=\"43\" ssid=\"27\">We focus on three types of subjectivity clues.</S>\n    <S sid=\"44\" ssid=\"28\">The first are hapax legomena, the set of words that appear just once in the corpus.</S>\n    <S sid=\"45\" ssid=\"29\">We refer to them here as unique words.</S>\n    <S sid=\"46\" ssid=\"30\">The set of all unique words is a feature with high frequency and significantly higher precision than baseline (Section 3.2).</S>\n    <S sid=\"47\" ssid=\"31\">The second are collocations (Section 3.3).</S>\n    <S sid=\"48\" ssid=\"32\">We demonstrate a straightforward method for automatically identifying collocational clues of subjectivity in texts.</S>\n    <S sid=\"49\" ssid=\"33\">The method is first used to identify fixed n-grams, such as of the century and get out of here.</S>\n    <S sid=\"50\" ssid=\"34\">Interestingly, many include noncontent words that are typically on stop lists of NLP systems (e.g., of, the, get, out, here in the above examples).</S>\n    <S sid=\"51\" ssid=\"35\">The method is then used to identify an unusual form of collocation: One or more positions in the collocation may be filled by any word (of an appropriate part of speech) that is unique in the test data.</S>\n    <S sid=\"52\" ssid=\"36\">The third type of subjectivity clue we examine here are adjective and verb features identified using the results of a method for clustering words according to distributional similarity (Lin 1998) (Section 3.4).</S>\n    <S sid=\"53\" ssid=\"37\">We hypothesized that two words may be distributionally similar because they are both potentially subjective (e.g., tragic, sad, and poignant are identified from bizarre).</S>\n    <S sid=\"54\" ssid=\"38\">In addition, we use distributional similarity to improve estimates of unseen events: A word is selected or discarded based on the precision of it together with its n most similar neighbors.</S>\n    <S sid=\"55\" ssid=\"39\">We show that the various subjectivity clues perform better and worse on the same data sets, exhibiting an important consistency in performance (Section 4.2).</S>\n    <S sid=\"56\" ssid=\"40\">In addition to learning and evaluating clues associated with subjectivity, we address disambiguating them in context, that is, identifying instances of clues that are subjective in context (Sections 4.3 and 4.4).</S>\n    <S sid=\"57\" ssid=\"41\">We find that the density of clues in the surrounding context is an important influence.</S>\n    <S sid=\"58\" ssid=\"42\">Using two types of annotations serves us well here, too.</S>\n    <S sid=\"59\" ssid=\"43\">It enables us to use manual judgments to identify parameters for disambiguating instances of automatically identified clues.</S>\n    <S sid=\"60\" ssid=\"44\">High-density clues are high precision in both the expression-level and document-level data.</S>\n    <S sid=\"61\" ssid=\"45\">In addition, we give the results of a new annotation study showing that most high-density clues are in subjective text spans (Section 4.5).</S>\n    <S sid=\"62\" ssid=\"46\">Finally, we use the clues together to perform documentlevel classification, to further demonstrate the utility of the acquired knowledge (Section 4.6).</S>\n    <S sid=\"63\" ssid=\"47\">At the end of the article, we discuss related work (Section 5) and conclusions (Section 6).</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"64\" ssid=\"1\">Subjective language is language used to express private states in the context of a text or conversation.</S>\n    <S sid=\"65\" ssid=\"2\">Private state is a general covering term for opinions, evaluations, emotions, and speculations (Quirk et al. 1985).</S>\n    <S sid=\"66\" ssid=\"3\">The following are examples of subjective sentences from a variety of document types.</S>\n    <S sid=\"67\" ssid=\"4\">The first two examples are from Usenet newsgroup messages: The next one is from an editorial: (3) We stand in awe of the Woodstock generation’s ability to be unceasingly fascinated by the subject of itself.</S>\n    <S sid=\"68\" ssid=\"5\">(“Bad Acid,” Wall Street Journal, August 17, 1989) The next example is from a book review: The last one is from a news story: (5) “The cost of health care is eroding our standard of living and sapping industrial strength,” complains Walter Maher, a Chrysler health-and-benefits specialist.</S>\n    <S sid=\"69\" ssid=\"6\">(Kenneth H. Bacon, “Business and Labor Reach a Consensus on Need to Overhaul Health-Care System,” Wall Street Journal, November 1, 1989) In contrast, the following are examples of objective sentences, sentences without significant expressions of subjectivity: A particular model of linguistic subjectivity underlies the current and past research in this area by Wiebe and colleagues.</S>\n    <S sid=\"70\" ssid=\"7\">It is most fully presented in Wiebe and Rapaport (1986, 1988, 1991) and Wiebe (1990, 1994).</S>\n    <S sid=\"71\" ssid=\"8\">It was developed to support NLP research and combines ideas from several sources in fields outside NLP, especially linguistics and literary theory.</S>\n    <S sid=\"72\" ssid=\"9\">The most direct influences on the model were Dolezel (1973) (types of subjectivity clues), Uspensky (1973) (types of point of view), Kuroda (1973, 1976) (pragmatics of point of view), Chatman (1978) (story versus discourse), Cohn (1978) (linguistic styles for presenting consciousness), Fodor (1979) (linguistic description of opaque contexts), and especially Banfield (1982) (theory of subjectivity versus communication).1 The remainder of this section sketches our conceptualization of subjectivity and describes the annotation projects it underlies.</S>\n    <S sid=\"73\" ssid=\"10\">Subjective elements are linguistic expressions of private states in context.</S>\n    <S sid=\"74\" ssid=\"11\">Subjective elements are often lexical (examples are stand in awe, unceasingly, fascinated in (3) and eroding, sapping, and complains in (5)).</S>\n    <S sid=\"75\" ssid=\"12\">They may be single words (e.g., complains) or more complex expressions (e.g., stand in awe, what a NP).</S>\n    <S sid=\"76\" ssid=\"13\">Purely syntactic or morphological devices may also be subjective elements (e.g., fronting, parallelism, changes in aspect).</S>\n    <S sid=\"77\" ssid=\"14\">A subjective element expresses the subjectivity of a source, who may be the writer or someone mentioned in the text.</S>\n    <S sid=\"78\" ssid=\"15\">For example, the source of fascinating in (4) is the writer, while the source of the subjective elements in (5) is Maher (according to the writer).</S>\n    <S sid=\"79\" ssid=\"16\">In addition, a subjective element usually has a target, that is, what the subjectivity is about or directed toward.</S>\n    <S sid=\"80\" ssid=\"17\">In (4), the target is a tale; in (5), the target of Maher’s subjectivity is the cost of health care.</S>\n    <S sid=\"81\" ssid=\"18\">Note our parenthetical above—“according to the writer”—concerning Maher’s subjectivity.</S>\n    <S sid=\"82\" ssid=\"19\">Maher is not directly speaking to us but is being quoted by the writer.</S>\n    <S sid=\"83\" ssid=\"20\">Thus, the source is a nested source, which we notate (writer, Maher); this represents the fact that the subjectivity is being attributed to Maher by the writer.</S>\n    <S sid=\"84\" ssid=\"21\">Since sources are not directly addressed by the experiments presented in this article, we merely illustrate the idea here with an example, to give the reader an idea: The Foreign Ministry said Thursday that it was “surprised, to put it mildly” by the U.S. State Department’s criticism of Russia’s human rights record and objected in particular to the “odious” section on Chechnya.</S>\n    <S sid=\"85\" ssid=\"22\">(Moscow Times, March 8, 2002] Let us consider some of the subjective elements in this sentence, along with their sources: surprised, to put it mildly: (writer, Foreign Ministry, Foreign Ministry) to put it mildly: (writer, Foreign Ministry) criticism: (writer, Foreign Ministry, Foreign Ministry, U.S. State Department) objected: (writer, Foreign Ministry) odious: (writer, Foreign Ministry) Consider surprised, to put it mildly.</S>\n    <S sid=\"86\" ssid=\"23\">This refers to a private state of the Foreign Ministry (i.e., it is very surprised).</S>\n    <S sid=\"87\" ssid=\"24\">This is in the context of The Foreign Ministry said, which is in a sentence written by the writer.</S>\n    <S sid=\"88\" ssid=\"25\">This gives us the three-level source (writer, Foreign Ministry, Foreign Ministry).</S>\n    <S sid=\"89\" ssid=\"26\">The phrase to put it mildly, which expresses sarcasm, is attributed to the Foreign Ministry by the writer (i.e., according to the writer, the Foreign Ministry said this).</S>\n    <S sid=\"90\" ssid=\"27\">So its source is (writer, Foreign Ministry).</S>\n    <S sid=\"91\" ssid=\"28\">The subjective element criticism has a deeply nested source: According to the writer, the Foreign Ministry said it is surprised by the U.S. State Department’s criticism.</S>\n    <S sid=\"92\" ssid=\"29\">The nested-source representation allows us to pinpoint the subjectivity in a sentence.</S>\n    <S sid=\"93\" ssid=\"30\">For example, there is no subjectivity attributed directly to the writer in the above sentence: At the level of the writer, the sentence merely says that someone said something and objected to something (without evaluating or questioning this).</S>\n    <S sid=\"94\" ssid=\"31\">If the sentence started The magnificent Foreign Ministry said... , then we would have an additional subjective element, magnificent, with source (writer).</S>\n    <S sid=\"95\" ssid=\"32\">Note that subjective does not mean not true.</S>\n    <S sid=\"96\" ssid=\"33\">Consider the sentence John criticized Mary for smoking.</S>\n    <S sid=\"97\" ssid=\"34\">The verb criticized is a subjective element, expressing negative evaluation, with nested source (writer, John).</S>\n    <S sid=\"98\" ssid=\"35\">But this does not mean that John does not believe that Mary smokes.</S>\n    <S sid=\"99\" ssid=\"36\">(In addition, the fact that John criticized Mary is being presented as true by the writer.)</S>\n    <S sid=\"100\" ssid=\"37\">Similarly, objective does not mean true.</S>\n    <S sid=\"101\" ssid=\"38\">A sentence is objective if the language used to convey the information suggests that facts are being presented; in the context of the discourse, material is objectively presented as if it were true.</S>\n    <S sid=\"102\" ssid=\"39\">Whether or not the source truly believes the information, and whether or not the information is in fact true, are considerations outside the purview of a theory of linguistic subjectivity.</S>\n    <S sid=\"103\" ssid=\"40\">An aspect of subjectivity highlighted when we are working with NLP applications is ambiguity.</S>\n    <S sid=\"104\" ssid=\"41\">Many words with subjective usages may be used objectively.</S>\n    <S sid=\"105\" ssid=\"42\">Examples are sapping and eroding.</S>\n    <S sid=\"106\" ssid=\"43\">In (5), they are used subjectively, but one can easily imagine objective usages, in a scientific domain, for example.</S>\n    <S sid=\"107\" ssid=\"44\">Thus, an NLP system may not merely consult a list of lexical items to accurately identify subjective language but must disambiguate words, phrases, and sentences in context.</S>\n    <S sid=\"108\" ssid=\"45\">In our terminology, a potential subjective element (PSE) is a linguistic element that may be used to express subjectivity.</S>\n    <S sid=\"109\" ssid=\"46\">A subjective element is an instance of a potential subjective element, in a particular context, that is indeed subjective in that context (Wiebe 1994).</S>\n    <S sid=\"110\" ssid=\"47\">In this article, we focus on learning lexical items that are associated with subjectivity (i.e., PSEs) and then using them in concert to disambiguate instances of them (i.e., to determine whether the instances are subjective elements).</S>\n    <S sid=\"111\" ssid=\"48\">In our subjectivity annotation projects, we do not give the annotators lists of particular words and phrases to look for.</S>\n    <S sid=\"112\" ssid=\"49\">Rather, we ask them to label sentences according to their interpretations in context.</S>\n    <S sid=\"113\" ssid=\"50\">As a result, the annotators consider a large variety of expressions when performing annotations.</S>\n    <S sid=\"114\" ssid=\"51\">We use data that have been manually annotated at the expression level, the sentence level, and the document level.</S>\n    <S sid=\"115\" ssid=\"52\">For diversity, we use data from the Wall Street Journal Treebank as well as data from a corpus of Usenet newsgroup messages.</S>\n    <S sid=\"116\" ssid=\"53\">Table 1 summarizes the data sets and annotations used in this article.</S>\n    <S sid=\"117\" ssid=\"54\">None of the datasets overlap.</S>\n    <S sid=\"118\" ssid=\"55\">The annotation types listed in the table are those used in the experiments presented in this article.</S>\n    <S sid=\"119\" ssid=\"56\">In our first subjectivity annotation project (Wiebe, Bruce, and O’Hara 1999; Bruce and Wiebe 1999), a corpus of sentences from the Wall Street Journal Treebank Corpus (Marcus, Santorini, and Marcinkiewicz 1993) (corpus WSJ-SE in Table 1) was annotated at the sentence level by multiple judges.</S>\n    <S sid=\"120\" ssid=\"57\">The judges were instructed to classify a sentence as subjective if it contained any significant expressions of subjectivity, attributed to either the writer or someone mentioned in the text, and to classify the sentence as objective, otherwise.</S>\n    <S sid=\"121\" ssid=\"58\">After multiple rounds of training, the annotators independently annotated a fresh test set of 500 sentences from WSJ-SE.</S>\n    <S sid=\"122\" ssid=\"59\">They achieved an average pairwise kappa score of 0.70 over the entire test set, an average pairwise kappa score of 0.80 for the 85% of the test set for which the annotators were somewhat sure of their judgments, and an average pairwise kappa score of 0.88 for the 70% of the test set for which the annotators were very sure of their judgments.</S>\n    <S sid=\"123\" ssid=\"60\">We later asked the same annotators to identify the subjective elements in WSJSE.</S>\n    <S sid=\"124\" ssid=\"61\">Specifically, each annotator was given the subjective sentences he identified in the previous study and asked to put brackets around the words he believed caused the sentence to be classified as subjective.2 For example (subjective elements are in parentheses): They paid (yet) more for (really good stuff).</S>\n    <S sid=\"125\" ssid=\"62\">(Perhaps you’ll forgive me) for reposting his response.</S>\n    <S sid=\"126\" ssid=\"63\">No other instructions were given to the annotators and no training was performed for the expression-level task.</S>\n    <S sid=\"127\" ssid=\"64\">A single round of tagging was performed, with no communication between annotators.</S>\n    <S sid=\"128\" ssid=\"65\">There are techniques for analyzing agreement when annotations involve segment boundaries (Litman and Passonneau 1995; Marcu, Romera, and Amorortu 1999), but our focus in this article is on words.</S>\n    <S sid=\"129\" ssid=\"66\">Thus, our analyses are at the word level: Each word is classified as either appearing in a subjective element or not.</S>\n    <S sid=\"130\" ssid=\"67\">Punctuation and numbers are excluded from the analyses.</S>\n    <S sid=\"131\" ssid=\"68\">The kappa value for word agreement in this study is 0.42.</S>\n    <S sid=\"132\" ssid=\"69\">Another two-level annotation project was performed in Wiebe et al. (2001), this time involving document-level and expression-level annotations of newsgroup data (NG-FE in Table 1).</S>\n    <S sid=\"133\" ssid=\"70\">In that project, we were interested in annotating flames, inflammatory messages in newsgroups or listservs.</S>\n    <S sid=\"134\" ssid=\"71\">Note that inflammatory language is a kind of subjective language.</S>\n    <S sid=\"135\" ssid=\"72\">The annotators were instructed to mark a message as a flame if the main intention of the message is a personal attack and the message contains insulting or abusive language.</S>\n    <S sid=\"136\" ssid=\"73\">After multiple rounds of training, three annotators independently annotated a fresh test set of 88 messages from NG-FE.</S>\n    <S sid=\"137\" ssid=\"74\">The average pairwise percentage agreement is 92% and the average pairwise kappa value is 0.78.</S>\n    <S sid=\"138\" ssid=\"75\">These results are comparable to those of Spertus (1997), who reports 98% agreement on noninflammatory messages and 64% agreement on inflammatory messages.</S>\n    <S sid=\"139\" ssid=\"76\">Two of the annotators were then asked to identify the flame elements in the entire corpus NG-FE.</S>\n    <S sid=\"140\" ssid=\"77\">Flame elements are the subset of subjective elements that are perceived to be inflammatory.</S>\n    <S sid=\"141\" ssid=\"78\">The two annotators were asked to do this in the entire corpus, even those messages not identified as flames, because messages that were not judged to be flames at the document level may contain some individual inflammatory phrases.</S>\n    <S sid=\"142\" ssid=\"79\">As above, no training was performed for the expression-level task, and a single round of tagging was performed, without communication between annotators.</S>\n    <S sid=\"143\" ssid=\"80\">Agreement was measured in the same way as in the subjective-element study above.</S>\n    <S sid=\"144\" ssid=\"81\">The kappa value for flame element annotations in corpus NG-FE is 0.46.</S>\n    <S sid=\"145\" ssid=\"82\">An additional annotation project involved a single annotator, who performed subjective-element annotations on the newsgroup corpus NG-SE.</S>\n    <S sid=\"146\" ssid=\"83\">The agreement results above suggest that good levels of agreement can be achieved at higher levels of classification (sentence and document), but agreement at the expression level is more challenging.</S>\n    <S sid=\"147\" ssid=\"84\">The agreement values are lower for the expression-level annotations but are still much higher than that expected by chance.</S>\n    <S sid=\"148\" ssid=\"85\">Note that our word-based analysis of agreement is a tough measure, because it requires that exactly the same words be identified by both annotators.</S>\n    <S sid=\"149\" ssid=\"86\">Consider the following example from WSJ-SE: D: (played the role well) (obligatory ragged jeans a thicket of long hair and rejection of all things conventional) M: played the role (well) (obligatory) (ragged) jeans a (thicket) of long hair and (rejection) of (all things conventional) Judge D in the example consistently identifies entire phrases as subjective, while judge M prefers to select discrete lexical items.</S>\n    <S sid=\"150\" ssid=\"87\">Despite such differences between annotators, the expression-level annotations proved very useful for exploring hypotheses and generating features, as described below.</S>\n    <S sid=\"151\" ssid=\"88\">Since this article was written, a new annotation project has been completed.</S>\n    <S sid=\"152\" ssid=\"89\">A 10,000-sentence corpus of English-language versions of world news articles has been annotated with detailed subjectivity information as part of a project investigating multiple-perspective question answering (Wiebe et al. 2003).</S>\n    <S sid=\"153\" ssid=\"90\">These annotations are much more detailed than the annotations used in this article (including, for example, the source of each private state).</S>\n    <S sid=\"154\" ssid=\"91\">The interannotator agreement scores for the new corpus are high and are improvements over the results of the studies described above (Wilson and Wiebe 2003).</S>\n    <S sid=\"155\" ssid=\"92\">The current article uses existing document-level subjective classes, namely editorials, letters to the editor, Arts &amp; Leisure reviews, and Viewpoints in the Wall Street Journal.</S>\n    <S sid=\"156\" ssid=\"93\">These are subjective classes in the sense that they are text categories for which subjectivity is a key aspect.</S>\n    <S sid=\"157\" ssid=\"94\">We refer to them collectively as opinion pieces.</S>\n    <S sid=\"158\" ssid=\"95\">All other types of documents in the Wall Street Journal are collectively referred to as nonopinion pieces.</S>\n    <S sid=\"159\" ssid=\"96\">Note that opinion pieces are not 100% subjective.</S>\n    <S sid=\"160\" ssid=\"97\">For example, editorials contain objective sentences presenting facts supporting the writer’s argument, and reviews contain sentences objectively presenting facts about the product beign reviewed.</S>\n    <S sid=\"161\" ssid=\"98\">Similarly, nonopinion pieces are not 100% objective.</S>\n    <S sid=\"162\" ssid=\"99\">News reports present opinions and reactions to reported events (van Dijk 1988); they often contain segments starting with expressions such as critics claim and supporters argue.</S>\n    <S sid=\"163\" ssid=\"100\">In addition, quoted-speech sentences in which individuals express their subjectivity are often included (Barzilay et al. 2000).</S>\n    <S sid=\"164\" ssid=\"101\">For concreteness, let us consider WSJ-SE, which, recall, has been manually annotated at the sentence level.</S>\n    <S sid=\"165\" ssid=\"102\">In WSJ-SE, 70% of the sentences in opinion pieces are subjective and 30% are objective.</S>\n    <S sid=\"166\" ssid=\"103\">In nonopinion pieces, 44% of the sentences are subjective and only 56% are objective.</S>\n    <S sid=\"167\" ssid=\"104\">Thus, while there is a higher concentration of subjective sentences in opinion versus nonopinion pieces, there are many subjective sentences in nonopinion pieces and objective sentences in opinion pieces.</S>\n    <S sid=\"168\" ssid=\"105\">An inspection of some data reveals that some editorial and review articles are not marked as such by the Wall Street Journal.</S>\n    <S sid=\"169\" ssid=\"106\">For example, there are articles whose purpose is to present an argument rather than cover a news story, but they are not explicitly labeled as editorials by the Wall Street Journal.</S>\n    <S sid=\"170\" ssid=\"107\">Thus, the opinion piece annotations of data sets OP1 and OP2 in Table 1 have been manually refined.</S>\n    <S sid=\"171\" ssid=\"108\">The annotation instructions were simply to identify any additional opinion pieces that were not marked as such.</S>\n    <S sid=\"172\" ssid=\"109\">To test the reliability of this annotation, two judges independently annotated two Wall Street Journal files, W9-22 and W9-33, each containing approximately 160,000 words.</S>\n    <S sid=\"173\" ssid=\"110\">This is an “annotation lite” task: With no training, the annotators achieved kappa values of 0.94 and 0.95, and each spent an average of three hours per Wall Street Journal file.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"174\" ssid=\"1\">The goal in this section is to learn lexical subjectivity clues of various types, single words as well as collocations.</S>\n    <S sid=\"175\" ssid=\"2\">Some require no training data, some are learned using the expression-level subjective-element annotations as training data, and some are learned using the document-level opinion piece annotations as training data (i.e., opinion piece versus nonopinion piece).</S>\n    <S sid=\"176\" ssid=\"3\">All of the clues are evaluated with respect to the document-level opinion piece annotations.</S>\n    <S sid=\"177\" ssid=\"4\">While these evaluations are our focus, because many more opinion piece than subjective-element data exist, we do evaluate the clues learned from the opinion piece data on the subjective-element data as well.</S>\n    <S sid=\"178\" ssid=\"5\">Thus, we cross-validate the results both ways between the two types of annotations.</S>\n    <S sid=\"179\" ssid=\"6\">Throughout this section, we evaluate sets of clues directly, by measuring the proportion of clues that appear in subjective documents or expressions, seeking those that appear more often than expected.</S>\n    <S sid=\"180\" ssid=\"7\">In later sections, the clues are used together to find subjective sentences and to perform text categorization.</S>\n    <S sid=\"181\" ssid=\"8\">The following paragraphs give details of the evaluation and experimental design used in this section.</S>\n    <S sid=\"182\" ssid=\"9\">The proportion of clues in subjective documents or expressions is their precision.</S>\n    <S sid=\"183\" ssid=\"10\">Specifically, the precision of a set S with respect to opinion pieces is number of instances of members of S in opinion pieces total number of instances of members of S in the data The precision of a set S with respect to subjective elements is number of instances of members of S in subjective elements total number of instances of members of S in the data In the above, S is a set of types (not tokens).</S>\n    <S sid=\"184\" ssid=\"11\">The counts are of tokens (i.e., instances or occurrences) of members of S. Why use a set rather than individual items?</S>\n    <S sid=\"185\" ssid=\"12\">Many good clues of subjectivity occur with low frequency (Wiebe, McKeever, and Bruce 1998).</S>\n    <S sid=\"186\" ssid=\"13\">In fact, as we shall see below, uniqueness in the corpus is an informative feature for subjectivity classification.</S>\n    <S sid=\"187\" ssid=\"14\">Thus, we do not want to discard low-frequency clues, because they are a valuable source of information, and we do not want to evaluate individual low-frequency lexical items, because the results would be unreliable.</S>\n    <S sid=\"188\" ssid=\"15\">Our strategy is thus to identify and evaluate sets of words and phrases, rather than individual items.</S>\n    <S sid=\"189\" ssid=\"16\">What kinds of results may we expect?</S>\n    <S sid=\"190\" ssid=\"17\">We cannot expect absolutely high precision with respect to the opinion piece classifications, even for strong clues, for three reasons.</S>\n    <S sid=\"191\" ssid=\"18\">First, for our purposes, the data are noisy.</S>\n    <S sid=\"192\" ssid=\"19\">As mentioned above, while the proportion of subjective sentences is higher in opinion than in nonopinion pieces, the proportions are not 100 and 0: Opinion pieces contain objective sentences, and nonopinion pieces contain subjective sentences.</S>\n    <S sid=\"193\" ssid=\"20\">Second, we are trying to learn lexical items associated with subjectivity, that is, PSEs.</S>\n    <S sid=\"194\" ssid=\"21\">As discussed above, many words and phrases with subjective usages have objective usages as well.</S>\n    <S sid=\"195\" ssid=\"22\">Thus, even in perfect data with no noise, we would not expect 100% precision.</S>\n    <S sid=\"196\" ssid=\"23\">(This is the motivation for the work on density presented in section 4.4.)</S>\n    <S sid=\"197\" ssid=\"24\">Third, the distribution of opinions and nonopinions is highly skewed in favor of nonopinions: Only 9% of the articles in the combination of OP1 and OP2 are opinion pieces.</S>\n    <S sid=\"198\" ssid=\"25\">In this work, increases in precision over a baseline precision are used as evidence that promising sets of PSEs have been found.</S>\n    <S sid=\"199\" ssid=\"26\">Our main baseline for comparison is the number of word instances in opinion pieces, divided by the total number of word instances: Baseline Precision = number of word instances in opinion pieces total number of word instances Frequencies and increases in precision of unique words in subjective-element data.</S>\n    <S sid=\"200\" ssid=\"27\">Baseline frequency is the total number of words, and baseline precision is the proportion of words in subjective elements.</S>\n    <S sid=\"201\" ssid=\"28\">Words and phrases with higher proportions than this appear more than expected in opinion pieces.</S>\n    <S sid=\"202\" ssid=\"29\">To further evaluate the quality of a set of PSEs, we also perform the following significance test.</S>\n    <S sid=\"203\" ssid=\"30\">For a set of PSEs in a given data set, we test the significance of the difference between (1) the proportion of words in opinion pieces that are PSEs and (2) the proportion of words in nonopinion pieces that are PSEs, using the z-significance test for two proportions.</S>\n    <S sid=\"204\" ssid=\"31\">Before we continue, there are a few more technical items to mention concerning the data preparation and experimental design: In this section, we show that low-frequency words are associated with subjectivity in both the subjective-element and opinion piece data.</S>\n    <S sid=\"205\" ssid=\"32\">Apparently, people are creative when they are being opinionated.</S>\n    <S sid=\"206\" ssid=\"33\">Table 2 gives results for unique words in subjective-element data.</S>\n    <S sid=\"207\" ssid=\"34\">Recall that unique words are those that appear just once in the corpus, that is, hapax legomena.</S>\n    <S sid=\"208\" ssid=\"35\">The first row of Table 2 gives the frequency of unique words in WSJ-SE, followed by the percentage-point improvements in precision over baseline for unique words in subjective elements marked by two annotators (denoted as D and M in the table).</S>\n    <S sid=\"209\" ssid=\"36\">The second row gives baseline frequency and precisions.</S>\n    <S sid=\"210\" ssid=\"37\">Baseline frequency is the total number of words in WSJ-SE.</S>\n    <S sid=\"211\" ssid=\"38\">Baseline precision for an annotator is the proportion of words included in subjective elements by that annotator.</S>\n    <S sid=\"212\" ssid=\"39\">Specifically, consider annotator M. The baseline precision of words in subjective elements marked by M is 0.08, Frequencies and increases in precision for words that appear exactly once in the data sets composing OP1.</S>\n    <S sid=\"213\" ssid=\"40\">For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces.</S>\n    <S sid=\"214\" ssid=\"41\">W9-04 W9-10 W9-22 W9-33 freq +prec freq +prec freq +prec freq +prec Unique words 4,794 +.15 4,763 +.16 4,274 +.11 4,567 +.11 Baseline 156,421 .19 156,334 .18 155,135 .13 153,634 .14 but the precision of unique words in these same annotations is 0.20, 0.12 points higher than the baseline.</S>\n    <S sid=\"215\" ssid=\"42\">This is a 150% improvement over the baseline.</S>\n    <S sid=\"216\" ssid=\"43\">The number of unique words in opinion pieces is also higher than expected.</S>\n    <S sid=\"217\" ssid=\"44\">Table 3 compares the precision of the set of unique words to the baseline precision (i.e., the precision of the set of all words that appear in the corpus) in the four WSJ files composing OP1.</S>\n    <S sid=\"218\" ssid=\"45\">Before this analysis was performed, numbers were removed from the data (we are not interested in the fact that, say, the number 163,213.01 appears just once in the corpus).</S>\n    <S sid=\"219\" ssid=\"46\">The number of words in each data set and baseline precisions are listed at the bottom of the table.</S>\n    <S sid=\"220\" ssid=\"47\">The freq columns give total frequencies.</S>\n    <S sid=\"221\" ssid=\"48\">The +prec columns show the percentage-point improvements in precision over baseline.</S>\n    <S sid=\"222\" ssid=\"49\">For example, in W9-10, unique words have precision 0.34: 0.18 baseline plus an improvement over baseline of 0.16.</S>\n    <S sid=\"223\" ssid=\"50\">The difference in the proportion of words that are unique in opinion pieces and the proportion of words that are unique in nonopinion pieces is highly significant, with p &lt; 0.001 (z ≥ 22) for all of the data sets.</S>\n    <S sid=\"224\" ssid=\"51\">Note that not only does the set of unique words have higher than baseline precision, the set is a frequent feature.</S>\n    <S sid=\"225\" ssid=\"52\">The question arises, how does corpus size affect the precision of the set of unique words?</S>\n    <S sid=\"226\" ssid=\"53\">Presumably, uniqueness in a larger corpus is more meaningful than uniqueness in a smaller one.</S>\n    <S sid=\"227\" ssid=\"54\">The results in Figure 1 provide evidence that it is.</S>\n    <S sid=\"228\" ssid=\"55\">The y-axis in Figure 1 represents increase in precision over baseline and the x-axis represents corpus size.</S>\n    <S sid=\"229\" ssid=\"56\">Five graphs are plotted, one for the set of words that appear exactly once (uniques), one for the set of words that appear exactly twice ( freq2), one for the set of words that appear exactly three times ( freq3), etc.</S>\n    <S sid=\"230\" ssid=\"57\">In Figure 1, increases in precision are given for corpora of size n, where n = 20,40,..., 2420, 2440 documents.</S>\n    <S sid=\"231\" ssid=\"58\">Each data point is an average over 25 sample corpora of size n. The sample corpora were chosen from the concatenation of OP1 and OP2, in which 9% of the documents are opinion pieces.</S>\n    <S sid=\"232\" ssid=\"59\">The sample corpora were created by randomly selecting documents from the large corpus, preserving the 9% distribution of opinion pieces.</S>\n    <S sid=\"233\" ssid=\"60\">At the smallest corpus size (containing 20 documents), the average number of words is 9,617.</S>\n    <S sid=\"234\" ssid=\"61\">At the largest corpus size (containing 2440 documents), the average is 1,225,186 words.</S>\n    <S sid=\"235\" ssid=\"62\">As can be seen in the figure, the precision of unique and other low-frequency words increases with corpus size, with increases tapering off at the largest corpus size tested.</S>\n    <S sid=\"236\" ssid=\"63\">Words with frequency 2 also realize a nice increase, although one that is not as dramatic, in precision over baseline.</S>\n    <S sid=\"237\" ssid=\"64\">Even words of frequency 3, 4, and 5 show modest increases.</S>\n    <S sid=\"238\" ssid=\"65\">To help us understand the importance of low-frequency words in large as opposed to small data sets, we can consider the following analogy.</S>\n    <S sid=\"239\" ssid=\"66\">With collectible trading cards, rare cards are the most valuable.</S>\n    <S sid=\"240\" ssid=\"67\">However, if we have some cards and are trying to determine thier value, looking in only a few packs of cards will not tell us if any of our cards are valuable.</S>\n    <S sid=\"241\" ssid=\"68\">Only by looking at many packs of cards can we make a determination as to which are the rare ones.</S>\n    <S sid=\"242\" ssid=\"69\">Only in samples of sufficient size is uniqueness informative.</S>\n    <S sid=\"243\" ssid=\"70\">The results in this section suggest that an NLP system using uniqueness features to recognize subjectivity should determine uniqueness with respect to the test data augmented with an additional store of (unannotated) data.</S>\n    <S sid=\"244\" ssid=\"71\">In this section, we describe experiments in identifying potentially subjective collocations.</S>\n    <S sid=\"245\" ssid=\"72\">Collocations are selected from the subjective-element data (i.e., NG-SE, NG-FE, and WSJ-SE), using the union of the annotators’ tags for the data sets tagged by multiple taggers.</S>\n    <S sid=\"246\" ssid=\"73\">The results are then evaluated on opinion piece data.</S>\n    <S sid=\"247\" ssid=\"74\">The selection procedure is as follows.</S>\n    <S sid=\"248\" ssid=\"75\">First, all 1-grams, 2-grams, 3-grams, and 4-grams are extracted from the data.</S>\n    <S sid=\"249\" ssid=\"76\">In this work, each constituent of an n-gram is a word-stem, part-of-speech pair.</S>\n    <S sid=\"250\" ssid=\"77\">For example, (in-prep the-det can-noun) is a 3-gram that matches trigrams consisting of preposition in, followed by determiner the, and ending with noun can.</S>\n    <S sid=\"251\" ssid=\"78\">A subset of the n-grams are then selected based on precision.</S>\n    <S sid=\"252\" ssid=\"79\">The precision of an n-gram is the number of subjective instances of that n-gram in the data divided by the total number of instances of that n-gram in the data.</S>\n    <S sid=\"253\" ssid=\"80\">An instance of an n-gram is subjective if each word occurs in a subjective element in the data. n-grams are selected based on two criteria.</S>\n    <S sid=\"254\" ssid=\"81\">First, the precision of the n-gram must be greater than the baseline precision (i.e., the proportion of all word instances that Wiebe, Wilson, Bruce, Bell, and Martin Learning Subjective Language are in subjective elements).</S>\n    <S sid=\"255\" ssid=\"82\">Second, the precision of the n-gram must be greater than the maximum precision of its constituents.</S>\n    <S sid=\"256\" ssid=\"83\">This criterion is used to avoid selecting unnecessarily long collocations.</S>\n    <S sid=\"257\" ssid=\"84\">For example, scumbag is a strongly subjective clue.</S>\n    <S sid=\"258\" ssid=\"85\">If be a scumbag does not have higher precision than scumbag alone, we do not want to select it.</S>\n    <S sid=\"259\" ssid=\"86\">Specifically, let (W1, W2) be a bigram consisting of consecutive words W1 and W2.</S>\n    <S sid=\"260\" ssid=\"87\">(W1,W2) is identified as a potential subjective element if prec(W1,W2) ≥ 0.1 and: For trigrams, we extend the second condition as follows.</S>\n    <S sid=\"261\" ssid=\"88\">Let (W1, W2, W3) be a trigram consisting of consecutive words W1, W2, and W3.</S>\n    <S sid=\"262\" ssid=\"89\">The condition is then The selection of 4-grams is similar to the selection of 3-grams, comparing the 4-gram first with the maximum of the precisions of word W1 and trigram (W2, W3, W4) and then with the maximum of the precisions of trigram (W1,W2,W3) and word W4.</S>\n    <S sid=\"263\" ssid=\"90\">We call the n-gram collocations identified as above fixed-n-grams.</S>\n    <S sid=\"264\" ssid=\"91\">We also define a type of collocation called a unique generalized n-gram (ugen-ngram).</S>\n    <S sid=\"265\" ssid=\"92\">Such collocations have placeholders for unique words.</S>\n    <S sid=\"266\" ssid=\"93\">As will be seen below, these are our highest-precision features.</S>\n    <S sid=\"267\" ssid=\"94\">To find and select such generalized collocations, we first find every word that appears just once in the corpus and replace it with a new word, UNIQUE (but remembering the part of speech of the original word).</S>\n    <S sid=\"268\" ssid=\"95\">In essence, we treat the set of single-instance words as a single, frequently occurring word (which occurs with various parts of speech).</S>\n    <S sid=\"269\" ssid=\"96\">Precisely the same method used for extracting and selecting n-grams above is used to obtain the potentially subjective collocations with one or more positions filled by a UNIQUE, part-of-speech pair.</S>\n    <S sid=\"270\" ssid=\"97\">To test the ugen-n-grams extracted from the subjective-element training data using the method outlined above, we assess their precision with respect to opinion piece data.</S>\n    <S sid=\"271\" ssid=\"98\">As with the training data, all unique words in the test data are replaced by UNIQUE.</S>\n    <S sid=\"272\" ssid=\"99\">When a ugen-n-gram is matched against the test data, the UNIQUE fillers match words (of the appropriate parts of speech) that are unique in the test data.</S>\n    <S sid=\"273\" ssid=\"100\">Table 4 shows the results of testing the fixed-n-gram and the ugen-n-gram patterns identified as described above on the four data sets composing OP1.</S>\n    <S sid=\"274\" ssid=\"101\">The freq columns give total frequencies, and the +prec columns show the improvements in precision from the baseline.</S>\n    <S sid=\"275\" ssid=\"102\">The number of words in each data set and baseline precisions are given at the bottom of the table.</S>\n    <S sid=\"276\" ssid=\"103\">For all n-gram features besides the fixed-4-grams and ugen-4-grams, the proportion of features in opinion pieces is significantly greater than the proportion of features in nonopinion pieces.3 The question arises, how much overlap is there between instances of fixed-n-grams and instances of ugen-n-grams?</S>\n    <S sid=\"277\" ssid=\"104\">In the test data of Table 4, there are a total of 8,577 fixed-n-grams instances.</S>\n    <S sid=\"278\" ssid=\"105\">Only 59 of these, fewer than 1% are contained (wholly or in part) in ugen-n-gram instances.</S>\n    <S sid=\"279\" ssid=\"106\">This small intersection set shows that two different types of potentially subjective collocations are being recognized.</S>\n    <S sid=\"280\" ssid=\"107\">Frequencies and increases in precision of fixed-n-gram and ugen-n-gram collocations learned from the subjective-element data.</S>\n    <S sid=\"281\" ssid=\"108\">For each data set, baseline frequency is the total number of words, and baseline precision is the proportion of words in opinion pieces.</S>\n    <S sid=\"282\" ssid=\"109\">Randomly selected examples of our learned collocations that appear in the test data are given in Tables 5 and 6.</S>\n    <S sid=\"283\" ssid=\"110\">It is interesting to note that the unique generalized collocations were learned from the training data by their matching different unique words from the ones they match in the test data.</S>\n    <S sid=\"284\" ssid=\"111\">In this section, we identify adjective and verb PSEs using distributional similarity.</S>\n    <S sid=\"285\" ssid=\"112\">Opinion-piece data are used for training, and (a different set of) opinion-piece data and the subjective-element data are used for testing.</S>\n    <S sid=\"286\" ssid=\"113\">With distributional similarity, words are judged to be more or less similar based on their distributional patterning in text (Lee 1999; Lee and Pereira 1999).</S>\n    <S sid=\"287\" ssid=\"114\">Our trainingPrec(s) is the precision of s in the training data validationPrec(s) is the precision of s in the validation data testPrec(s) is the precision of s in the test data (similarly for trainingFreq, validationFreq, and testFreq) S = the set of all adjectives (verbs) in the training data for T in [0.01,0.04,...,0.70]: for n in [2,3,...,40]: for n in [2,3,...,40]: if validationPrec(RT,n) &gt; 0.28 (0.23 for verbs) Algorithm for selecting adjective and verb features using distributional similarity. motivation for experimenting with it to identify PSEs was twofold.</S>\n    <S sid=\"288\" ssid=\"115\">First, we hypothesized that words might be distributionally similar because they share pragmatic usages, such as expressing subjectivity, even if they are not close synonyms.</S>\n    <S sid=\"289\" ssid=\"116\">Second, as shown above, low-frequency words appear more often in subjective texts than expected.</S>\n    <S sid=\"290\" ssid=\"117\">We did not want to discard all low-frequency words from consideration but cannot effectively judge the suitability of individual words.</S>\n    <S sid=\"291\" ssid=\"118\">Thus, to decide whether to retain a word as a PSE, we consider the precision not of the individual word, but of the word together with a cluster of words similar to it.</S>\n    <S sid=\"292\" ssid=\"119\">Many variants of distributional similarity have been used in NLP (Lee 1999; Lee and Pereira 1999).</S>\n    <S sid=\"293\" ssid=\"120\">Dekang Lin’s (1998) method is used here.</S>\n    <S sid=\"294\" ssid=\"121\">In contrast to many implementations, which focus exclusively on verb-noun relationships, Lin’s method incorporates a variety of syntactic relations.</S>\n    <S sid=\"295\" ssid=\"122\">This is important for subjectivity recognition, because PSEs are not limited to verb-noun relationships.</S>\n    <S sid=\"296\" ssid=\"123\">In addition, Lin’s results are freely available.</S>\n    <S sid=\"297\" ssid=\"124\">A set of seed words begins the process.</S>\n    <S sid=\"298\" ssid=\"125\">For each seed si, the precision of the set {si}UCi,n in the training data is calculated, where Ci,n is the set of n words most similar to si, according to Lin’s (1998) method.</S>\n    <S sid=\"299\" ssid=\"126\">If the precision of {si} U Ci,n is greater than a threshold T, then the words in this set are retained as PSEs.</S>\n    <S sid=\"300\" ssid=\"127\">If it is not, neither si nor the words in Ci,n are retained.</S>\n    <S sid=\"301\" ssid=\"128\">The union of the retained sets will be denoted RT,n, that is, the union of all sets {si} U Ci,n with precision on the training set &gt; T. In Wiebe (2000), the seeds (the sis) were extracted from the subjective-element annotations in corpus WSJ-SE.</S>\n    <S sid=\"302\" ssid=\"129\">Specifically, the seeds were the adjectives that appear at least once in a subjective element in WSJ-SE.</S>\n    <S sid=\"303\" ssid=\"130\">In this article, the opinion piece corpus is used to move beyond the manual annotations and small corpus of the earlier work, and a much looser criterion is used to choose the initial seeds: All of the adjectives (verbs) in the training data are used.</S>\n    <S sid=\"304\" ssid=\"131\">The algorithm for the process is given in Figure 2.</S>\n    <S sid=\"305\" ssid=\"132\">There is one small difference for adjectives and verbs noted in the figure, that is, the precision threshold of 0.28 for adjectives versus 0.23 for verbs.</S>\n    <S sid=\"306\" ssid=\"133\">These thresholds were determined using validation data.</S>\n    <S sid=\"307\" ssid=\"134\">Seeds and their clusters are assessed on a training set for many parameter settings (cluster size n from 2 through 40, and precision threshold T from 0.01 through 0.70 by .03).</S>\n    <S sid=\"308\" ssid=\"135\">As mentioned above, each (n, T) parameter pair yields a set of adjectives RT,n, that is, the union of all sets {si} U Ci,n with precision on the training set &gt; T. A subset, ADJpses, of those sets is chosen based on precision and frequency in a validation set.</S>\n    <S sid=\"309\" ssid=\"136\">Finally, the ADJpses are tested on the test set.</S>\n    <S sid=\"310\" ssid=\"137\">Table 7 shows the results for four opinion piece test sets.</S>\n    <S sid=\"311\" ssid=\"138\">Multiple trainingvalidation data set pairs are used for each test set, as given in Table 7.</S>\n    <S sid=\"312\" ssid=\"139\">The results are for the union of the adjectives (verbs) chosen for each pair.</S>\n    <S sid=\"313\" ssid=\"140\">The freq columns give total frequencies, and the +prec columns show the improvements in precision from the baseline.</S>\n    <S sid=\"314\" ssid=\"141\">For each data set, the difference between the proportion of instances of ADJpses in opinion pieces and the proportion in nonopinion pieces is significant (p &lt; 0.001, z &gt; 9.2).</S>\n    <S sid=\"315\" ssid=\"142\">The same is true for VERBpses (p &lt; 0.001, z &gt; 4.1).</S>\n    <S sid=\"316\" ssid=\"143\">In the interests of testing consistency, Table 8 shows the results of assessing the adjective and verb features generated from opinion piece data (ADJpses and VERBpses in Table 7) on the subjective-element data.</S>\n    <S sid=\"317\" ssid=\"144\">The left side of the table gives baseline figures for each set of subjective-element annotations.</S>\n    <S sid=\"318\" ssid=\"145\">The right side of the table gives the average frequencies and increases in precision over baseline for the ADJpses and VERBpses sets on the subjective-element data.</S>\n    <S sid=\"319\" ssid=\"146\">The baseline figures in the table are the frequencies and precisions of the sets of adjectives and verbs that appear at least once in a subjective element.</S>\n    <S sid=\"320\" ssid=\"147\">Since these sets include words that appear just once in the corpus (and thus have 100% precision), the baseline precision is a challenging one.</S>\n    <S sid=\"321\" ssid=\"148\">Testing the VERBpses and ADJpses on the subjective-element data reveals some interesting consistencies for these subjectivity clues.</S>\n    <S sid=\"322\" ssid=\"149\">The precision increases of the VERBpses on the subjective-element data are comparable to their increases on the opinion piece data.</S>\n    <S sid=\"323\" ssid=\"150\">Similarly, the precision increases of the ADJpses on the subjective-element data are as good as or better than the performance of this set of PSEs on the opinion piece data.</S>\n    <S sid=\"324\" ssid=\"151\">Finally, the precisions increases for the ADJpses are higher than for the VERBpses on all data sets.</S>\n    <S sid=\"325\" ssid=\"152\">This is again consistent with the higher performance of the ADJpses sets in the opinion piece data sets.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"326\" ssid=\"1\">In this section, we examine the various types of clues used together.</S>\n    <S sid=\"327\" ssid=\"2\">In preparation for this work, all instances in OP1 and OP2 of all of the PSEs identified as described in Section 3 have been automatically identified.</S>\n    <S sid=\"328\" ssid=\"3\">All training to define the PSE instances in OP1 was performed on data separate from OP1, and all training to define the PSE instances in OP2 was performed on data separate from OP2.</S>\n    <S sid=\"329\" ssid=\"4\">Table 9 summarizes the results from previous sections in which the opinion piece data are used for testing.</S>\n    <S sid=\"330\" ssid=\"5\">The performance of the various features is consistently good or bad on the same data sets: the performance is better for all features on W9-10 and W9-04 than on W9-22 and W9-33 (except for the ugen-4-grams, which occur with very low frequency, and the verbs, which have low frequency in W9-10).</S>\n    <S sid=\"331\" ssid=\"6\">This is so despite the fact that the features were generated using different procedures and data: The Algorithm for calculating density in subjective-element data. adjectives and verbs were generated from WSJ document-level opinion piece classifications; the n-gram features were generated from newsgroup and WSJ expression-level subjective-element classifications; and the unique unigram feature requires no training.</S>\n    <S sid=\"332\" ssid=\"7\">This consistency in performance suggests that the results are not brittle.</S>\n    <S sid=\"333\" ssid=\"8\">In Wiebe (1994), whether a PSE is interpreted to be subjective depends, in part, on how subjective the surrounding context is.</S>\n    <S sid=\"334\" ssid=\"9\">We explore this idea in the current work, assessing whether PSEs are more likely to be subjective if they are surrounded by subjective elements.</S>\n    <S sid=\"335\" ssid=\"10\">In particular, we experiment with a density feature to decide whether or not a PSE instance is subjective: If a sufficient number of subjective elements are nearby, then the PSE instance is considered to be subjective; otherwise, it is discarded.</S>\n    <S sid=\"336\" ssid=\"11\">The density parameters are a window size W and a frequency threshold T. In this section, we explore the density of manually annotated PSEs in subjectiveelement data and choose density parameters to use in Section 4.4, in which we apply them to automatically identified PSEs in opinion piece data.</S>\n    <S sid=\"337\" ssid=\"12\">The process for calculating density in the subjective-element data is given in Figure 3.</S>\n    <S sid=\"338\" ssid=\"13\">The PSEs are defined to be all adjectives, verbs, modals, nouns, and adverbs that appear at least once in a subjective element, with the exception of some stop words (line 0 of Figure 3).</S>\n    <S sid=\"339\" ssid=\"14\">Note that these PSEs depend only on the subjective-element manual annotations, not on the automatically identified features used elsewhere in the article or on the document-level opinion piece classes.</S>\n    <S sid=\"340\" ssid=\"15\">PSEinsts is the set of PSE instances to be disambiguated (line 1).</S>\n    <S sid=\"341\" ssid=\"16\">HiDensity (initialized on line 2) will be the subset of PSEinsts that are retained.</S>\n    <S sid=\"342\" ssid=\"17\">In the loop, the density of each PSE instance P is calculated.</S>\n    <S sid=\"343\" ssid=\"18\">This is the number of subjective elements that begin or end in the W words preceding or following P (line 6).</S>\n    <S sid=\"344\" ssid=\"19\">P is retained if its density is at least T (line 7).</S>\n    <S sid=\"345\" ssid=\"20\">Lines 8–9 of the algorithm assess the precision of the original (PSEinsts) and new (HiDensity) sets of PSE instances.</S>\n    <S sid=\"346\" ssid=\"21\">If prec(HiDensity) is greater than prec(PSEinsts), then there is evidence that the number of subjective elements near a PSE instance is related to its subjectivity in context.</S>\n    <S sid=\"347\" ssid=\"22\">To create more data points for this analysis, WSJ-SE was split into two (WSJ-SE1 and WSJ-SE2) and annotations of the two judges are considered separately.</S>\n    <S sid=\"348\" ssid=\"23\">WSJ-SE2-D, for example, refers to D’s annotations of WSJ-SE2.</S>\n    <S sid=\"349\" ssid=\"24\">The process in Figure 3 was repeated for different parameter settings (T in [1, 2,4,..., 48] and W in [1, 10, 20, ... , 490]) on each of the SE data sets.</S>\n    <S sid=\"350\" ssid=\"25\">To find good parameter settings, the results for each data set were sorted into five-point precision intervals and then sorted by frequency within each interval.</S>\n    <S sid=\"351\" ssid=\"26\">Information for the top three precision intervals for each data set are shown in Table 10, specifically, the parameter values (i.e., T and W) and the frequency and precision of the most frequent result in each interval.</S>\n    <S sid=\"352\" ssid=\"27\">The intervals are in the rows labeled Range.</S>\n    <S sid=\"353\" ssid=\"28\">For example, the top three precision intervals for WSJ-SE1-M, 0.87-0.92, 0.82-0.87, and 0.77-0.82 (no parameter values yield higher precision than 0.92).</S>\n    <S sid=\"354\" ssid=\"29\">The top of Table 10 gives baseline frequencies and precisions, which are IPSEinstsl and prec(PSEinsts), respectively, in line 8 of Figure 3.</S>\n    <S sid=\"355\" ssid=\"30\">The parameter values exhibit a range of frequencies and precisions, with the expected trade-off between precision and frequency.</S>\n    <S sid=\"356\" ssid=\"31\">We choose the following parameters to test in Section 4.4: For each data set, for each precision interval whose lower bound is at least 10 percentage points higher than the baseline for that data set, the top two (T, W) pairs yielding the highest frequencies in that interval are chosen.</S>\n    <S sid=\"357\" ssid=\"32\">Among the five data sets, a total of 45 parameter pairs were so selected.</S>\n    <S sid=\"358\" ssid=\"33\">This exercise was completed once, without experimenting with different parameter settings.</S>\n    <S sid=\"359\" ssid=\"34\">In this section, density is exploited to find subjective instances of automatically identified PSEs.</S>\n    <S sid=\"360\" ssid=\"35\">The process is shown in Figure 4.</S>\n    <S sid=\"361\" ssid=\"36\">There are only two differences between the algorithms in Figures 3 and 4.</S>\n    <S sid=\"362\" ssid=\"37\">First, in Figure 3, density is defined in terms of the number of subjective elements nearby.</S>\n    <S sid=\"363\" ssid=\"38\">However, subjective-element annotations are not available in test data.</S>\n    <S sid=\"364\" ssid=\"39\">Thus in Figure 4, density is defined in terms of the Algorithm for calculating density in opinion piece (OP) data number of other PSE instances nearby, where PSEinsts consists of all instances of the automatically identified PSEs described in Section 3, for which results are given in Table 9.</S>\n    <S sid=\"365\" ssid=\"40\">Second, in Figure 4, we assess precision with respect to the document-level classes (lines 7–8).</S>\n    <S sid=\"366\" ssid=\"41\">The test data are OP1.</S>\n    <S sid=\"367\" ssid=\"42\">An interesting question arose when we were defining the PSE instances: What should be done with words that are identified to be PSEs (or parts of PSEs) according to multiple criteria?</S>\n    <S sid=\"368\" ssid=\"43\">For example, sunny, radiant, and exhilarating are all unique in corpus OP1, and are all members of the adjective PSE feature defined for testing on OP1.</S>\n    <S sid=\"369\" ssid=\"44\">Collocations add additional complexity.</S>\n    <S sid=\"370\" ssid=\"45\">For example, consider the sequence and splendidly, which appears in the test data.</S>\n    <S sid=\"371\" ssid=\"46\">The sequence and splendidly matches the ugen-2-gram (and-conj U-adj), and the word splendidly is unique.</S>\n    <S sid=\"372\" ssid=\"47\">In addition, a sequence may match more than one n-gram feature.</S>\n    <S sid=\"373\" ssid=\"48\">For example, is it that matches three fixed-n-gram features: is it, is it that, and it that.</S>\n    <S sid=\"374\" ssid=\"49\">In the current experiments, the more PSEs a word matches, the more weight it is given.</S>\n    <S sid=\"375\" ssid=\"50\">The hypothesis behind this treatment is that additional matches represent additional evidence that a PSE instance is subjective.</S>\n    <S sid=\"376\" ssid=\"51\">This hypothesis is realized as follows: Each match of each member of each type of PSE is considered to be a PSE instance.</S>\n    <S sid=\"377\" ssid=\"52\">Thus, among them, there are 11 members in PSEinsts for the five phrases sunny, radiant, exhilarating, and splendidly, and is it that, one for each of the matches mentioned above.</S>\n    <S sid=\"378\" ssid=\"53\">The process in Figure 4 was conducted with the 45 parameter pair values (T and W) chosen from the subjective-element data as described in Section 4.3.</S>\n    <S sid=\"379\" ssid=\"54\">Table 11 shows results for a subset of the 45 parameters, namely, the most frequent parameter pair chosen from the top three precision intervals for each training set.</S>\n    <S sid=\"380\" ssid=\"55\">The bottom of the table gives a baseline frequency and a baseline precision in OP1, defined as |PSEinsts| and prec(PSEinsts), respectively, in line 7 of Figure 4.</S>\n    <S sid=\"381\" ssid=\"56\">The density features result in substantial increases in precision.</S>\n    <S sid=\"382\" ssid=\"57\">Of the 45 parameter pairs, the minimum percentage increase over baseline is 22%.</S>\n    <S sid=\"383\" ssid=\"58\">Fully 24% of the 45 parameter pairs yield increases of 200% or more; 38% yield increases between 100% and 199%, and 38% yield increases between 22% and 99%.</S>\n    <S sid=\"384\" ssid=\"59\">In addition, the increases are significant.</S>\n    <S sid=\"385\" ssid=\"60\">Using the set of high-density PSEs defined by the parameter pair with the least increase over baseline, we tested the difference in the proportion of PSEs in opinion pieces that are high-density and the proportion of PSEs in nonopinion pieces that are high-density.</S>\n    <S sid=\"386\" ssid=\"61\">The difference between these two proportions is highly significant (z = 46.2, p &lt; 0.0001).</S>\n    <S sid=\"387\" ssid=\"62\">Notice that, except for one blip (T, W = 6,10 under WSJ-SE-M), the precisions decrease and the frequencies increase as we go down each column in Table 11.</S>\n    <S sid=\"388\" ssid=\"63\">The same pattern can be observed with all 45 parameter pairs (results not included here because of space considerations).</S>\n    <S sid=\"389\" ssid=\"64\">But the parameter pairs are ordered in Table 11 based on performance in the manually annotated subjective-element data, not based on performance in the test data.</S>\n    <S sid=\"390\" ssid=\"65\">For example, the entry in the first row, first column (T, W = 10, 20) is the parameter pair giving the highest frequency in the top precision interval of WSJ-SE-M (frequency and precision in WSJ-SE-M, using the process of Figure 3).</S>\n    <S sid=\"391\" ssid=\"66\">Thus, the relative precisions and frequencies of the parameter pairs are carried over from the training to the test data.</S>\n    <S sid=\"392\" ssid=\"67\">This is quite a strong result, given that the PSEs in the training data are from manual annotations, while the PSEs in the test data are our automatically identified features.</S>\n    <S sid=\"393\" ssid=\"68\">To assess the subjectivity of sentences with high-density PSEs, we extracted the 133 sentences in corpus OP2 that contain at least one high-density PSE and manually annotated them.</S>\n    <S sid=\"394\" ssid=\"69\">We refer to these sentences as the system-identified sentences.</S>\n    <S sid=\"395\" ssid=\"70\">We chose the density-parameter pair (T, W = 12,30), based on its precision and frequency in OP1.</S>\n    <S sid=\"396\" ssid=\"71\">This parameter setting yields results that have relatively high precision and low frequency.</S>\n    <S sid=\"397\" ssid=\"72\">We chose a low-frequency setting to make the annotation study feasible.</S>\n    <S sid=\"398\" ssid=\"73\">The extracted sentences were independently annotated by two judges.</S>\n    <S sid=\"399\" ssid=\"74\">One is a coauthor of this article (judge 1), and the other has performed subjectivity annotation before, but is not otherwise involved in this research (judge 2).</S>\n    <S sid=\"400\" ssid=\"75\">Sentences were annotated according to the coding instructions of Wiebe, Bruce, and O’Hara (1999) which, recall, are to classify a sentence as subjective if there is a significant expression of subjectivity of either the writer or someone mentioned in the text, in the sentence.</S>\n    <S sid=\"401\" ssid=\"76\">In addition to the subjective and objective classes, a judge can tag a sentence as unsure if he or she is unsure of his or her rating or considers the sentence to be borderline.</S>\n    <S sid=\"402\" ssid=\"77\">An equal number (133) of other sentences were randomly selected from the corpus to serve as controls.</S>\n    <S sid=\"403\" ssid=\"78\">The 133 system-identified sentences and the 133 control sentences were randomly mixed together.</S>\n    <S sid=\"404\" ssid=\"79\">The judges were asked to annotate all 266 sentences, not knowing which were system-identified and which were control.</S>\n    <S sid=\"405\" ssid=\"80\">Each sentence was presented with the sentence that precedes it and the sentence that follows it in the corpus, to provide some context for interpretation.</S>\n    <S sid=\"406\" ssid=\"81\">Table 12 shows examples of the system-identified sentences.</S>\n    <S sid=\"407\" ssid=\"82\">Sentences classified by both judges as objective are marked oo and those classified by both judges as subjective are marked ss.</S>\n    <S sid=\"408\" ssid=\"83\">Bathed in cold sweat, I watched these Dantesque scenes, holding tightly the damp hand of Edek or Waldeck who, like me, were convinced that there was no God.</S>\n    <S sid=\"409\" ssid=\"84\">“The Japanese are amazed that a company like this exists in Japan,” says Kimindo Kusaka, head of the Softnomics Center, a Japanese management-research organization.</S>\n    <S sid=\"410\" ssid=\"85\">And even if drugs were legal, what evidence do you have that the habitual drug user wouldn’t continue to rob and steal to get money for clothes, food or shelter?</S>\n    <S sid=\"411\" ssid=\"86\">The moral cost of legalizing drugs is great, but it is a cost that apparently lies outside the narrow scope of libertarian policy prescriptions.</S>\n    <S sid=\"412\" ssid=\"87\">I doubt that one exists.</S>\n    <S sid=\"413\" ssid=\"88\">They were upset at his committee’s attempt to pacify the program critics by cutting the surtax paid by the more affluent elderly and making up the loss by shifting more of the burden to the elderly poor and by delaying some benefits by a year.</S>\n    <S sid=\"414\" ssid=\"89\">Judge 1 classified 103 of the system-identified sentences as subjective, 16 as objective, and 14 as unsure.</S>\n    <S sid=\"415\" ssid=\"90\">Judge 2 classified 102 of the system-identified sentences as subjective, 27 as objective; and 4 as unsure.</S>\n    <S sid=\"416\" ssid=\"91\">The contingency table is given in Table 13.4 The kappa value using all three classes is 0.60, reflecting the highly skewed distribution in favor of subjective sentences, and the disagreement on the lower-frequency classes (unsure and objective).</S>\n    <S sid=\"417\" ssid=\"92\">Consistent with the findings in Wiebe, Bruce, and O’Hara (1999), the kappa value for agreement on the sentences for which neither judge is unsure is very high: 0.86.</S>\n    <S sid=\"418\" ssid=\"93\">A different breakdown of the sentences is illuminating.</S>\n    <S sid=\"419\" ssid=\"94\">For 98 of the sentences (call them SS), judges 1 and 2 tag the sentence as subjective.</S>\n    <S sid=\"420\" ssid=\"95\">Among the other sentences, 20 appear in a block of contiguous system-identified sentences that includes a member of SS.</S>\n    <S sid=\"421\" ssid=\"96\">For example, in Table 12, (2.a) and (2.c) are in SS and (2.b) is in the same block of subjective sentences as they are.</S>\n    <S sid=\"422\" ssid=\"97\">Similarly, (3.a) is in SS and (3.b) is in the same block.</S>\n    <S sid=\"423\" ssid=\"98\">Among the remaining 15 sentences, 6 are adjacent to subjective sentences that were not identified by our system (so were not annotated by the judges).</S>\n    <S sid=\"424\" ssid=\"99\">All of those sentences contain significant expressions of subjectivity of the writer or someone mentioned in the text, the criterion used in this work for classifying a sentence as subjective.</S>\n    <S sid=\"425\" ssid=\"100\">Samples are shown in Table 14.</S>\n    <S sid=\"426\" ssid=\"101\">Thus, 93% of the sentences identified by the system are subjective or are near subjective sentences.</S>\n    <S sid=\"427\" ssid=\"102\">All the sentences, together with their tags and the sentences adjacent to them, are available on the Web at www.cs.pitt.edu/˜wiebe.</S>\n    <S sid=\"428\" ssid=\"103\">In this section, we assess the usefulness of the PSEs identified in Section 3 and listed in Table 9 by using them to perform document-level classification of opinion pieces.</S>\n    <S sid=\"429\" ssid=\"104\">Opinion-piece classification is a difficult task for two reasons.</S>\n    <S sid=\"430\" ssid=\"105\">First, as discussed in Section 2.1, both opinionated and factual documents tend to be composed of a mixture of subjective and objective language.</S>\n    <S sid=\"431\" ssid=\"106\">Second, the natural distribution of documents in our data is heavily skewed toward nonopinion pieces.</S>\n    <S sid=\"432\" ssid=\"107\">Despite these hurdles, using only our PSEs, we achieve positive results in opinion-piece classification using the basic knearest-neighbor (KNN) algorithm with leave-one-out cross-validation (Mitchell 1997).</S>\n    <S sid=\"433\" ssid=\"108\">Given a document, the basic KNN algorithm classifies the document according to the majority classification of the document’s k closest neighbors.</S>\n    <S sid=\"434\" ssid=\"109\">For our purposes, each document is characterized by one feature, the count of all PSE instances (regardless of type) in the document, normalized by document length in words.</S>\n    <S sid=\"435\" ssid=\"110\">The distance between two documents is simply the absolute value of the difference between the normalized PSE counts for the two documents.</S>\n    <S sid=\"436\" ssid=\"111\">With leave-one-out cross-validation, the set of n documents to be classified is divided into a training set of size n−1 and a validation set of size 1.</S>\n    <S sid=\"437\" ssid=\"112\">The one document in the validation set is then classified according to the majority classification of its k closest-neighbor documents in the training set.</S>\n    <S sid=\"438\" ssid=\"113\">This process is repeated until every document is classified.</S>\n    <S sid=\"439\" ssid=\"114\">Which value to use for k is chosen during a preprocessing phase.</S>\n    <S sid=\"440\" ssid=\"115\">During the preprocessing phase, we run the KNN algorithm with leave-one-out cross-validation on a separate training set, for odd values of k from 1 to 15.</S>\n    <S sid=\"441\" ssid=\"116\">The value of k that results in the best classification during the preprocessing phase is the one used for later KNN classification.</S>\n    <S sid=\"442\" ssid=\"117\">For the classification experiment, the data set OP1 was used in the preprocessing phase to select the value of k, and then classification was performed on the 1,222 documents in OP2.</S>\n    <S sid=\"443\" ssid=\"118\">During training on OP1, k equal to 15 resulted in the best classification.</S>\n    <S sid=\"444\" ssid=\"119\">On the test set, OP2, we achieved a classification accuracy of 0.939; the baseline accuracy for choosing the most frequent class (nonopinion pieces) was 0.915.</S>\n    <S sid=\"445\" ssid=\"120\">Our classification accuracy represents a 28% reduction in error and is significantly better than baseline according to McNemar’s test (Everitt 1997).</S>\n    <S sid=\"446\" ssid=\"121\">The positive results from the opinion piece classification show the usefulness of the various PSE features when used together.</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"447\" ssid=\"1\">There has been much work in other fields, including linguistics, literary theory, psychology, philosophy, and content analysis, involving subjective language.</S>\n    <S sid=\"448\" ssid=\"2\">As mentioned in Section 2, the conceptualization underlying our manual annotations is based on work in literary theory and linguistics, most directly Dole˘zel (1973), Uspensky (1973), Kuroda (1973, 1976), Chatman (1978), Cohn (1978), Fodor (1979), and Banfield (1982).</S>\n    <S sid=\"449\" ssid=\"3\">We also mentioned existing knowledge resources such as affective lexicons (General-Inquirer 2000; Heise 2000) and annotations in more general-purpose lexicons (e.g., the attitude adverb features in Comlex [Macleod, Grishman, and Meyers 1998]).</S>\n    <S sid=\"450\" ssid=\"4\">Such knowledge may be used in future work to complement the work presented in this article, for example, to seed the distributional-similarity process described in Section 3.4.</S>\n    <S sid=\"451\" ssid=\"5\">There is also work in fields such as content analysis and psychology on statistically characterizing texts in terms of word lists manually developed for distinctions related to subjectivity.</S>\n    <S sid=\"452\" ssid=\"6\">For example, Hart (1984) performs counts on a manually developed list of words and rhetorical devices (e.g., “sacred” terms such as freedom) in political speeches to explore potential reasons for public reactions.</S>\n    <S sid=\"453\" ssid=\"7\">Anderson and McMaster (1998) use fixed sets of high-frequency words to assign connotative scores to documents and sections of documents along dimensions such as how pleasant, acrimonious, pious, or confident, the text is.</S>\n    <S sid=\"454\" ssid=\"8\">What distinguishes our work from work on subjectivity in other fields is that we focus on (1) automatically learning knowledge from corpora, (2) automatically performing contextual disambiguation, and (3) using knowledge of subjectivity in NLP applications.</S>\n    <S sid=\"455\" ssid=\"9\">This article expands and integrates the work reported in Wiebe and Wilson (2002), Wiebe, Wilson, and Bell (2001), Wiebe et al. (2001) and Wiebe (2000).</S>\n    <S sid=\"456\" ssid=\"10\">Previous work in NLP on the same or related tasks includes sentence-level and document-level subjectivity classifications.</S>\n    <S sid=\"457\" ssid=\"11\">At the sentence level, Wiebe, Bruce, and O’Hara (1999) developed a machine learning system to classify sentences as subjective or objective.</S>\n    <S sid=\"458\" ssid=\"12\">The accuracy of the system was more than 20 percentage points higher than a baseline accuracy.</S>\n    <S sid=\"459\" ssid=\"13\">Five part-of-speech features, two lexical features, and a paragraph feature were used.</S>\n    <S sid=\"460\" ssid=\"14\">These results suggested to us that there are clues to subjectivity that might be learned automatically from text and motivated the work reported in the current article.</S>\n    <S sid=\"461\" ssid=\"15\">The system was tested in 10-fold cross validation experiments using corpus WSJ-SE, a small corpus of only 1,001 sentences.</S>\n    <S sid=\"462\" ssid=\"16\">As discussed in Section 1, a main goal of our current work is to exploit existing document-level annotations, because they enable us to use much larger data sets, they were created outside our research group, and they allow us to assess consistency of performance by cross-validating between our manual annotations and the existing document-level annotations.</S>\n    <S sid=\"463\" ssid=\"17\">Because the document-level data are not annotated at the sentence level, sentence-level classification is not highlighted in this article.</S>\n    <S sid=\"464\" ssid=\"18\">The new sentence annotation study to evaluate sentences with high-density features (Section 4.5) uses different data from WSJ-SE, because some of the features (n-grams and density parameters) were identified using WSJ-SE as training data.</S>\n    <S sid=\"465\" ssid=\"19\">Other previous work in NLP has addressed related document-level classifications.</S>\n    <S sid=\"466\" ssid=\"20\">Spertus (1997) developed a system for recognizing inflammatory messages.</S>\n    <S sid=\"467\" ssid=\"21\">As mentioned earlier in the article, inflammatory language is a type of subjective language, so the task she addresses is closely related to ours.</S>\n    <S sid=\"468\" ssid=\"22\">She uses machine learning to select among manually developed features.</S>\n    <S sid=\"469\" ssid=\"23\">In contrast, the focus in our work is on automatically identifying features from the data.</S>\n    <S sid=\"470\" ssid=\"24\">A number of projects investigating genre detection include editorials as one of the targeted genres.</S>\n    <S sid=\"471\" ssid=\"25\">For example, in Karlgren and Cutting (1994), editorials are one of fifteen categories, and in Kessler, Nunberg, and Sch¨utze (1997), editorials are one of six.</S>\n    <S sid=\"472\" ssid=\"26\">Given the goal of these works to perform genre detection in general, they use low-level features that are not specific to editorials.</S>\n    <S sid=\"473\" ssid=\"27\">Neither shows significant improvements for editorial recognition.</S>\n    <S sid=\"474\" ssid=\"28\">Argamon, Koppel, and Avneri (1998) address a slightly different task, though it does involve editorials.</S>\n    <S sid=\"475\" ssid=\"29\">Their goal is to distinguish not only, for example, news from editorials, but also these categories in different publications.</S>\n    <S sid=\"476\" ssid=\"30\">Their best results are distinguishing among the news categories of different publications; their lowest results involve editorials.</S>\n    <S sid=\"477\" ssid=\"31\">Because we focus specifically on distinguishing opinion pieces from nonopinion pieces, our results are better than theirs for those categories.</S>\n    <S sid=\"478\" ssid=\"32\">In addition, in contrast to the above studies, the focus of our work is on learning features of subjectivity.</S>\n    <S sid=\"479\" ssid=\"33\">We perform opinion piece recognition in order to assess the usefulness of the various features when used together.</S>\n    <S sid=\"480\" ssid=\"34\">Other previous NLP research has used features similar to ours for other NLP tasks.</S>\n    <S sid=\"481\" ssid=\"35\">Low-frequency words have been used as features in information extraction (Weeber, Vos, and Baayen 2000) and text categorization (Copeck et al. 2000).</S>\n    <S sid=\"482\" ssid=\"36\">A number of researchers have worked on mining collocations from text to extend lexicographic resources for machine translation and word sense disambiguation (e.g., Smajda 1993; Lin 1999; Biber 1993).</S>\n    <S sid=\"483\" ssid=\"37\">In Samuel, Carberry, and Vijay-Shanker’s (1998) work on identifying collocations for dialog-act recognition, a filter similar to ours was used to eliminate redundant n-gram features: n-grams were eliminated if they contained substrings with the same entropy score as or a better entropy score than the n-gram.</S>\n    <S sid=\"484\" ssid=\"38\">While it is common in studies of collocations to omit low-frequency words and expressions from analysis, because they give rise to invalid or unrealistic statistical measures (Church and Hanks, 1990), we are able to identify higher-precision collocations by including placeholders for unique words (i.e., the ugen-n-grams).</S>\n    <S sid=\"485\" ssid=\"39\">We are not aware of other work that uses such collocations as we do.</S>\n    <S sid=\"486\" ssid=\"40\">Features identified using distributional similarity have previously been used for syntactic and semantic disambiguation (Hindle 1990; Dagan, Pereira, and Lee 1994) and to develop lexical resources from corpora (Lin 1998; Riloff and Jones 1999).</S>\n    <S sid=\"487\" ssid=\"41\">We are not aware of other work identifying and using density parameters as described in this article.</S>\n    <S sid=\"488\" ssid=\"42\">Since our experiments, other related work in NLP has been performed.</S>\n    <S sid=\"489\" ssid=\"43\">Some of this work addresses related but different classification tasks.</S>\n    <S sid=\"490\" ssid=\"44\">Three studies classify reviews as positive or negative (Turney 2002; Pang, Lee, and Vaithyanathan 2002; Dave, Lawrence, Pennock 2003).</S>\n    <S sid=\"491\" ssid=\"45\">The input is assumed to be a review, so this task does not include finding subjective documents in the first place.</S>\n    <S sid=\"492\" ssid=\"46\">The first study listed above (Turney 2002) uses a variation of the semantic similarity procedure presented in Wiebe (2000) (Section 3.4).</S>\n    <S sid=\"493\" ssid=\"47\">The third (Dave, Lawrence, and Pennock 2003) uses ngram features identified with a variation of the procedure presented in Wiebe, Wilson, and Bell (2001) (Section 3.3).</S>\n    <S sid=\"494\" ssid=\"48\">Tong (2001) addresses finding sentiment timelines, that is, tracking sentiments over time in multiple documents.</S>\n    <S sid=\"495\" ssid=\"49\">For clues of subjectivity, he uses manually developed lexical rules, rather than automatically learning them from corpora.</S>\n    <S sid=\"496\" ssid=\"50\">Similarly, Gordon et al. (2003) use manually developed grammars to detect some types of subjective language.</S>\n    <S sid=\"497\" ssid=\"51\">Agrawal et al. (2003) partition newsgroup authors into camps based on quotation links.</S>\n    <S sid=\"498\" ssid=\"52\">They do not attempt to recognize subjective language.</S>\n    <S sid=\"499\" ssid=\"53\">The most closely related new work is Riloff, Wiebe, and Wilson (2003), Riloff and Wiebe (2003) and Yu and Hatzivassiloglou (2003).</S>\n    <S sid=\"500\" ssid=\"54\">The first two focus on finding additional types of subjective clues (nouns and extraction patterns identified using extraction pattern bootstrapping).</S>\n    <S sid=\"501\" ssid=\"55\">Yu and Hatzivassiloglou (2003) perform opinion text classification.</S>\n    <S sid=\"502\" ssid=\"56\">They also use existing WSJ document classes for training and testing, but they do not include the entire corpus in their experiments, as we do.</S>\n    <S sid=\"503\" ssid=\"57\">Their opinion piece class consists only of editorials and letters to the editor, and their nonopinion class consists only of business and news.</S>\n    <S sid=\"504\" ssid=\"58\">They report an average F-measure of 96.5%.</S>\n    <S sid=\"505\" ssid=\"59\">Our result of 94% accuracy on document level classification is almost comparable.</S>\n    <S sid=\"506\" ssid=\"60\">They also perform sentence-level classification.</S>\n    <S sid=\"507\" ssid=\"61\">We anticipate that knowledge of subjective language may be usefully exploited in a number of NLP application areas and hope that the work presented in this article will encourage others to experiment with subjective language in their applications.</S>\n    <S sid=\"508\" ssid=\"62\">More generally, there are many types of artificial intelligence systems for which state-ofaffairs types such as beliefs and desires are central, including systems that perform plan recognition for understanding narratives (Dyer 1982; Lehnert et al. 1983), for argument understanding (Alvarado, Dyer, and Flowers 1986), for understanding stories from different perspectives (Carbonell 1979), and for generating language under different pragmatic constraints (Hovy 1987).</S>\n    <S sid=\"509\" ssid=\"63\">Knowledge of linguistic subjectivity could enhance the abilities of such systems to recognize and generate expressions referring to such states of affairs in natural text.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"510\" ssid=\"1\">Knowledge of subjective language promises to be beneficial for many NLP applications including information extraction, question answering, text categorization, and summarization.</S>\n    <S sid=\"511\" ssid=\"2\">This article has presented the results of an empirical study in acquiring knowledge of subjective language from corpora in which a number of feature types were learned and evaluated on different types of data with positive results.</S>\n    <S sid=\"512\" ssid=\"3\">We showed that unique words are subjective more often than expected and that unique words are valuable clues to subjectivity.</S>\n    <S sid=\"513\" ssid=\"4\">We also presented a procedure for automatically identifying potentially subjective collocations, including fixed collocations and collocations with placeholders for unique words.</S>\n    <S sid=\"514\" ssid=\"5\">In addition, we used the results of a method for clustering words according to distributional similarity (Lin 1998) to identify adjectival and verbal clues of subjectivity.</S>\n    <S sid=\"515\" ssid=\"6\">Table 9 summarizes the results of testing all of the above types of PSEs.</S>\n    <S sid=\"516\" ssid=\"7\">All show increased precision in the evaluations.</S>\n    <S sid=\"517\" ssid=\"8\">Together, they show consistency in performance.</S>\n    <S sid=\"518\" ssid=\"9\">In almost all cases they perform better or worse on the same data sets, despite the fact that different kinds of data and procedures are used to learn them.</S>\n    <S sid=\"519\" ssid=\"10\">In addition, PSEs learned using expression-level subjective-element data have precisions higher than baseline on document-level opinion piece data, and vice versa.</S>\n    <S sid=\"520\" ssid=\"11\">Having a large stable of PSEs, it was important to disambiguate whether or not PSE instances are subjective in the contexts in which they appear.</S>\n    <S sid=\"521\" ssid=\"12\">We discovered that the density of other potentially subjective expressions in the surrounding context is important.</S>\n    <S sid=\"522\" ssid=\"13\">If a clue is surrounded by a sufficient number of other clues, then it is more likely to be subjective than if there were not.</S>\n    <S sid=\"523\" ssid=\"14\">Parameter values were selected using training data manually annotated at the expression level for subjective elements and then tested on data annotated at the document level for opinion pieces.</S>\n    <S sid=\"524\" ssid=\"15\">All of the selected parameters led to increases in precision on the test data, and most lead to increases over 100%.</S>\n    <S sid=\"525\" ssid=\"16\">Once again we found consistency between expression-level and document-level annotations.</S>\n    <S sid=\"526\" ssid=\"17\">PSE sets defined by density have high precision in both the subjective-element data and the opinion piece data.</S>\n    <S sid=\"527\" ssid=\"18\">The large differences between training and testing suggest that our results are not brittle.</S>\n    <S sid=\"528\" ssid=\"19\">Using a density feature selected from a training set, sentences containing highdensity PSEs were extracted from a separate test set, and manually annotated by two judges.</S>\n    <S sid=\"529\" ssid=\"20\">Fully 93% of the sentences extracted were found to be subjective or to be near subjective sentences.</S>\n    <S sid=\"530\" ssid=\"21\">Admittedly, the chosen density feature is a high-precision, lowfrequency one.</S>\n    <S sid=\"531\" ssid=\"22\">But since the process is fully automatic, the feature could be applied to more unannotated text to identify regions containing subjective sentences.</S>\n    <S sid=\"532\" ssid=\"23\">In addition, because the precision and frequency of the density features are stable across data sets, lower-precision but higher-frequency options are available.</S>\n    <S sid=\"533\" ssid=\"24\">Finally, the value of the various types of PSEs was demonstrated with the task of opinion piece classification.</S>\n    <S sid=\"534\" ssid=\"25\">Using the k-nearest-neighbor classification algorithm with leave-one-out cross-validation, a classification accuracy of 94% was achieved on a large test set, with a reduction in error of 28% from the baseline.</S>\n    <S sid=\"535\" ssid=\"26\">Future work is required to determine how to exploit density features to improve the performance of text categorization algorithms.</S>\n    <S sid=\"536\" ssid=\"27\">Another area of future work is searching for clues to objectivity, such as the politeness features used by Spertus (1997).</S>\n    <S sid=\"537\" ssid=\"28\">Still another is identifying the type of a subjective expression (e.g., positive or negative evaluative), extending work such as Hatzivassiloglou and McKeown (1997) on classifying lexemes to the classification of instances in context (compare, e.g., “great!” and “oh great.”) In addition, it would be illuminating to apply our system to data annotated with discourse trees (Carlson, Marcu, and Okurowski 2001).</S>\n    <S sid=\"538\" ssid=\"29\">We hypothesize that most objective sentences identified by our system are dominated in the discourse by subjective sentences and that we are moving toward identifying subjective discourse segments.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"8\">\n    <S sid=\"539\" ssid=\"1\">We thank the anonymous reviewers for their helpful and constructive comments.</S>\n    <S sid=\"540\" ssid=\"2\">This research was supported in part by the Office of Naval Research under grants N00014-95-1-0776 and N00014-01-1-0381.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "02bcc48887cb5f2bd67b97faf063fd88",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Learning Subjective Language"
                }
            }
        },
        {
            "index": 3,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "investigate for primarily unsupervised sequence modeling. Prior knowledge is specified declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system’s error trends.",
                    "filename": "Prototype-Driven_Learning_For_Sequence_Models",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/N06-1041/Documents_xml/N06-1041.xml",
                    "title": "Prototype-Driven Learning For Sequence Models"
                },
                "llm": {
                    "descriptions": [
                        "Abstract:\n\nThis paper introduces prototype-driven learning, a semi-supervised approach that specifies prototypical examples for each target label without labeling any complete documents or sentences. The method leverages distributional similarity to link words with their prototypes and encodes these links as features in a log-linear generative model trained on unlabeled data. Tested on three tasks—English and Chinese part-of-speech tagging and classified ad field segmentation—the approach significantly reduces error rates by incorporating prototype features and distributional similarity measures. The results demonstrate that prototype-driven learning can effectively induce sequence models, offering a compact and efficient way to incorporate prior knowledge into model learning. This method reconciles the benefits of distributional methods in unsupervised settings with sequence models' success in supervised contexts, with the potential to minimize the effort in learning models.",
                        "Abstract:\n\nThis paper presents a semi-supervised approach to learning sequence models called prototype-driven learning. Unlike traditional supervised learning which requires extensive labeled data, and unsupervised learning which relies on the model structure, prototype-driven learning uses prototypical examples to define target labels. This approach minimizes the effort involved in learning by leveraging distributional similarity to link words to their prototypes, and encoding these links as features in a log-linear generative model. The method does not require fully labeled documents, but instead uses a list of prototype examples for each label to guide the learning process. Experiments conducted on English and Chinese part-of-speech tagging and classified advertisement field segmentation demonstrate that distributional prototype features can substantially reduce error rates by linking words to similar prototypes and encouraging the tags of words to be consistent with those of their prototypes. The results suggest that prototype-driven learning is an efficient way to incorporate prior knowledge into the model, achieving significant improvements over baseline models and approaching the performance of fully supervised systems with considerably less labeled data.",
                        "**Abstract:**\n\nThis paper introduces prototype-driven learning, a semi-supervised approach for sequence modeling tasks, such as part-of-speech tagging and information extraction from classified ads. Unlike supervised learning, which heavily relies on labeled examples, and unsupervised learning, which depends on model structure, prototype-driven learning specifies prototypical examples for target labels, without necessarily labeling any documents or sentences. This approach offers a compact way to incorporate prior knowledge about the task at hand.\n\nThe authors propose a log-linear generative model that includes distributional prototype features, where distributional similarity links words to similar prototypes, serving as features. The model's performance is enhanced by this mechanism, which reflects the intuition that words similar to a prototype should generally share the same labels. Extensive experiments on English and Chinese part-of-speech tagging and a classified ads information extraction task demonstrate that incorporating prototype features results in substantial error rate reductions across all tasks. For instance, it raises English part-of-speech tagging accuracy from 41.3% to 80.5% with only three prototypes per tag.\n\nThe results suggest that this method of specifying prior knowledge is not only effective but also efficient in learning, achieving comparable or superior performance to more knowledge-intensive systems. Prototype-driven learning reconciles the strengths of distributional methods in unsupervised word clustering with the advantages of sequence models in supervised learning, leveraging distributional similarity to guide sequence model training. This research provides a promising direction for reducing the effort in model training while maintaining or improving performance on language tasks.",
                        "**Abstract:**\n\nThis paper introduces prototype-driven learning, a semi-supervised learning approach for sequence modeling tasks such as part-of-speech (POS) tagging and information extraction. This method specifies prototypical examples for each target label, rather than fully labeling documents or sentences. Prototype lists are argued to be compact and a minimal requirement for specifying a new annotation task. The approach leverages distributional similarity to link words to prototypical examples, integrating these links as features in a log-linear generative model trained on unlabeled data. Experiments conducted on English and Chinese POS tagging and classified advertisement field segmentation demonstrate that distributional prototype features significantly reduce error rates, enhancing the model's ability to predict label sequences that align with the target structure. The results indicate that prototype-driven learning is effective in specifying target labeling schemes and in bridging the gap between distributional methods in unsupervised clustering and sequence models in supervised settings."
                    ],
                    "filename": "prototypedriven_learning_for_seq-write_xml_paper_abstract",
                    "generation_prompt_nickname": "write_xml_paper_abstract",
                    "generation_prompt_text": "Read the following academic paper (in XML format) and create an abstract for it.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Prototype-Driven Learning For Sequence Models</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Learning, broadly taken, involves choosing a good model from a large space of possible models.</S>\n    <S sid=\"8\" ssid=\"2\">In supervised learning, model behavior is primarily determined by labeled examples, whose production requires a certain kind of expertise and, typically, a substantial commitment of resources.</S>\n    <S sid=\"9\" ssid=\"3\">In unsupervised learning, model behavior is largely determined by the structure of the model.</S>\n    <S sid=\"10\" ssid=\"4\">Designing models to exhibit a certain target behavior requires another, rare kind of expertise and effort.</S>\n    <S sid=\"11\" ssid=\"5\">Unsupervised learning, while minimizing the usage of labeled data, does not necessarily minimize total effort.</S>\n    <S sid=\"12\" ssid=\"6\">We therefore consider here how to learn models with the least effort.</S>\n    <S sid=\"13\" ssid=\"7\">In particular, we argue for a certain kind of semi-supervised learning, which we call prototype-driven learning.</S>\n    <S sid=\"14\" ssid=\"8\">In prototype-driven learning, we specify prototypical examples for each target label or label configuration, but do not necessarily label any documents or sentences.</S>\n    <S sid=\"15\" ssid=\"9\">For example, when learning a model for Penn treebank-style part-of-speech tagging in English, we may list the 45 target tags and a few examples of each tag (see figure 4 for a concrete prototype list for this task).</S>\n    <S sid=\"16\" ssid=\"10\">This manner of specifying prior knowledge about the task has several advantages.</S>\n    <S sid=\"17\" ssid=\"11\">First, is it certainly compact (though it remains to be proven that it is effective).</S>\n    <S sid=\"18\" ssid=\"12\">Second, it is more or less the minimum one would have to provide to a human annotator in order to specify a new annotation task and policy (compare, for example, with the list in figure 2, which suggests an entirely different task).</S>\n    <S sid=\"19\" ssid=\"13\">Indeed, prototype lists have been used pedagogically to summarize tagsets to students (Manning and Sch¨utze, 1999).</S>\n    <S sid=\"20\" ssid=\"14\">Finally, natural language does exhibit proform and prototype effects (Radford, 1988), which suggests that learning by analogy to prototypes may be effective for language tasks.</S>\n    <S sid=\"21\" ssid=\"15\">In this paper, we consider three sequence modeling tasks: part-of-speech tagging in English and Chinese and a classified ads information extraction task.</S>\n    <S sid=\"22\" ssid=\"16\">Our general approach is to use distributional similarity to link any given word to similar prototypes.</S>\n    <S sid=\"23\" ssid=\"17\">For example, the word reported may be linked to said, which is in turn a prototype for the part-of-speech VBD.</S>\n    <S sid=\"24\" ssid=\"18\">We then encode these prototype links as features in a log-linear generative model, which is trained to fit unlabeled data (see section 4.1).</S>\n    <S sid=\"25\" ssid=\"19\">Distributional prototype features provide substantial error rate reductions on all three tasks.</S>\n    <S sid=\"26\" ssid=\"20\">For example, on English part-of-speech tagging with three prototypes per tag, adding prototype features to the baseline raises per-position accuracy from 41.3% to 80.5%.</S>\n  </SECTION>\n  <SECTION title=\"2 Tasks and Related Work: Tagging\" number=\"2\">\n    <S sid=\"27\" ssid=\"1\">For our part-of-speech tagging experiments, we used data from the English and Chinese Penn treebanks (Marcus et al., 1994; Ircs, 2002).</S>\n    <S sid=\"28\" ssid=\"2\">Example sentences are shown in figure 1(a) and (b).</S>\n    <S sid=\"29\" ssid=\"3\">A great deal of research has investigated the unsupervised and semisupervised induction of part-of-speech models, especially in English, and there is unfortunately only space to mention some highly related work here.</S>\n    <S sid=\"30\" ssid=\"4\">One approach to unsupervised learning of partof-speech models is to induce HMMs from unlabeled data in a maximum-likelihood framework.</S>\n    <S sid=\"31\" ssid=\"5\">For example, Merialdo (1991) presents experiments learning HMMs using EM.</S>\n    <S sid=\"32\" ssid=\"6\">Merialdo’s results most famously show that re-estimation degrades accuracy unless almost no examples are labeled.</S>\n    <S sid=\"33\" ssid=\"7\">Less famously, his results also demonstrate that reestimation can improve tagging accuracies to some degree in the fully unsupervised case.</S>\n    <S sid=\"34\" ssid=\"8\">One recent and much more successful approach to part-of-speech learning is contrastive estimation, presented in Smith and Eisner (2005).</S>\n    <S sid=\"35\" ssid=\"9\">They utilize task-specific comparison neighborhoods for part-ofspeech tagging to alter their objective function.</S>\n    <S sid=\"36\" ssid=\"10\">Both of these works require specification of the legal tags for each word.</S>\n    <S sid=\"37\" ssid=\"11\">Such dictionaries are large and embody a great deal of lexical knowledge.</S>\n    <S sid=\"38\" ssid=\"12\">A prototype list, in contrast, is extremely compact.</S>\n  </SECTION>\n  <SECTION title=\"3 Tasks and Related Work: Extraction\" number=\"3\">\n    <S sid=\"39\" ssid=\"1\">Grenager et al. (2005) presents an unsupervised approach to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see figure 1(c)).</S>\n    <S sid=\"40\" ssid=\"2\">Labels in this domain tend to be “sticky” in that the correct annotation tends to consist of multi-element fields of the same label.</S>\n    <S sid=\"41\" ssid=\"3\">The overall approach of Grenager et al. (2005) typifies the process involved in fully unsupervised learning on new domain: they first alter the structure of their HMM so that diagonal transitions are preferred, then modify the transition structure to explicitly model boundary tokens, and so on.</S>\n    <S sid=\"42\" ssid=\"4\">Given enough refinement set of the CLASSIFIEDS data.</S>\n    <S sid=\"43\" ssid=\"5\">The BOUNDARY field is not present in the original annotation, but added to model boundaries (see Section 5.3).</S>\n    <S sid=\"44\" ssid=\"6\">The starred tokens are the results of collapsing of basic entities during pre-processing as is done in (Grenager et al., 2005) ments the model learns to segment with a reasonable match to the target structure.</S>\n    <S sid=\"45\" ssid=\"7\">In section 5.3, we discuss an approach to this task which does not require customization of model structure, but rather centers on feature engineering.</S>\n  </SECTION>\n  <SECTION title=\"4 Approach\" number=\"4\">\n    <S sid=\"46\" ssid=\"1\">In the present work, we consider the problem of learning sequence models over text.</S>\n    <S sid=\"47\" ssid=\"2\">For each document x = [xi], we would like to predict a sequence of labels y = [yi], where xi E X and yi E Y.</S>\n    <S sid=\"48\" ssid=\"3\">We construct a generative model, p(x, y|0), where 0 are the model’s parameters, and choose parameters to maximize the log-likelihood of our observed data D: We take our model family to be chain-structured Markov random fields (MRFs), the undirected equivalent of HMMs.</S>\n    <S sid=\"49\" ssid=\"4\">Our joint probability model over (x, y) is given by where φ(c) is a potential over a clique c, taking the form exp {θT f(c)}, and f(c) is the vector of features active over c. In our sequence models, the cliques are over the edges/transitions (yi−1, yi) and nodes/emissions (xi, yi).</S>\n    <S sid=\"50\" ssid=\"5\">See figure 3 for an example from the English POS tagging domain.</S>\n    <S sid=\"51\" ssid=\"6\">Note that the only way an MRF differs from a conditional random field (CRF) (Lafferty et al., 2001) is that the partition function is no longer observation dependent; we are modeling the joint probability of x and y instead of y given x.</S>\n    <S sid=\"52\" ssid=\"7\">As a result, learning an MRF is slightly harder than learning a CRF; we discuss this issue in section 4.4.</S>\n    <S sid=\"53\" ssid=\"8\">We assume prior knowledge about the target structure via a prototype list, which specifies the set of target labels Y and, for each label y E Y, a set of prototypes words, py E Py.</S>\n    <S sid=\"54\" ssid=\"9\">See figures 2 and 4 for examples of prototype lists.1 1Note that this setting differs from the standard semisupervised learning setup, where a small number of fully labeled examples are given and used in conjunction with a larger amount of unlabeled data.</S>\n    <S sid=\"55\" ssid=\"10\">In our prototype-driven approach, we never provide a single fully labeled example sequence.</S>\n    <S sid=\"56\" ssid=\"11\">See section 5.3 for further comparison of this setting to semi-supervised learning.</S>\n    <S sid=\"57\" ssid=\"12\">Broadly, we would like to learn sequence models which both explain the observed data and meet our prior expectations about target structure.</S>\n    <S sid=\"58\" ssid=\"13\">A straightforward way to implement this is to constrain each prototype word to take only its given label(s) at training time.</S>\n    <S sid=\"59\" ssid=\"14\">As we show in section 5, this does not work well in practice because this constraint on the model is very sparse.</S>\n    <S sid=\"60\" ssid=\"15\">In providing a prototype, however, we generally mean something stronger than a constraint on that word.</S>\n    <S sid=\"61\" ssid=\"16\">In particular, we may intend that words which are in some sense similar to a prototype generally be given the same label(s) as that prototype.</S>\n    <S sid=\"62\" ssid=\"17\">In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words (Sch¨utze, 1995; Clark, 2001).</S>\n    <S sid=\"63\" ssid=\"18\">The underlying linguistic idea is that replacing a word with another word of the same syntactic category should preserve syntactic well-formedness (Radford, 1988).</S>\n    <S sid=\"64\" ssid=\"19\">We present more details in section 5, but for now assume that a similarity function over word types is given.</S>\n    <S sid=\"65\" ssid=\"20\">Suppose further that for each non-prototype word type w, we have a subset of prototypes, Sw, which are known to be distributionally similar to w (above some threshold).</S>\n    <S sid=\"66\" ssid=\"21\">We would like our model to relate the tags of w to those of Sw. One approach to enforcing the distributional assumption in a sequence model is by supplementing the training objective (here, data likelihood) with a penalty term that encourages parameters for which each w’s posterior distribution over tags is compatible with it’s prototypes Sw. For example, we might maximize, where t|w is the model’s distribution of tags for word w. The disadvantage of a penalty-based approach is that it is difficult to construct the penalty term in a way which produces exactly the desired behavior.</S>\n    <S sid=\"67\" ssid=\"22\">Instead, we introduce distributional prototypes into the learning process as features in our log-linear model.</S>\n    <S sid=\"68\" ssid=\"23\">Concretely, for each prototype z, we introduce a predicate PROTO = z which becomes active at each w for which z E Sw (see figure 3).</S>\n    <S sid=\"69\" ssid=\"24\">One advantage of this approach is that it allows the strength of the distributional constraint to be calibrated along with any other features; it was also more successful in our experiments.</S>\n    <S sid=\"70\" ssid=\"25\">So far we have ignored the issue of how we learn model parameters θ which maximize L(θ; D).</S>\n    <S sid=\"71\" ssid=\"26\">If our model family were HMMs, we could use the EM algorithm to perform a local search.</S>\n    <S sid=\"72\" ssid=\"27\">Since we have a log-linear formulation, we instead use a gradientbased search.</S>\n    <S sid=\"73\" ssid=\"28\">In particular, we use L-BFGS (Liu and Nocedal, 1989), a standard numerical optimization technique, which requires the ability to evaluate L(θ; D) and its gradient at a given θ.</S>\n    <S sid=\"74\" ssid=\"29\">The density p(x|θ) is easily calculated up to the global constant Z(θ) using the forward-backward algorithm (Rabiner, 1989).</S>\n    <S sid=\"75\" ssid=\"30\">The partition function is given by sumptions about the clique potentials, but can in all cases be bounded by Where K is a suitably chosen large constant.</S>\n    <S sid=\"76\" ssid=\"31\">We can efficiently compute ˆZ`(θ) for fixed ` using a generalization of the forward-backward algorithm to the lattice of all observations x of length ` (see Smith and Eisner (2005) for an exposition).</S>\n    <S sid=\"77\" ssid=\"32\">Similar to supervised maximum entropy problems, the partial derivative of L(θ; D) with respect to each parameter θj (associated with feature fj) is given by a difference in feature expectations: The first expectation is the expected count of the feature under the model’s p(y|x, θ) and is again easily computed with the forward-backward algorithm, just as for CRFs or HMMs.</S>\n    <S sid=\"78\" ssid=\"33\">The second expectation is the expectation of the feature under the model’s joint distribution over all x, y pairs, and is harder to calculate.</S>\n    <S sid=\"79\" ssid=\"34\">Again assuming that sentences beyond a certain length have negligible mass, we calculate the expectation of the feature for each fixed length ` and take a (truncated) weighted sum: For fixed `, we can calculate Ex,y|`,θfj using the lattice of all inputs of length `.</S>\n    <S sid=\"80\" ssid=\"35\">The quantity p(|x |= `) is simply ˆZ`(θ)/ ˆZ(θ).</S>\n    <S sid=\"81\" ssid=\"36\">As regularization, we use a diagonal Gaussian prior with variance σ2 = 0.5, which gave relatively good performance on all tasks.</S>\n  </SECTION>\n  <SECTION title=\"5 Experiments\" number=\"5\">\n    <S sid=\"82\" ssid=\"1\">We experimented with prototype-driven learning in three domains: English and Chinese part-of-speech tagging and classified advertisement field segmentation.</S>\n    <S sid=\"83\" ssid=\"2\">At inference time, we used maximum posterior decoding,2 which we found to be uniformly but slightly superior to Viterbi decoding.</S>\n    <S sid=\"84\" ssid=\"3\">For our English part-of-speech tagging experiments, we used the WSJ portion of the English Penn treebank (Marcus et al., 1994).</S>\n    <S sid=\"85\" ssid=\"4\">We took our data to be either the first 48K tokens (2000 sentences) or 193K tokens (8000 sentences) starting from section 2.</S>\n    <S sid=\"86\" ssid=\"5\">We used a trigram tagger of the model form outlined in section 4.1 with the same set of spelling features reported in Smith and Eisner (2005): exact word type, character suffixes of length up to 3, initial-capital, contains-hyphen, and contains-digit.</S>\n    <S sid=\"87\" ssid=\"6\">Our only edge features were tag trigrams.</S>\n    <S sid=\"88\" ssid=\"7\">With just these features (our baseline BASE) the problem is symmetric in the 45 model labels.</S>\n    <S sid=\"89\" ssid=\"8\">In order to break initial symmetry we initialized our potentials to be near one, with some random noise.</S>\n    <S sid=\"90\" ssid=\"9\">To evaluate in this setting, model labels must be mapped to target labels.</S>\n    <S sid=\"91\" ssid=\"10\">We followed the common approach in the literature, greedily mapping each model label to a target label in order to maximize per-position accuracy on the dataset.</S>\n    <S sid=\"92\" ssid=\"11\">The results of BASE, reported in table 1, depend upon random initialization; averaging over 10 runs gave an average per-position accuracy of 41.3% on the larger training set.</S>\n    <S sid=\"93\" ssid=\"12\">We automatically extracted the prototype list by taking our data and selecting for each annotated label the top three occurring word types which were not given another label more often.</S>\n    <S sid=\"94\" ssid=\"13\">This resulted in 116 prototypes for the 193K token setting.3 For comparison, there are 18,423 word types occurring in this data.</S>\n    <S sid=\"95\" ssid=\"14\">Incorporating the prototype list in the simplest possible way, we fixed prototype occurrences in the data to their respective annotation labels.</S>\n    <S sid=\"96\" ssid=\"15\">In this case, the model is no longer symmetric, and we no longer require random initialization or post-hoc mapping of labels.</S>\n    <S sid=\"97\" ssid=\"16\">Adding prototypes in this way gave an accuracy of 68.8% on all tokens, but only 47.7% on non-prototype occurrences, which is only a marginal improvement over BASE.</S>\n    <S sid=\"98\" ssid=\"17\">It appears as though the prototype information is not spreading to non-prototype words.</S>\n    <S sid=\"99\" ssid=\"18\">In order to remedy this, we incorporated distributional similarity features.</S>\n    <S sid=\"100\" ssid=\"19\">Similar to (Sch¨utze, 1995), we collect for each word type a context vector of the counts of the most frequent 500 words, conjoined with a direction and distance (e.g +1,-2).</S>\n    <S sid=\"101\" ssid=\"20\">We then performed an SVD on the matrix to obtain a reduced rank approximation.</S>\n    <S sid=\"102\" ssid=\"21\">We used the dot product between left singular vectors as a measure of distributional similarity.</S>\n    <S sid=\"103\" ssid=\"22\">For each word w, we find the set of prototype words with similarity exceeding a fixed threshold of 0.35.</S>\n    <S sid=\"104\" ssid=\"23\">For each of these prototypes z, we add a predicate PROTO = z to each occurrence of w. For example, we might add PROTO = said to each token of reported (as in figure 3).4 Each prototype word is also its own prototype (since a word has maximum similarity to itself), so when we lock the prototype to a label, we are also pushing all the words distributionally similar to that prototype towards that label.5 3To be clear: this method of constructing a prototype list required statistics from the labeled data.</S>\n    <S sid=\"105\" ssid=\"24\">However, we believe it to be a fair and necessary approach for several reasons.</S>\n    <S sid=\"106\" ssid=\"25\">First, we wanted our results to be repeatable.</S>\n    <S sid=\"107\" ssid=\"26\">Second, we did not want to overly tune this list, though experiments below suggest that tuning could greatly reduce the error rate.</S>\n    <S sid=\"108\" ssid=\"27\">Finally, it allowed us to run on Chinese, where the authors have no expertise.</S>\n    <S sid=\"109\" ssid=\"28\">4Details of distributional similarity features: To extract context vectors, we used a window of size 2 in either direction and use the first 250 singular vectors.</S>\n    <S sid=\"110\" ssid=\"29\">We collected counts from all the WSJ portion of the Penn Treebank as well as the entire BLIPP corpus.</S>\n    <S sid=\"111\" ssid=\"30\">We limited each word to have similarity features for its top 5 most similar prototypes.</S>\n    <S sid=\"112\" ssid=\"31\">5Note that the presence of a prototype feature does not ensure every instance of that word type will be given its prototype’s label; pressure from “edge” features or other prototype features can cause occurrences of a word type to be given different labels.</S>\n    <S sid=\"113\" ssid=\"32\">However, rare words with a single prototype feature are almost always given that prototype’s label.</S>\n    <S sid=\"114\" ssid=\"33\">This setting, PROTO+SIM, brings the all-tokens accuracy up to 80.5%, which is a 37.5% error reduction over PROTO.</S>\n    <S sid=\"115\" ssid=\"34\">For non-prototypes, the accuracy increases to 67.8%, an error reduction of 38.4% over PROTO.</S>\n    <S sid=\"116\" ssid=\"35\">The overall error reduction from BASE to PROTO+SIM on all-token accuracy is 66.7%.</S>\n    <S sid=\"117\" ssid=\"36\">Table 5 lists the most common confusions for PROTO+SIM.</S>\n    <S sid=\"118\" ssid=\"37\">The second, third, and fourth most common confusions are characteristic of fully supervised taggers (though greater in number here) and are difficult.</S>\n    <S sid=\"119\" ssid=\"38\">For instance, both JJs and NNs tend to occur after determiners and before nouns.</S>\n    <S sid=\"120\" ssid=\"39\">The CD and DT confusion is a result of our prototype list not containing a contains-digit prototype for CD, so the predicate fails to be linked to CDs.</S>\n    <S sid=\"121\" ssid=\"40\">Of course in a realistic, iterative design setting, we could have altered the prototype list to include a contains-digit prototype for CD and corrected this confusion.</S>\n    <S sid=\"122\" ssid=\"41\">Figure 6 shows the marginal posterior distribution over label pairs (roughly, the bigram transition matrix) according to the treebank labels and the PROTO+SIM model run over the training set (using a collapsed tag set for space).</S>\n    <S sid=\"123\" ssid=\"42\">Note that the broad structure is recovered to a reasonable degree.</S>\n    <S sid=\"124\" ssid=\"43\">It is difficult to compare our results to other systems which utilize a full or partial tagging dictionary, since the amount of provided knowledge is substantially different.</S>\n    <S sid=\"125\" ssid=\"44\">The best comparison is to Smith and Eisner (2005) who use a partial tagging dictionary.</S>\n    <S sid=\"126\" ssid=\"45\">In order to compare with their results, we projected the tagset to the coarser set of 17 that they used in their experiments.</S>\n    <S sid=\"127\" ssid=\"46\">On 24K tokens, our PROTO+SIM model scored 82.2%.</S>\n    <S sid=\"128\" ssid=\"47\">When Smith and Eisner (2005) limit their tagging dictionary to words which occur at least twice, their best performing neighborhood model achieves 79.5%.</S>\n    <S sid=\"129\" ssid=\"48\">While these numbers seem close, for comparison, their tagging dictionary contained information about the allowable tags for 2,125 word types (out of 5,406 types) and the their system must only choose, on average, between 4.4 tags for a word.</S>\n    <S sid=\"130\" ssid=\"49\">Our prototype list, however, contains information about only 116 word types and our tagger must on average choose between 16.9 tags, a much harder task.</S>\n    <S sid=\"131\" ssid=\"50\">When Smith and Eisner (2005) include tagging dictionary entries for all words in the first half of their 24K tokens, giving tagging knowledge for 3,362 word types, they do achieve a higher accuracy of 88.1%.</S>\n    <S sid=\"132\" ssid=\"51\">We also tested our POS induction system on the Chinese POS data in the Chinese Treebank (Ircs, 2002).</S>\n    <S sid=\"133\" ssid=\"52\">The model is wholly unmodified from the English version except that the suffix features are removed since, in Chinese, suffixes are not a reliable indicator of part-of-speech as in English (Tseng et al., 2005).</S>\n    <S sid=\"134\" ssid=\"53\">Since we did not have access to a large auxiliary unlabeled corpus that was segmented, our distributional model was built only from the treebank text, and the distributional similarities are presumably degraded relative to the English.</S>\n    <S sid=\"135\" ssid=\"54\">On 60K word tokens, BASE gave an accuracy of 34.4, PROTO gave 39.0, and PROTO+SIM gave 57.4, similar in order if not magnitude to the English case.</S>\n    <S sid=\"136\" ssid=\"55\">We believe the performance for Chinese POS tagging is not as high as English for two reasons: the general difficulty of Chinese POS tagging (Tseng et al., 2005) and the lack of a larger segmented corpus from which to build distributional models.</S>\n    <S sid=\"137\" ssid=\"56\">Nonetheless, the addition of distributional similarity features does reduce the error rate by 35% from BASE.</S>\n    <S sid=\"138\" ssid=\"57\">We tested our framework on the CLASSIFIEDS data described in Grenager et al. (2005) under conditions similar to POS tagging.</S>\n    <S sid=\"139\" ssid=\"58\">An important characteristic of this domain (see figure 1(a)) is that the hidden labels tend to be “sticky,” in that fields tend to consist of runs of the same label, as in figure 1(c), in contrast with part-of-speech tagging, where we rarely see adjacent tokens given the same label.</S>\n    <S sid=\"140\" ssid=\"59\">Grenager et al. (2005) report that in order to learn this “sticky” structure, they had to alter the structure of their HMM so that a fixed mass is placed on each diagonal transition.</S>\n    <S sid=\"141\" ssid=\"60\">In this work, we learned this structure automatically though prototype similarity features without manually constraining the model (see</S>\n  </SECTION>\n  <SECTION title=\"ROOMATES ROOMATES ROOMATES UTILITIES UTILITIES UTILITIES RESTRICTIONS RESTRICTIONS RESTRICTIONS AVAILABLE AVAILABLE AVAILABLE SIZE SIZE SIZE PHOTOS PHOTOS PHOTOS RENT RENT RENT FEATURES FEATURES FEATURES CONTACT CONTACT CONTACT NEIGHBORHOOD NEIGHBORHOOD NEIGHBORHOOD ADDRESS ADDRESS ADDRESS\" number=\"6\">\n    <S sid=\"142\" ssid=\"1\">figure 8), though we did change the similarity function (see below).</S>\n    <S sid=\"143\" ssid=\"2\">On the test set of (Grenager et al., 2005), BASE scored an accuracy of 46.4%, comparable to Grenager et al. (2005)’s unsupervised HMM baseline.</S>\n    <S sid=\"144\" ssid=\"3\">Adding the prototype list (see figure 2) without distributional features yielded a slightly improved accuracy of 53.7%.</S>\n    <S sid=\"145\" ssid=\"4\">For this domain, we utilized a slightly different notion of distributional similarity: we are not interested in the syntactic behavior of a word type, but its topical content.</S>\n    <S sid=\"146\" ssid=\"5\">Therefore, when we collect context vectors for word types in this domain, we make no distinction by direction or distance and collect counts from a wider window.</S>\n    <S sid=\"147\" ssid=\"6\">This notion of distributional similarity is more similar to latent semantic indexing (Deerwester et al., 1990).</S>\n    <S sid=\"148\" ssid=\"7\">A natural consequence of this definition of distributional similarity is that many neighboring words will share the same prototypes.</S>\n    <S sid=\"149\" ssid=\"8\">Therefore distributional prototype features will encourage labels to persist, naturally giving the “sticky” effect of the domain.</S>\n    <S sid=\"150\" ssid=\"9\">Adding distributional similarity features to our model (PROTO+SIM) improves accuracy substantially, yielding 71.5%, a 38.4% error reduction over BASE.6 Another feature of this domain that Grenager et al. (2005) take advantage of is that end of sentence punctuation tends to indicate the end of a field and the beginning of a new one.</S>\n    <S sid=\"151\" ssid=\"10\">Grenager et al. (2005) experiment with manually adding boundary states and biasing transitions from these states to not self-loop.</S>\n    <S sid=\"152\" ssid=\"11\">We capture this “boundary” effect by simply adding a line to our protoype-list, adding a new BOUNDARY state (see figure 2) with a few (hand-chosen) prototypes.</S>\n    <S sid=\"153\" ssid=\"12\">Since we utilize a trigram tagger, we are able to naturally capture the effect that the BOUNDARY tokens typically indicate transitions between the fields before and after the boundary token.</S>\n    <S sid=\"154\" ssid=\"13\">As a post-processing step, when a token is tagged as a BOUNDARY</S>\n  </SECTION>\n  <SECTION title=\"FEATURES SIZE 11.2 FEATURES NBRHD 9.0 SIZE FEATURES 7.7 NBRHD FEATURES 6.4 ADDRESS NBRHD 5.3 UTILITIES FEATURES 5.3\" number=\"7\">\n    <S sid=\"155\" ssid=\"1\">token it is given the same label as the previous non-BOUNDARY token, which reflects the annotational convention that boundary tokens are given the same label as the field they terminate.</S>\n    <S sid=\"156\" ssid=\"2\">Adding the BOUNDARY label yields significant improvements, as indicated by the PROTO+SIM+BOUND setting in Table 5.3, surpassing the best unsupervised result of Grenager et al. (2005) which is 72.4%.</S>\n    <S sid=\"157\" ssid=\"3\">Furthermore, our PROTO+SIM+BOUND model comes close to the supervised HMM accuracy of 74.4% reported in Grenager et al. (2005).</S>\n    <S sid=\"158\" ssid=\"4\">We also compared our method to the most basic semi-supervised setting, where fully labeled documents are provided along with unlabeled ones.</S>\n    <S sid=\"159\" ssid=\"5\">Roughly 25% of the data had to be labeled in order to achieve an accuracy equal to our PROTO+SIM+BOUND model, suggesting that the use of prior knowledge in the prototype system is particularly efficient.</S>\n    <S sid=\"160\" ssid=\"6\">In table 5.3, we provide the top confusions made by our PROTO+SIM+BOUND model.</S>\n    <S sid=\"161\" ssid=\"7\">As can be seen, many of our confusions involve the FEATURE field, which serves as a general purpose background state, which often differs subtly from other fields such as SIZE.</S>\n    <S sid=\"162\" ssid=\"8\">For instance, the parenthical comment: ( master has walk - in closet with vanity ) is labeled as a SIZE field in the data, but our model proposed it as a FEATURE field.</S>\n    <S sid=\"163\" ssid=\"9\">NEIGHBORHOOD and ADDRESS is another natural confusion resulting from the fact that the two fields share much of the same vocabulary (e.g [ADDRESS 2525 Telegraph Ave.] vs. [NBRHD near Telegraph]).</S>\n    <S sid=\"164\" ssid=\"10\">Acknowledgments We would like to thank the anonymous reviewers for their comments.</S>\n    <S sid=\"165\" ssid=\"11\">This work is supported by a Microsoft / CITRIS grant and by an equipment donation from Intel.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusions\" number=\"8\">\n    <S sid=\"166\" ssid=\"1\">We have shown that distributional prototype features can allow one to specify a target labeling scheme in a compact and declarative way.</S>\n    <S sid=\"167\" ssid=\"2\">These features give substantial error reduction on several induction tasks by allowing one to link words to prototypes according to distributional similarity.</S>\n    <S sid=\"168\" ssid=\"3\">Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in unsupervised word clustering with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "a1826b5a92df07b5ab784a0f3d08b115",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Prototype-Driven Learning For Sequence Models"
                }
            }
        },
        {
            "index": 4,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.",
                    "filename": "Linguistic_Regularities_in_Continuous_Space_Word_Representations",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/N13-1090/Documents_xml/N13-1090.xml",
                    "title": "Linguistic Regularities in Continuous Space Word Representations"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores the syntactic and semantic regularities captured by continuous space word representations learned through a recurrent neural network language model (RNNLM). The authors propose a vector offset method based on cosine distance that effectively identifies relationships between words as constant vector offsets. They demonstrate that RNNLM-generated word vectors exhibit significant syntactic regularity and semantic information, surpassing traditional n-gram models and other existing approaches like Latent Semantic Analysis (LSA) and Hierarchical Log-Bilinear (HLBL) models. The paper introduces a new dataset for measuring syntactic analogy performance, achieving nearly 40% accuracy, as well as evaluates semantic generalization using the SemEval 2012 task, where it outperforms the previous state-of-the-art. The findings suggest that even unsupervised training on large text corpora can result in meaningful word representations that capture linguistic regularities.",
                        "This paper investigates the syntactic and semantic regularities captured in continuous space word representations learned by Recurrent Neural Network Language Models (RNNLMs). It demonstrates that these word vectors not only support language modeling but also encode meaningful linguistic patterns as constant vector offsets between related words. The authors propose a simple and effective vector offset method, based on cosine distance, to solve analogy questions that reflect these regularities. Experiments using this method on newly created datasets for syntactic analogies and SemEval 2012's semantic relations show that RNNLM word vectors outperform other models, including Latent Semantic Analysis (LSA) and Hierarchical Log-Bilinear Model (HLBL) vectors. Remarkably, these results are achieved through unsupervised training on large text corpora without explicit syntactic or semantic objectives, suggesting that the RNNLMs learn to generalize linguistic relationships as a byproduct of maximizing data likelihood.",
                        "This paper explores the syntactic and semantic regularities captured by continuous space word representations learned through recurrent neural network language models (RNNLM). The authors demonstrate that these learned word vectors exhibit reliable vector offsets between words sharing specific linguistic relationships, such as singular/plural forms or semantic connections highlighted in the SemEval 2012 task. A vector offset method using cosine distance effectively resolves analogy questions, reflecting both syntactic and semantic relations. The RNNLM-generated word vectors significantly outperform other models like Latent Semantic Analysis (LSA) in capturing these linguistic regularities. The paper also introduces a new dataset for evaluating syntactic understanding and shows that RNNLM outperforms the state-of-the-art on the SemEval 2012 semantic task. The study concludes that the vector offset method is a powerful tool for identifying linguistic patterns in word representations, with the RNNLM's success attributed to unsupervised learning on a large corpus of text data.",
                        "In this study, the authors explore how neural network language models, specifically recurrent neural networks (RNNs), learn word representations as high-dimensional vectors that exhibit syntactic and semantic regularities. The paper demonstrates that these regularities manifest as constant vector offsets between words sharing specific linguistic relationships. The research introduces a vector offset method based on cosine distance, which effectively solves analogy questions, revealing inherent syntactic and semantic properties of the word vectors. Through extensive testing with syntactic analogy questions and the SemEval-2012 Task 2 for semantic relation measurement, the authors show that RNN-generated vectors capture linguistic regularities more effectively than other models. The RNN approach outperforms the previous state-of-the-art methods in both syntactic and semantic tasks, despite the models being trained on large text datasets without explicit syntactic or semantic objectives. The findings suggest a robust ability of RNN language models to generalize linguistic knowledge through unsupervised learning."
                    ],
                    "filename": "linguistic_regularities_in_conti-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 154 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Linguistic Regularities in Continuous Space Word Representations</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"8\" ssid=\"1\">A defining feature of neural network language models is their representation of words as high dimensional real valued vectors.</S>\n    <S sid=\"9\" ssid=\"2\">In these models (Bengio et al., 2003; Schwenk, 2007; Mikolov et al., 2010), words are converted via a learned lookuptable into real valued vectors which are used as the inputs to a neural network.</S>\n    <S sid=\"10\" ssid=\"3\">As pointed out by the original proposers, one of the main advantages of these models is that the distributed representation achieves a level of generalization that is not possible with classical n-gram language models; whereas a n-gram model works in terms of discrete units that have no inherent relationship to one another, a continuous space model works in terms of word vectors where similar words are likely to have similar vectors.</S>\n    <S sid=\"11\" ssid=\"4\">Thus, when the model parameters are adjusted in response to a particular word or word-sequence, the improvements will carry over to occurrences of similar words and sequences.</S>\n    <S sid=\"12\" ssid=\"5\">By training a neural network language model, one obtains not just the model itself, but also the learned word representations, which may be used for other, potentially unrelated, tasks.</S>\n    <S sid=\"13\" ssid=\"6\">This has been used to good effect, for example in (Collobert and Weston, 2008; Turian et al., 2010) where induced word representations are used with sophisticated classifiers to improve performance in many NLP tasks.</S>\n    <S sid=\"14\" ssid=\"7\">In this work, we find that the learned word representations in fact capture meaningful syntactic and semantic regularities in a very simple way.</S>\n    <S sid=\"15\" ssid=\"8\">Specifically, the regularities are observed as constant vector offsets between pairs of words sharing a particular relationship.</S>\n    <S sid=\"16\" ssid=\"9\">For example, if we denote the vector for word i as xi, and focus on the singular/plural relation, we observe that x apple−xapples ≈ xcar−xcars, xfamily−xfamilies ≈ xcar−xcars, and so on.</S>\n    <S sid=\"17\" ssid=\"10\">Perhaps more surprisingly, we find that this is also the case for a variety of semantic relations, as measured by the SemEval 2012 task of measuring relation similarity.</S>\n    <S sid=\"18\" ssid=\"11\">Atlanta, Georgia, 9–14 June 2013. c�2013 Association for Computational Linguistics The remainder of this paper is organized as follows.</S>\n    <S sid=\"19\" ssid=\"12\">In Section 2, we discuss related work; Section 3 describes the recurrent neural network language model we used to obtain word vectors; Section 4 discusses the test sets; Section 5 describes our proposed vector offset method; Section 6 summarizes our experiments, and we conclude in Section 7.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"20\" ssid=\"1\">Distributed word representations have a long history, with early proposals including (Hinton, 1986; Pollack, 1990; Elman, 1991; Deerwester et al., 1990).</S>\n    <S sid=\"21\" ssid=\"2\">More recently, neural network language models have been proposed for the classical language modeling task of predicting a probability distribution over the “next” word, given some preceding words.</S>\n    <S sid=\"22\" ssid=\"3\">These models were first studied in the context of feed-forward networks (Bengio et al., 2003; Bengio et al., 2006), and later in the context of recurrent neural network models (Mikolov et al., 2010; Mikolov et al., 2011b).</S>\n    <S sid=\"23\" ssid=\"4\">This early work demonstrated outstanding performance in terms of word-prediction, but also the need for more computationally efficient models.</S>\n    <S sid=\"24\" ssid=\"5\">This has been addressed by subsequent work using hierarchical prediction (Morin and Bengio, 2005; Mnih and Hinton, 2009; Le et al., 2011; Mikolov et al., 2011b; Mikolov et al., 2011a).</S>\n    <S sid=\"25\" ssid=\"6\">Also of note, the use of distributed topic representations has been studied in (Hinton and Salakhutdinov, 2006; Hinton and Salakhutdinov, 2010), and (Bordes et al., 2012) presents a semantically driven method for obtaining word representations.</S>\n  </SECTION>\n  <SECTION title=\"3 Recurrent Neural Network Model\" number=\"3\">\n    <S sid=\"26\" ssid=\"1\">The word representations we study are learned by a recurrent neural network language model (Mikolov et al., 2010), as illustrated in Figure 1.</S>\n    <S sid=\"27\" ssid=\"2\">This architecture consists of an input layer, a hidden layer with recurrent connections, plus the corresponding weight matrices.</S>\n    <S sid=\"28\" ssid=\"3\">The input vector w(t) represents input word at time t encoded using 1-of-N coding, and the output layer y(t) produces a probability distribution over words.</S>\n    <S sid=\"29\" ssid=\"4\">The hidden layer s(t) maintains a representation of the sentence history.</S>\n    <S sid=\"30\" ssid=\"5\">The input vector w(t) and the output vector y(t) have dimensionality of the vocabulary.</S>\n    <S sid=\"31\" ssid=\"6\">The values in the hidden and output layers are computed as follows: where In this framework, the word representations are found in the columns of U, with each column representing a word.</S>\n    <S sid=\"32\" ssid=\"7\">The RNN is trained with backpropagation to maximize the data log-likelihood under the model.</S>\n    <S sid=\"33\" ssid=\"8\">The model itself has no knowledge of syntax or morphology or semantics.</S>\n    <S sid=\"34\" ssid=\"9\">Remarkably, training such a purely lexical model to maximize likelihood will induce word representations with striking syntactic and semantic properties.</S>\n  </SECTION>\n  <SECTION title=\"4 Measuring Linguistic Regularity\" number=\"4\">\n    <S sid=\"35\" ssid=\"1\">To understand better the syntactic regularities which are inherent in the learned representation, we created a test set of analogy questions of the form “a is to b as c is to ” testing base/comparative/superlative forms of adjectives; singular/plural forms of common nouns; possessive/non-possessive forms of common nouns; and base, past and 3rd person present tense forms of verbs.</S>\n    <S sid=\"36\" ssid=\"2\">More precisely, we tagged 267M words of newspaper text with Penn Treebank POS tags (Marcus et al., 1993).</S>\n    <S sid=\"37\" ssid=\"3\">We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB).</S>\n    <S sid=\"38\" ssid=\"4\">We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1.</S>\n    <S sid=\"39\" ssid=\"5\">The total test set size is 8000.</S>\n    <S sid=\"40\" ssid=\"6\">The test set is available online.</S>\n    <S sid=\"41\" ssid=\"7\">1 In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information.</S>\n    <S sid=\"42\" ssid=\"8\">The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing.</S>\n    <S sid=\"43\" ssid=\"9\">Each relation is exemplified by 3 or 4 gold word pairs.</S>\n    <S sid=\"44\" ssid=\"10\">Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds.</S>\n    <S sid=\"45\" ssid=\"11\">This can be viewed as another analogy problem.</S>\n    <S sid=\"46\" ssid=\"12\">For example, take the ClassInclusion:Singular Collective relation with the prototypical word pair clothing:shirt.</S>\n    <S sid=\"47\" ssid=\"13\">To measure the degree that a target word pair dish:bowl has the same relation, we form the analogy “clothing is to shirt as dish is to bowl,” and ask how valid it is.</S>\n  </SECTION>\n  <SECTION title=\"5 The Vector Offset Method\" number=\"5\">\n    <S sid=\"48\" ssid=\"1\">As we have seen, both the syntactic and semantic tasks have been formulated as analogy questions.</S>\n    <S sid=\"49\" ssid=\"2\">We have found that a simple vector offset method based on cosine distance is remarkably effective in solving these questions.</S>\n    <S sid=\"50\" ssid=\"3\">In this method, we assume relationships are present as vector offsets, so that in the embedding space, all pairs of words sharing a particular relation are related by the same constant offset.</S>\n    <S sid=\"51\" ssid=\"4\">This is illustrated in Figure 2.</S>\n    <S sid=\"52\" ssid=\"5\">In this model, to answer the analogy question a:b c:d where d is unknown, we find the embedding vectors xa, xb, xc (all normalized to unit norm), and compute y = xb − xa + xc. y is the continuous space representation of the word we expect to be the best answer.</S>\n    <S sid=\"53\" ssid=\"6\">Of course, no word might exist at that exact position, so we then search for the word whose embedding vector has the greatest cosine similarity to y and output it: provided.</S>\n    <S sid=\"54\" ssid=\"7\">We have explored several related methods and found that the proposed method performs well for both syntactic and semantic relations.</S>\n    <S sid=\"55\" ssid=\"8\">We note that this measure is qualitatively similar to relational similarity model of (Turney, 2012), which predicts similarity between members of the word pairs (xb, xd), (x,-, xd) and dis-similarity for (xa, xd).</S>\n  </SECTION>\n  <SECTION title=\"6 Experimental Results\" number=\"6\">\n    <S sid=\"56\" ssid=\"1\">To evaluate the vector offset method, we used vectors generated by the RNN toolkit of Mikolov (2012).</S>\n    <S sid=\"57\" ssid=\"2\">Vectors of dimensionality 80, 320, and 640 were generated, along with a composite of several systems, with total dimensionality 1600.</S>\n    <S sid=\"58\" ssid=\"3\">The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary.</S>\n    <S sid=\"59\" ssid=\"4\">Table 2 shows results for both RNNLM and LSA vectors on the syntactic task.</S>\n    <S sid=\"60\" ssid=\"5\">LSA was trained on the same data as the RNN.</S>\n    <S sid=\"61\" ssid=\"6\">We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly.</S>\n    <S sid=\"62\" ssid=\"7\">2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those.</S>\n    <S sid=\"63\" ssid=\"8\">This resulted in a 36k word vocabulary, and a test set with 6632 questions.</S>\n    <S sid=\"64\" ssid=\"9\">Turian’s Collobert and Weston based vectors do poorly on this task, whereas the Hierarchical Log-Bilinear Model vectors of (Mnih and Hinton, 2009) do essentially as well as the RNN vectors.</S>\n    <S sid=\"65\" ssid=\"10\">These representations were trained on 37M words of data and this may indicate a greater robustness of the HLBL method.</S>\n    <S sid=\"66\" ssid=\"11\">We conducted similar experiments with the semantic test set.</S>\n    <S sid=\"67\" ssid=\"12\">For each target word pair in a relation category, the model measures its relational similarity to each of the prototypical word pairs, and then uses the average as the final score.</S>\n    <S sid=\"68\" ssid=\"13\">The results are evaluated using the two standard metrics defined in the task, Spearman’s rank correlation coefficient p and MaxDiff accuracy.</S>\n    <S sid=\"69\" ssid=\"14\">In both cases, larger values are better.</S>\n    <S sid=\"70\" ssid=\"15\">To compare to previous systems, we report the average over all 69 relations in the test set.</S>\n    <S sid=\"71\" ssid=\"16\">From Table 4, we see that as with the syntactic regularity study, the RNN-based representations perform best.</S>\n    <S sid=\"72\" ssid=\"17\">In this case, however, Turian’s CW vectors are comparable in performance to the HLBL vectors.</S>\n    <S sid=\"73\" ssid=\"18\">With the RNN vectors, the performance improves as the number of dimensions increases.</S>\n    <S sid=\"74\" ssid=\"19\">Surprisingly, we found that even though the RNN vectors are not trained or tuned specifically for this task, the model achieves better results (RNN-320, RNN640 &amp; RNN-1600) than the previously best performing system, UTD-NB (Rink and Harabagiu, 2012).</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"7\">\n    <S sid=\"75\" ssid=\"1\">We have presented a generally applicable vector offset method for identifying linguistic regularities in continuous space word representations.</S>\n    <S sid=\"76\" ssid=\"2\">We have shown that the word representations learned by a RNNLM do an especially good job in capturing these regularities.</S>\n    <S sid=\"77\" ssid=\"3\">We present a new dataset for measuring syntactic performance, and achieve almost 40% correct.</S>\n    <S sid=\"78\" ssid=\"4\">We also evaluate semantic generalization on the SemEval 2012 task, and outperform the previous state-of-the-art.</S>\n    <S sid=\"79\" ssid=\"5\">Surprisingly, both results are the byproducts of an unsupervised maximum likelihood training criterion that simply operates on a large amount of text data.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "dace8ac16046887958a96d6bd49b1028",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Linguistic Regularities in Continuous Space Word Representations"
                }
            }
        },
        {
            "index": 5,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Over the last few years, two of the main research directions in machine learning of natural language processing have been the study of semi-supervised learning algorithms as a way to train classifiers when the labeled data is scarce, and the study of ways to exploit knowledge and global information in structured learning tasks. In this paper, we suggest a method for incorporating domain knowledge in semi-supervised learning algorithms. Our novel framework unifies can exploit several kinds of specific The experimental results presented in the information extraction domain demonstrate that applying constraints helps the model to generate better feedback during learning, and hence the framework allows for high performance learning with significantly less training data than was possible before on these tasks.",
                    "filename": "Guiding_Semi-Supervision_with_Constraint-Driven_Learning",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P07-1036/Documents_xml/P07-1036.xml",
                    "title": "Guiding Semi-Supervision with Constraint-Driven Learning"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces a novel constraint-driven learning protocol for semi-supervised learning in Natural Language Processing (NLP) systems. The proposed framework unifies unary, dictionary-based, and n-ary constraints to encode structural information and interdependencies among possible labels. The protocol is designed to guide semi-supervised learning by capturing different levels of constraint violation and can be integrated with various learning models, including Hidden Markov Models (HMMs). Experiments on information extraction tasks demonstrate significant reductions in the number of training examples required, and the method notably outperforms traditional semi-supervised approaches. The paper emphasizes the importance of soft constraints over hard constraints and shows that incorporating constraints during both the learning and inference stages leads to improved model performance, particularly when labeled data is limited.",
                        "This paper introduces a novel semi-supervised learning approach that uses constraint-driven learning to guide model training when labeled data is limited. By integrating domain-specific knowledge through unary, dictionary-based, and n-ary constraints, the proposed method effectively captures structural information and interdependencies among labels. The authors present a formalism for constraint-based learning that accounts for different levels of constraint violation. Experiments using Hidden Markov Models (HMMs) on information extraction tasks demonstrate significant reductions in the amount of labeled data required. The framework shows that incorporating constraints not only enhances the quality of \"training\" examples but also improves the overall performance of the semi-supervised learning process, outperforming traditional methods.",
                        "Abstract:\n\nThis paper introduces a novel constraint-based learning protocol to enhance semi-supervised learning in Natural Language Processing (NLP) systems, which typically struggle with acquiring large amounts of labeled data. The proposed framework, which is model-agnostic and can be used with any learning algorithm, incorporates unary, dictionary-based, and n-ary constraints to unify different types of structural information and interdependencies among possible labels. The method allows for different levels of constraint violation, aiming to leverage domain knowledge more effectively. Experiments with Hidden Markov Models (HMMs) on two information extraction tasks demonstrate the advantages of the approach, particularly in reducing the number of labeled examples needed. The results indicate significant improvements in model performance when constraints are used during both the learning and inference stages, with the most substantial gains observed when labeled data is scarce. The framework holds promise for advancing semi-supervised learning by effectively utilizing constraints to guide the learning process.",
                        "This paper introduces a novel semi-supervised learning protocol that employs constraint-driven learning (CODL) to guide the learning process using a variety of constraints. The authors propose a formalism that allows for the unification of unary, dictionary-based, and n-ary constraints, capturing different levels of constraint violation. The proposed framework is flexible and can be applied to any learning model, including Hidden Markov Models (HMMs), and is shown to significantly reduce the number of training examples required for information extraction tasks. Experiments demonstrate that the constraint-driven semi-supervised learning process not only improves the learning model but also outperforms traditional semi-supervised approaches. The paper emphasizes the effectiveness of constraints in semi-supervised learning, particularly when labeled data is scarce."
                    ],
                    "filename": "guiding_semisupervision_with_con-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 121 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Guiding Semi-Supervision with Constraint-Driven Learning</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"4\" ssid=\"1\">Natural Language Processing (NLP) systems typically require large amounts of knowledge to achieve good performance.</S>\n    <S sid=\"5\" ssid=\"2\">Acquiring labeled data is a difficult and expensive task.</S>\n    <S sid=\"6\" ssid=\"3\">Therefore, an increasing attention has been recently given to semi-supervised learning, where large amounts of unlabeled data are used to improve the models learned from a small training set (Collins and Singer, 1999; Thelen and Riloff, 2002).</S>\n    <S sid=\"7\" ssid=\"4\">The hope is that semi-supervised or even unsupervised approaches, when given enough knowledge about the structure of the problem, will be competitive with the supervised models trained on large training sets.</S>\n    <S sid=\"8\" ssid=\"5\">However, in the general case, semi-supervised approaches give mixed results, and sometimes even degrade the model performance (Nigam et al., 2000).</S>\n    <S sid=\"9\" ssid=\"6\">In many cases, improving semi-supervised models was done by seeding these models with domain information taken from dictionaries or ontology (Cohen and Sarawagi, 2004; Collins and Singer, 1999; Haghighi and Klein, 2006; Thelen and Riloff, 2002).</S>\n    <S sid=\"10\" ssid=\"7\">On the other hand, in the supervised setting, it has been shown that incorporating domain and problem specific structured information can result in substantial improvements (Toutanova et al., 2005; Roth and Yih, 2005).</S>\n    <S sid=\"11\" ssid=\"8\">This paper proposes a novel constraints-based learning protocol for guiding semi-supervised learning.</S>\n    <S sid=\"12\" ssid=\"9\">We develop a formalism for constraints-based learning that unifies several kinds of constraints: unary, dictionary based and n-ary constraints, which encode structural information and interdependencies among possible labels.</S>\n    <S sid=\"13\" ssid=\"10\">One advantage of our formalism is that it allows capturing different levels of constraint violation.</S>\n    <S sid=\"14\" ssid=\"11\">Our protocol can be used in the presence of any learning model, including those that acquire additional statistical constraints from observed data while learning (see Section 5.</S>\n    <S sid=\"15\" ssid=\"12\">In the experimental part of this paper we use HMMs as the underlying model, and exhibit significant reduction in the number of training examples required in two information extraction problems.</S>\n    <S sid=\"16\" ssid=\"13\">As is often the case in semi-supervised learning, the algorithm can be viewed as a process that improves the model by generating feedback through labeling unlabeled examples.</S>\n    <S sid=\"17\" ssid=\"14\">Our algorithm pushes this intuition further, in that the use of constraints allows us to better exploit domain information as a way to label, along with the current learned model, unlabeled examples.</S>\n    <S sid=\"18\" ssid=\"15\">Given a small amount of labeled data and a large unlabeled pool, our framework initializes the model with the labeled data and then repeatedly: This way, we can generate better “training” examples during the semi-supervised learning process.</S>\n    <S sid=\"19\" ssid=\"16\">The core of our approach, (1), is described in Section 5.</S>\n    <S sid=\"20\" ssid=\"17\">The task is described in Section 3 and the Experimental study in Section 6.</S>\n    <S sid=\"21\" ssid=\"18\">It is shown there that the improvement on the training examples via the constraints indeed boosts the learned model and the proposed method significantly outperforms the traditional semi-supervised framework.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">In the semi-supervised domain there are two main approaches for injecting domain specific knowledge.</S>\n    <S sid=\"23\" ssid=\"2\">One is using the prior knowledge to accurately tailor the generative model so that it captures the domain structure.</S>\n    <S sid=\"24\" ssid=\"3\">For example, (Grenager et al., 2005) proposes Diagonal Transition Models for sequential labeling tasks where neighboring words tend to have the same labels.</S>\n    <S sid=\"25\" ssid=\"4\">This is done by constraining the HMM transition matrix, which can be done also for other models, such as CRF.</S>\n    <S sid=\"26\" ssid=\"5\">However (Roth and Yih, 2005) showed that reasoning with more expressive, non-sequential constraints can improve the performance for the supervised protocol.</S>\n    <S sid=\"27\" ssid=\"6\">A second approach has been to use a small highaccuracy set of labeled tokens as a way to seed and bootstrap the semi-supervised learning.</S>\n    <S sid=\"28\" ssid=\"7\">This was used, for example, by (Thelen and Riloff, 2002; Collins and Singer, 1999) in information extraction, and by (Smith and Eisner, 2005) in POS tagging.</S>\n    <S sid=\"29\" ssid=\"8\">(Haghighi and Klein, 2006) extends the dictionarybased approach to sequential labeling tasks by propagating the information given in the seeds with contextual word similarity.</S>\n    <S sid=\"30\" ssid=\"9\">This follows a conceptually similar approach by (Cohen and Sarawagi, 2004) that uses a large named-entity dictionary, where the similarity between the candidate named-entity and its matching prototype in the dictionary is encoded as a feature in a supervised classifier.</S>\n    <S sid=\"31\" ssid=\"10\">In our framework, dictionary lookup approaches are viewed as unary constraints on the output states.</S>\n    <S sid=\"32\" ssid=\"11\">We extend these kinds of constraints and allow for more general, n-ary constraints.</S>\n    <S sid=\"33\" ssid=\"12\">In the supervised learning setting it has been established that incorporating global information can significantly improve performance on several NLP tasks, including information extraction and semantic role labeling.</S>\n    <S sid=\"34\" ssid=\"13\">(Punyakanok et al., 2005; Toutanova et al., 2005; Roth and Yih, 2005).</S>\n    <S sid=\"35\" ssid=\"14\">Our formalism is most related to this last work.</S>\n    <S sid=\"36\" ssid=\"15\">But, we develop a semi-supervised learning protocol based on this formalism.</S>\n    <S sid=\"37\" ssid=\"16\">We also make use of soft constraints and, furthermore, extend the notion of soft constraints to account for multiple levels of constraints’ violation.</S>\n    <S sid=\"38\" ssid=\"17\">Conceptually, although not technically, the most related work to ours is (Shen et al., 2005) that, in a somewhat ad-hoc manner uses soft constraints to guide an unsupervised model that was crafted for mention tracking.</S>\n    <S sid=\"39\" ssid=\"18\">To the best of our knowledge, we are the first to suggest a general semi-supervised protocol that is driven by soft constraints.</S>\n    <S sid=\"40\" ssid=\"19\">We propose learning with constraints - a framework that combines the approaches described above in a unified and intuitive way.</S>\n  </SECTION>\n  <SECTION title=\"3 Tasks, Examples and Datasets\" number=\"3\">\n    <S sid=\"41\" ssid=\"1\">In Section 4 we will develop a general framework for semi-supervised learning with constraints.</S>\n    <S sid=\"42\" ssid=\"2\">However, it is useful to illustrate the ideas on concrete problems.</S>\n    <S sid=\"43\" ssid=\"3\">Therefore, in this section, we give a brief introduction to the two domains on which we tested our algorithms.</S>\n    <S sid=\"44\" ssid=\"4\">We study two information extraction problems in each of which, given text, a set of pre-defined fields is to be identified.</S>\n    <S sid=\"45\" ssid=\"5\">Since the fields are typically related and interdependent, these kinds of applications provide a good test case for an approach like ours.1 The first task is to identify fields from citations (McCallum et al., 2000) .</S>\n    <S sid=\"46\" ssid=\"6\">The data originally included 500 labeled references, and was later extended with 5,000 unannotated citations collected from papers found on the Internet (Grenager et al., 2005).</S>\n    <S sid=\"47\" ssid=\"7\">Given a citation, the task is to extract the each open bracket.</S>\n    <S sid=\"48\" ssid=\"8\">The correct assignment was shown in (a).</S>\n    <S sid=\"49\" ssid=\"9\">While the predicted label assignment (b) is generally coherent, some constraints are violated.</S>\n    <S sid=\"50\" ssid=\"10\">Most obviously, punctuation marks are ignored as cues for state transitions.</S>\n    <S sid=\"51\" ssid=\"11\">The constraint “Fields cannot end with stop words (such as “the”)” may be also good. fields that appear in the given reference.</S>\n    <S sid=\"52\" ssid=\"12\">See Fig.</S>\n    <S sid=\"53\" ssid=\"13\">1.</S>\n    <S sid=\"54\" ssid=\"14\">There are 13 possible fields including author, title, location, etc.</S>\n    <S sid=\"55\" ssid=\"15\">To gain an insight to how the constraints can guide semi-supervised learning, assume that the sentence shown in Figure 1 appears in the unlabeled data pool.</S>\n    <S sid=\"56\" ssid=\"16\">Part (a) of the figure shows the correct labeled assignment and part (b) shows the assignment labeled by a HMM trained on 30 labels.</S>\n    <S sid=\"57\" ssid=\"17\">However, if we apply the constraint that state transition can occur only on punctuation marks, the same HMM model parameters will result in the correct labeling (a).</S>\n    <S sid=\"58\" ssid=\"18\">Therefore, by adding the improved labeled assignment we can generate better training samples during semi-supervised learning.</S>\n    <S sid=\"59\" ssid=\"19\">In fact, the punctuation marks are only some of the constraints that can be applied to this problem.</S>\n    <S sid=\"60\" ssid=\"20\">The set of constraints we used in our experiments appears in Table 1.</S>\n    <S sid=\"61\" ssid=\"21\">Note that some of the constraints are non-local and are very intuitive for people, yet it is very difficult to inject this knowledge into most models.</S>\n    <S sid=\"62\" ssid=\"22\">The second problem we consider is extracting fields from advertisements (Grenager et al., 2005).</S>\n    <S sid=\"63\" ssid=\"23\">The dataset consists of 8,767 advertisements for apartment rentals in the San Francisco Bay Area downloaded in June 2004 from the Craigslist website.</S>\n    <S sid=\"64\" ssid=\"24\">In the dataset, only 302 entries have been labeled with 12 fields, including size, rent, neighborhood, features, and so on.</S>\n    <S sid=\"65\" ssid=\"25\">The data was preprocessed using regular expressions for phone numbers, email addresses and URLs.</S>\n    <S sid=\"66\" ssid=\"26\">The list of the constraints for this domain is given in Table 1.</S>\n    <S sid=\"67\" ssid=\"27\">We implement some global constraints and include unary constraints which were largely imported from the list of seed words used in (Haghighi and Klein, 2006).</S>\n    <S sid=\"68\" ssid=\"28\">We slightly modified the seedwords due to difference in preprocessing.</S>\n  </SECTION>\n  <SECTION title=\"4 Notation and Definitions\" number=\"4\">\n    <S sid=\"69\" ssid=\"1\">given an input sequence x = (x1,... , xN), the task is to find the best assignment to the output variables y = (y1, ... , yM).</S>\n    <S sid=\"70\" ssid=\"2\">We denote X to be the space of the possible input sequences and Y to be the set of possible output sequences.</S>\n    <S sid=\"71\" ssid=\"3\">We define a structured output classifier as a function h : X  Y that uses a global scoring function f : X × Y  R to assign scores to each possible input/output pair.</S>\n    <S sid=\"72\" ssid=\"4\">Given an input x, a desired function f will assign the correct output y the highest score among all the possible outputs.</S>\n    <S sid=\"73\" ssid=\"5\">The global scoring function is often decomposed as a weighted sum of feature functions, This decomposition applies both to discriminative linear models and to generative models such as HMMs and CRFs, in which case the linear sum corresponds to log likelihood assigned to the input/output pair by the model (for details see (Roth, 1999) for the classification case and (Collins, 2002) for the structured case).</S>\n    <S sid=\"74\" ssid=\"6\">Even when not dictated by the model, the feature functions fi(x, y) used are local to allow inference tractability.</S>\n    <S sid=\"75\" ssid=\"7\">Local feature function can capture some context for each input or output variable, yet it is very limited to allow dynamic programming decoding during inference.</S>\n    <S sid=\"76\" ssid=\"8\">Now, consider a scenario where we have a set of constraints C1, ... , CK.</S>\n    <S sid=\"77\" ssid=\"9\">We define a constraint C : X × Y  {0, 1} as a function that indicates whether the input/output sequence violates some desired properties.</S>\n    <S sid=\"78\" ssid=\"10\">When the constraints are hard, the solution is given by from citations and advertisements.</S>\n    <S sid=\"79\" ssid=\"11\">Some constraints (represented in the first block of each domain) are global and are relatively difficult to inject into traditional models.</S>\n    <S sid=\"80\" ssid=\"12\">While all the constraints hold for the vast majority of the data, some of them are violated by some correct labeled assignments. where 1C(x) is a subset of Y for which all Ci assign the value 1 for the given (x, y).</S>\n    <S sid=\"81\" ssid=\"13\">When the constraints are soft, we want to incur some penalty for their violation.</S>\n    <S sid=\"82\" ssid=\"14\">Moreover, we want to incorporate into our cost function a measure for the amount of violation incurred by violating the constraint.</S>\n    <S sid=\"83\" ssid=\"15\">A generic way to capture this intuition is to introduce a distance function d(y, 1Ci(x)) between the space of outputs that respect the constraint,1Ci(x), and the given output sequence y.</S>\n    <S sid=\"84\" ssid=\"16\">One possible way to implement this distance function is as the minimal Hamming distance to a sequence that respects the constraint Ci, that is: d(y, 1Ci(x)) = min(y'E1C(.))</S>\n    <S sid=\"85\" ssid=\"17\">H(y, y').</S>\n    <S sid=\"86\" ssid=\"18\">If the penalty for violating the soft constraint Ci is pi, we write the score function as: We refer to d(y, 1C(x)) as the valuation of the constraint C on (x, y).</S>\n    <S sid=\"87\" ssid=\"19\">The intuition behind (1) is as follows.</S>\n    <S sid=\"88\" ssid=\"20\">Instead of merely maximizing the model’s likelihood, we also want to bias the model using some knowledge.</S>\n    <S sid=\"89\" ssid=\"21\">The first term of (1) is used to learn from data.</S>\n    <S sid=\"90\" ssid=\"22\">The second term biases the mode by using the knowledge encoded in the constraints.</S>\n    <S sid=\"91\" ssid=\"23\">Note that we do not normalize our objective function to be a true probability distribution.</S>\n  </SECTION>\n  <SECTION title=\"5 Learning and Inference with Constraints\" number=\"5\">\n    <S sid=\"92\" ssid=\"1\">In this section we present a new constraint-driven learning algorithm (CODL) for using constraints to guide semi-supervised learning.</S>\n    <S sid=\"93\" ssid=\"2\">The task is to learn the parameter vector A by using the new objective function (1).</S>\n    <S sid=\"94\" ssid=\"3\">While our formulation allows us to train also the coefficients of the constraints valuation, pi, we choose not to do it, since we view this as a way to bias (or enforce) the prior knowledge into the learned model, rather than allowing the data to brush it away.</S>\n    <S sid=\"95\" ssid=\"4\">Our experiments demonstrate that the proposed approach is robust to inaccurate approximation of the prior knowledge (assigning the same penalty to all the pi ).</S>\n    <S sid=\"96\" ssid=\"5\">We note that in the presence of constraints, the inference procedure (for finding the output y that maximizes the cost function) is usually done with search techniques (rather than Viterbi decoding, see (Toutanova et al., 2005; Roth and Yih, 2005) for a discussion), we chose beamsearch decoding.</S>\n    <S sid=\"97\" ssid=\"6\">The semi-supervised learning with constraints is done with an EM-like procedure.</S>\n    <S sid=\"98\" ssid=\"7\">We initialize the model with traditional supervised learning (ignoring the constraints) on a small labeled set.</S>\n    <S sid=\"99\" ssid=\"8\">Given an unlabeled set U, in the estimation step, the traditional EM algorithm assigns a distribution over labeled assignments Y of each x E U, and in the maximization step, the set of model parameters is learned from the distributions assigned in the estimation step.</S>\n    <S sid=\"100\" ssid=\"9\">However, in the presence of constraints, assigning the complete distributions in the estimation step is infeasible since the constraints reshape the distribution in an arbitrary way.</S>\n    <S sid=\"101\" ssid=\"10\">As in existing methods for training a model by maximizing a linear cost function (maximize likelihood or discriminative maximization), the distribution over y is represented as the set of scores assigned to it; rather than considering the score assigned to all y's, we truncate the distribution to the top K assignments as returned by the search.</S>\n    <S sid=\"102\" ssid=\"11\">Given a set of K top assignments yi, , yK, we approximate the estimation step by assigning uniform probability to the top K candidates, and zero to the other output sequences.</S>\n    <S sid=\"103\" ssid=\"12\">We denote this algorithm top-K hard EM.</S>\n    <S sid=\"104\" ssid=\"13\">In this paper, we use beamsearch to generate K candidates according to (1).</S>\n    <S sid=\"105\" ssid=\"14\">Our training algorithm is summarized in Figure 2.</S>\n    <S sid=\"106\" ssid=\"15\">Several things about the algorithm should be clarified: the Top-K-Inference procedure in line 7, the learning procedure in line 9, and the new parameter estimation in line 9.</S>\n    <S sid=\"107\" ssid=\"16\">The Top-K-Inference is a procedure that returns the K labeled assignments that maximize the new objective function (1).</S>\n    <S sid=\"108\" ssid=\"17\">In our case we used the topK elements in the beam, but this could be applied to any other inference procedure.</S>\n    <S sid=\"109\" ssid=\"18\">The fact that the constraints are used in the inference procedure (in particular, for generating new training examples) allows us to use a learning algorithm that ignores the constraints, which is a lot more efficient (although algorithms that do take the constraints into account can be used too).</S>\n    <S sid=\"110\" ssid=\"19\">We used maximum likelihood estimation of A but, in general, perceptron or quasiNewton can also be used.</S>\n    <S sid=\"111\" ssid=\"20\">It is known that traditional semi-supervised training can degrade the learned model’s performance.</S>\n    <S sid=\"112\" ssid=\"21\">(Nigam et al., 2000) has suggested to balance the contribution of labeled and unlabeled data to the parameters.</S>\n    <S sid=\"113\" ssid=\"22\">The intuition is that when iteratively estimating the parameters with EM, we disallow the parameters to drift too far from the supervised model.</S>\n    <S sid=\"114\" ssid=\"23\">The parameter re-estimation in line 9, uses a similar intuition, but instead of weighting data instances, we introduced a smoothing parameter -y which controls the convex combination of models induced by the labeled and the unlabeled data.</S>\n    <S sid=\"115\" ssid=\"24\">Unlike the technique mentioned above which focuses on naive Bayes, our method allows us to weight linear models generated by different learning algorithms.</S>\n    <S sid=\"116\" ssid=\"25\">Another way to look the algorithm is from the self-training perspective (McClosky et al., 2006).</S>\n    <S sid=\"117\" ssid=\"26\">Similarly to self-training, we use the current model to generate new training examples from the unlaTop-K-Inference, we use beamsearch to find the Kbest solution according to Eq.</S>\n    <S sid=\"118\" ssid=\"27\">(1). beled set.</S>\n    <S sid=\"119\" ssid=\"28\">However, there are two important differences.</S>\n    <S sid=\"120\" ssid=\"29\">One is that in self-training, once an unlabeled sample was labeled, it is never labeled again.</S>\n    <S sid=\"121\" ssid=\"30\">In our case all the samples are relabeled in each iteration.</S>\n    <S sid=\"122\" ssid=\"31\">In self-training it is often the case that only high-confidence samples are added to the labeled data pool.</S>\n    <S sid=\"123\" ssid=\"32\">While we include all the samples in the training pool, we could also limit ourselves to the high-confidence samples.</S>\n    <S sid=\"124\" ssid=\"33\">The second difference is that each unlabeled example generates K labeled instances.</S>\n    <S sid=\"125\" ssid=\"34\">The case of one iteration of top-1 hard EM is equivalent to self training, where all the unlabeled samples are added to the labeled pool.</S>\n    <S sid=\"126\" ssid=\"35\">There are several possible benefits to using K &gt; 1 samples.</S>\n    <S sid=\"127\" ssid=\"36\">(1) It effectively increases the training set by a factor of K (albeit by somewhat noisy examples).</S>\n    <S sid=\"128\" ssid=\"37\">In the structured scenario, each of the top-K assignments is likely to have some good components so generating top-K assignments helps leveraging the noise.</S>\n    <S sid=\"129\" ssid=\"38\">(2) Given an assignment that does not satisfy some constraints, using top-K allows for multiple ways to correct it.</S>\n    <S sid=\"130\" ssid=\"39\">For example, consider the output 11101000 with the constraint that it should belong to the language 1*0*.</S>\n    <S sid=\"131\" ssid=\"40\">If the two top scoring corrections are 11111000 and 11100000, considering only one of those can negatively bias the model.</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments and Results\" number=\"6\">\n    <S sid=\"132\" ssid=\"1\">In this section, we present empirical results of our algorithms on two domains: citations and advertisements.</S>\n    <S sid=\"133\" ssid=\"2\">Both problems are modeled with a simple token-based HMM.</S>\n    <S sid=\"134\" ssid=\"3\">We stress that token-based HMM cannot represent many of our constraints.</S>\n    <S sid=\"135\" ssid=\"4\">The function d(y, 1C(x)) used is an approximation of a Hamming distance function, discussed in Section 7.</S>\n    <S sid=\"136\" ssid=\"5\">For both domains, and all the experiments, -y was set to 0.1.</S>\n    <S sid=\"137\" ssid=\"6\">The constraints violation penalty p is set to − log 10−4 and − log 10−1 for citations and advertisements, resp.2 Note that all constraints share the same penalty.</S>\n    <S sid=\"138\" ssid=\"7\">The number of semi-supervised training cycles (line 3 of Figure 2) was set to 5.</S>\n    <S sid=\"139\" ssid=\"8\">The constraints for the two domains are listed in Table 1.</S>\n    <S sid=\"140\" ssid=\"9\">We trained models on training sets of size varying from 5 to 300 for the citations and from 5 to 100 for the advertisements.</S>\n    <S sid=\"141\" ssid=\"10\">Additionally, in all the semi-supervised experiments, 1000 unlabeled examples are used.</S>\n    <S sid=\"142\" ssid=\"11\">We report token-based3 accuracy on 100 held-out examples (which do not overlap neither with the training nor with the unlabeled data).</S>\n    <S sid=\"143\" ssid=\"12\">We ran 5 experiments in each setting, randomly choosing the training set.</S>\n    <S sid=\"144\" ssid=\"13\">The results reported below are the averages over these 5 runs.</S>\n    <S sid=\"145\" ssid=\"14\">To verify our claims we implemented several baselines.</S>\n    <S sid=\"146\" ssid=\"15\">The first baseline is the supervised learning protocol denoted by sup.</S>\n    <S sid=\"147\" ssid=\"16\">The second baseline was a traditional top-1 Hard EM also known as truncated EM4 (denoted by H for Hard).</S>\n    <S sid=\"148\" ssid=\"17\">In the third baseline, denoted H&amp;W, we balanced the weight of the supervised and unsupervised models as described in line 9 of Figure 2.</S>\n    <S sid=\"149\" ssid=\"18\">We compare these baselines to our proposed protocol, H&amp;W&amp;C, where we added the constraints to guide the H&amp;W protocol.</S>\n    <S sid=\"150\" ssid=\"19\">We experimented with two flavors of the algorithm: the top-1 and the top-K version.</S>\n    <S sid=\"151\" ssid=\"20\">In the top-K version, the algorithm uses K-best predictions (K=50) for each instance in order to update the model as described in Figure 2.</S>\n    <S sid=\"152\" ssid=\"21\">The experimental results for both domains are in given Table 2.</S>\n    <S sid=\"153\" ssid=\"22\">As hypothesized, hard EM sometimes from citations and advertisements.</S>\n    <S sid=\"154\" ssid=\"23\">N is the number of labeled samples.</S>\n    <S sid=\"155\" ssid=\"24\">H is the traditional hard-EM and H&amp;W weighs labeled and unlabeled data as mentioned in Sec.</S>\n    <S sid=\"156\" ssid=\"25\">5.</S>\n    <S sid=\"157\" ssid=\"26\">Our proposed model is H&amp;W&amp;C, which uses constraints in the learning procedure.</S>\n    <S sid=\"158\" ssid=\"27\">I refers to using constraints during inference at evaluation time.</S>\n    <S sid=\"159\" ssid=\"28\">Note that adding constraints improves the accuracy during both learning and inference. degrade the performance.</S>\n    <S sid=\"160\" ssid=\"29\">Indeed, with 300 labeled examples in the citations domain, the performance decreases from 86.1 to 80.7.</S>\n    <S sid=\"161\" ssid=\"30\">The usefulness of injecting constraints in semi-supervised learning is exhibited in the two right most columns: using constraints H&amp;W&amp;C improves the performance over H&amp;W quite significantly.</S>\n    <S sid=\"162\" ssid=\"31\">We carefully examined the contribution of using constraints to the learning stage and the testing stage, and two separate results are presented: testing with constraints (denoted I for inference) and without constraints (no I).</S>\n    <S sid=\"163\" ssid=\"32\">The I results are consistently better.</S>\n    <S sid=\"164\" ssid=\"33\">And, it is also clear from Table 2, that using constraints in training always improves the model and the amount of improvement depends on the amount of labeled data.</S>\n    <S sid=\"165\" ssid=\"34\">Figure 3 compares two protocols on the advertisements domain: H&amp;W+I, where we first run the H&amp;W protocol and then apply the constraints during testing stage, and H&amp;W&amp;C+I, which uses constraints to guide the model during learning and uses it also in testing.</S>\n    <S sid=\"166\" ssid=\"35\">Although injecting constraints in the learning process helps, testing with constraints is more important than using constraints during learning, especially when the labeled data size is large.</S>\n    <S sid=\"167\" ssid=\"36\">This confirms results reported for the supervised learning case in (Punyakanok et al., 2005; Roth and Yih, 2005).</S>\n    <S sid=\"168\" ssid=\"37\">However, as shown, our proposed algorithm H&amp;W&amp;C for training with constraints is critical when the amount labeled data is small.</S>\n    <S sid=\"169\" ssid=\"38\">Figure 4 further strengthens this point.</S>\n    <S sid=\"170\" ssid=\"39\">In the citations domain, H&amp;W&amp;C+I achieves with 20 labeled samples similar performance to the supervised version without constraints with 300 labeled samples.</S>\n    <S sid=\"171\" ssid=\"40\">(Grenager et al., 2005) and (Haghighi and Klein, 2006) also report results for semi-supervised learning for these domains.</S>\n    <S sid=\"172\" ssid=\"41\">However, due to different preprocessing, the comparison is not straightforward.</S>\n    <S sid=\"173\" ssid=\"42\">For the citation domain, when 20 labeled and 300 unlabeled samples are available, (Grenager et al., 2005) observed an increase from 65.2% to 71.3%.</S>\n    <S sid=\"174\" ssid=\"43\">Our improvement is from 70.1% to 79.4%.</S>\n    <S sid=\"175\" ssid=\"44\">For the advertisement domain, they observed no improvement, while our model improves from 68.1% to 74.6% with 20 labeled samples.</S>\n    <S sid=\"176\" ssid=\"45\">Moreover, we successfully use out-of-domain data (web data) to improve our model, while they report that this data did not improve their unsupervised model.</S>\n    <S sid=\"177\" ssid=\"46\">(Haghighi and Klein, 2006) also worked on one of our data sets.</S>\n    <S sid=\"178\" ssid=\"47\">Their underlying model, Markov Random Fields, allows more expressive features.</S>\n    <S sid=\"179\" ssid=\"48\">Nevertheless, when they use only unary constraints they get 53.75%.</S>\n    <S sid=\"180\" ssid=\"49\">When they use their final model, along with a mechanism for extending the prototypes to other tokens, they get results that are comparable to our model with 10 labeled examples.</S>\n    <S sid=\"181\" ssid=\"50\">Additionally, in their framework, it is not clear how to use small amounts of labeled data when available.</S>\n    <S sid=\"182\" ssid=\"51\">Our model outperforms theirs once we add 10 more examples.</S>\n  </SECTION>\n  <SECTION title=\"7 Soft Constraints\" number=\"7\">\n    <S sid=\"183\" ssid=\"1\">This section discusses the importance of using soft constraints rather than hard constraints, the choice of Hamming distance for d(y, 1C(x)) and how we approximate it.</S>\n    <S sid=\"184\" ssid=\"2\">We use two constraints to illustrate the ideas.</S>\n    <S sid=\"185\" ssid=\"3\">(C1): “state transitions can only occur on punctuation marks or newlines”, and (C2): “the field TITLE must appear”.</S>\n    <S sid=\"186\" ssid=\"4\">First, we claim that defining d(y, 1C(x)) to be the Hamming distance is superior to using a binary value, d(y, 1C(x)) = 0 if y E 1C(x) and 1 otherwise.</S>\n    <S sid=\"187\" ssid=\"5\">Consider, for example, the constraint C1 in the advertisements domain.</S>\n    <S sid=\"188\" ssid=\"6\">While the vast majority of the instances satisfy the constraint, some violate it in more than one place.</S>\n    <S sid=\"189\" ssid=\"7\">Therefore, once the binary distance is set to 1, the algorithm looses the ability to discriminate constraint violations in other locations of the same instance.</S>\n    <S sid=\"190\" ssid=\"8\">This may hurt the performance in both the inference and the learning stage.</S>\n    <S sid=\"191\" ssid=\"9\">Computing the Hamming distance exactly can be a computationally hard problem.</S>\n    <S sid=\"192\" ssid=\"10\">Furthermore, it is unreasonable to implement the exact computation for each constraint.</S>\n    <S sid=\"193\" ssid=\"11\">Therefore, we implemented a generic approximation for the hamming distance assuming only that we are given a boolean function OC(yN) that returns whether labeling the token xN with state yN violates constraint with respect to an already labeled consider the prefix x1, x2, x3, x4, which contains no punctuation or newlines and was labeled AUTH, AUTH, DATE, DATE.</S>\n    <S sid=\"194\" ssid=\"12\">This labeling violates C1, the minimal hamming distance is 2, and our approximation gives 1, (since there is only one transition that violates the constraint.)</S>\n    <S sid=\"195\" ssid=\"13\">For constraints which cannot be validated based on prefix information, our approximation resorts to binary violation count.</S>\n    <S sid=\"196\" ssid=\"14\">For instance, the constraint C2 cannot be implemented with prefix information when the assignment is not complete.</S>\n    <S sid=\"197\" ssid=\"15\">Otherwise, it would mean that the field TITLE should appear as early as possible in the assignment.</S>\n    <S sid=\"198\" ssid=\"16\">While (Roth and Yih, 2005) showed the significance of using hard constraints, our experiments show that using soft constraints is a superior option.</S>\n    <S sid=\"199\" ssid=\"17\">For example, in the advertisements domain, C1 holds for the large majority of the gold-labeled instances, but is sometimes violated.</S>\n    <S sid=\"200\" ssid=\"18\">In supervised training with 100 labeled examples on this domain, sup gave 76.3% accuracy.</S>\n    <S sid=\"201\" ssid=\"19\">When the constraint violation penalty p was infinity (equivalent to hard constraint), the accuracy improved to 78.7%, but when the penalty was set to −log(0.1), the accuracy of the model jumped to 80.6%.</S>\n  </SECTION>\n  <SECTION title=\"8 Conclusions and Future Work\" number=\"8\">\n    <S sid=\"202\" ssid=\"1\">We proposed to use constraints as a way to guide semi-supervised learning.</S>\n    <S sid=\"203\" ssid=\"2\">The framework developed is general both in terms of the representation and expressiveness of the constraints, and in terms of the underlying model being learned – HMM in the current implementation.</S>\n    <S sid=\"204\" ssid=\"3\">Moreover, our framework is a useful tool when the domain knowledge cannot be expressed by the model.</S>\n    <S sid=\"205\" ssid=\"4\">The results show that constraints improve not only the performance of the final inference stage but also propagate useful information during the semisupervised learning process and that training with the constraints is especially significant when the number of labeled training data is small.</S>\n    <S sid=\"206\" ssid=\"5\">Acknowledgments: This work is supported by NSF SoDHCER-0613885 and by a grant from Boeing.</S>\n    <S sid=\"207\" ssid=\"6\">Part of this work was done while Dan Roth visited the Technion, Israel, supported by a Lady Davis Fellowship.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "44d2fb16ea22c2ea9cc01a347e91ff7f",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Guiding Semi-Supervision with Constraint-Driven Learning"
                }
            }
        },
        {
            "index": 6,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Machine learning approaches to coreference resolution are typically supervised, and require expensive labeled data. Some unsupervised approaches have been proposed (e.g., Haghighi and Klein (2007)), but they are less accurate. In this paper, we present the first unsupervised approach that is competitive with supervised ones. This is made possible by performing joint inference across mentions, in contrast to the pairwise classification typically used in supervised methods, and by using Markov logic as a representation language, which enables us to easily express relations like apposition and predicate nominals. On MUC and ACE datasets, our model outperforms Haghigi and Klein’s one using only a fraction of the training data, and often matches or exceeds the accuracy of state-of-the-art supervised models.",
                    "filename": "Joint_Unsupervised_Coreference_Resolution_with_Markov_Logic",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D08-1068/Documents_xml/D08-1068.xml",
                    "title": "Joint Unsupervised Coreference Resolution with Markov Logic"
                },
                "llm": {
                    "descriptions": [
                        "Abstract:\n\nThis paper presents a novel unsupervised coreference resolution system employing Markov logic for joint inference among mentions of entities in text. Coreference resolution is a critical subtask in natural language processing (NLP) applications such as information extraction and question answering. Traditional supervised approaches are hampered by the need for labeled training data, while unsupervised methods, which could leverage abundant unlabeled text, have lagged in accuracy. The proposed system addresses this gap by using Markov logic to capture complex relations like apposition and predicate nominals, enabling joint inference that uses \"easy\" decisions to inform \"hard\" ones. The system's performance is evaluated using standard MUC and ACE datasets, showing that even with minimal training data, the system surpasses the previous state-of-the-art unsupervised systems and matches or exceeds supervised models' accuracy. This advancement opens the door to accurate coreference resolution without the need for extensive labeled datasets, and the flexibility of the Markov logic framework provides a pathway for future enhancements through the incorporation of additional linguistic and world knowledge.",
                        "Abstract:\n\nThis paper presents a novel unsupervised coreference resolution system that approaches the accuracy of supervised models. Coreference resolution, identifying mentions in text that refer to the same entity, is vital for various natural language processing tasks. While supervised approaches have been successful, they require labeled data, which limits their applicability. Unsupervised methods, which use plentiful unlabeled data, have lagged behind in performance. The authors address this gap by employing Markov logic—a combination of probabilistic graphical models and first-order logic—to perform joint inference on mentions, exploiting relations like apposition and predicate nominals. They introduce the first general-purpose unsupervised learning algorithm for Markov logic networks (MLNs) and apply it to coreference resolution. The proposed system is evaluated on standard MUC and ACE datasets, outperforming the previous state-of-the-art unsupervised systems and achieving accuracy comparable to the best supervised methods. The system's success is attributed to its use of joint inference, syntactic relations, and a head mixture model. Future work includes incorporating additional knowledge sources, joint entity detection and coreference resolution, and integrating coreference resolution with other NLP tasks.",
                        "**Abstract:**\n\nThis paper presents an innovative approach to unsupervised coreference resolution by incorporating joint inference and utilizing Markov logic—a formal language that combines aspects of probabilistic graphical models and first-order logic. The authors have developed the first general-purpose unsupervised learning algorithm for Markov logic and applied it to the task of coreference resolution, which involves identifying when different noun phrases refer to the same entity in a text.\n\nThe proposed system exploits relations among mentions, such as apposition and predicate nominals, to perform joint inference without requiring labeled training data. The method was evaluated on standard MUC and ACE datasets and showed significant improvements over previous unsupervised models, even rivaling the best supervised models in terms of accuracy. The authors' full model outperformed the state-of-the-art unsupervised system by Haghighi and Klein (2007) and supervised approaches by McCallum & Wellner (2005).\n\nThe results indicate the potential of Markov logic as a powerful framework for joint inference in natural language processing tasks, and the authors suggest future research directions, including the incorporation of additional linguistic and world knowledge, joint entity detection and coreference resolution, and integration with other NLP tasks. The research contributes to the advancement of unsupervised learning techniques in NLP and demonstrates a robust, scalable approach to coreference resolution.",
                        "**Abstract:**\n\nThis paper presents an innovative unsupervised coreference resolution system that uses Markov logic, a combination of probabilistic graphical models and first-order logic, to perform joint inference among text mentions. Coreference resolution, a crucial subtask in natural language processing (NLP), involves identifying noun phrases that refer to the same entity across a document. Current supervised approaches to coreference resolution require labeled training data and are limited by their inability to utilize the vast amounts of unlabeled text available. The proposed system overcomes this limitation by using unsupervised learning, which does not rely on labeled data.\n\nThe authors developed a general-purpose unsupervised learning algorithm for Markov logic and applied it to coreference resolution, testing it on standard MUC and ACE datasets. Their system employs a Markov logic network (MLN) that leverages syntactic relations such as apposition and predicate nominals, which have been previously underutilized due to unsupervised learning challenges.\n\nExperimental results demonstrate that the proposed system outperforms the state-of-the-art unsupervised models and achieves accuracy comparable to supervised models. The system's success is attributed to its use of joint inference and the ability to incorporate relations among mentions. This represents a significant advancement in the field, as it is the first unsupervised system to reach such accuracy levels, holding promise for future enhancements and broader applications in NLP tasks."
                    ],
                    "filename": "joint_unsupervised_coreference_r-write_xml_paper_abstract",
                    "generation_prompt_nickname": "write_xml_paper_abstract",
                    "generation_prompt_text": "Read the following academic paper (in XML format) and create an abstract for it.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Joint Unsupervised Coreference Resolution with Markov Logic</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">The goal of coreference resolution is to identify mentions (typically noun phrases) that refer to the same entities.</S>\n    <S sid=\"7\" ssid=\"2\">This is a key subtask in many NLP applications, including information extraction, question answering, machine translation, and others.</S>\n    <S sid=\"8\" ssid=\"3\">Supervised learning approaches treat the problem as one of classification: for each pair of mentions, predict whether they corefer or not (e.g., McCallum &amp; Wellner (2005)).</S>\n    <S sid=\"9\" ssid=\"4\">While successful, these approaches require labeled training data, consisting of mention pairs and the correct decisions for them.</S>\n    <S sid=\"10\" ssid=\"5\">This limits their applicability.</S>\n    <S sid=\"11\" ssid=\"6\">Unsupervised approaches are attractive due to the availability of large quantities of unlabeled text.</S>\n    <S sid=\"12\" ssid=\"7\">However, unsupervised coreference resolution is much more difficult.</S>\n    <S sid=\"13\" ssid=\"8\">Haghighi and Klein’s (2007) model, the most sophisticated to date, still lags supervised ones by a substantial margin.</S>\n    <S sid=\"14\" ssid=\"9\">Extending it appears difficult, due to the limitations of its Dirichlet process-based representation.</S>\n    <S sid=\"15\" ssid=\"10\">The lack of label information in unsupervised coreference resolution can potentially be overcome by performing joint inference, which leverages the “easy” decisions to help make related “hard” ones.</S>\n    <S sid=\"16\" ssid=\"11\">Relations that have been exploited in supervised coreference resolution include transitivity (McCallum &amp; Wellner, 2005) and anaphoricity (Denis &amp; Baldridge, 2007).</S>\n    <S sid=\"17\" ssid=\"12\">However, there is little work to date on joint inference for unsupervised resolution.</S>\n    <S sid=\"18\" ssid=\"13\">We address this problem using Markov logic, a powerful and flexible language that combines probabilistic graphical models and first-order logic (Richardson &amp; Domingos, 2006).</S>\n    <S sid=\"19\" ssid=\"14\">Markov logic allows us to easily build models involving relations among mentions, like apposition and predicate nominals.</S>\n    <S sid=\"20\" ssid=\"15\">By extending the state-of-the-art algorithms for inference and learning, we developed the first general-purpose unsupervised learning algorithm for Markov logic, and applied it to unsupervised coreference resolution.</S>\n    <S sid=\"21\" ssid=\"16\">We test our approach on standard MUC and ACE datasets.</S>\n    <S sid=\"22\" ssid=\"17\">Our basic model, trained on a minimum of data, suffices to outperform Haghighi and Klein’s (2007) one.</S>\n    <S sid=\"23\" ssid=\"18\">Our full model, using apposition and other relations for joint inference, is often as accurate as the best supervised models, or more.</S>\n    <S sid=\"24\" ssid=\"19\">We begin by reviewing the necessary background on Markov logic.</S>\n    <S sid=\"25\" ssid=\"20\">We then describe our Markov logic network for joint unsupervised coreference resolution, and the learning and inference algorithms we used.</S>\n    <S sid=\"26\" ssid=\"21\">Finally, we present our experiments and results.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"27\" ssid=\"1\">Most existing supervised learning approaches for coreference resolution are suboptimal since they resolve each mention pair independently, only imposing transitivity in postprocessing (Ng, 2005).</S>\n    <S sid=\"28\" ssid=\"2\">Moreover, many of them break up the resolution step into subtasks (e.g., first determine whether a mention is anaphoric, then classify whether it is coreferent with an antecedent), which further forsakes opportunities for joint inference that have been shown to be helpful (Poon &amp; Domingos, 2007).</S>\n    <S sid=\"29\" ssid=\"3\">Using graph partitioning, McCallum &amp; Wellner (2005) incorporated transitivity into pairwise classification and achieved the state-of-the-art result on the MUC-6 dataset, but their approach can only leverage one binary relation at a time, not arbitrary relations among mentions.</S>\n    <S sid=\"30\" ssid=\"4\">Denis &amp; Baldridge (2007) determined anaphoricity and pairwise classification jointly using integer programming, but they did not incorporate transitivity or other relations.</S>\n    <S sid=\"31\" ssid=\"5\">While potentially more appealing, unsupervised learning is very challenging, and unsupervised coreference resolution systems are still rare to this date.</S>\n    <S sid=\"32\" ssid=\"6\">Prior to our work, the best performance in unsupervised coreference resolution was achieved by Haghighi &amp; Klein (2007), using a nonparametric Bayesian model based on hierarchical Dirichlet processes.</S>\n    <S sid=\"33\" ssid=\"7\">At the heart of their system is a mixture model with a few linguistically motivated features such as head words, entity properties and salience.</S>\n    <S sid=\"34\" ssid=\"8\">Their approach is a major step forward in unsupervised coreference resolution, but extending it is challenging.</S>\n    <S sid=\"35\" ssid=\"9\">The main advantage of Dirichlet processes is that they are exchangeable, allowing parameters to be integrated out, but Haghighi and Klein forgo this when they introduce salience.</S>\n    <S sid=\"36\" ssid=\"10\">Their model thus requires Gibbs sampling over both assignments and parameters, which can be very expensive.</S>\n    <S sid=\"37\" ssid=\"11\">Haghighi and Klein circumvent this by making approximations that potentially hurt accuracy.</S>\n    <S sid=\"38\" ssid=\"12\">At the same time, the Dirichlet process prior favors skewed cluster sizes and a number of clusters that grows logarithmically with the number of data points, neither of which seems generally appropriate for coreference resolution.</S>\n    <S sid=\"39\" ssid=\"13\">Further, deterministic or strong non-deterministic dependencies cause Gibbs sampling to break down (Poon &amp; Domingos, 2006), making it difficult to leverage many linguistic regularities.</S>\n    <S sid=\"40\" ssid=\"14\">For example, apposition (as in “Bill Gates, the chairman of Microsoft”) suggests coreference, and thus the two mentions it relates should always be placed in the same cluster.</S>\n    <S sid=\"41\" ssid=\"15\">However, Gibbs sampling can only move one mention at a time from one cluster to another, and this is unlikely to happen, because it would require breaking the apposition rule.</S>\n    <S sid=\"42\" ssid=\"16\">Blocked sampling can alleviate this problem by sampling multiple mentions together, but it requires that the block size be predetermined to a small fixed number.</S>\n    <S sid=\"43\" ssid=\"17\">When we incorporate apposition and other regularities the blocks can become arbitrarily large, making this infeasible.</S>\n    <S sid=\"44\" ssid=\"18\">For example, suppose we also want to leverage predicate nominals (i.e., the subject and the predicating noun of a copular verb are likely coreferent).</S>\n    <S sid=\"45\" ssid=\"19\">Then a sentence like “He is Bill Gates, the chairman of Microsoft” requires a block of four mentions: “He”, “Bill Gates”, “the chairman of Microsoft”, and “Bill Gates, the chairman of Microsoft”.</S>\n    <S sid=\"46\" ssid=\"20\">Similar difficulties occur with other inference methods.</S>\n    <S sid=\"47\" ssid=\"21\">Thus, extending Haghighi and Klein’s model to include richer linguistic features is a challenging problem.</S>\n    <S sid=\"48\" ssid=\"22\">Our approach is instead based on Markov logic, a powerful representation for joint inference with uncertainty (Richardson &amp; Domingos, 2006).</S>\n    <S sid=\"49\" ssid=\"23\">Like Haghighi and Klein’s, our model is cluster-based rather than pairwise, and implicitly imposes transitivity.</S>\n    <S sid=\"50\" ssid=\"24\">We do not predetermine anaphoricity of a mention, but rather fuse it into the integrated resolution process.</S>\n    <S sid=\"51\" ssid=\"25\">As a result, our model is inherently joint among mentions and subtasks.</S>\n    <S sid=\"52\" ssid=\"26\">It shares several features with Haghighi &amp; Klein’s model, but removes or refines features where we believe it is appropriate to.</S>\n    <S sid=\"53\" ssid=\"27\">Most importantly, our model leverages apposition and predicate nominals, which Haghighi &amp; Klein did not use.</S>\n    <S sid=\"54\" ssid=\"28\">We show that this can be done very easily in our framework, and yet results in very substantial accuracy gains.</S>\n    <S sid=\"55\" ssid=\"29\">It is worth noticing that Markov logic is also well suited for joint inference in supervised systems (e.g., transitivity, which took McCallum &amp; Wellner (2005) nontrivial effort to incorporate, can be handled in Markov logic with the addition of a single formula (Poon &amp; Domingos, 2008)).</S>\n  </SECTION>\n  <SECTION title=\"3 Markov Logic\" number=\"3\">\n    <S sid=\"56\" ssid=\"1\">In many NLP applications, there exist rich relations among objects, and recent work in statistical relational learning (Getoor &amp; Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy.</S>\n    <S sid=\"57\" ssid=\"2\">One of the most powerful representations for joint inference is Markov logic, a probabilistic extension of first-order logic (Richardson &amp; Domingos, 2006).</S>\n    <S sid=\"58\" ssid=\"3\">A Markov logic network (MLN) is a set of weighted first-order clauses.</S>\n    <S sid=\"59\" ssid=\"4\">Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause.</S>\n    <S sid=\"60\" ssid=\"5\">The weight of a feature is the weight of the firstorder clause that originated it.</S>\n    <S sid=\"61\" ssid=\"6\">The probability of a state x in such a network is given by P(x) = (1/Z) exp (Ez wzfz(x)), where Z is a normalization constant, wz is the weight of the ith clause, fz = 1 if the ith clause is true, and fz = 0 otherwise.</S>\n    <S sid=\"62\" ssid=\"7\">Markov logic makes it possible to compactly specify probability distributions over complex relational domains.</S>\n    <S sid=\"63\" ssid=\"8\">Efficient inference can be performed using MC-SAT (Poon &amp; Domingos, 2006).</S>\n    <S sid=\"64\" ssid=\"9\">MC-SAT is a “slice sampling” Markov chain Monte Carlo algorithm.</S>\n    <S sid=\"65\" ssid=\"10\">Slice sampling introduces auxiliary variables u that decouple the original ones x, and alternately samples u conditioned on x and viceversa.</S>\n    <S sid=\"66\" ssid=\"11\">To sample from the slice (the set of states x consistent with the current u), MC-SAT calls SampleSAT (Wei et al., 2004), which uses a combination of satisfiability testing and simulated annealing.</S>\n    <S sid=\"67\" ssid=\"12\">The advantage of using a satisfiability solver (WalkSAT) is that it efficiently finds isolated modes in the distribution, and as a result the Markov chain mixes very rapidly.</S>\n    <S sid=\"68\" ssid=\"13\">The slice sampling scheme ensures that detailed balance is (approximately) preserved.</S>\n    <S sid=\"69\" ssid=\"14\">MC-SAT is orders of magnitude faster than previous MCMC algorithms like Gibbs sampling, making efficient sampling possible on a scale that was previAlgorithm 1 MC-SAT(clauses, weights, num samples) x(c) Satisfy(hard clauses) for i 1 to num samples do M 0 for all ck E clauses satisfied by x(z−1) do With probability 1 — e−lk add ck to M ously out of reach.</S>\n    <S sid=\"70\" ssid=\"15\">Algorithm 1 gives pseudo-code for MC-SAT.</S>\n    <S sid=\"71\" ssid=\"16\">At iteration i — 1, the factor 0k for clause ck is either elk if ck is satisfied in x(z−1), or 1 otherwise.</S>\n    <S sid=\"72\" ssid=\"17\">MC-SAT first samples the auxiliary variable uk uniformly from (0, 0k), then samples a new state uniformly from the set of states that satisfy 0k &gt; uk for all k (the slice).</S>\n    <S sid=\"73\" ssid=\"18\">Equivalently, for each k, with probability 1 — e−lk the next state must satisfy ck.</S>\n    <S sid=\"74\" ssid=\"19\">In general, we can factorize the probability distribution in any way that facilitates inference, sample the uk’s, and make sure that the next state is drawn uniformly from solutions that satisfy 0k &gt; uk for all factors.</S>\n    <S sid=\"75\" ssid=\"20\">MC-SAT, like most existing relational inference algorithms, grounds all predicates and clauses, thus requiring memory and time exponential in the predicate and clause arities.</S>\n    <S sid=\"76\" ssid=\"21\">We developed a general method for producing a “lazy” version of relational inference algorithms (Poon &amp; Domingos, 2008), which carries exactly the same inference steps as the original algorithm, but only maintains a small subset of “active” predicates/clauses, grounding more as needed.</S>\n    <S sid=\"77\" ssid=\"22\">We showed that Lazy-MC-SAT, the lazy version of MC-SAT, reduced memory and time by orders of magnitude in several domains.</S>\n    <S sid=\"78\" ssid=\"23\">We use Lazy-MC-SAT in this paper.</S>\n    <S sid=\"79\" ssid=\"24\">Supervised learning for Markov logic maximizes the conditional log-likelihood L(x, y) = log P(Y = yJX = x), where Y represents the non-evidence predicates, X the evidence predicates, and x, y their values in the training data.</S>\n    <S sid=\"80\" ssid=\"25\">For simplicity, from now on we omit X, whose values are fixed and always conditioned on.</S>\n    <S sid=\"81\" ssid=\"26\">The optimization problem is convex and a global optimum can be found using gradient descent, with the gradient being where ni is the number of true groundings of clause i.</S>\n    <S sid=\"82\" ssid=\"27\">The expected count can be approximated as where yk are samples generated by MC-SAT.</S>\n    <S sid=\"83\" ssid=\"28\">To combat overfitting, a Gaussian prior is imposed on all weights.</S>\n    <S sid=\"84\" ssid=\"29\">In practice, it is difficult to tune the learning rate for gradient descent, especially when the number of groundings varies widely among clauses.</S>\n    <S sid=\"85\" ssid=\"30\">Lowd &amp; Domingos (2007) used a preconditioned scaled conjugate gradient algorithm (PSCG) to address this problem.</S>\n    <S sid=\"86\" ssid=\"31\">This estimates the optimal step size in each step as where g is the gradient, d the conjugate update direction, and A a parameter that is automatically tuned to trade off second-order information with gradient descent.</S>\n    <S sid=\"87\" ssid=\"32\">H is the Hessian matrix, with the (i, j)th entry being The Hessian can be approximated with the same samples used for the gradient.</S>\n    <S sid=\"88\" ssid=\"33\">Its negative inverse diagonal is used as the preconditioner.1 The open-source Alchemy package (Kok et al., 2007) provides implementations of existing algorithms for Markov logic.</S>\n    <S sid=\"89\" ssid=\"34\">In Section 5, we develop the first general-purpose unsupervised learning algorithm for Markov logic by extending the existing algorithms to handle hidden predicates.2 1Lowd &amp; Domingos showed that α can be computed more efficiently, without explicitly approximating or storing the Hessian.</S>\n    <S sid=\"90\" ssid=\"35\">Readers are referred to their paper for details.</S>\n    <S sid=\"91\" ssid=\"36\">2Alchemy includes a discriminative EM algorithm, but it assumes that only a few values are missing, and cannot handle completely hidden predicates.</S>\n    <S sid=\"92\" ssid=\"37\">Kok &amp; Domingos (2007) applied Markov logic to relational clustering, but they used hard EM.</S>\n  </SECTION>\n  <SECTION title=\"4 An MLN for Joint Unsupervised Coreference Resolution\" number=\"4\">\n    <S sid=\"93\" ssid=\"1\">In this section, we present our MLN for joint unsupervised coreference resolution.</S>\n    <S sid=\"94\" ssid=\"2\">Our model deviates from Haghighi &amp; Klein’s (2007) in several important ways.</S>\n    <S sid=\"95\" ssid=\"3\">First, our MLN does not model saliences for proper nouns or nominals, as their influence is marginal compared to other features; for pronoun salience, it uses a more intuitive and simpler definition based on distance, and incorporated it as a prior.</S>\n    <S sid=\"96\" ssid=\"4\">Another difference is in identifying heads.</S>\n    <S sid=\"97\" ssid=\"5\">For the ACE datasets, Haghighi and Klein used the gold heads; for the MUC-6 dataset, where labels are not available, they crudely picked the rightmost token in a mention.</S>\n    <S sid=\"98\" ssid=\"6\">We show that a better way is to determine the heads using head rules in a parser.</S>\n    <S sid=\"99\" ssid=\"7\">This improves resolution accuracy and is always applicable.</S>\n    <S sid=\"100\" ssid=\"8\">Crucially, our MLN leverages syntactic relations such as apposition and predicate nominals, which are not used by Haghighi and Klein.</S>\n    <S sid=\"101\" ssid=\"9\">In our approach, what it takes is just adding two formulas to the MLN.</S>\n    <S sid=\"102\" ssid=\"10\">As common in previous work, we assume that true mention boundaries are given.</S>\n    <S sid=\"103\" ssid=\"11\">We do not assume any other labeled information.</S>\n    <S sid=\"104\" ssid=\"12\">In particular, we do not assume gold name entity recognition (NER) labels, and unlike Haghighi &amp; Klein (2007), we do not assume gold mention types (for ACE datasets, they also used gold head words).</S>\n    <S sid=\"105\" ssid=\"13\">We determined the head of a mention either by taking its rightmost token, or by using the head rules in a parser.</S>\n    <S sid=\"106\" ssid=\"14\">We detected pronouns using a list.</S>\n    <S sid=\"107\" ssid=\"15\">The main query predicate is InClust(m, c!</S>\n    <S sid=\"108\" ssid=\"16\">), which is true iff mention m is in cluster c. The “t!” notation signifies that for each m, this predicate is true for a unique value of c. The main evidence predicate is Head(m, t!</S>\n    <S sid=\"109\" ssid=\"17\">), where m is a mention and t a token, and which is true iff t is the head of m. A key component in our MLN is a simple head mixture model, where the mixture component priors are represented by the unit clause InClust(+m, +c) and the head distribution is represented by the head prediction rule All free variables are implicitly universally quantified.</S>\n    <S sid=\"110\" ssid=\"18\">The “+” notation signifies that the MLN contains an instance of the rule, with a separate weight, for each value combination of the variables with a plus sign.</S>\n    <S sid=\"111\" ssid=\"19\">By convention, at each inference step we name each non-empty cluster after the earliest mention it contains.</S>\n    <S sid=\"112\" ssid=\"20\">This helps break the symmetry among mentions, which otherwise produces multiple optima and makes learning unnecessarily harder.</S>\n    <S sid=\"113\" ssid=\"21\">To encourage clustering, we impose an exponential prior on the number of non-empty clusters with weight −1.</S>\n    <S sid=\"114\" ssid=\"22\">The above model only clusters mentions with the same head, and does not work well for pronouns.</S>\n    <S sid=\"115\" ssid=\"23\">To address this, we introduce the predicate IsPrn(m), which is true iff the mention m is a pronoun, and adapt the head prediction rule as follows: This is always false when m is a pronoun, and thus applies only to non-pronouns.</S>\n    <S sid=\"116\" ssid=\"24\">Pronouns tend to resolve with mentions that are semantically compatible with them.</S>\n    <S sid=\"117\" ssid=\"25\">Thus we introduce predicates that represent entity type, number, and gender: Type(x, e!</S>\n    <S sid=\"118\" ssid=\"26\">), Number(x, n!</S>\n    <S sid=\"119\" ssid=\"27\">), Gender(x, g!</S>\n    <S sid=\"120\" ssid=\"28\">), where x can be either a cluster or mention, e E {Person,Organization,Location,Other}, n E {Singular,Plural} and g E {Male, Female, Neuter}.</S>\n    <S sid=\"121\" ssid=\"29\">Many of these are known for pronouns, and some can be inferred from simple linguistic cues (e.g., “Ms.</S>\n    <S sid=\"122\" ssid=\"30\">Galen” is a singular female person, while “XYZ Corp.” is an organization).3 Entity type assignment is represented by the unit clause Type(+x, +e) and similarly for number and gender.</S>\n    <S sid=\"123\" ssid=\"31\">A mention should agree with its cluster in entity type.</S>\n    <S sid=\"124\" ssid=\"32\">This is ensured by the hard rule (which has infinite weight and must be satisfied) InClust(m, c) ==&gt; (Type(m, e) &lt;---&gt; Type(c, e)) 3We used the following cues: Mr., Ms., Jr., Inc., Corp., corporation, company.</S>\n    <S sid=\"125\" ssid=\"33\">The proportions of known properties range from 14% to 26%.</S>\n    <S sid=\"126\" ssid=\"34\">There are similar hard rules for number and gender.</S>\n    <S sid=\"127\" ssid=\"35\">Different pronouns prefer different entity types, as represented by which only applies to pronouns, and whose weight is positive if pronoun t is likely to assume entity type e and negative otherwise.</S>\n    <S sid=\"128\" ssid=\"36\">There are similar rules for number and gender.</S>\n    <S sid=\"129\" ssid=\"37\">Aside from semantic compatibility, pronouns tend to resolve with nearby mentions.</S>\n    <S sid=\"130\" ssid=\"38\">To model this, we impose an exponential prior on the distance (number of mentions) between a pronoun and its antecedent, with weight −1.4 This is similar to Haghighi and Klein’s treatment of salience, but simpler.</S>\n    <S sid=\"131\" ssid=\"39\">Syntactic relations among mentions often suggest coreference.</S>\n    <S sid=\"132\" ssid=\"40\">Incorporating such relations into our MLN is straightforward.</S>\n    <S sid=\"133\" ssid=\"41\">We illustrate this with two examples: apposition and predicate nominals.</S>\n    <S sid=\"134\" ssid=\"42\">We introduce a predicate for apposition, Appo(x, y), where x, y are mentions, and which is true iff y is an appositive of x.</S>\n    <S sid=\"135\" ssid=\"43\">We then add the rule Appo(x, y) ==&gt; (InClust(x, c) &lt;---&gt; InClust(y, c)) which ensures that x, y are in the same cluster if y is an appositive of x.</S>\n    <S sid=\"136\" ssid=\"44\">Similarly, we introduce a predicate for predicate nominals, PredNom(x, y), and the corresponding rule.5 The weights of both rules can be learned from data with a positive prior mean.</S>\n    <S sid=\"137\" ssid=\"45\">For simplicity, in this paper we treat them as hard constraints.</S>\n    <S sid=\"138\" ssid=\"46\">We also consider a rule-based system that clusters non-pronouns by their heads, and attaches a pronoun to the cluster which has no known conflicting type, number, or gender, and contains the closest antecedent for the pronoun.</S>\n    <S sid=\"139\" ssid=\"47\">This system can be encoded in an MLN with just four rules.</S>\n    <S sid=\"140\" ssid=\"48\">Three of them are the ones for enforcing agreement in type, number, and gender between a cluster and its members, as defined in the base MLN.</S>\n    <S sid=\"141\" ssid=\"49\">The fourth rule is With a large but not infinite weight (e.g., 100), this rule has the effect of clustering non-pronouns by their heads, except when it violates the hard rules.</S>\n    <S sid=\"142\" ssid=\"50\">The MLN can also include the apposition and predicate-nominal rules.</S>\n    <S sid=\"143\" ssid=\"51\">As in the base MLN, we impose the same exponential prior on the number of non-empty clusters and that on the distance between a pronoun and its antecedent.</S>\n    <S sid=\"144\" ssid=\"52\">This simple MLN is remarkably competitive, as we will see in the experiment section.</S>\n  </SECTION>\n  <SECTION title=\"5 Learning and Inference\" number=\"5\">\n    <S sid=\"145\" ssid=\"1\">Unsupervised learning in Markov logic maximizes the conditional log-likelihood where Z are unknown predicates.</S>\n    <S sid=\"146\" ssid=\"2\">In our coreference resolution MLN, Y includes Head and known groundings of Type, Number and Gender, Z includes InClust and unknown groundings of Type, Number, Gender, and X includes IsPrn, Appo and PredNom.</S>\n    <S sid=\"147\" ssid=\"3\">(For simplicity, from now on we drop X from the formula.)</S>\n    <S sid=\"148\" ssid=\"4\">With Z, the optimization problem is no longer convex.</S>\n    <S sid=\"149\" ssid=\"5\">However, we can still find a local optimum using gradient descent, with the gradient being where ni is the number of true groundings of the ith clause.</S>\n    <S sid=\"150\" ssid=\"6\">We extended PSCG for unsupervised learning.</S>\n    <S sid=\"151\" ssid=\"7\">The gradient is the difference of two expectations, each of which can be approximated using samples generated by MC-SAT.</S>\n    <S sid=\"152\" ssid=\"8\">The (i, j)th entry of and the step size can be computed accordingly.</S>\n    <S sid=\"153\" ssid=\"9\">Since our problem is no longer convex, the negative diagonal Hessian may contain zero or negative entries, so we first took the absolute values of the diagonal and added 1, then used the inverse as the preconditioner.</S>\n    <S sid=\"154\" ssid=\"10\">We also adjusted A more conservatively than Lowd &amp; Domingos (2007).</S>\n    <S sid=\"155\" ssid=\"11\">Notice that when the objects form independent subsets (in our cases, mentions in each document), we can process them in parallel and then gather sufficient statistics for learning.</S>\n    <S sid=\"156\" ssid=\"12\">We developed an efficient parallelized implementation of our unsupervised learning algorithm using the message-passing interface (MPI).</S>\n    <S sid=\"157\" ssid=\"13\">Learning in MUC-6 took only one hour, and in ACE-2004 two and a half.</S>\n    <S sid=\"158\" ssid=\"14\">To reduce burn-in time, we initialized MC-SAT with the state returned by MaxWalkSAT (Kautz et al., 1997), rather than a random solution to the hard clauses.</S>\n    <S sid=\"159\" ssid=\"15\">In the existing implementation in Alchemy (Kok et al., 2007), SampleSAT flips only one atom in each step, which is inefficient for predicates with unique-value constraints (e.g., Head(m, c!)).</S>\n    <S sid=\"160\" ssid=\"16\">Such predicates can be viewed as multi-valued predicates (e.g., Head(m) with value ranging over all c’s) and are prevalent in NLP applications.</S>\n    <S sid=\"161\" ssid=\"17\">We adapted SampleSAT to flip two or more atoms in each step so that the unique-value constraints are automatically satisfied.</S>\n    <S sid=\"162\" ssid=\"18\">By default, MC-SAT treats each ground clause as a separate factor while determining the slice.</S>\n    <S sid=\"163\" ssid=\"19\">This can be very inefficient for highly correlated clauses.</S>\n    <S sid=\"164\" ssid=\"20\">For example, given a non-pronoun mention m currently in cluster c and with head t, among the mixture prior rules involving m InClust(m, c) is the only one that is satisfied, and among those head-prediction rules involving m, ¬IsPrn(m)∧InClust(m, c)∧Head(m, t) is the only one that is satisfied; the factors for these rules multiply to = exp(wm, + wm,�,t), where wm,r is the weight for InClust(m, c), and wm,�,t is the weight for ¬IsPrn(m) ∧ InClust(m, c) ∧ Head(m, t), since an unsatisfied rule contributes a factor of e0 = 1.</S>\n    <S sid=\"165\" ssid=\"21\">We extended MC-SAT to treat each set of mutually exclusive and exhaustive rules as a single factor.</S>\n    <S sid=\"166\" ssid=\"22\">E.g., for the above m, MC-SAT now samples u uniformly from (0, 0), and requires that in the next state 0' be no less than u. Equivalently, the new cluster and head for m should satisfy wm,c, + wm,c,,t, &gt; log(u).</S>\n    <S sid=\"167\" ssid=\"23\">We extended SampleSAT so that when it considers flipping any variable involved in such constraints (e.g., c or t above), it ensures that their new values still satisfy these constraints.</S>\n    <S sid=\"168\" ssid=\"24\">The final clustering is found using the MaxWalkSAT weighted satisfiability solver (Kautz et al., 1997), with the appropriate extensions.</S>\n    <S sid=\"169\" ssid=\"25\">We first ran a MaxWalkSAT pass with only finite-weight formulas, then ran another pass with all formulas.</S>\n    <S sid=\"170\" ssid=\"26\">We found that this significantly improved the quality of the results that MaxWalkSAT returned.</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments\" number=\"6\">\n    <S sid=\"171\" ssid=\"1\">We implemented our method as an extension to the Alchemy system (Kok et al., 2007).</S>\n    <S sid=\"172\" ssid=\"2\">Since our learning uses sampling, all results are the average of five runs using different random seeds.</S>\n    <S sid=\"173\" ssid=\"3\">Our optimization problem is not convex, so initialization is important.</S>\n    <S sid=\"174\" ssid=\"4\">The core of our model (head mixture) tends to cluster non-pronouns with the same head.</S>\n    <S sid=\"175\" ssid=\"5\">Therefore, we initialized by setting all weights to zero, and running the same learning algorithm on the base MLN, while assuming that in the ground truth, nonpronouns are clustered by their heads.</S>\n    <S sid=\"176\" ssid=\"6\">(Effectively, the corresponding InClust atoms are assigned to appropriate values and are included in Y rather than Z during learning.)</S>\n    <S sid=\"177\" ssid=\"7\">We used 30 iterations of PSCG for learning.</S>\n    <S sid=\"178\" ssid=\"8\">(In preliminary experiments, additional iterations had little effect on coreference accuracy.)</S>\n    <S sid=\"179\" ssid=\"9\">We generated 100 samples using MC-SAT for each expectation approximation.6 We conducted experiments on MUC-6, ACE-2004, and ACE Phrase-2 (ACE-2).</S>\n    <S sid=\"180\" ssid=\"10\">We evaluated our systems using two commonly-used scoring programs: MUC (Vilain et al., 1995) and B3 (Amit &amp; Baldwin, 1998).</S>\n    <S sid=\"181\" ssid=\"11\">To gain more insight, we also report pairwise resolution scores and mean absolute error in the number of clusters.</S>\n    <S sid=\"182\" ssid=\"12\">The MUC-6 dataset consists of 30 documents for testing and 221 for training.</S>\n    <S sid=\"183\" ssid=\"13\">To evaluate the contribution of the major components in our model, we conducted five experiments, each differing from the previous one in a single aspect.</S>\n    <S sid=\"184\" ssid=\"14\">We emphasize that our approach is unsupervised, and thus the data only contains raw text plus true mention boundaries.</S>\n    <S sid=\"185\" ssid=\"15\">MLN-1 In this experiment, the base MLN was used, and the head was chosen crudely as the rightmost token in a mention.</S>\n    <S sid=\"186\" ssid=\"16\">Our system was run on each test document separately, using a minimum of training data (the document itself).</S>\n    <S sid=\"187\" ssid=\"17\">MLN-30 Our system was trained on all 30 test documents together.</S>\n    <S sid=\"188\" ssid=\"18\">This tests how much can be gained by pooling information.</S>\n    <S sid=\"189\" ssid=\"19\">MLN-H The heads were determined using the head rules in the Stanford parser (Klein &amp; Manning, 2003), plus simple heuristics to handle suffixes such as “Corp.” and “Inc.” MLN-HA The apposition rule was added.</S>\n    <S sid=\"190\" ssid=\"20\">MLN-HAN The predicate-nominal rule was added.</S>\n    <S sid=\"191\" ssid=\"21\">This is our full model.</S>\n    <S sid=\"192\" ssid=\"22\">We also compared with two rule-based MLNs: RULE chose the head crudely as the rightmost token in a mention, and did not include the apposition rule and predicate-nominal rule; RULE-HAN chose the head using the head rules in the Stanford parser, and included the apposition rule and predicate-nominal rule.</S>\n    <S sid=\"193\" ssid=\"23\">Past results on ACE were obtained on different releases of the datasets, e.g., Haghighi and Klein (2007) used the ACE-2004 training corpus, Ng (2005) and Denis and Baldridge (2007) used ACE Phrase-2, and Culotta et al. (2007) used the ACE2004 formal test set.</S>\n    <S sid=\"194\" ssid=\"24\">In this paper, we used the ACE-2004 training corpus and ACE Phrase-2 (ACE2) to enable direct comparisons with Haghighi &amp; Klein (2007), Ng (2005), and Denis and Baldridge (2007).</S>\n    <S sid=\"195\" ssid=\"25\">Due to license restrictions, we were not able to obtain the ACE-2004 formal test set and so cannot compare directly to Culotta et al. (2007).</S>\n    <S sid=\"196\" ssid=\"26\">The English version of the ACE-2004 training corpus contains two sections, BNEWS and NWIRE, with 220 and 128 documents, respectively.</S>\n    <S sid=\"197\" ssid=\"27\">ACE-2 contains a training set and a test set.</S>\n    <S sid=\"198\" ssid=\"28\">In our experiments, we only used the test set, which contains three sections, BNEWS, NWIRE, and NPAPER, with 51, 29, and 17 documents, respectively.</S>\n  </SECTION>\n  <SECTION title=\"6.3 Results\" number=\"7\">\n    <S sid=\"199\" ssid=\"1\">Table 1 compares our system with previous approaches on the MUC-6 dataset, in MUC scores.</S>\n    <S sid=\"200\" ssid=\"2\">Our approach greatly outperformed Haghighi &amp; Klein (2007), the state-of-the-art unsupervised system.</S>\n    <S sid=\"201\" ssid=\"3\">Our system, trained on individual documents, achieved an F1 score more than 7% higher than theirs trained on 60 documents, and still outperformed it trained on 381 documents.</S>\n    <S sid=\"202\" ssid=\"4\">Training on the 30 test documents together resulted in a significant gain.</S>\n    <S sid=\"203\" ssid=\"5\">(We also ran experiments using more documents, and the results were similar.)</S>\n    <S sid=\"204\" ssid=\"6\">Better head identification (MLN-H) led to a large improvement in accuracy, which is expected since for mentions with a right modifier, the rightmost tokens confuse rather than help coreference (e.g., “the chairman of Microsoft”).</S>\n    <S sid=\"205\" ssid=\"7\">Notice that with this improvement our system already outperforms a state-of-theart supervised system (McCallum &amp; Wellner, 2005).</S>\n    <S sid=\"206\" ssid=\"8\">Leveraging apposition resulted in another large improvement, and predicate nominals also helped.</S>\n    <S sid=\"207\" ssid=\"9\">Our full model scores about 9% higher than Haghighi &amp; Klein (2007), and about 6% higher than McCallum &amp; Wellner (2005).</S>\n    <S sid=\"208\" ssid=\"10\">To our knowledge, this is the best coreference accuracy reported on MUC-6 to date.7 The B3 scores of MLN-HAN on the MUC-6 dataset are 77.4 (precision), 67.6 (recall) and 72.2 (F1).</S>\n    <S sid=\"209\" ssid=\"11\">(The other systems did not report B3.)</S>\n    <S sid=\"210\" ssid=\"12\">Interestingly, the rule-based MLN (RULE) sufficed to outperform Haghighi &amp; Klein (2007), and by using better heads and the apposition and predicate-nominal rules (RULE-HAN), it outperformed McCallum &amp; Wellner (2005), the supervised system.</S>\n    <S sid=\"211\" ssid=\"13\">The MLNs with learning (MLN-30 and MLN-HAN), on the other hand, substantially outperformed the corresponding rule-based ones.</S>\n    <S sid=\"212\" ssid=\"14\">Table 2 compares our system to Haghighi &amp; Klein (2007) on the ACE-2004 training set in MUC scores.</S>\n    <S sid=\"213\" ssid=\"15\">Again, our system outperformed theirs by a large margin.</S>\n    <S sid=\"214\" ssid=\"16\">The B3 scores of MLN-HAN on the ACE2004 dataset are 71.6 (precision), 68.4 (recall) and 70.0 (F1) for BNEWS, and 75.7 (precision), 69.2 (recall) and 72.3 (F1) for NWIRE.</S>\n    <S sid=\"215\" ssid=\"17\">(Haghighi &amp; Klein (2007) did not report B3.)</S>\n    <S sid=\"216\" ssid=\"18\">Due to license restrictions, we could not compare directly to Culotta et al. (2007), who reported overall B3-F1 of 79.3 on the formal test set.</S>\n    <S sid=\"217\" ssid=\"19\">Tables 3 and 4 compare our system to two recent supervised systems, Ng (2005) and Denis &amp; Baldridge (2007).</S>\n    <S sid=\"218\" ssid=\"20\">Our approach significantly outperformed Ng (2005).</S>\n    <S sid=\"219\" ssid=\"21\">It tied with Denis &amp; Baldridge (2007) on NWIRE, and was somewhat less accurate on BNEWS and NPAPER.</S>\n    <S sid=\"220\" ssid=\"22\">Luo et al. (2004) pointed out that one can obtain a very high MUC score simply by lumping all mentions together.</S>\n    <S sid=\"221\" ssid=\"23\">B3 suffers less from this problem but is not perfect.</S>\n    <S sid=\"222\" ssid=\"24\">Thus we also report pairwise resolution scores (Table 5), the gold number of clusters, and our mean absolute error in the number of clusters (Table 6).</S>\n    <S sid=\"223\" ssid=\"25\">Systems that simply merge all mentions will have exceedingly low pairwise precision (far below 50%), and very large errors in the number of clusters.</S>\n    <S sid=\"224\" ssid=\"26\">Our system has fairly good pairwise precisions and small mean error in the number of clusters, which verifies that our results are sound.</S>\n    <S sid=\"225\" ssid=\"27\">Many of our system’s remaining errors involve nominals.</S>\n    <S sid=\"226\" ssid=\"28\">Additional features should be considered to distinguish mentions that have the same head but are different entities.</S>\n    <S sid=\"227\" ssid=\"29\">For pronouns, many remaining errors can be corrected using linguistic knowledge like binding theory and salience hierarchy.</S>\n    <S sid=\"228\" ssid=\"30\">Our heuristics for identifying appositives and predicate nominals also make many errors, which often can be fixed with additional name entity recognition capabilities (e.g., given “Mike Sullivan, VOA News”, it helps to know that the former is a person and the latter an organization).</S>\n    <S sid=\"229\" ssid=\"31\">The most challenging case involves phrases with different heads that are both proper nouns (e.g., “Mr.</S>\n    <S sid=\"230\" ssid=\"32\">Bush” and “the White House”).</S>\n    <S sid=\"231\" ssid=\"33\">Handling these cases requires domain knowledge and/or more powerful joint inference.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"8\">\n    <S sid=\"232\" ssid=\"1\">This paper introduces the first unsupervised coreference resolution system that is as accurate as supervised systems.</S>\n    <S sid=\"233\" ssid=\"2\">It performs joint inference among mentions, using relations like apposition and predicate nominals.</S>\n    <S sid=\"234\" ssid=\"3\">It uses Markov logic as a representation language, which allows it to be easily extended to incorporate additional linguistic and world knowledge.</S>\n    <S sid=\"235\" ssid=\"4\">Future directions include incorporating additional knowledge, conducting joint entity detection and coreference resolution, and combining coreference resolution with other NLP tasks.</S>\n  </SECTION>\n  <SECTION title=\"8 Acknowledgements\" number=\"9\">\n    <S sid=\"236\" ssid=\"1\">We thank the anonymous reviewers for their comments.</S>\n    <S sid=\"237\" ssid=\"2\">This research was funded by DARPA contracts NBCHD030010/02-000225, FA8750-07-D-0185, and HR001107-C-0060, DARPA grant FA8750-05-2-0283, NSF grant IIS-0534881, and ONR grant N-00014-05-1-0313 and N00014-08-1-0670.</S>\n    <S sid=\"238\" ssid=\"3\">The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, NSF, ONR, or the United States Government.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "fbfea2202236ea96a440edafe17be5d5",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Joint Unsupervised Coreference Resolution with Markov Logic"
                }
            }
        },
        {
            "index": 7,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different For the German–English speech we analyze the effect of various syscomponents. On the French–English Canadian the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.",
                    "filename": "The_Alignment_Template_Approach_To_Statistical_Machine_Translation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J04-4002/Documents_xml/J04-4002.xml",
                    "title": "The Alignment Template Approach To Statistical Machine Translation"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents the alignment template approach to statistical machine translation (SMT), which is more flexible and powerful than traditional word-based methods. It allows for many-to-many relationships between words, capturing word context and explicit learning of local word order changes. The authors describe a log-linear modeling approach, which facilitates the integration of multiple knowledge sources and eases system extension.\n\nThe alignment template system is evaluated on German-English, French-English, and Chinese-English translation tasks, showing significant improvements over single-word-based models. For the German-English VERBMOBiL task, the system's various components are analyzed. The French-English Canadian HANSARDS task sees the alignment template system achieving better results than single-word-based models. In the Chinese-English NIST machine translation evaluation, the system outperforms all competing research and commercial systems, indicating its effectiveness across different language pairs.\n\nThe paper details the learning process of phrasal translations, feature functions used, and the search algorithm implemented. Additionally, the authors discuss potential improvements, such as incorporating recursive structures of natural languages and learning nonconsecutive phrases for enhanced generalization.",
                        "Abstract:\n\nThis paper presents an innovative phrase-based statistical machine translation approach known as the alignment template approach. Unlike traditional methods that rely on word-to-word correspondence, this approach supports flexible many-to-many relationships between words, considering the context of words and explicitly learning local changes in word order between source and target languages. The alignment template model is based on a log-linear framework, which allows for the easy integration of additional feature functions and is described using a log-linear modeling approach, generalizing the source-channel approach.\n\nThe paper discusses the learning process for phrasal translations, the feature functions used, and the search algorithm implemented. It evaluates the alignment template approach on three distinct tasks: German–English speech translation, French–English Canadian HANSARDS, and Chinese–English translation in the NIST 2002 evaluation. The approach outperformed single-word-based translation models and achieved significantly better results than competing research and commercial systems.\n\nThe alignment template system is fully data-driven and does not require linguistically annotated corpora, offering an important advantage over syntax-based translation models. The paper also suggests that future improvements may come from learning nonconsecutive phrases and better generalization methods for learned phrase pairs, moving towards models that account for the recursive structure of natural languages.",
                        "This academic paper presents the alignment template approach to statistical machine translation (SMT), which enables phrase-based SMT by allowing many-to-many relations between words and considering the context in translation models. The approach employs a log-linear modeling framework, facilitating ease of extension and integration of various features. The paper details the learning of phrasal translations, feature functions, and the search algorithm used to find the most probable translation.\n\nThe evaluation of the approach is conducted on three translation tasks: German–English (VERBMOBiL task), French–English (Canadian HANSARDS task), and Chinese–English (NIST 2002 evaluation). The alignment template system shows significantly better results than single-word-based models and outperforms other research and commercial systems in the NIST evaluation.\n\nThe paper also discusses the challenges inherent in machine translation, such as the complexity of natural languages, the ambiguity of word meanings, and the necessity to make decisions with incomplete knowledge. The authors suggest that statistical decision theory is an effective means to address these challenges. They elaborate on the methods for learning statistical alignment models, the construction of phrase translation lexica, and the implementation of a dynamic-programming-based beam search algorithm for translation.\n\nThe authors conclude that their approach is effective for different language pairs and offers potential for further improvements by considering the recursive structure of natural languages and better generalization methods for learned phrase pairs.",
                        "This academic paper introduces the alignment template approach to statistical machine translation (SMT), a phrase-based approach that accounts for the context of words and allows for learning explicit local changes in word order between source and target languages. The approach is based on a log-linear modeling framework, which simplifies system extension. The paper describes the learning process for phrasal translations, the feature functions used, and the search algorithm. Evaluation is conducted on three tasks: German–English speech (VERBMOBiL), French–English Canadian HANSARDS, and Chinese–English National Institute of Standards and Technology (NIST) machine translation evaluation, with the alignment template system outperforming single-word-based models in all cases.\n\nThe paper also discusses the challenges of machine translation, such as the complexity of natural languages and the need for decisions based on incomplete knowledge. It emphasizes the use of statistical techniques to improve system quality, highlighting the role of large training data sets and improved modeling and search methods. The paper presents detailed methodologies for learning word alignments, extracting phrasal translations, and applying alignment templates, stressing the data-driven nature of the model and the lack of need for linguistically annotated corpora. The search algorithm described employs a dynamic-programming-based beam search allowing for a balance between efficiency and quality.\n\nOverall, the paper contributes to SMT by illustrating the effectiveness of the alignment template approach, providing a detailed account of its methodology, and showcasing its superior performance across different language pairs and tasks."
                    ],
                    "filename": "the_alignment_template_approach_-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 169 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">The Alignment Template Approach To Statistical Machine Translation</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"10\" ssid=\"1\">A phrase-based statistical machine translation approach — the alignment template approach — is described.</S>\n    <S sid=\"11\" ssid=\"2\">This translation approach allows for general many-to-many relations between words.</S>\n    <S sid=\"12\" ssid=\"3\">Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly.</S>\n    <S sid=\"13\" ssid=\"4\">The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach.</S>\n    <S sid=\"14\" ssid=\"5\">Thereby, the model is easier to extend than classical statistical machine translation systems.</S>\n    <S sid=\"15\" ssid=\"6\">We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm.</S>\n    <S sid=\"16\" ssid=\"7\">The evaluation of this approach is performed on three different tasks.</S>\n    <S sid=\"17\" ssid=\"8\">For the German–English speech VERBMOBiL task, we analyze the effect of various system components.</S>\n    <S sid=\"18\" ssid=\"9\">On the French–English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model.</S>\n    <S sid=\"19\" ssid=\"10\">In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"20\" ssid=\"1\">Machine translation (MT) is a hard problem, because natural languages are highly complex, many words have various meanings and different possible translations, sentences might have various readings, and the relationships between linguistic entities are often vague.</S>\n    <S sid=\"21\" ssid=\"2\">In addition, it is sometimes necessary to take world knowledge into account.</S>\n    <S sid=\"22\" ssid=\"3\">The number of relevant dependencies is much too large and those dependencies are too complex to take them all into account in a machine translation system.</S>\n    <S sid=\"23\" ssid=\"4\">Given these boundary conditions, a machine translation system has to make decisions (produce translations) given incomplete knowledge.</S>\n    <S sid=\"24\" ssid=\"5\">In such a case, a principled approach to solving that problem is to use the concepts of statistical decision theory to try to make optimal decisions given incomplete knowledge.</S>\n    <S sid=\"25\" ssid=\"6\">This is the goal of statistical machine translation.</S>\n    <S sid=\"26\" ssid=\"7\">The use of statistical techniques in machine translation has led to dramatic improvements in the quality of research systems in recent years.</S>\n    <S sid=\"27\" ssid=\"8\">For example, the statistical approaches of the VERBMOBiL evaluations (Wahlster 2000) or the U.S. National Institute of Standards and Technology (NIST)/TIDES MT evaluations 2001 through 20031 obtain the best results.</S>\n    <S sid=\"28\" ssid=\"9\">In addition, the field of statistical machine translation is rapidly progressing, and the quality of systems is getting better and better.</S>\n    <S sid=\"29\" ssid=\"10\">An important factor in these improvements is definitely the availability of large amounts of data for training statistical models.</S>\n    <S sid=\"30\" ssid=\"11\">Yet the modeling, training, and search methods have also improved since the field of statistical machine translation was pioneered by IBM in the late 1980s and early 1990s (Brown et al. 1990; Brown et al.</S>\n    <S sid=\"31\" ssid=\"12\">1993; Berger et al. 1994).</S>\n    <S sid=\"32\" ssid=\"13\">This article focuses on an important improvement, namely, the use of (generalized) phrases instead of just single words as the core elements of the statistical translation model.</S>\n    <S sid=\"33\" ssid=\"14\">We describe in Section 2 the basics of our statistical translation model.</S>\n    <S sid=\"34\" ssid=\"15\">We suggest the use of a log-linear model to incorporate the various knowledge sources into an overall translation system and to perform discriminative training of the free model parameters.</S>\n    <S sid=\"35\" ssid=\"16\">This approach can be seen as a generalization of the originally suggested source–channel modeling framework for statistical machine translation.</S>\n    <S sid=\"36\" ssid=\"17\">In Section 3, we describe the statistical alignment models used to obtain a word alignment and techniques for learning phrase translations from word alignments.</S>\n    <S sid=\"37\" ssid=\"18\">Here, the term phrase just refers to a consecutive sequence of words occurring in text and has to be distinguished from the use of the term in a linguistic sense.</S>\n    <S sid=\"38\" ssid=\"19\">The learned bilingual phrases are not constrained by linguistic phrase boundaries.</S>\n    <S sid=\"39\" ssid=\"20\">Compared to the word-based statistical translation models in Brown et al. (1993), this model is based on a (statistical) phrase lexicon instead of a single-word-based lexicon.</S>\n    <S sid=\"40\" ssid=\"21\">Looking at the results of the recent machine translation evaluations, this approach seems currently to give the best results, and an increasing number of researchers are working on different methods for learning phrase translation lexica for machine translation purposes (Marcu and Wong 2002; Venugopal, Vogel, and Waibel 2003; Tillmann 2003; Koehn, Och, and Marcu 2003).</S>\n    <S sid=\"41\" ssid=\"22\">Our approach to learning a phrase translation lexicon works in two stages: In the first stage, we compute an alignment between words, and in the second stage, we extract the aligned phrase pairs.</S>\n    <S sid=\"42\" ssid=\"23\">In our machine translation system, we then use generalized versions of these phrases, called alignment templates, that also include the word alignment and use word classes instead of the words themselves.</S>\n    <S sid=\"43\" ssid=\"24\">In Section 4, we describe the various components of the statistical translation model.</S>\n    <S sid=\"44\" ssid=\"25\">The backbone of the translation model is the alignment template feature function, which requires that a translation of a new sentence be composed of a set of alignment templates that covers the source sentence and the produced translation.</S>\n    <S sid=\"45\" ssid=\"26\">Other feature functions score the well-formedness of the produced target language sentence (i.e., language model feature functions), the number of produced words, or the order of the alignment templates.</S>\n    <S sid=\"46\" ssid=\"27\">Note that all components of our statistical machine translation model are purely data-driven and that there is no need for linguistically annotated corpora.</S>\n    <S sid=\"47\" ssid=\"28\">This is an important advantage compared to syntax-based translation models (Yamada and Knight 2001; Gildea 2003; Charniak, Knight, and Yamada 2003) that require a parser for source or target language.</S>\n    <S sid=\"48\" ssid=\"29\">In Section 5, we describe in detail our search algorithm and discuss an efficient implementation.</S>\n    <S sid=\"49\" ssid=\"30\">We use a dynamic-programming-based beam search algorithm that allows a trade-off between efficiency and quality.</S>\n    <S sid=\"50\" ssid=\"31\">We also discuss the use of heuristic functions to reduce the number of search errors for a fixed beam size.</S>\n    <S sid=\"51\" ssid=\"32\">In Section 6, we describe various results obtained on different tasks.</S>\n    <S sid=\"52\" ssid=\"33\">For the German–English VERBMOBiL task, we analyze the effect of various system compoArchitecture of the translation approach based on a log-linear modeling approach. nents.</S>\n    <S sid=\"53\" ssid=\"34\">On the French–English Canadian HANSARDS task, the alignment template system obtains significantly better results than a single-word-based translation model.</S>\n    <S sid=\"54\" ssid=\"35\">In the Chinese–English 2002 NIST machine translation evaluation it yields results that are significantly better statistically than all competing research and commercial translation systems.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"55\" ssid=\"1\">We are given a source (French) sentence f =f1J = f1, ... ,fj, ... , fJ, which is to be translated into a target (English) sentence e = eI1 = e1, ... , ei, ... , eI.</S>\n    <S sid=\"56\" ssid=\"2\">Among all possible target sentences, we will choose the sentence with the highest probability:2 The argmax operation denotes the search problem, that is, the generation of the output sentence in the target language.</S>\n    <S sid=\"57\" ssid=\"3\">As an alternative to the often used source–channel approach (Brown et al. 1993), we directly model the posterior probability Pr(eI1  |f J1) (Och and Ney 2002).</S>\n    <S sid=\"58\" ssid=\"4\">An especially well-founded framework for doing this is the maximum-entropy framework (Berger, Della Pietra, and Della Pietra 1996).</S>\n    <S sid=\"59\" ssid=\"5\">In this framework, we have a set of M feature functions hm(eI1,fJ1), m = 1, ... , M. For each feature function, there exists a model 2 The notational convention employed in this article is as follows.</S>\n    <S sid=\"60\" ssid=\"6\">We use the symbol Pr(·) to denote general probability distributions with (nearly) no specific assumptions.</S>\n    <S sid=\"61\" ssid=\"7\">In contrast, for model-based probability distributions, we use the generic symbol p(·).</S>\n    <S sid=\"62\" ssid=\"8\">This approach has been suggested by Papineni, Roukos, and Ward (1997, 1998) for a natural language understanding task.</S>\n    <S sid=\"63\" ssid=\"9\">We obtain the following decision rule: Hence, the time-consuming renormalization in equation (3) is not needed in search.</S>\n    <S sid=\"64\" ssid=\"10\">The overall architecture of the log-linear modeling approach is summarized in Figure 1.</S>\n    <S sid=\"65\" ssid=\"11\">A standard criterion on a parallel training corpus consisting of S sentence pairs {(fs, es): s = 1,. .</S>\n    <S sid=\"66\" ssid=\"12\">.</S>\n    <S sid=\"67\" ssid=\"13\">, S} for log-linear models is the maximum class posterior probability criterion, which can be derived from the maximum-entropy principle: This corresponds to maximizing the equivocation or maximizing the likelihood of the direct-translation model.</S>\n    <S sid=\"68\" ssid=\"14\">This direct optimization of the posterior probability in Bayes’ decision rule is referred to as discriminative training (Ney 1995) because we directly take into account the overlap in the probability distributions.</S>\n    <S sid=\"69\" ssid=\"15\">The optimization problem under this criterion has very nice properties: There is one unique global optimum, and there are algorithms (e.g. gradient descent) that are guaranteed to converge to the global optimum.</S>\n    <S sid=\"70\" ssid=\"16\">Yet the ultimate goal is to obtain good translation quality on unseen test data.</S>\n    <S sid=\"71\" ssid=\"17\">An alternative training criterion therefore directly optimizes translation quality as measured by an automatic evaluation criterion (Och 2003).</S>\n    <S sid=\"72\" ssid=\"18\">Typically, the translation probability Pr(eI1  |f J1) is decomposed via additional hidden variables.</S>\n    <S sid=\"73\" ssid=\"19\">To include these dependencies in our log-linear model, we extend the feature functions to include the dependence on the additional hidden variable.</S>\n    <S sid=\"74\" ssid=\"20\">Using for example the alignment aJ1 as hidden variable, we obtain M feature functions of the form hm(eI1,f J 1, aJ1), m = 1, ... , M and the following model: Obviously, we can perform the same step for translation models with an even richer set of hidden variables than only the alignment aJ1.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"75\" ssid=\"1\">In this section, we describe methods for learning the single-word and phrase-based translation lexica that are the basis of the machine translation system described in Section 4.</S>\n    <S sid=\"76\" ssid=\"2\">First, we introduce the basic concepts of statistical alignment models, which are used to learn word alignment.</S>\n    <S sid=\"77\" ssid=\"3\">Then, we describe how these alignments can be used to learn bilingual phrasal translations.</S>\n    <S sid=\"78\" ssid=\"4\">In (statistical) alignment models Pr(f J 1,aJ 1 eI1), a “hidden” alignment a = aJ1 is introduced that describes a mapping from a source position j to a target position aj.</S>\n    <S sid=\"79\" ssid=\"5\">The relationship between the translation model and the alignment model is given by The alignment aJ1 may contain alignments aj = 0 with the “empty” word e0 to account for source words that are not aligned with any target word.</S>\n    <S sid=\"80\" ssid=\"6\">In general, the statistical model depends on a set of unknown parameters θ that is learned from training data.</S>\n    <S sid=\"81\" ssid=\"7\">To express the dependence of the model on the parameter set, we use the following notation: A detailed description of different specific statistical alignment models can be found in Brown et al. (1993) and Och and Ney (2003).</S>\n    <S sid=\"82\" ssid=\"8\">Here, we use the hidden Markov model (HMM) alignment model (Vogel, Ney, and Tillmann 1996) and Model 4 of Brown et al. (1993) to compute the word alignment for the parallel training corpus.</S>\n    <S sid=\"83\" ssid=\"9\">To train the unknown parameters θ, we are given a parallel training corpus consisting of S sentence pairs j(fs, es): s = 1, ... , Sj.</S>\n    <S sid=\"84\" ssid=\"10\">For each sentence pair (fs, es), the alignment variable is denoted by a = aJ1.</S>\n    <S sid=\"85\" ssid=\"11\">The unknown parameters θ are determined by maximizing the likelihood on the parallel training corpus: This optimization can be performed using the expectation maximization (EM) algorithm (Dempster, Laird, and Rubin 1977).</S>\n    <S sid=\"86\" ssid=\"12\">For a given sentence pair there are a large number of alignments.</S>\n    <S sid=\"87\" ssid=\"13\">The alignment ˆaJ1 that has the highest probability (under a certain model) is also called the Viterbi alignment (of that model): A detailed comparison of the quality of these Viterbi alignments for various statistical alignment models compared to human-made word alignments can be found in Och and Ney (2003).</S>\n    <S sid=\"88\" ssid=\"14\">The baseline alignment model does not allow a source word to be aligned with two or more target words.</S>\n    <S sid=\"89\" ssid=\"15\">Therefore, lexical correspondences like the German compound word Zahnarzttermin for dentist’s appointment cause problems because a single source word must be mapped onto two or more target words.</S>\n    <S sid=\"90\" ssid=\"16\">Therefore, the resulting Viterbi alignment of the standard alignment models has a systematic loss in recall.</S>\n    <S sid=\"91\" ssid=\"17\">Here, we Example of a (symmetrized) word alignment (VERBMOBIL task). describe various methods for performing a symmetrization of our directed statistical alignment models by applying a heuristic postprocessing step that combines the alignments in both translation directions (source to target, target to source).</S>\n    <S sid=\"92\" ssid=\"18\">Figure 2 shows an example of a symmetrized alignment.</S>\n    <S sid=\"93\" ssid=\"19\">To solve this problem, we train in both translation directions.</S>\n    <S sid=\"94\" ssid=\"20\">For each sentence pair, we compute two Viterbi alignments aJ1 and bI1.</S>\n    <S sid=\"95\" ssid=\"21\">Let A1 = f(aj, j)  |aj &gt; 01 and A2 = f(i, bi)  |bi &gt; 01 denote the sets of alignments in the two Viterbi alignments.</S>\n    <S sid=\"96\" ssid=\"22\">To increase the quality of the alignments, we can combine (symmetrize) A1 and A2 into one alignment matrix A using one of the following combination methods: alignment A1 or in the alignment A2 if neither fj nor ei have an alignment in A, or if the following conditions both hold: Obviously, the intersection yields an alignment consisting of only one-to-one alignments with a higher precision and a lower recall.</S>\n    <S sid=\"97\" ssid=\"23\">The union yields a higher recall and a lower precision of the combined alignment.</S>\n    <S sid=\"98\" ssid=\"24\">The refined alignment method is often able to improve precision and recall compared to the nonsymmetrized alignments.</S>\n    <S sid=\"99\" ssid=\"25\">Whether a higher precision or a higher recall is preferred depends on the final application of the word alignment.</S>\n    <S sid=\"100\" ssid=\"26\">For the purpose of statistical MT, it seems that a higher recall is more important.</S>\n    <S sid=\"101\" ssid=\"27\">Therefore, we use the union or the refined combination method to obtain a symmetrized alignment matrix.</S>\n    <S sid=\"102\" ssid=\"28\">The resulting symmetrized alignments are then used to train single-word-based translation lexica p(e  |f) by computing relative frequencies using the count N(e,f) of how many times e and f are aligned divided by the count N(f ) of how many times the word f occurs: In this section, we present a method for learning relationships between whole phrases of m source language words and n target language words.</S>\n    <S sid=\"103\" ssid=\"29\">This algorithm, which will be called phrase-extract, takes as input a general word alignment matrix (Section 3.2).</S>\n    <S sid=\"104\" ssid=\"30\">The output is a set of bilingual phrases.</S>\n    <S sid=\"105\" ssid=\"31\">In the following, we describe the criterion that defines the set of phrases that is consistent with the word alignment matrix: Hence, the set of all bilingual phrases that are consistent with the alignment is constituted by all bilingual phrase pairs in which all words within the source language phrase are aligned only with the words of the target language phrase and the words of the target language phrase are aligned only with the words of the source language phrase.</S>\n    <S sid=\"106\" ssid=\"32\">Note that we require that at least one word in the source language phrase be aligned with at least one word of the target language phrase.</S>\n    <S sid=\"107\" ssid=\"33\">As a result there are no empty source or target language phrases that would correspond to the “empty word” of the word-based statistical alignment models.</S>\n    <S sid=\"108\" ssid=\"34\">These phrases can be computed straightforwardly by enumerating all possible phrases in one language and checking whether the aligned words in the other language are consecutive, with the possible exception of words that are not aligned at all.</S>\n    <S sid=\"109\" ssid=\"35\">Figure 3 gives the algorithm phrase-extract that computes the phrases.</S>\n    <S sid=\"110\" ssid=\"36\">The algorithm takes into account possibly unaligned words at the boundaries of the source or target language phrases.</S>\n    <S sid=\"111\" ssid=\"37\">Table 1 shows the bilingual phrases containing between two and seven words that result from the application of this algorithm to the alignment of Figure 2.</S>\n    <S sid=\"112\" ssid=\"38\">Examples of two- to seven-word bilingual phrases obtained by applying the algorithm phrase-extract to the alignment of Figure 2. ja , yes , ja , ich yes , I ja , ich denke mal yes , I think ja , ich denke mal , yes , I think , ja , ich denke mal , also yes, I think, well ,ich , I , ich denke mal , I think , ich denke mal, , I think , , ich denke mal, also , I think, well , ich denke mal, also wir , I think, well we ich denke mal I think ich denke mal, I think, ich denke mal, also I think, well ich denke mal, also wir I think, well we ich denke mal , also wir wollten I think, well we plan to denke mal, think , denke mal , also think, well denke mal , also wir think, well we denke mal, also wir wollten think, well we plan to , also , well , also wir , well we , also wir wollten , well we plan to also wir well we also wir wollten well we plan to wir wollten we plan to in unserer in our in unserer Abteilung in our department in unserer Abteilung ein neues Netzwerk a new network in our department in unserer Abteilung ein neues Netzwerk set up a new network in our department aufbauen unserer Abteilung our department ein neues a new ein neues Netzwerk a new network ein neues Netzwerk aufbauen set up a new network neues Netzwerk new network It should be emphasized that this constraint to consecutive phrases limits the expressive power.</S>\n    <S sid=\"113\" ssid=\"39\">If a consecutive phrase in one language is translated into two or three nonconsecutive phrases in the other language, there is no corresponding bilingual phrase pair learned by this approach.</S>\n    <S sid=\"114\" ssid=\"40\">In principle, this approach to learning phrases from a word-aligned corpus could be extended straightforwardly to handle nonconsecutive phrases in source and target language as well.</S>\n    <S sid=\"115\" ssid=\"41\">Informal experiments have shown that allowing for nonconsecutive phrases significantly increases the number of extracted phrases and especially increases the percentage of wrong phrases.</S>\n    <S sid=\"116\" ssid=\"42\">Therefore, we consider only consecutive phrases.</S>\n    <S sid=\"117\" ssid=\"43\">In the following, we add generalization capability to the bilingual phrase lexicon by replacing words with word classes and also by storing the alignment information for each phrase pair.</S>\n    <S sid=\"118\" ssid=\"44\">These generalized and alignment-annotated phrase pairs are called alignment templates.</S>\n    <S sid=\"119\" ssid=\"45\">Formally, an alignment template z is a triple (FJy1 , EI�1 , ˜A) Algorithm phrase-extract for extracting phrases from a word-aligned sentence pair.</S>\n    <S sid=\"120\" ssid=\"46\">Here quasi-consecutive(TP) is a predicate that tests whether the set of words TP is consecutive, with the possible exception of words that are not aligned. that describes the alignment A˜ between a source class sequence FJy1 and a target class sequence EI�1 .</S>\n    <S sid=\"121\" ssid=\"47\">If each word corresponds to one class, an alignment template corresponds to a bilingual phrase together with an alignment within this phrase.</S>\n    <S sid=\"122\" ssid=\"48\">Figure 4 shows examples of alignment templates.</S>\n    <S sid=\"123\" ssid=\"49\">The alignment A˜ is represented as a matrix with J' · (I' + 1) binary elements.</S>\n    <S sid=\"124\" ssid=\"50\">A matrix element with value 1 means that the words at the corresponding positions are aligned, and the value 0 means that the words are not aligned.</S>\n    <S sid=\"125\" ssid=\"51\">If a source word is not aligned with a target word, then it is aligned with the empty word e0, which is at the imaginary position i = 0.</S>\n    <S sid=\"126\" ssid=\"52\">The classes used in FJy1 and EI�1 are automatically trained bilingual classes using the method described in Och (1999) and constitute a partition of the vocabulary of source and target language.</S>\n    <S sid=\"127\" ssid=\"53\">In general, we are not limited to disjoint classes as long as each specific instance of a word is disambiguated, that is, uniquely belongs to a specific class.</S>\n    <S sid=\"128\" ssid=\"54\">In the following, we use the class function C to map words to their classes.</S>\n    <S sid=\"129\" ssid=\"55\">Hence, it would be possible to employ parts-of-speech or semantic categories instead of the automatically trained word classes used here.</S>\n    <S sid=\"130\" ssid=\"56\">The use of classes instead of the words themselves has the advantage of better generalization.</S>\n    <S sid=\"131\" ssid=\"57\">For example, if there exist classes in source and target language that contain town names, it is possible that an alignment template learned using a specific town name can be generalized to other town names.</S>\n    <S sid=\"132\" ssid=\"58\">In the following, e˜ and f˜ denote target and source phrases, respectively.</S>\n    <S sid=\"133\" ssid=\"59\">To train the probability of applying an alignment template p(z = (FJy1 , EI~1 , ˜A)  |f˜), we use an extended version of the algorithm phrase-extract from Section 3.3.</S>\n    <S sid=\"134\" ssid=\"60\">All bilingual phrases that are consistent with the alignment are extracted together with the alignment within this bilingual phrase.</S>\n    <S sid=\"135\" ssid=\"61\">Thus, we obtain a count N(z) of how often an alignment template occurred in the aligned training corpus.</S>\n    <S sid=\"136\" ssid=\"62\">The probability of using an alignment template to translate a specific source language phrase f˜ is estimated by means of relative frequency: To reduce the memory requirement of the alignment templates, we compute these probabilities only for phrases up to a certain maximal length in the source language.</S>\n    <S sid=\"137\" ssid=\"63\">Depending on the size of the corpus, the maximal length in the experiments is between four and seven words.</S>\n    <S sid=\"138\" ssid=\"64\">In addition, we remove alignment templates that have a probability lower than a certain threshold.</S>\n    <S sid=\"139\" ssid=\"65\">In the experiments, we use a threshold of 0.01.</S>\n    <S sid=\"140\" ssid=\"66\">It should be emphasized that this algorithm for computing aligned phrase pairs and their associated probabilities is very easy to implement.</S>\n    <S sid=\"141\" ssid=\"67\">The joint translation model suggested by Marcu and Wong (2002) tries to learn phrases as part of a full EM algorithm, which leads to very large memory requirements and a rather complicated training algorithm.</S>\n    <S sid=\"142\" ssid=\"68\">A comparison of the two approaches can be found in Koehn, Och, and Marcu (2003).</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"143\" ssid=\"1\">To describe our translation model based on the alignment templates described in the previous section in a formal way, we first decompose both the source sentence f1J and the target sentence eI1 into a sequence of phrases (k = 1,...,K): Note that there are a large number of possible segmentations of a sentence pair into K phrase pairs.</S>\n    <S sid=\"144\" ssid=\"2\">In the following, we will describe the model for a specific segmentation.</S>\n    <S sid=\"145\" ssid=\"3\">Eventually, however, a model can be described in which the specific segmentation is not known when new text is translated.</S>\n    <S sid=\"146\" ssid=\"4\">Hence, as part of the overall search process (Section 5), we also search for the optimal segmentation.</S>\n    <S sid=\"147\" ssid=\"5\">To allow possible reordering of phrases, we introduce an alignment on the phrase level πK1 between the source phrases f˜1K and the target phrases ˜eK1.</S>\n    <S sid=\"148\" ssid=\"6\">Hence, πK1 is a permutation of the phrase positions 1, ... , K and indicates that the phrases ˜ek and ˜fπk are translations of one another.</S>\n    <S sid=\"149\" ssid=\"7\">We assume that for the translation between these phrases a specific alignment template zk is used: ˜ek zk ˜fπk ←→ Hence, our model has the following hidden variables: Figure 5 gives an example of the word alignment and phrase alignment of a German–English sentence pair.</S>\n    <S sid=\"150\" ssid=\"8\">We describe our model using a log-linear modeling approach.</S>\n    <S sid=\"151\" ssid=\"9\">Hence, all knowledge sources are described as feature functions that include the given source language string f J1, the target language string eI1, and the above-stated hidden variables.</S>\n    <S sid=\"152\" ssid=\"10\">Hence, we have the following functional form of all feature functions: Figure 6 gives an overview of the decisions made in the alignment template model.</S>\n    <S sid=\"153\" ssid=\"11\">First, the source sentence words fJ1 are grouped into phrases f˜1K.</S>\n    <S sid=\"154\" ssid=\"12\">For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to πK1 ).</S>\n    <S sid=\"155\" ssid=\"13\">Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z).</S>\n    <S sid=\"156\" ssid=\"14\">Finally, the sequence of phrases ˜eK1 constitutes the sequence of words eI1.</S>\n    <S sid=\"157\" ssid=\"15\">Dependencies in the alignment template model.</S>\n    <S sid=\"158\" ssid=\"16\">Och and Ney The Alignment Template Approach to Statistical Machine Translation 4.1.1 Alignment Template Selection.</S>\n    <S sid=\"159\" ssid=\"17\">To score the use of an alignment template, we use the probability p(z  |f˜) defined in Section 3.</S>\n    <S sid=\"160\" ssid=\"18\">We establish a corresponding feature Here, jπk−1 + 1 is the position of the first word of alignment template zk in the source language sentence and jπk is the position of the last word of that alignment template.</S>\n    <S sid=\"161\" ssid=\"19\">Note that this feature function requires that a translation of a new sentence be composed of a set of alignment templates that covers both the source sentence and the produced translation.</S>\n    <S sid=\"162\" ssid=\"20\">There is no notion of “empty phrase” that corresponds to the “empty word” in word-based statistical alignment models.</S>\n    <S sid=\"163\" ssid=\"21\">The alignment on the phrase level is actually a permutation, and no insertions or deletions are allowed.</S>\n    <S sid=\"164\" ssid=\"22\">4.1.2 Word Selection.</S>\n    <S sid=\"165\" ssid=\"23\">For scoring the use of target language words, we use a lexicon probability p(e  |f), which is estimated using relative frequencies as described in Section 3.2.</S>\n    <S sid=\"166\" ssid=\"24\">The target word e depends on the aligned source words.</S>\n    <S sid=\"167\" ssid=\"25\">If we denote the resulting word alignment matrix by A := AπKAK and the predicted word class for word For p(ei  |{fj  |(i,j) ∈ A}) we use a uniform mixture of a single-word model p(e  |f), which is constrained to predict only words that are in the predicted word class Ei: A disadvantage of this model is that the word order is ignored in the translation model.</S>\n    <S sid=\"168\" ssid=\"26\">The translations the day after tomorrow or after the day tomorrow for the German word ¨ubermorgen receive an identical probability.</S>\n    <S sid=\"169\" ssid=\"27\">Yet the first one should obtain a significantly higher probability.</S>\n    <S sid=\"170\" ssid=\"28\">Hence, we also include a dependence on the word positions in the lexicon model p(e  |f, i, j): Here, [(i', j) ∈ A] is 1 if (i', j) ∈ A and 0 otherwise.</S>\n    <S sid=\"171\" ssid=\"29\">As a result, the word ei depends not only on the aligned French word fj, but also on the number of preceding French words aligned with ei and on the number of the preceding English words aligned with fj.</S>\n    <S sid=\"172\" ssid=\"30\">This model distinguishes the positions within a phrasal translation.</S>\n    <S sid=\"173\" ssid=\"31\">The number of parameters of p(e  |f,i,j) is significantly higher than that of p(e  |f) alone.</S>\n    <S sid=\"174\" ssid=\"32\">Hence, there is a data estimation problem especially for words that rarely occur.</S>\n    <S sid=\"175\" ssid=\"33\">Therefore, we linearly interpolate the models p(e  |f) and p(e  |f, i, j). very often a monotone alignment is a correct alignment.</S>\n    <S sid=\"176\" ssid=\"34\">Hence, the feature function hAL measures the “amount of nonmonotonicity” by summing over the distance (in the source language) of alignment templates that are consecutive in the target language: Here, jπ0 is defined to equal 0 and jπK+1−1 is defined to equal J.</S>\n    <S sid=\"177\" ssid=\"35\">The above-stated sum includes k = K + 1 to include the distance from the end position of the last phrase to the end of sentence.</S>\n    <S sid=\"178\" ssid=\"36\">The sequence of K = 6 alignment templates in Figure 5 corresponds to the following sum of seven jump distances: 0 + 0 + 1 + 3 + 2 + 0 + 0 = 6.</S>\n    <S sid=\"179\" ssid=\"37\">4.1.4 Language Model Features.</S>\n    <S sid=\"180\" ssid=\"38\">As a default language model feature, we use a standard backing-off word-based trigram language model (Ney, Generet, and Wessel 1995): The use of the language model feature in equation (18) helps take long-range dependencies better into account.</S>\n    <S sid=\"181\" ssid=\"39\">Without this feature, we typically observe that the produced sentences tend to be too short.</S>\n    <S sid=\"182\" ssid=\"40\">4.1.6 Conventional Lexicon.</S>\n    <S sid=\"183\" ssid=\"41\">We also use a feature that counts how many entries of a conventional lexicon co-occur in the given sentence pair.</S>\n    <S sid=\"184\" ssid=\"42\">Therefore, the weight for the provided conventional dictionary can be learned: The intuition is that the conventional dictionary LEX is more reliable than the automatically trained lexicon and therefore should get a larger weight.</S>\n    <S sid=\"185\" ssid=\"43\">4.1.7 Additional Features.</S>\n    <S sid=\"186\" ssid=\"44\">A major advantage of the log-linear modeling approach used is that we can add numerous features that deal with specific problems of the baseline statistical MT system.</S>\n    <S sid=\"187\" ssid=\"45\">Here, we will restrict ourselves to the described set of features.</S>\n    <S sid=\"188\" ssid=\"46\">Yet we could use grammatical features that relate certain grammatical dependencies of source and target language.</S>\n    <S sid=\"189\" ssid=\"47\">For example, using a function k(·) that counts how many arguments the main verb of a sentence has in the source or target sentence, we can define the following feature, which has a nonzero value if the verb in each of the two sentences has the same number of arguments: In the same way, we can introduce semantic features or pragmatic features such as the dialogue act classification.</S>\n    <S sid=\"190\" ssid=\"48\">For the three different tasks on which we report results, we use two different training approaches.</S>\n    <S sid=\"191\" ssid=\"49\">For the VERBMOBiL task, we train the model parameters λM1 according to the maximum class posterior probability criterion (equation (4)).</S>\n    <S sid=\"192\" ssid=\"50\">For the French– English HANSARDS task and the Chinese–English NIST task, we simply tune the model parameters by coordinate descent on held-out data with respect to the automatic evaluation metric employed, using as a starting point the model parameters obtained on the VERBMOBiL task.</S>\n    <S sid=\"193\" ssid=\"51\">Note that this tuning depends on the starting point of the model parameters and is not guaranteed to converge to the global optimum on the training data.</S>\n    <S sid=\"194\" ssid=\"52\">As a result, this approach is limited to a very small number of model parameters.</S>\n    <S sid=\"195\" ssid=\"53\">An efficient algorithm for performing this tuning for a larger number of model parameters can be found in Och (2003).</S>\n    <S sid=\"196\" ssid=\"54\">A standard approach to training the log-linear model parameters of the maximum class posterior probability criterion is the GIS (Generalized Iterative Scaling) algorithm (Darroch and Ratcliff 1972).</S>\n    <S sid=\"197\" ssid=\"55\">To apply this algorithm, we have to solve various practical problems.</S>\n    <S sid=\"198\" ssid=\"56\">The renormalization needed in equation (3) requires a sum over many possible sentences, for which we do not know of an efficient algorithm.</S>\n    <S sid=\"199\" ssid=\"57\">Hence, we approximate this sum by extracting a large set of highly probable sentences as a sample from the space of all possible sentences (n-best approximation).</S>\n    <S sid=\"200\" ssid=\"58\">The set of considered sentences is computed by means of an appropriately extended version of the search algorithm described in Section 5.</S>\n    <S sid=\"201\" ssid=\"59\">Using an n-best approximation, we might face the problem that the parameters trained with the GIS algorithm yield worse translation results even on the training corpus.</S>\n    <S sid=\"202\" ssid=\"60\">This can happen because with the modified model scaling factors, the n-best list can change significantly and can include sentences that have not been taken into account in training.</S>\n    <S sid=\"203\" ssid=\"61\">Using these sentences, the new model parameters might perform worse than the old model parameters.</S>\n    <S sid=\"204\" ssid=\"62\">To avoid this problem, we proceed as follows.</S>\n    <S sid=\"205\" ssid=\"63\">In a first step, we perform a search, compute an n-best list, and use this n-best list to train the model parameters.</S>\n    <S sid=\"206\" ssid=\"64\">Second, we use the new model parameters in a new search and compute a new n-best list, which is combined with the existing n-best list.</S>\n    <S sid=\"207\" ssid=\"65\">Third, using this extended n-best list, new model parameters are computed.</S>\n    <S sid=\"208\" ssid=\"66\">This process is iterated until the resulting n-best list does not change.</S>\n    <S sid=\"209\" ssid=\"67\">In this algorithm, convergence is guaranteed, as in the limit the n-best list will contain all possible translations.</S>\n    <S sid=\"210\" ssid=\"68\">In practice, the algorithm converges after five to seven iterations.</S>\n    <S sid=\"211\" ssid=\"69\">In our experiments this final n-best list contains about 500–1000 alternative translations.</S>\n    <S sid=\"212\" ssid=\"70\">We might have the problem that none of the given reference translations is part of the n-best list because the n-best list is too small or because the search algorithm performs pruning which in principle limits the possible translations that can be produced given a certain input sentence.</S>\n    <S sid=\"213\" ssid=\"71\">To solve this problem, we define as reference translation for maximum-entropy training each sentence that has the minimal number of word errors with respect to any of the reference translations in the n-best list.</S>\n    <S sid=\"214\" ssid=\"72\">More details of the training procedure can be found in Och and Ney (2002).</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"215\" ssid=\"1\">In this section, we describe an efficient search architecture for the alignment template model.</S>\n    <S sid=\"216\" ssid=\"2\">In general, the search problem for statistical MT even using only Model 1 of Brown et al. (1993) is NP-complete (Knight 1999).</S>\n    <S sid=\"217\" ssid=\"3\">Therefore, we cannot expect to develop efficient search algorithms that are guaranteed to solve the problem without search errors.</S>\n    <S sid=\"218\" ssid=\"4\">Yet for practical applications it is acceptable to commit some search errors (Section 6.1.2).</S>\n    <S sid=\"219\" ssid=\"5\">Hence, the art of developing a search algorithm lies in finding suitable approximations and heuristics that allow an efficient search without committing too many search errors.</S>\n    <S sid=\"220\" ssid=\"6\">In the development of the search algorithm described in this section, our main aim is that the search algorithm should be efficient.</S>\n    <S sid=\"221\" ssid=\"7\">It should be possible to translate a sentence of reasonable length within a few seconds of computing time.</S>\n    <S sid=\"222\" ssid=\"8\">We accept that the search algorithm sometimes results in search errors, as long as the impact on translation quality is minor.</S>\n    <S sid=\"223\" ssid=\"9\">Yet it should be possible to reduce the number of search errors by increasing computing time.</S>\n    <S sid=\"224\" ssid=\"10\">In the limit, it should be possible to search without search errors.</S>\n    <S sid=\"225\" ssid=\"11\">The search algorithm should not impose any principal limitations.</S>\n    <S sid=\"226\" ssid=\"12\">We also expect that the search algorithm be able to scale up to very long sentences with an acceptable computing time.</S>\n    <S sid=\"227\" ssid=\"13\">To meet these aims, it is necessary to have a mechanism that restricts the search effort.</S>\n    <S sid=\"228\" ssid=\"14\">We accomplish such a restriction by searching in a breadth-first manner with pruning: beam search.</S>\n    <S sid=\"229\" ssid=\"15\">In pruning, we constrain the set of considered translation candidates (the “beam”) only to the promising ones.</S>\n    <S sid=\"230\" ssid=\"16\">We compare in beam search those hypotheses that cover different parts of the input sentence.</S>\n    <S sid=\"231\" ssid=\"17\">This makes the comparison of the probabilities problematic.</S>\n    <S sid=\"232\" ssid=\"18\">Therefore, we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation (Section 5.6) (Garcia-Varea, Casacuberta, and Ney 1998; Garcia-Varea et al. 2001), as does the original IBM stack search decoder (Berger et al.</S>\n    <S sid=\"233\" ssid=\"19\">1994).</S>\n    <S sid=\"234\" ssid=\"20\">All these simplifications ultimately make the search problem simpler but introduce fundamental search errors.</S>\n    <S sid=\"235\" ssid=\"21\">In the following, we describe our search algorithm based on the concept of beam search, which allows a trade-off between efficiency and quality by adjusting the size of the beam.</S>\n    <S sid=\"236\" ssid=\"22\">The search algorithm can be easily adapted to other phrase-based translation models.</S>\n    <S sid=\"237\" ssid=\"23\">For single-word-based search in MT, a similar algorithm has been described in Tillmann and Ney (2003).</S>\n    <S sid=\"238\" ssid=\"24\">Putting everything together and performing search in maximum approximation, we obtain the following decision rule: Using the four feature functions AT, AL, WRD, and LM, we obtain the following decision rule:3 Here, we have grouped the contributions of the various feature functions into those for each word (from LM and WRD, expression (24)), those for every alignment template (from AT and AL, expression (25)), and those for the end of sentence (expression (26)), which includes a term logp(EOS  |eI−1,eI) for the end-of-sentence language model probability.</S>\n    <S sid=\"239\" ssid=\"25\">To extend this decision rule for the word penalty (WP) feature function, we simply obtain an additional term AWP for each word.</S>\n    <S sid=\"240\" ssid=\"26\">The class-based 5-gram language model (CLM) can be included like the trigram language model.</S>\n    <S sid=\"241\" ssid=\"27\">Note that all these feature functions decompose nicely into contributions for each produced target language word or for each covered source language word.</S>\n    <S sid=\"242\" ssid=\"28\">This makes it possible to develop an efficient dynamic programming search algorithm.</S>\n    <S sid=\"243\" ssid=\"29\">Not all feature functions have this nice property: For the conventional lexicon feature function (LEX), we obtain an additional term in our decision rule which depends on the full sentence.</S>\n    <S sid=\"244\" ssid=\"30\">Therefore, this feature function will not be integrated in the dynamic programming search but instead will be used to rerank the set of candidate translations produced by the search.</S>\n    <S sid=\"245\" ssid=\"31\">We have to structure the search space in a suitable way to search efficiently.</S>\n    <S sid=\"246\" ssid=\"32\">In our search algorithm, we generate search hypotheses that correspond to prefixes of target language sentences.</S>\n    <S sid=\"247\" ssid=\"33\">Each hypothesis is the translation of a part of the source language sentence.</S>\n    <S sid=\"248\" ssid=\"34\">A hypothesis is extended by appending one target word.</S>\n    <S sid=\"249\" ssid=\"35\">The set of all hypotheses can be structured as a graph with a source node representing the sentence start, goal nodes representing complete translations, and intermediate nodes representing partial translations.</S>\n    <S sid=\"250\" ssid=\"36\">There is a directed edge between hypotheses n1 and n2 if the hypothesis n2 is obtained by appending one word to hypothesis n1.</S>\n    <S sid=\"251\" ssid=\"37\">Each edge has associated costs resulting from the contributions of all feature functions.</S>\n    <S sid=\"252\" ssid=\"38\">Finally, our search problem can be reformulated as finding the optimal path through this graph.</S>\n    <S sid=\"253\" ssid=\"39\">˜ In the first step, we determine the set of all source phrases in f for which an applicable alignment template exists.</S>\n    <S sid=\"254\" ssid=\"40\">Every possible application of an alignment template z = (FJy1 , EI~1 , ˜A) to a subsequence f j+J�−1 of the source sentence is called an alignment j template instantiation Z = (z, j).</S>\n    <S sid=\"255\" ssid=\"41\">Hence, the set of all alignment template instantiations for the source sentence fJ1 is If the source sentence contains words that have not been seen in the training data, we introduce a new alignment template that performs a one-to-one translation of each of these words by itself.</S>\n    <S sid=\"256\" ssid=\"42\">In the second step, we determine a set of probable target language words for each target word position in the alignment template instantiation.</S>\n    <S sid=\"257\" ssid=\"43\">Only these words are then hypothesized in the search.</S>\n    <S sid=\"258\" ssid=\"44\">We call this selection of highly probable words observation pruning (Tillmann and Ney 2000).</S>\n    <S sid=\"259\" ssid=\"45\">As a criterion for a word e at position i in the alignment template instantiation, we use In our experiments, we hypothesize only the five best-scoring words.</S>\n    <S sid=\"260\" ssid=\"46\">A decision is a triple d = (Z, e,l) consisting of an alignment template instantiation Z, the generated word e, and the index l of the generated word in Z.</S>\n    <S sid=\"261\" ssid=\"47\">A hypothesis n corresponds to a valid sequence of decisions di1.</S>\n    <S sid=\"262\" ssid=\"48\">The possible decisions are as follows: The resulting decision score corresponds to the contribution of expression (26).</S>\n    <S sid=\"263\" ssid=\"49\">Any valid and complete sequence of decisions dI+1 1 uniquely corresponds to a certain translation eI1, a segmentation into K phrases, a phrase alignment πK1 , and a sequence of alignment template instantiations zK1 .</S>\n    <S sid=\"264\" ssid=\"50\">The sum of the decision scores is equal to the corresponding score described in expressions (24)–(26).</S>\n    <S sid=\"265\" ssid=\"51\">A straightforward representation of all hypotheses would be the prefix tree of all possible sequences of decisions.</S>\n    <S sid=\"266\" ssid=\"52\">Obviously, there would be a large redundancy in this search space representation, because there are many search nodes that are indistinguishable in the sense that the subtrees following these search nodes are identical.</S>\n    <S sid=\"267\" ssid=\"53\">We can recombine these identical search nodes; that is, we have to maintain only the most probable hypothesis (Bellman 1957).</S>\n    <S sid=\"268\" ssid=\"54\">In general, the criterion for recombining a set of nodes is that the hypotheses can be distinguished by neither language nor translation model.</S>\n    <S sid=\"269\" ssid=\"55\">In performing recombination, Algorithm for breadth-first search with pruning. we obtain a search graph instead of a search tree.</S>\n    <S sid=\"270\" ssid=\"56\">The exact criterion for performing recombination for the alignment templates is described in Section 5.5.</S>\n    <S sid=\"271\" ssid=\"57\">Theoretically, we could use any graph search algorithm to search the optimal path in the search space.</S>\n    <S sid=\"272\" ssid=\"58\">We use a breadth-first search algorithm with pruning.</S>\n    <S sid=\"273\" ssid=\"59\">This approach offers very good possibilities for adjusting the trade-off between quality and efficiency.</S>\n    <S sid=\"274\" ssid=\"60\">In pruning, we always compare hypotheses that have produced the same number of target words.</S>\n    <S sid=\"275\" ssid=\"61\">Figure 7 shows a structogram of the algorithm.</S>\n    <S sid=\"276\" ssid=\"62\">As the search space increases exponentially, it is not possible to explicitly represent it.</S>\n    <S sid=\"277\" ssid=\"63\">Therefore, we represent the search space implicitly, using the functions Extend and Recombine.</S>\n    <S sid=\"278\" ssid=\"64\">The function Extend produces new hypotheses extending the current hypothesis by one word.</S>\n    <S sid=\"279\" ssid=\"65\">Some hypotheses might be identical or indistinguishable by the language and translation models.</S>\n    <S sid=\"280\" ssid=\"66\">These are recombined by the function Recombine.</S>\n    <S sid=\"281\" ssid=\"67\">We expand the search space such that only hypotheses with the same number of target language words are recombined.</S>\n    <S sid=\"282\" ssid=\"68\">In the pruning step, we use two different types of pruning.</S>\n    <S sid=\"283\" ssid=\"69\">First, we perform pruning relative to the score Qˆ of the current best hypothesis.</S>\n    <S sid=\"284\" ssid=\"70\">We ignore all hypotheses that have a probability lower than log(tp)+ˆQ, where tp is an adjustable pruning parameter.</S>\n    <S sid=\"285\" ssid=\"71\">This type of pruning can be performed when the hypothesis extensions are computed.</S>\n    <S sid=\"286\" ssid=\"72\">Second, in histogram pruning (Steinbiss, Tran, and Ney 1994), we maintain only the best Np hypotheses.</S>\n    <S sid=\"287\" ssid=\"73\">The two pruning parameters tp and Np have to be optimized with respect to the trade-off between efficiency and quality.</S>\n    <S sid=\"288\" ssid=\"74\">In this section, we describe various issues involved in performing an efficient implementation of a search algorithm for the alignment template approach.</S>\n    <S sid=\"289\" ssid=\"75\">A very important design decision in the implementation is the representation of a hypothesis.</S>\n    <S sid=\"290\" ssid=\"76\">Theoretically, it would be possible to represent search hypotheses only by the associated decision and a back-pointer to the previous hypothesis.</S>\n    <S sid=\"291\" ssid=\"77\">Yet this would be a very inefficient representation for the implementation of the operations that have to be performed in the search.</S>\n    <S sid=\"292\" ssid=\"78\">The hypothesis representation should contain all information required to perform efficiently the computations needed in the search but should contain no more information than that, to keep the memory consumption small.</S>\n    <S sid=\"293\" ssid=\"79\">In search, we produce hypotheses n, each of which contains the following information: We compare in beam search those hypotheses that cover different parts of the input sentence.</S>\n    <S sid=\"294\" ssid=\"80\">This makes the comparison of the probabilities problematic.</S>\n    <S sid=\"295\" ssid=\"81\">Therefore, we integrate an admissible estimation of the remaining probabilities to arrive at a complete translation.</S>\n    <S sid=\"296\" ssid=\"82\">Details of the heuristic function for the alignment templates are provided in the next section.</S>\n    <S sid=\"297\" ssid=\"83\">To improve the comparability of search hypotheses, we introduce heuristic functions.</S>\n    <S sid=\"298\" ssid=\"84\">A heuristic function estimates the probabilities of reaching the goal node from a certain search node.</S>\n    <S sid=\"299\" ssid=\"85\">An admissible heuristic function is always an optimistic estimate; that is, for each search node, the product of edge probabilities of reaching a goal node is always equal to or smaller than the estimated probability.</S>\n    <S sid=\"300\" ssid=\"86\">For an A*-based search algorithm, a good heuristic function is crucial to being able to translate long sentences.</S>\n    <S sid=\"301\" ssid=\"87\">For a beam search algorithm, the heuristic function has a different motivation.</S>\n    <S sid=\"302\" ssid=\"88\">It is used to improve the scoring of search hypotheses.</S>\n    <S sid=\"303\" ssid=\"89\">The goal is to make the probabilities of all hypotheses more comparable, in order to minimize the chance that the hypothesis leading to the optimal translation is pruned away.</S>\n    <S sid=\"304\" ssid=\"90\">Heuristic functions for search in statistical MT have been used in Wang and Waibel (1997) and Och, Ueffing, and Ney (2001).</S>\n    <S sid=\"305\" ssid=\"91\">Wang and Waibel (1997) have described a simple heuristic function for Model 2 of Brown et al. (1993) that was not admissible.</S>\n    <S sid=\"306\" ssid=\"92\">Och, Ueffing, and Ney (2001) have described an admissible heuristic function for Model 4 of Brown et al. (1993) and an almost-admissible heuristic function that is empirically obtained.</S>\n    <S sid=\"307\" ssid=\"93\">We have to keep in mind that a heuristic function is helpful only if the overhead introduced in computing the heuristic function is more than compensated for by the gain obtained through a better pruning of search hypotheses.</S>\n    <S sid=\"308\" ssid=\"94\">The heuristic functions described in the following are designed such that their computation can be performed efficiently.</S>\n    <S sid=\"309\" ssid=\"95\">The basic idea for developing a heuristic function for an alignment model is that all source sentence positions that have not been covered so far still have to be translated to complete the sentence.</S>\n    <S sid=\"310\" ssid=\"96\">If we have an estimation rX(j) of the optimal score for translating position j, then the value of the heuristic function RX(n) for a node n can be inferred by summing over the contribution for every position j that is not in the coverage vector c(n) (here X denotes different possibilities to choose the heuristic The situation in the case of the alignment template approach is more complicated, as not every word is translated alone, but typically the words are translated in context.</S>\n    <S sid=\"311\" ssid=\"97\">Therefore, the basic quantity for the heuristic function in the case of the alignment template approach is a function r(Z) that assigns to every alignment template instantiation Z a maximal probability.</S>\n    <S sid=\"312\" ssid=\"98\">Using r(Z), we can induce a position-dependent heuristic function r(j): Here, J(Z) denotes the number of source language words produced by the alignment template instantiation Z and j(Z) denotes the position of the first source language word.</S>\n    <S sid=\"313\" ssid=\"99\">It can be easily shown that if r(Z) is admissible, then r(j) is also admissible.</S>\n    <S sid=\"314\" ssid=\"100\">We have to show that for all nonoverlapping sequences ZK1 the following holds: Here, k(j) denotes the phrase index k that includes the target language word position j.</S>\n    <S sid=\"315\" ssid=\"101\">In the following, we develop various heuristic functions r(Z) of increasing complexity.</S>\n    <S sid=\"316\" ssid=\"102\">The simplest realization of a heuristic function r(Z) takes into account only the prior probability of an alignment template instantiation: The language model can be incorporated by considering that for each target word there exists an optimal language model probability: Here, we assume a trigram language model.</S>\n    <S sid=\"317\" ssid=\"103\">In general, it is necessary to maximize over all possible different language model histories.</S>\n    <S sid=\"318\" ssid=\"104\">We can also combine the language model and the lexicon model into one heuristic function: To include the phrase alignment probability in the heuristic function, we compute the minimum sum of all jump widths that is needed to complete the translation.</S>\n    <S sid=\"319\" ssid=\"105\">This sum can be computed efficiently using the algorithm shown in Figure 8.</S>\n    <S sid=\"320\" ssid=\"106\">Then, an admissible heuristic function for the jump width is obtained by Combining all the heuristic functions for the various models, we obtain as final heuristic function for a search hypothesis n</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"321\" ssid=\"1\">We present results on the VERBMOBiL task, which is a speech translation task in the domain of appointment scheduling, travel planning, and hotel reservation (Wahlster 2000).</S>\n    <S sid=\"322\" ssid=\"2\">Table 2 shows the corpus statistics for this task.</S>\n    <S sid=\"323\" ssid=\"3\">We use a training corpus, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus.</S>\n    <S sid=\"324\" ssid=\"4\">On average, 3.32 reference translations for the development corpus and 5.14 reference translations for the test corpus are used.</S>\n    <S sid=\"325\" ssid=\"5\">A standard vocabulary had been defined for the various speech recognizers used in VERBMOBiL.</S>\n    <S sid=\"326\" ssid=\"6\">However, not all words of this vocabulary were observed in the training corpus.</S>\n    <S sid=\"327\" ssid=\"7\">Therefore, the translation vocabulary was extended semiautomatically by adding about 13,000 German–English entries from an online bilingual lexicon available on the Web.</S>\n    <S sid=\"328\" ssid=\"8\">The resulting lexicon contained not only word-word entries, but also multi-word translations, especially for the large number of German compound words.</S>\n    <S sid=\"329\" ssid=\"9\">To counteract the sparseness of the training data, a couple of straightforward rule-based preprocessing steps were applied before any other type of processing: So far, in machine translation research there is no generally accepted criterion for the evaluation of experimental results.</S>\n    <S sid=\"330\" ssid=\"10\">Therefore, we use various criteria.</S>\n    <S sid=\"331\" ssid=\"11\">In the following experiments, we use: In the following, we analyze the effect of various system components: alignment template length, search pruning, and language model n-gram size.</S>\n    <S sid=\"332\" ssid=\"12\">A systematic evaluation of the alignment template system comparing it with other translation approaches (e.g., rule-based) has been performed in the VERBMOBiL project and is described in Tessiore and von Hahn (2000).</S>\n    <S sid=\"333\" ssid=\"13\">There, the alignment-template-based system achieved a significantly larger number of “approximately correct” translations than the competing translation systems (Ney, Och, and Vogel 2001).</S>\n    <S sid=\"334\" ssid=\"14\">6.1.1 Effect of Alignment Template Length.</S>\n    <S sid=\"335\" ssid=\"15\">Table 3 shows the effect of constraining the maximum length of the alignment templates in the source language.</S>\n    <S sid=\"336\" ssid=\"16\">Typically, it is necessary to restrict the alignment template length to keep memory requirements low.</S>\n    <S sid=\"337\" ssid=\"17\">We see that using alignment templates with only one or two words in the source languages results in very bad translation quality.</S>\n    <S sid=\"338\" ssid=\"18\">Yet using alignment templates with lengths as small as three words yields optimal results. algorithm misses the most probable translation and produces a translation which is less probable.</S>\n    <S sid=\"339\" ssid=\"19\">As we typically cannot efficiently compute the probability of the optimal translation, we cannot efficiently compute the number of search errors.</S>\n    <S sid=\"340\" ssid=\"20\">Yet we can compute a lower bound on the number of search errors by comparing the translation found under specific pruning thresholds with the best translation that we have found using very conservative pruning thresholds.</S>\n    <S sid=\"341\" ssid=\"21\">Tables 4 and 5 show the effect of the pruning parameter tp with the histogram pruning parameter Np = 50,000.</S>\n    <S sid=\"342\" ssid=\"22\">Tables 6 and 7 show the effect of the pruning parameter Np with the pruning parameter tp = 10−12.</S>\n    <S sid=\"343\" ssid=\"23\">In all four tables, we provide the results for using no heuristic functions and three variants of an increasingly informative heuristic function.</S>\n    <S sid=\"344\" ssid=\"24\">The first is an estimate of the alignment template and the lexicon probability (AT+WRD), the second adds an estimate of the language model (+LM) probability, and the third also adds the alignment probability (+AL).</S>\n    <S sid=\"345\" ssid=\"25\">These heuristic functions are described in Section 5.6.</S>\n    <S sid=\"346\" ssid=\"26\">Without a heuristic function, even more than a hundred seconds per sentence cannot guarantee search-error-free translation.</S>\n    <S sid=\"347\" ssid=\"27\">We draw the conclusion that a good heuristic function is very important to obtaining an efficient search algorithm.</S>\n    <S sid=\"348\" ssid=\"28\">In addition, the search errors have a more severe effect on the error rates if we do not use a heuristic function.</S>\n    <S sid=\"349\" ssid=\"29\">If we compare the error rates in Table 7, which correspond to about 55 search errors in Table 6, we obtain an mWER of 36.7% (53 search errors) using no heuristic function and an mWER of 32.6% (57 search errors) using the combined heuristic function.</S>\n    <S sid=\"350\" ssid=\"30\">The reason is that without a heuristic function, often the “easy” part of the input sentence is translated first.</S>\n    <S sid=\"351\" ssid=\"31\">This yields severe reordering errors. n-gram-based language models.</S>\n    <S sid=\"352\" ssid=\"32\">Ideally, we would like to take into account long-range dependencies.</S>\n    <S sid=\"353\" ssid=\"33\">Yet long n-grams are seen rarely and are therefore rarely used on unseen data.</S>\n    <S sid=\"354\" ssid=\"34\">Therefore, we expect that extending the history length will at some point not improve further translation quality.</S>\n    <S sid=\"355\" ssid=\"35\">Table 8 shows the effect of the length of the language model history on translation quality.</S>\n    <S sid=\"356\" ssid=\"36\">We see that the language model perplexity improves from 4,781 for a unigram model to 29.9 for a trigram model.</S>\n    <S sid=\"357\" ssid=\"37\">The corresponding translation quality improves from an mWER of 45.9% to an mWER of 31.8%.</S>\n    <S sid=\"358\" ssid=\"38\">The largest effect seems to come from taking into account the bigram dependence, which achieves an mWER of 32.9%.</S>\n    <S sid=\"359\" ssid=\"39\">If we perform log-linear interpolation of a trigram model with a class-based 5-gram model, we observe an additional small improvement in translation quality to an mWER of 30.9%.</S>\n    <S sid=\"360\" ssid=\"40\">The HANSARDS task involves the proceedings of the Canadian parliament, which are kept by law in both French and English.</S>\n    <S sid=\"361\" ssid=\"41\">About three million parallel sentences of this bilingual data have been made available by the Linguistic Data Consortium (LDC).</S>\n    <S sid=\"362\" ssid=\"42\">Here, we use a subset of the data containing only sentences of up to 30 words.</S>\n    <S sid=\"363\" ssid=\"43\">Table 9 shows the training and test corpus statistics.</S>\n    <S sid=\"364\" ssid=\"44\">The results for French to English and for English to French are shown in Table 10.</S>\n    <S sid=\"365\" ssid=\"45\">Because of memory limitations, the maximum alignment template length has been restricted to four words.</S>\n    <S sid=\"366\" ssid=\"46\">We compare here against the single-word-based search for Model 4 described in Tillmann (2001).</S>\n    <S sid=\"367\" ssid=\"47\">We see that the alignment template approach obtains significantly better results than the single-word-based search.</S>\n    <S sid=\"368\" ssid=\"48\">Various statistical, example-based, and rule-based MT systems for a Chinese–English news domain were evaluated in the NIST 2002 MT evaluation.4 Using the alignment template approach described in this article, we participated in these evaluations.</S>\n    <S sid=\"369\" ssid=\"49\">The problem domain is the translation of Chinese news text into English.</S>\n    <S sid=\"370\" ssid=\"50\">Table 11 gives an overview on the training and test data.</S>\n    <S sid=\"371\" ssid=\"51\">The English vocabulary consists of fullform words that have been converted to lowercase letters.</S>\n    <S sid=\"372\" ssid=\"52\">The number of sentences has been artificially increased by adding certain parts of the original training material more than once to the training corpus, in order to give larger weight to those parts of the training corpus that consist of high-quality aligned Chinese news text and are therefore expected to be especially helpful for the translation of the test data.</S>\n    <S sid=\"373\" ssid=\"53\">The Chinese language poses special problems because the boundaries of Chinese words are not marked.</S>\n    <S sid=\"374\" ssid=\"54\">Chinese text is provided as a sequence of characters, and it is unclear which characters have to be grouped together to obtain entities that can be interpreted as words.</S>\n    <S sid=\"375\" ssid=\"55\">For statistical MT, it would be possible to ignore this fact and treat the Chinese characters as elementary units and translate them into English.</S>\n    <S sid=\"376\" ssid=\"56\">Yet preliminary experiments showed that the existing alignment models produce better results if the Chinese characters are segmented in a preprocessing step into single words.</S>\n    <S sid=\"377\" ssid=\"57\">We use the LDC segmentation tool.5 For the English corpus, the following preprocessing steps are applied.</S>\n    <S sid=\"378\" ssid=\"58\">First, the corpus is tokenized; it is then segmented into sentences, and all uppercase characters are converted to lowercase.</S>\n    <S sid=\"379\" ssid=\"59\">As the final evaluation criterion does not distinguish case, it is not necessary to deal with the case information.</S>\n    <S sid=\"380\" ssid=\"60\">Then, the preprocessed Chinese and English corpora are sentence aligned in which the lengths of the source and target sentences are significantly different.</S>\n    <S sid=\"381\" ssid=\"61\">From the resulting corpus, we automatically replace translations.</S>\n    <S sid=\"382\" ssid=\"62\">In addition, only sentences with less than 60 words in English and Chinese are used.</S>\n    <S sid=\"383\" ssid=\"63\">To improve the translation of Chinese numbers, we use a categorization of Chinese number and date expressions.</S>\n    <S sid=\"384\" ssid=\"64\">For the statistical learning, all number and date expressions are replaced with one of two generic symbols, $number or $date.</S>\n    <S sid=\"385\" ssid=\"65\">The number and date expressions are subjected to a rule-based translation by simple lexicon lookup.</S>\n    <S sid=\"386\" ssid=\"66\">The translation of the number and date expressions is inserted into the output using the alignment information.</S>\n    <S sid=\"387\" ssid=\"67\">For Chinese and English, this categorization is implemented independently of the other language.</S>\n    <S sid=\"388\" ssid=\"68\">To evaluate MT quality on this task, NIST made available the NIST-09 evaluation tool.</S>\n    <S sid=\"389\" ssid=\"69\">This tool provides a modified BLEU score by computing a weighted precision of n-grams modified by a length penalty for very short translations.</S>\n    <S sid=\"390\" ssid=\"70\">Table 12 shows the results of the official evaluation performed by NIST in June 2002.</S>\n    <S sid=\"391\" ssid=\"71\">With a score of 7.65, the results obtained were statistically significantly better than any other competing approach.</S>\n    <S sid=\"392\" ssid=\"72\">Differences in the NIST score larger than 0.12 are statistically significant at the 95% level.</S>\n    <S sid=\"393\" ssid=\"73\">We conclude that the developed alignment template approach is also applicable to unrelated language pairs such as Chinese–English and that the developed statistical models indeed seem to be largely language-independent.</S>\n    <S sid=\"394\" ssid=\"74\">Table 13 shows various example translations.</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"395\" ssid=\"1\">We have presented a framework for statistical MT for natural languages which is more general than the widely used source–channel approach.</S>\n    <S sid=\"396\" ssid=\"2\">It allows a baseline MT been achieved in 1995 in the economic construction of China’s fourteen border cities open to foreigners.</S>\n    <S sid=\"397\" ssid=\"3\">Translation Xinhua News Agency, Beijing, February 12—China’s opening up to the outside world of the 1995 in the fourteen border pleased to obtain the construction of the economy.</S>\n    <S sid=\"398\" ssid=\"4\">Reference Foreign Investment in Jiangsu’s Agriculture on the Increase Translation To increase the operation of foreign investment in Jiangsu agriculture Reference According to the data provided today by the Ministry of Foreign Trade and Economic Cooperation, as of November this year, China has actually utilized 46.959 billion US dollars of foreign capital, including 40.007 billion US dollars of direct investment from foreign businessmen.</S>\n    <S sid=\"399\" ssid=\"5\">Translation The external economic and trade cooperation Department today provided that this year, the foreign capital actually utilized by China on November to US $46.959 billion, including of foreign company direct investment was US $40.007 billion.</S>\n    <S sid=\"400\" ssid=\"6\">Reference According to officials from the Provincial Department of Agriculture and Forestry of Jiangsu, the ”Three-Capital” ventures approved by agencies within the agricultural system of Jiangsu Province since 1994 have numbered more than 500 and have utilized over 700 million US dollars worth of foreign capital, respectively three times and seven times more than in 1993.</S>\n    <S sid=\"401\" ssid=\"7\">Translation Jiangsu Province for the Secretaries said that, from the 1994 years, Jiangsu Province system the approval of the “three-funded” enterprises, there are more than 500, foreign investment utilization rate of more than US $700 million, 1993 years before three and seven.</S>\n    <S sid=\"402\" ssid=\"8\">Reference The actual amount of foreign capital has also increased more than 30% as compared with the same period last year.</S>\n    <S sid=\"403\" ssid=\"9\">Translation The actual amount of foreign investment has increased by more than 30% compared with the same period last year.</S>\n    <S sid=\"404\" ssid=\"10\">Reference Import and Export in Pudong New District Exceeding 9 billion US dollars This Year Translation Foreign trade imports and exports of this year to the Pudong new Region exceeds US $9 billion system to be extended easily by adding new feature functions.</S>\n    <S sid=\"405\" ssid=\"11\">We have described the alignment template approach for statistical machine translation, which uses two different alignment levels: a phrase-level alignment between phrases and a wordlevel alignment between single words.</S>\n    <S sid=\"406\" ssid=\"12\">As a result the context of words has a greater influence, and the changes in word order from source to target language can be learned explicitly.</S>\n    <S sid=\"407\" ssid=\"13\">An advantage of this method is that machine translation is learned fully automatically through the use of a bilingual training corpus.</S>\n    <S sid=\"408\" ssid=\"14\">We have shown that the presented approach is capable of achieving better translation results on various tasks compared to other statistical, example-based, or rule-based translation systems.</S>\n    <S sid=\"409\" ssid=\"15\">This is especially interesting, as our system is structured simpler than many competing systems.</S>\n    <S sid=\"410\" ssid=\"16\">We expect that better translation can be achieved by using models that go beyond the flat phrase segmentation that we perform in our model.</S>\n    <S sid=\"411\" ssid=\"17\">A promising avenue is to gradually extend the model to take into account to some extent the recursive structure of natural languages using ideas from Wu and Wong (1998) or Alshawi, Bangalore, and Douglas (2000).</S>\n    <S sid=\"412\" ssid=\"18\">We expect other improvements as well from learning nonconsecutive phrases in source or target language and from better generalization methods for the learned-phrase pairs.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"9\">\n    <S sid=\"413\" ssid=\"1\">The work reported here was carried out while the first author was with the Lehrstuhl f¨ur Informatik VI, Computer Science Department, RWTH Aachen–University of Technology.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "64a3b848e7ded622f3586958f682eac9",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "The Alignment Template Approach To Statistical Machine Translation"
                }
            }
        },
        {
            "index": 8,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a stochastic parsing system consisting of a Lexical-Functional Grammar (LFG), a constraint-based parser and a stochastic disambiguation model. We report on the results of applying this system to parsing the UPenn Wall Street Journal (WSJ) treebank. The model combines full and partial parsing techniques to reach full grammar coverage on unseen data. The treebank annotations are used to provide partially labeled data for discriminative statistical estimation using exponential models. Disambiguation performance is evaluated by measuring matches of predicate-argument relations on two distinct test sets. On a gold standard of manually annotated f-structures for a subset of the WSJ treebank, this evaluation reaches 79% F-score. An evaluation on a gold standard of dependency relations for",
                    "filename": "Parsing_The_Wall_Street_Journal_Using_A_Lexical-Functional_Grammar_And_Discriminative_Estimation_Techniques",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P02-1035/Documents_xml/P02-1035.xml",
                    "title": "Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a novel approach to parsing the Wall Street Journal using a Lexical-Functional Grammar (LFG) combined with discriminative estimation techniques. The authors extend a constraint-based parser with partial parsing techniques to tackle the problem of grammar coverage, achieving 100% coverage on unseen data. They exploit the WSJ treebank for discriminative estimation of an exponential model on LFG parses, using partially labeled data to refine the estimation process. The parser, coupled with a stochastic disambiguator, achieves an F-score of 79% using an LFG annotation scheme and 76% under a dependency relations metric. The paper details the robust parsing techniques, the statistical model, and empirical evaluation results, demonstrating significant improvements over existing methods.",
                        "This paper presents a novel approach to statistical parsing by scaling up a system that utilizes a hand-coded, linguistically fine-grained Lexical-Functional Grammar (LFG) in conjunction with discriminative estimation techniques for the Wall Street Journal (WSJ) treebank. The authors address the challenge of grammar coverage by integrating full and partial parsing techniques, achieving 100% coverage on unseen data. The paper outlines how the rudimentary functional annotations from the WSJ treebank are leveraged to provide partially labeled data for discriminative estimation of a probability model on LFG parses. The empirical evaluation demonstrates that the parsing system attains an F-score of 79% on WSJ data and 76% on the Brown corpus using predicate-argument matching as the criterion for assessing quality. The results also suggest a significant reduction in error rate, indicating the effectiveness of the discriminative model in parsing performance.",
                        "This paper presents a pioneering approach to scaling up stochastic parsing systems employing hand-coded, linguistically fine-grained Lexical-Functional Grammar (LFG) with discriminative estimation techniques on the Wall Street Journal (WSJ) treebank. The authors extend LFG and integrate partial parsing strategies to achieve 100% grammar coverage, overcoming limitations in functional annotations within standard treebanks. By utilizing treebank annotations to define a gold standard for discriminative estimation, they leverage partially labeled data to train an exponential model on LFG parses. The system attains an F-score of 79% on WSJ data and 76% on a Brown corpus test set, demonstrating the efficacy of their approach in handling fine-grained linguistic structures for statistical parsing.",
                        "This paper presents a novel approach to statistical parsing by scaling up a combined system of hand-coded, linguistically fine-grained Lexical-Functional Grammar (LFG) with a stochastic disambiguation model to work with the Wall Street Journal (WSJ) treebank data. The authors tackle the problem of grammar coverage by using full and partial parsing techniques, achieving 100% coverage on unseen data. They exploit the WSJ treebank for discriminative estimation of an exponential model on LFG parses, using partially labeled data to estimate a probability model on fine-grained parses. The parser achieves an F-score of 79% on WSJ data and 76% on the Brown corpus, showing the effectiveness of the method in disambiguating and parsing large-scale, real-world linguistic data."
                    ],
                    "filename": "parsing_the_wall_street_journal_-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 116 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"8\" ssid=\"1\">Statistical parsing using combined systems of handcoded linguistically fine-grained grammars and stochastic disambiguation components has seen considerable progress in recent years.</S>\n    <S sid=\"9\" ssid=\"2\">However, such attempts have so far been confined to a relatively small scale for various reasons.</S>\n    <S sid=\"10\" ssid=\"3\">Firstly, the rudimentary character of functional annotations in standard treebanks has hindered the direct use of such data for statistical estimation of linguistically fine-grained statistical parsing systems.</S>\n    <S sid=\"11\" ssid=\"4\">Rather, parameter estimation for such models had to resort to unsupervised techniques (Bouma et al., 2000; Riezler et al., 2000), or training corpora tailored to the specific grammars had to be created by parsing and manual disambiguation, resulting in relatively small training sets of around 1,000 sentences (Johnson et al., 1999).</S>\n    <S sid=\"12\" ssid=\"5\">Furthermore, the effort involved in coding broadcoverage grammars by hand has often led to the specialization of grammars to relatively small domains, thus sacrificing grammar coverage (i.e. the percentage of sentences for which at least one analysis is found) on free text.</S>\n    <S sid=\"13\" ssid=\"6\">The approach presented in this paper is a first attempt to scale up stochastic parsing systems based on linguistically fine-grained handcoded grammars to the UPenn Wall Street Journal (henceforth WSJ) treebank (Marcus et al., 1994).</S>\n    <S sid=\"14\" ssid=\"7\">The problem of grammar coverage, i.e. the fact that not all sentences receive an analysis, is tackled in our approach by an extension of a fullfledged Lexical-Functional Grammar (LFG) and a constraint-based parser with partial parsing techniques.</S>\n    <S sid=\"15\" ssid=\"8\">In the absence of a complete parse, a socalled “FRAGMENT grammar” allows the input to be analyzed as a sequence of well-formed chunks.</S>\n    <S sid=\"16\" ssid=\"9\">The set of fragment parses is then chosen on the basis of a fewest-chunk method.</S>\n    <S sid=\"17\" ssid=\"10\">With this combination of full and partial parsing techniques we achieve 100% grammar coverage on unseen data.</S>\n    <S sid=\"18\" ssid=\"11\">Another goal of this work is the best possible exploitation of the WSJ treebank for discriminative estimation of an exponential model on LFG parses.</S>\n    <S sid=\"19\" ssid=\"12\">We define discriminative or conditional criteria with respect to the set of grammar parses consistent with the treebank annotations.</S>\n    <S sid=\"20\" ssid=\"13\">Such data can be gathered by applying labels and brackets taken from the treebank annotation to the parser input.</S>\n    <S sid=\"21\" ssid=\"14\">The rudimentary treebank annotations are thus used to provide partially labeled data for discriminative estimation of a probability model on linguistically fine-grained parses.</S>\n    <S sid=\"22\" ssid=\"15\">Concerning empirical evaluation of disambiguation performance, we feel that an evaluation measuring matches of predicate-argument relations is more appropriate for assessing the quality of our LFGbased system than the standard measure of matching labeled bracketing on section 23 of the WSJ treebank.</S>\n    <S sid=\"23\" ssid=\"16\">The first evaluation we present measures matches of predicate-argument relations in LFG fstructures (henceforth the LFG annotation scheme) to a gold standard of manually annotated f-structures for a representative subset of the WSJ treebank.</S>\n    <S sid=\"24\" ssid=\"17\">The evaluation measure counts the number of predicateargument relations in the f-structure of the parse selected by the stochastic model that match those in the gold standard annotation.</S>\n    <S sid=\"25\" ssid=\"18\">Our parser plus stochastic disambiguator achieves 79% F-score under this evaluation regime.</S>\n    <S sid=\"26\" ssid=\"19\">Furthermore, we employ another metric which maps predicate-argument relations in LFG fstructures to the dependency relations (henceforth the DR annotation scheme) proposed by Carroll et al. (1999).</S>\n    <S sid=\"27\" ssid=\"20\">Evaluation with this metric measures the matches of dependency relations to Carroll et al.’s gold standard corpus.</S>\n    <S sid=\"28\" ssid=\"21\">For a direct comparison of our results with Carroll et al.’s system, we computed an F-score that does not distinguish different types of dependency relations.</S>\n    <S sid=\"29\" ssid=\"22\">Under this measure we obtain 76% F-score.</S>\n    <S sid=\"30\" ssid=\"23\">This paper is organized as follows.</S>\n    <S sid=\"31\" ssid=\"24\">Section 2 describes the Lexical-Functional Grammar, the constraint-based parser, and the robustness techniques employed in this work.</S>\n    <S sid=\"32\" ssid=\"25\">In section 3 we present the details of the exponential model on LFG parses and the discriminative statistical estimation technique.</S>\n    <S sid=\"33\" ssid=\"26\">Experimental results are reported in section 4.</S>\n    <S sid=\"34\" ssid=\"27\">A discussion of results is in section 5.</S>\n  </SECTION>\n  <SECTION title=\"2 Robust Parsing using LFG\" number=\"2\">\n    <S sid=\"35\" ssid=\"1\">The grammar used for this project was developed in the ParGram project (Butt et al., 1999).</S>\n    <S sid=\"36\" ssid=\"2\">It uses LFG as a formalism, producing c(onstituent)-structures (trees) and f(unctional)-structures (attribute value matrices) as output.</S>\n    <S sid=\"37\" ssid=\"3\">The c-structures encode constituency.</S>\n    <S sid=\"38\" ssid=\"4\">F-structures encode predicate-argument relations and other grammatical information, e.g., number, tense.</S>\n    <S sid=\"39\" ssid=\"5\">The XLE parser (Maxwell and Kaplan, 1993) was used to produce packed representations, specifying all possible grammar analyses of the input.</S>\n    <S sid=\"40\" ssid=\"6\">The grammar has 314 rules with regular expression right-hand sides which compile into a collection of finite-state machines with a total of 8,759 states and 19,695 arcs.</S>\n    <S sid=\"41\" ssid=\"7\">The grammar uses several lexicons and two guessers: one guesser for words recognized by the morphological analyzer but not in the lexicons and one for those not recognized.</S>\n    <S sid=\"42\" ssid=\"8\">As such, most nouns, adjectives, and adverbs have no explicit lexical entry.</S>\n    <S sid=\"43\" ssid=\"9\">The main verb lexicon contains 9,652 verb stems and 23,525 subcategorization frame-verb stem entries; there are also lexicons for adjectives and nouns with subcategorization frames and for closed class items.</S>\n    <S sid=\"44\" ssid=\"10\">For estimation purposes using the WSJ treebank, the grammar was modified to parse part of speech tags and labeled bracketing.</S>\n    <S sid=\"45\" ssid=\"11\">A stripped down version of the WSJ treebank was created that used only those POS tags and labeled brackets relevant for determining grammatical relations.</S>\n    <S sid=\"46\" ssid=\"12\">The WSJ labeled brackets are given LFG lexical entries which constrain both the c-structure and the f-structure of the parse.</S>\n    <S sid=\"47\" ssid=\"13\">For example, the WSJ’s ADJP-PRD label must correspond to an AP in the c-structure and an XCOMP in the f-structure.</S>\n    <S sid=\"48\" ssid=\"14\">In this version of the corpus, all WSJ labels with -SBJ are retained and are restricted to phrases corresponding to SUBJ in the LFG grammar; in addition, it contains NP under VP (OBJ and OBJth in the LFG grammar), all -LGS tags (OBL-AG), all -PRD tags (XCOMP), VP under VP (XCOMP), SBAR- (COMP), and verb POS tags under VP (V in the c-structure).</S>\n    <S sid=\"49\" ssid=\"15\">For example, our labeled bracketing of wsj 1305.mrg is [NP-SBJHis credibility] is/VBZ also [PP-PRD on the line] in the investment community.</S>\n    <S sid=\"50\" ssid=\"16\">Some mismatches between the WSJ labeled bracketing and the LFG grammar remain.</S>\n    <S sid=\"51\" ssid=\"17\">These often arise when a given constituent fills a grammatical role in more than one clause.</S>\n    <S sid=\"52\" ssid=\"18\">For example, in wsj 1303.mrg Japan’s Daiwa Securities Co. named Masahiro Dozen president., the noun phrase Masahiro Dozen is labeled as an NP-SBJ.</S>\n    <S sid=\"53\" ssid=\"19\">However, the LFG grammar treats it as the OBJ of the matrix clause.</S>\n    <S sid=\"54\" ssid=\"20\">As a result, the labeled bracketed version of this sentence does not receive a full parse, even though its unlabeled, string-only counterpart is wellformed.</S>\n    <S sid=\"55\" ssid=\"21\">Some other bracketing mismatches remain, usually the result of adjunct attachment.</S>\n    <S sid=\"56\" ssid=\"22\">Such mismatches occur in part because, besides minor modifications to match the bracketing for special constructions, e.g., negated infinitives, the grammar was not altered to mirror the idiosyncrasies of the WSJ bracketing.</S>\n    <S sid=\"57\" ssid=\"23\">To increase robustness, the standard grammar has been augmented with a FRAGMENT grammar.</S>\n    <S sid=\"58\" ssid=\"24\">This grammar parses the sentence as well-formed chunks specified by the grammar, in particular as Ss, NPs, PPs, and VPs.</S>\n    <S sid=\"59\" ssid=\"25\">These chunks have both c-structures and f-structures corresponding to them.</S>\n    <S sid=\"60\" ssid=\"26\">Any token that cannot be parsed as one of these chunks is parsed as a TOKEN chunk.</S>\n    <S sid=\"61\" ssid=\"27\">The TOKENs are also recorded in the c- and f-structures.</S>\n    <S sid=\"62\" ssid=\"28\">The grammar has a fewest-chunk method for determining the correct parse.</S>\n    <S sid=\"63\" ssid=\"29\">For example, if a string can be parsed as two NPs and a VP or as one NP and an S, the NP-S option is chosen.</S>\n    <S sid=\"64\" ssid=\"30\">A sample FRAGMENT c-structure and f-structure are shown in Fig.</S>\n    <S sid=\"65\" ssid=\"31\">1 for wsj 0231.mrg (The golden share was scheduled to expire at the beginning of), an incomplete sentence; the parser builds one S chunk and then one TOKEN for the stranded preposition.</S>\n    <S sid=\"66\" ssid=\"32\">A final capability of XLE that increases coverage of the standard-plus-fragment grammar is a SKIMMING technique.</S>\n    <S sid=\"67\" ssid=\"33\">Skimming is used to avoid timeouts and memory problems.</S>\n    <S sid=\"68\" ssid=\"34\">When the amount of time or memory spent on a sentence exceeds a threshhold, XLE goes into skimming mode for the constituents whose processing has not been completed.</S>\n    <S sid=\"69\" ssid=\"35\">When XLE skims these remaining constituents, it does a bounded amount of work per subtree.</S>\n    <S sid=\"70\" ssid=\"36\">This guarantees that XLE finishes processing a sentence in a polynomial amount of time.</S>\n    <S sid=\"71\" ssid=\"37\">In parsing section 23, 7.2% of the sentences were skimmed; 26.1% of these resulted in full parses, while 73.9% were FRAGMENT parses.</S>\n    <S sid=\"72\" ssid=\"38\">The grammar coverage achieved 100% of section 23 as unseen unlabeled data: 74.7% as full parses, 25.3% FRAGMENT and/or SKIMMED parses.</S>\n  </SECTION>\n  <SECTION title=\"3 Discriminative Statistical Estimation from Partially Labeled Data\" number=\"3\">\n    <S sid=\"73\" ssid=\"1\">We employed the well-known family of exponential models for stochastic disambiguation.</S>\n    <S sid=\"74\" ssid=\"2\">In this paper we are concerned with conditional exponential models of the form: where X(y) is the set of parses for sentence y, Zλ(y) = PxEX(y) eλ'f(x) is a normalizing constant, λ = (λ1, ... , λn) E IRn is a vector of log-parameters, f = (f1, ... , fn) is a vector of property-functions fi : X IR for i = 1, ... , n on the set of parses X, and λ f(x) is the vector dot product Pni=1 λifi(x).</S>\n    <S sid=\"75\" ssid=\"3\">In our experiments, we used around 1000 complex property-functions comprising information about c-structure, f-structure, and lexical elements in parses, similar to the properties used in Johnson et al. (1999).</S>\n    <S sid=\"76\" ssid=\"4\">For example, there are property functions for c-structure nodes and c-structure subtrees, indicating attachment preferences.</S>\n    <S sid=\"77\" ssid=\"5\">High versus low attachment is indicated by property functions counting the number of recursively embedded phrases.</S>\n    <S sid=\"78\" ssid=\"6\">Other property functions are designed to refer to f-structure attributes, which correspond to grammatical functions in LFG, or to atomic attributevalue pairs in f-structures.</S>\n    <S sid=\"79\" ssid=\"7\">More complex property functions are designed to indicate, for example, the branching behaviour of c-structures and the (non)parallelism of coordinations on both c-structure and f-structure levels.</S>\n    <S sid=\"80\" ssid=\"8\">Furthermore, properties refering to lexical elements based on an auxiliary distribution approach as presented in Riezler et al. (2000) are included in the model.</S>\n    <S sid=\"81\" ssid=\"9\">Here tuples of head words, argument words, and grammatical relations are extracted from the training sections of the WSJ, and fed into a finite mixture model for clustering grammatical relations.</S>\n    <S sid=\"82\" ssid=\"10\">The clustering model itself is then used to yield smoothed probabilities as values for property functions on head-argument-relation tuples of LFG parses.</S>\n    <S sid=\"83\" ssid=\"11\">Discriminative estimation techniques have recently received great attention in the statistical machine learning community and have already been applied to statistical parsing (Johnson et al., 1999; Collins, 2000; Collins and Duffy, 2001).</S>\n    <S sid=\"84\" ssid=\"12\">In discriminative estimation, only the conditional relation of an analysis given an example is considered relevant, whereas in maximum likelihood estimation the joint probability of the training data to best describe observations is maximized.</S>\n    <S sid=\"85\" ssid=\"13\">Since the discriminative task is kept in mind during estimation, discriminative methods can yield improved performance.</S>\n    <S sid=\"86\" ssid=\"14\">In our case, discriminative criteria cannot be defined directly with respect to “correct labels” or “gold standard” parses since the WSJ annotations are not sufficient to disambiguate the more complex LFG parses.</S>\n    <S sid=\"87\" ssid=\"15\">However, instead of retreating to unsupervised estimation techniques or creating small LFG treebanks by hand, we use the labeled bracketing of the WSJ training sections to guide discriminative estimation.</S>\n    <S sid=\"88\" ssid=\"16\">That is, discriminative criteria are defined with respect to the set ofparses consistent with the WSJ annotations.1 The objective function in our approach, denoted by P(λ), is the joint of the negative log-likelihood −L(λ) and a Gaussian regularization term −G(λ) on the parameters λ.</S>\n    <S sid=\"89\" ssid=\"17\">Let {(yj, zj)Imj=1 be a set of training data, consisting of pairs of sentences y and partial annotations z, let X(y, z) be the set of parses for sentence y consistent with annotation z, and let X(y) be the set of all parses produced by the grammar for sentence y.</S>\n    <S sid=\"90\" ssid=\"18\">Furthermore, let p[f] denote the expectation of function f under distribution p. Then P(λ) can be defined for a conditional exponential model pλ(zIy) as: Intuitively, the goal of estimation is to find model pa'An earlier approach using partially labeled data for estimating stochastics parsers is Pereira and Schabes’s (1992) work on training PCFG from partially bracketed data.</S>\n    <S sid=\"91\" ssid=\"19\">Their approach differs from the one we use here in that Pereira and Schabes take an EM-based approach maximizing the joint likelihood of the parses and strings of their training data, while we maximize the conditional likelihood of the sets of parses given the corresponding strings in a discriminative estimation setting. rameters which make the two expectations in the last equation equal, i.e. which adjust the model parameters to put all the weight on the parses consistent with the annotations, modulo a penalty term from the Gaussian prior for too large or too small weights.</S>\n    <S sid=\"92\" ssid=\"20\">Since a closed form solution for such parameters is not available, numerical optimization methods have to be used.</S>\n    <S sid=\"93\" ssid=\"21\">In our experiments, we applied a conjugate gradient routine, yielding a fast converging optimization algorithm where at each iteration the negative log-likelihood P(λ) and the gradient vector have to be evaluated.2 For our task the gradient takes the form: The derivatives in the gradient vector intuitively are again just a difference of two expectations Note also that this expression shares many common terms with the likelihood function, suggesting an efficient implementation of the optimization routine.</S>\n  </SECTION>\n  <SECTION title=\"4 Experimental Evaluation\" number=\"4\">\n    <S sid=\"94\" ssid=\"1\">The basic training data for our experiments are sections 02-21 of the WSJ treebank.</S>\n    <S sid=\"95\" ssid=\"2\">As a first step, all sections were parsed, and the packed parse forests unpacked and stored.</S>\n    <S sid=\"96\" ssid=\"3\">For discriminative estimation, this data set was restricted to sentences which receive a full parse (in contrast to a FRAGMENT or SKIMMED parse) for both its partially labeled and its unlabeled variant.</S>\n    <S sid=\"97\" ssid=\"4\">Furthermore, only sentences 2An alternative numerical method would be a combination of iterative scaling techniques with a conditional EM algorithm (Jebara and Pentland, 1998).</S>\n    <S sid=\"98\" ssid=\"5\">However, it has been shown experimentally that conjugate gradient techniques can outperform iterative scaling techniques by far in running time (Minka, 2001). which received at most 1,000 parses were used.</S>\n    <S sid=\"99\" ssid=\"6\">From this set, sentences of which a discriminative learner cannot possibly take advantage, i.e. sentences where the set of parses assigned to the partially labeled string was not a proper subset of the parses assigned the unlabeled string, were removed.</S>\n    <S sid=\"100\" ssid=\"7\">These successive selection steps resulted in a final training set consisting of 10,000 sentences, each with parses for partially labeled and unlabeled versions.</S>\n    <S sid=\"101\" ssid=\"8\">Altogether there were 150,000 parses for partially labeled input and 500,000 for unlabeled input.</S>\n    <S sid=\"102\" ssid=\"9\">For estimation, a simple property selection procedure was applied to the full set of around 1000 properties.</S>\n    <S sid=\"103\" ssid=\"10\">This procedure is based on a frequency cutoff on instantiations of properties for the parses in the labeled training set.</S>\n    <S sid=\"104\" ssid=\"11\">The result of this procedure is a reduction of the property vector to about half its size.</S>\n    <S sid=\"105\" ssid=\"12\">Furthermore, a held-out data set was created from section 24 of the WSJ treebank for experimental selection of the variance parameter of the prior distribution.</S>\n    <S sid=\"106\" ssid=\"13\">This set consists of 120 sentences which received only full parses, out of which the most plausible one was selected manually.</S>\n    <S sid=\"107\" ssid=\"14\">Two different sets of test data were used: (i) 700 sentences randomly extracted from section 23 of the WSJ treebank and given gold-standard f-structure annotations according to our LFG scheme, and (ii) 500 sentences from the Brown corpus given gold standard annotations by Carroll et al. (1999) according to their dependency relations (DR) scheme.3 Annotating the WSJ test set was bootstrapped by parsing the test sentences using the LFG grammar and also checking for consistency with the Penn Treebank annotation.</S>\n    <S sid=\"108\" ssid=\"15\">Starting from the (sometimes fragmentary) parser analyses and the Treebank annotations, gold standard parses were created by manual corrections and extensions of the LFG parses.</S>\n    <S sid=\"109\" ssid=\"16\">Manual corrections were necessary in about half of the cases.</S>\n    <S sid=\"110\" ssid=\"17\">The average sentence length of the WSJ f-structure bank is 19.8 words; the average number of predicate-argument relations in the goldstandard f-structures is 31.2.</S>\n    <S sid=\"111\" ssid=\"18\">Performance on the LFG-annotated WSJ test set was measured using both the LFG and DR metrics, thanks to an f-structure-to-DR annotation mapping.</S>\n    <S sid=\"112\" ssid=\"19\">Performance on the DR-annotated Brown test set was only measured using the DR metric.</S>\n    <S sid=\"113\" ssid=\"20\">The LFG evaluation metric is based on the comparison of full f-structures, represented as triples relation(predicate, argument).</S>\n    <S sid=\"114\" ssid=\"21\">The predicateargument relations of the f-structure for one parse of the sentence Meridian will pay a premium of $30.5 million to assume $2 billion in deposits. are shown in Fig.</S>\n    <S sid=\"115\" ssid=\"22\">2.</S>\n    <S sid=\"116\" ssid=\"23\">The DR annotation for our example sentence, obtained via a mapping from f-structures to Carroll et al’s annotation scheme, is shown in Fig.</S>\n    <S sid=\"117\" ssid=\"24\">3.</S>\n    <S sid=\"118\" ssid=\"25\">Superficially, the LFG and DR representations are very similar.</S>\n    <S sid=\"119\" ssid=\"26\">One difference between the annotation schemes is that the LFG representation in general specifies more relation tuples than the DR representation.</S>\n    <S sid=\"120\" ssid=\"27\">Also, multiple occurences of the same lexical item are indicated explicitly in the LFG representation but not in the DR representation.</S>\n    <S sid=\"121\" ssid=\"28\">The main conceptual difference between the two annotation schemes is the fact that the DR scheme crucially refers to phrase-structure properties and word order as well as to grammatical relations in the definition of dependency relations, whereas the LFG scheme abstracts away from serialization and phrase-structure.</S>\n    <S sid=\"122\" ssid=\"29\">Facts like this can make a correct mapping of LFG f-structures to DR relations problematic.</S>\n    <S sid=\"123\" ssid=\"30\">Indeed, we believe that we still underestimate by a few points because of DR mapping difficulties.</S>\n    <S sid=\"124\" ssid=\"31\">4 In our evaluation, we report F-scores for both types of annotation, LFG and DR, and for three types of parse selection, (i) lower bound: random choice of a parse from the set of analyses (averaged over 10 runs), (ii) upper bound: selection of the parse with the best F-score according to the annotation scheme used, and (iii) stochastic: the parse selected by the stochastic disambiguator.</S>\n    <S sid=\"125\" ssid=\"32\">The error reduction row lists the reduction in error rate relative to the upper and lower bounds obtained by the stochastic disambiguation model.</S>\n    <S sid=\"126\" ssid=\"33\">F-score is defined as 2 × precision × recall/(precision + recall).</S>\n    <S sid=\"127\" ssid=\"34\">The effect of the quality of the parses on disambiguation performance can be illustrated by breaking down the F-scores according to whether the parser yields full parses, FRAGMENT, SKIMMED, or SKIMMED+FRAGMENT parses for the test sentences.</S>\n    <S sid=\"128\" ssid=\"35\">The percentages of test examples which belong to the respective classes of quality are listed in the first row of Table 2.</S>\n    <S sid=\"129\" ssid=\"36\">F-scores broken down according to classes of parse quality are recorded in the following rows.</S>\n    <S sid=\"130\" ssid=\"37\">The first column shows F-scores for all parses in the test set, as in Table 1.</S>\n    <S sid=\"131\" ssid=\"38\">The second column shows the best F-scores when restricting attention to examples which receive only full parses.</S>\n    <S sid=\"132\" ssid=\"39\">The third column reports F-scores for examples which receive only non-full parses, i.e.</S>\n    <S sid=\"133\" ssid=\"40\">FRAGMENT or SKIMMED parses or SKIMMED+FRAGMENT parses.</S>\n    <S sid=\"134\" ssid=\"41\">Columns 4-6 break down non-full parses according to examples which receive only FRAGMENT, only SKIMMED, or only SKIMMED+FRAGMENT parses.</S>\n    <S sid=\"135\" ssid=\"42\">Results of the evaluation on Carroll et al.’s Brown test set are given in Table 3.</S>\n    <S sid=\"136\" ssid=\"43\">Evaluation results for the DR measure applied to the Brown corpus test set broken down according to parse-quality are shown in Table 2.</S>\n    <S sid=\"137\" ssid=\"44\">In Table 3 we show the DR measure along with an evaluation measure which facilitates a direct comparison of our results to those of Carroll et al. (1999).</S>\n    <S sid=\"138\" ssid=\"45\">Following Carroll et al. (1999), we count a dependency relation as correct if the gold standard has a relation with the same governor and dependent but perhaps with a different relation-type.</S>\n    <S sid=\"139\" ssid=\"46\">This dependency-only (DO) measure thus does not reflect mismatches between arguments and modifiers in a small number of cases.</S>\n    <S sid=\"140\" ssid=\"47\">Note that since for the evaluation on the Brown corpus, no heldout data were available to adjust the variance parameter of a Bayesian model, we used a plain maximumlikelihood model for disambiguation on this test set.</S>\n  </SECTION>\n  <SECTION title=\"5 Discussion\" number=\"5\">\n    <S sid=\"141\" ssid=\"1\">We have presented a first attempt at scaling up a stochastic parsing system combining a hand-coded linguistically fine-grained grammar and a stochastic disambiguation model to the WSJ treebank.</S>\n    <S sid=\"142\" ssid=\"2\">Full grammar coverage is achieved by combining specialized constraint-based parsing techniques for LFG grammars with partial parsing techniques.</S>\n    <S sid=\"143\" ssid=\"3\">Furthermore, a maximal exploitation of treebank annotations for estimating a distribution on fine-grained LFG parses is achieved by letting grammar analyses which are consistent with the WSJ labeled bracketing define a gold standard set for discriminative estimation.</S>\n    <S sid=\"144\" ssid=\"4\">The combined system trained on WSJ data achieves full grammar coverage and disambiguation performance of 79% F-score on WSJ data, and 76% F-score on the Brown corpus test set.</S>\n    <S sid=\"145\" ssid=\"5\">While disambiguation performance of around 79% F-score on WSJ data seems promising, from one perspective it only offers a 3% absolute improvement over a lower bound random baseline.</S>\n    <S sid=\"146\" ssid=\"6\">We think that the high lower bound measure highlights an important aspect of symbolic constraintbased grammars (in contrast to treebank grammars): the symbolic grammar already significantly restricts/disambiguates the range of possible analyses, giving the disambiguator a much narrower window in which to operate.</S>\n    <S sid=\"147\" ssid=\"7\">As such, it is more appropriate to assess the disambiguator in terms of reduction in error rate (36% relative to the upper bound) than in terms of absolute F-score.</S>\n    <S sid=\"148\" ssid=\"8\">Both the DR and LFG annotations broadly agree in their measure of error reduction.</S>\n    <S sid=\"149\" ssid=\"9\">The lower reduction in error rate relative to the upper bound for DR evaluation on the Brown corpus can be attributed to a corpus effect that has also been observed by Gildea (2001) for training and testing PCFGs on the WSJ and Brown corpora.5 Breaking down results according to parse quality shows that irrespective of evaluation measure and corpus, around 4% overall performance is lost due to non-full parses, i.e.</S>\n    <S sid=\"150\" ssid=\"10\">FRAGMENT, or SKIMMED, or SKIMMED+FRAGMENT parses.</S>\n    <S sid=\"151\" ssid=\"11\">Due to the lack of standard evaluation measures and gold standards for predicate-argument matching, a comparison of our results to other stochastic parsing systems is difficult.</S>\n    <S sid=\"152\" ssid=\"12\">To our knowledge, so far the only direct point of comparison is the parser of Carroll et al. (1999) which is also evaluated on Carroll et al.’s test corpus.</S>\n    <S sid=\"153\" ssid=\"13\">They report an F-score of 75.1% for a DO evaluation that ignores predicate labels, counting only dependencies.</S>\n    <S sid=\"154\" ssid=\"14\">Under this measure, our system achieves 76.1% F-score.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "48d6707f95da2aaac392fde21c42e570",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Parsing The Wall Street Journal Using A Lexical-Functional Grammar And Discriminative Estimation Techniques"
                }
            }
        },
        {
            "index": 9,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Sense induction seeks to automatically identify word senses directly from a corpus. A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning. Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense. Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words. The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical cooccurrences and to systematically assess their utility on the sense induction task. The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.",
                    "filename": "Bayesian_Word_Sense_Induction",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/E09-1013/Documents_xml/E09-1013.xml",
                    "title": "Bayesian Word Sense Induction"
                },
                "llm": {
                    "descriptions": [
                        "This academic paper presents a novel Bayesian approach to word sense induction, which is the automatic discovery of a word's possible meanings. Unlike word sense disambiguation, which operates with known senses, sense induction generates meaning distinctions directly from data. The proposed model, inspired by Latent Dirichlet Allocation, generates context words from local topics and can integrate lexical information, parts of speech, and dependencies. The authors demonstrate the model's effectiveness on a benchmark dataset, showing improvements over state-of-the-art systems. They also explore the influence of different features and the number of senses on the model's performance. The paper concludes with potential enhancements, including integrating hyperparameter estimation into the Gibbs sampling process and adopting an infinite Dirichlet model to determine the optimal number of senses.",
                        "This paper introduces a Bayesian approach to word sense induction, which is the task of automatically identifying all possible meanings of an ambiguous word. Unlike word sense disambiguation, which assumes known senses, sense induction infers sense distinctions directly from data, potentially offering more domain-specific relevance and adaptability to languages with limited lexical resources. The authors present a generative model that draws a distribution over senses and generates context words accordingly, assuming that different senses correspond to distinct lexical distributions. This approach is inspired by Latent Dirichlet Allocation but adjusted for local context around ambiguous words, rather than global document topics. The model integrates multiple information sources, such as lexical data, parts of speech, and dependencies. The proposed model is evaluated on a benchmark dataset, showing improvements over state-of-the-art methods. The authors suggest that the Bayesian framework could be adapted for other tasks requiring integration of multi-level information.",
                        "This paper introduces a Bayesian approach to word sense induction, which involves automatically identifying all possible senses of an ambiguous word. The authors propose a generative model that creates a distribution over senses for each ambiguous word and generates context words based on this distribution. Unlike previous methods which rely on fixed sense inventories from dictionaries or lexical resources, the Bayesian model infers sense distinctions directly from data, leading to more relevant and adaptable representations for specific tasks and domains. The model outperforms state-of-the-art systems on a benchmark dataset while also integrating multiple information sources, such as lexical information, parts of speech, and dependencies. This flexible model can be expanded beyond sense induction to other applications requiring multi-layered information integration.",
                        "This paper introduces a novel Bayesian approach to word sense induction, which is distinct from word sense disambiguation (WSD) in that it aims to discover all possible senses of an ambiguous word automatically. Unlike traditional WSD methods that rely on predetermined senses from lexical resources, sense induction benefits from the ability to adapt to new domains or languages with scarce resources and can better represent the granularity needed for specific applications. The proposed model uses Bayesian inference to generate context words based on a distribution over senses, assuming that different senses correspond to distinct lexical distributions. The model shows improvements over state-of-the-art systems on a benchmark dataset and demonstrates the ability to integrate multiple information sources such as parts of speech and dependencies. Furthermore, it has the potential to be applied to other tasks beyond sense induction."
                    ],
                    "filename": "bayesian_word_sense_induction-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 133 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Bayesian Word Sense Induction</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Sense induction is the task of discovering automatically all possible senses of an ambiguous word.</S>\n    <S sid=\"8\" ssid=\"2\">It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context.</S>\n    <S sid=\"9\" ssid=\"3\">Although the bulk of previous work has been devoted to the disambiguation problem1, there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD.</S>\n    <S sid=\"10\" ssid=\"4\">Since most disambiguation methods assign senses according to, and with the aid of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce.</S>\n    <S sid=\"11\" ssid=\"5\">A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.</S>\n    <S sid=\"12\" ssid=\"6\">In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.</S>\n    <S sid=\"13\" ssid=\"7\">There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.</S>\n    <S sid=\"14\" ssid=\"8\">Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993).</S>\n    <S sid=\"15\" ssid=\"9\">Sense induction is typically treated as an unsupervised clustering problem.</S>\n    <S sid=\"16\" ssid=\"10\">The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.</S>\n    <S sid=\"17\" ssid=\"11\">In other words, contexts that are grouped together in the same class represent a specific word sense.</S>\n    <S sid=\"18\" ssid=\"12\">In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.</S>\n    <S sid=\"19\" ssid=\"13\">For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution.</S>\n    <S sid=\"20\" ssid=\"14\">It is thus assumed that different senses will correspond to distinct lexical distributions.</S>\n    <S sid=\"21\" ssid=\"15\">In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning).</S>\n    <S sid=\"22\" ssid=\"16\">Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.</S>\n    <S sid=\"23\" ssid=\"17\">LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words.</S>\n    <S sid=\"24\" ssid=\"18\">The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic.</S>\n    <S sid=\"25\" ssid=\"19\">Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.</S>\n    <S sid=\"26\" ssid=\"20\">Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.</S>\n    <S sid=\"27\" ssid=\"21\">In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection.</S>\n    <S sid=\"28\" ssid=\"22\">We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.</S>\n    <S sid=\"29\" ssid=\"23\">For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem.</S>\n    <S sid=\"30\" ssid=\"24\">This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.</S>\n    <S sid=\"31\" ssid=\"25\">We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.</S>\n    <S sid=\"32\" ssid=\"26\">The remainder of this paper is structured as follows.</S>\n    <S sid=\"33\" ssid=\"27\">We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).</S>\n    <S sid=\"34\" ssid=\"28\">Section 5 describes the resources and evaluation methodology used in our experiments.</S>\n    <S sid=\"35\" ssid=\"29\">We discuss our results in Section 6, and conclude in Section 7.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"36\" ssid=\"1\">Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts.</S>\n    <S sid=\"37\" ssid=\"2\">Considerable latitude is allowed in selecting and representing the cooccurring contexts.</S>\n    <S sid=\"38\" ssid=\"3\">Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003).</S>\n    <S sid=\"39\" ssid=\"4\">The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).</S>\n    <S sid=\"40\" ssid=\"5\">In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.</S>\n    <S sid=\"41\" ssid=\"6\">A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007).</S>\n    <S sid=\"42\" ssid=\"7\">Graph-based methods have also been applied to the sense induction task.</S>\n    <S sid=\"43\" ssid=\"8\">In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences.</S>\n    <S sid=\"44\" ssid=\"9\">Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003).</S>\n    <S sid=\"45\" ssid=\"10\">Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature.</S>\n    <S sid=\"46\" ssid=\"11\">The inferred document-level topics can help determine coarsegrained sense distinctions.</S>\n    <S sid=\"47\" ssid=\"12\">Cai et al. (2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.</S>\n    <S sid=\"48\" ssid=\"13\">In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD.</S>\n    <S sid=\"49\" ssid=\"14\">Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm.</S>\n    <S sid=\"50\" ssid=\"15\">For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.</S>\n    <S sid=\"51\" ssid=\"16\">Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word.</S>\n    <S sid=\"52\" ssid=\"17\">Then, the word sense is selected based on the word, neighbor, and topic.</S>\n    <S sid=\"53\" ssid=\"18\">Boyd-Graber et al. (2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.</S>\n    <S sid=\"54\" ssid=\"19\">In this case the model discovers both the topics of the corpus and the senses assigned to each of its words.</S>\n    <S sid=\"55\" ssid=\"20\">Our own model is also inspired by LDA but crucially performs word sense induction, not disambiguation.</S>\n    <S sid=\"56\" ssid=\"21\">Unlike the work mentioned above, we do not rely on a pre-existing list of senses, and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the tendency to remain at a high level, document-like, setting.</S>\n    <S sid=\"57\" ssid=\"22\">In contrast, we make use of much smaller units of text (a few sentences, rather than a full document), and create an individual model for each (ambiguous) word type.</S>\n    <S sid=\"58\" ssid=\"23\">Our induced senses are few in number (typically less than ten).</S>\n    <S sid=\"59\" ssid=\"24\">This is in marked contrast to tens, and sometimes hundreds, of topics commonly used in document-modeling tasks.</S>\n    <S sid=\"60\" ssid=\"25\">Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch¨utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models.</S>\n    <S sid=\"61\" ssid=\"26\">Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.</S>\n    <S sid=\"62\" ssid=\"27\">Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies).</S>\n    <S sid=\"63\" ssid=\"28\">This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.</S>\n    <S sid=\"64\" ssid=\"29\">Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.</S>\n    <S sid=\"65\" ssid=\"30\">2003) have been task-specific and limited to only two layers of information.</S>\n    <S sid=\"66\" ssid=\"31\">Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.</S>\n  </SECTION>\n  <SECTION title=\"3 The Sense Induction Model\" number=\"3\">\n    <S sid=\"67\" ssid=\"1\">The core idea behind sense induction is that contextual information provides important cues regarding a word’s meaning.</S>\n    <S sid=\"68\" ssid=\"2\">The idea dates back to (at least) Firth (1957) (“You shall know a word by the company it keeps”), and underlies most WSD and lexicon acquisition work to date.</S>\n    <S sid=\"69\" ssid=\"3\">Under this premise, we should expect different senses to be signaled by different lexical distributions.</S>\n    <S sid=\"70\" ssid=\"4\">We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.</S>\n    <S sid=\"71\" ssid=\"5\">More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s. Each word wi in the context window is generated by first sampling a sense from the sense distribution, then choosing a word from the sense-context distribution.</S>\n    <S sid=\"72\" ssid=\"6\">P(si = j) denotes the probability that the jth sense was sampled for the ith word token and P(wi|si = j) the probability of context word wi under sense j.</S>\n    <S sid=\"73\" ssid=\"7\">The model thus specifies a distribution over words within a context window: where S is the number of senses.</S>\n    <S sid=\"74\" ssid=\"8\">We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.</S>\n    <S sid=\"75\" ssid=\"9\">The variables in the lower right corner refer to the number of samples. consists of Nc word tokens.</S>\n    <S sid=\"76\" ssid=\"10\">We shall write �(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c. Following Blei et al. (2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a.</S>\n    <S sid=\"77\" ssid=\"11\">The role of the hyperparameter a is to create a smoothed sense distribution.</S>\n    <S sid=\"78\" ssid=\"12\">We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002).</S>\n    <S sid=\"79\" ssid=\"13\">The hyperparmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed.</S>\n    <S sid=\"80\" ssid=\"14\">Our model is represented in graphical notation in Figure 1.</S>\n    <S sid=\"81\" ssid=\"15\">The model sketched above only takes word information into account.</S>\n    <S sid=\"82\" ssid=\"16\">Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002).</S>\n    <S sid=\"83\" ssid=\"17\">The first idea that comes to mind, is to use the same model while treating various features as word-like elements.</S>\n    <S sid=\"84\" ssid=\"18\">In other words, we could simply assume that the contexts we wish to model are the union of all our features.</S>\n    <S sid=\"85\" ssid=\"19\">Although straightforward, this solution is undesirable.</S>\n    <S sid=\"86\" ssid=\"20\">It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model.</S>\n    <S sid=\"87\" ssid=\"21\">For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser).</S>\n    <S sid=\"88\" ssid=\"22\">Layers containing more elements (e.g.</S>\n    <S sid=\"89\" ssid=\"23\">10 word window) would overwhelm rectangles represent different sources (layers) of information.</S>\n    <S sid=\"90\" ssid=\"24\">All layers share the same, instancespecific, sense distribution (0), but each have their own (multinomial) sense-feature distribution (�).</S>\n    <S sid=\"91\" ssid=\"25\">Shaded nodes represent observed features f; these can be words, parts of speech, collocations or dependencies. unconditional joint distribution P(s) of the unobserved variables (provided certain criteria are fulfilled).</S>\n    <S sid=\"92\" ssid=\"26\">In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).</S>\n    <S sid=\"93\" ssid=\"27\">From these assignments, we must determine the sense distribution of the instance as a whole.</S>\n    <S sid=\"94\" ssid=\"28\">This is the purpose of the Gibbs sampling procedure.</S>\n    <S sid=\"95\" ssid=\"29\">Specifically, in order to derive the update function used in the Gibbs sampler, we must provide the conditional probability of the i-th variable being assigned sense si in layer l, given the feature value fi of the context variable and the current sense assignments of all the other variables in the data (s−i): p(si|s−i, f) — p(fi|s, f −i,R) · p(si|s−i,a) (2) The probability of a single sense assignment, si, is proportional to the product of the likelihood (of feature fi, given the rest of the data) and the prior probability of the assignment. smaller ones (e.g.</S>\n    <S sid=\"96\" ssid=\"30\">1 word window).</S>\n    <S sid=\"97\" ssid=\"31\">Our solution is to treat each information source (or feature type) individually and then combine all of them together in a unified model.</S>\n    <S sid=\"98\" ssid=\"32\">Our underlying assumption is that the context window around the target word can have multiple representations, all of which share the same sense distribution.</S>\n    <S sid=\"99\" ssid=\"33\">We illustrate this in Figure 2 where each inner rectangle (layer) corresponds to a distinct feature type.</S>\n    <S sid=\"100\" ssid=\"34\">We will naively assume independence between multiple layers, even though this is clearly not the case in our task.</S>\n    <S sid=\"101\" ssid=\"35\">The idea here is to model each layer as faithfully as possible to the empirical data while at the same time combining information from all layers in estimating the sense distribution of each target instance.</S>\n  </SECTION>\n  <SECTION title=\"4 Inference\" number=\"4\">\n    <S sid=\"102\" ssid=\"1\">Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984).</S>\n    <S sid=\"103\" ssid=\"2\">The procedure begins by randomly initializing all unobserved random variables.</S>\n    <S sid=\"104\" ssid=\"3\">At each iteration, each random variable si is sampled from the conditional distribution P(si|s−i) where s−i refers to all variables other than si.</S>\n    <S sid=\"105\" ssid=\"4\">Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) · p(O |f-i, Rt)dO _ #(fi, si) + Rl For the likelihood term p(fi|s, f −i,R), integrating over all possible values of the multinomial featuresense distribution � gives us the rightmost term in Equation 3, which has an intuitive interpretation.</S>\n    <S sid=\"106\" ssid=\"5\">The term #(fi,si) indicates the number of times the feature-value fi was assigned sense si in the rest of the data.</S>\n    <S sid=\"107\" ssid=\"6\">Similarly, #(si) indicates the number of times the sense assignment si was observed in the data.</S>\n    <S sid=\"108\" ssid=\"7\">Rl is the Dirichlet prior for the featuresense distribution � in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.</S>\n    <S sid=\"109\" ssid=\"8\">Intuitively, the probability of a feature-value given a sense is directly proportional to the number of times we have seen that value and that senseassignment together in the data, taking into account a pseudo-count prior, expressed through R. This can also be viewed as a form of smoothing.</S>\n    <S sid=\"110\" ssid=\"9\">A similar approach is taken with regards to the prior probability p(si|s−i,a).</S>\n    <S sid=\"111\" ssid=\"10\">In this case, however, all layers must be considered: Here λl is the weight for the contribution of layer l, and αl is the portion of the Dirichlet prior for the sense distribution θ in the current layer.</S>\n    <S sid=\"112\" ssid=\"11\">Treating each layer individually, we integrate over the possible values of θ, obtaining a similar count-based term: where #l(si) indicates the number of elements in layer l assigned the sense si, #l indicates the number of elements in layer l, i.e., the size of the layer and S the number of senses.</S>\n    <S sid=\"113\" ssid=\"12\">To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.</S>\n    <S sid=\"114\" ssid=\"13\">This distributes α according to the relative size of each layer in the instance.</S>\n    <S sid=\"115\" ssid=\"14\">Placing these values in Equation 4 we obtain the following: #m+S·α Putting it all together, we arrive at the final update equation for the Gibbs sampling: Note that when dealing with a single layer, Equation 8 collapses to: where #m(si) indicates the number of elements (e.g., words) in the context window assigned to sense si.</S>\n    <S sid=\"116\" ssid=\"15\">This is identical to the update equation in the original, word-based LDA model.</S>\n    <S sid=\"117\" ssid=\"16\">The sampling algorithm gives direct estimates of s for every context element.</S>\n    <S sid=\"118\" ssid=\"17\">However, in view of our task, we are more interested in estimating θ, the sense-context distribution which can be obtained as in Equation 7, but taking into account all sense assignments, without removing assignment i.</S>\n    <S sid=\"119\" ssid=\"18\">Our system labels each instance with the single, most probable sense.</S>\n  </SECTION>\n  <SECTION title=\"5 Evaluation Setup\" number=\"5\">\n    <S sid=\"120\" ssid=\"1\">In this section we discuss our experimental set-up for assessing the performance of the model presented above.</S>\n    <S sid=\"121\" ssid=\"2\">We give details on our training procedure, describe our features, and explain how our system output was evaluated.</S>\n    <S sid=\"122\" ssid=\"3\">Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words.</S>\n    <S sid=\"123\" ssid=\"4\">For example, nouns represent 45% of the content words in the British National Corpus.</S>\n    <S sid=\"124\" ssid=\"5\">Moreover, for many tasks and applications (e.g., web queries, Jansen et al. 2000) nouns are the most frequent and most important part-of-speech.</S>\n    <S sid=\"125\" ssid=\"6\">For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).</S>\n    <S sid=\"126\" ssid=\"7\">The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ).</S>\n    <S sid=\"127\" ssid=\"8\">It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns.</S>\n    <S sid=\"128\" ssid=\"9\">The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense.</S>\n    <S sid=\"129\" ssid=\"10\">This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses.</S>\n    <S sid=\"130\" ssid=\"11\">We used two corpora for training as we wanted to evaluate our model’s performance across different domains.</S>\n    <S sid=\"131\" ssid=\"12\">The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.</S>\n    <S sid=\"132\" ssid=\"13\">This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.</S>\n    <S sid=\"133\" ssid=\"14\">The second, in-domain, corpus was built from selected portions of the Wall Street Journal.</S>\n    <S sid=\"134\" ssid=\"15\">We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.</S>\n    <S sid=\"135\" ssid=\"16\">Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).</S>\n    <S sid=\"136\" ssid=\"17\">Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods.</S>\n    <S sid=\"137\" ssid=\"18\">Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).</S>\n    <S sid=\"138\" ssid=\"19\">Here, no attempt is made to match the induced senses against the labels of the gold standard.</S>\n    <S sid=\"139\" ssid=\"20\">Under the second scheme, the gold standard is partitioned into a test and training corpus.</S>\n    <S sid=\"140\" ssid=\"21\">The latter is used to derive a mapping of the induced senses to the gold standard labels.</S>\n    <S sid=\"141\" ssid=\"22\">The mapping is then used to calculate the system’s F-Score on the test corpus.</S>\n    <S sid=\"142\" ssid=\"23\">Unfortunately, the first scheme failed to discriminate among participating systems.</S>\n    <S sid=\"143\" ssid=\"24\">The onecluster-per-word baseline outperformed all systems, except one, which was only marginally better.</S>\n    <S sid=\"144\" ssid=\"25\">The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.</S>\n    <S sid=\"145\" ssid=\"26\">For the purposes of this work, therefore, we focused on the second evaluation scheme.</S>\n    <S sid=\"146\" ssid=\"27\">Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores.</S>\n    <S sid=\"147\" ssid=\"28\">Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context.</S>\n    <S sid=\"148\" ssid=\"29\">Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).</S>\n    <S sid=\"149\" ssid=\"30\">These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation).</S>\n    <S sid=\"150\" ssid=\"31\">In all cases, we use the lemmatized version of the word(s).</S>\n    <S sid=\"151\" ssid=\"32\">The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word).</S>\n    <S sid=\"152\" ssid=\"33\">This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.</S>\n    <S sid=\"153\" ssid=\"34\">For instances containing more than one occurrence of the target word, we disambiguate the first occurrence.</S>\n    <S sid=\"154\" ssid=\"35\">Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sensecluster.3</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments\" number=\"6\">\n    <S sid=\"155\" ssid=\"1\">Model Selection The framework presented in Section 3 affords great flexibility in modeling the empirical data.</S>\n    <S sid=\"156\" ssid=\"2\">This however entails that several parameters must be instantiated.</S>\n    <S sid=\"157\" ssid=\"3\">More precisely, our model is conditioned on the Dirichlet hyperparameters α and β and the number of senses S. Additional parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights.</S>\n    <S sid=\"158\" ssid=\"4\">Our strategy in this paper is to fix α and β and explore the consequences of varying S. The value for the α hyperparameter was set to 0.02.</S>\n    <S sid=\"159\" ssid=\"5\">This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets.</S>\n    <S sid=\"160\" ssid=\"6\">We experimented with α values ranging from 0.005 to 1.</S>\n    <S sid=\"161\" ssid=\"7\">The β parameter was set to 0.1 (in all layers).</S>\n    <S sid=\"162\" ssid=\"8\">This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).</S>\n    <S sid=\"163\" ssid=\"9\">For simplicity, we used uniform weights for the layers.</S>\n    <S sid=\"164\" ssid=\"10\">The Gibbs sampler was run for 2,000 iterations.</S>\n    <S sid=\"165\" ssid=\"11\">Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs.</S>\n    <S sid=\"166\" ssid=\"12\">Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive.</S>\n    <S sid=\"167\" ssid=\"13\">We experimented with values ranging from three to nine senses.</S>\n    <S sid=\"168\" ssid=\"14\">Figure 3 shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively.</S>\n    <S sid=\"169\" ssid=\"15\">Here, we are using the optimal combination of layers for each system (which we discuss in the following section in detail).</S>\n    <S sid=\"170\" ssid=\"16\">For the model trained on WSJ, performance peaks at four senses, which is similar to the average ambiguity in the test data.</S>\n    <S sid=\"171\" ssid=\"17\">For the model trained on the BNC, however, the best results are obtained using twice as many senses.</S>\n    <S sid=\"172\" ssid=\"18\">Using fewer senses with the BNC-trained system can result in a drop in accuracy of almost 2%.</S>\n    <S sid=\"173\" ssid=\"19\">This is due to the shift in domain.</S>\n    <S sid=\"174\" ssid=\"20\">As the sense-divisions of the learning domain do not match those of the target domain, finer granularity is required in order to encompass all the relevant distinctions.</S>\n    <S sid=\"175\" ssid=\"21\">Table 1 illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.</S>\n    <S sid=\"176\" ssid=\"22\">The most probable words for each sense are also shown.</S>\n    <S sid=\"177\" ssid=\"23\">Firstly, note that the model infers some plausible senses for drug on the WSJ corpus (top half of Table 1).</S>\n    <S sid=\"178\" ssid=\"24\">Sense 1 corresponds to the “enforcement” sense of drug, Sense 2 refers to “medication”, Sense 3 to the “drug industry” and Sense 4 to “drugs research”.</S>\n    <S sid=\"179\" ssid=\"25\">The inferred senses for drug on the BNC (bottom half of Table 1) are more fine grained.</S>\n    <S sid=\"180\" ssid=\"26\">For example, the model finds distinct senses for “medication” (Sense 1 and 7) and “illegal substance” (Senses 2, 4, 6, 7).</S>\n    <S sid=\"181\" ssid=\"27\">It also finds a separate sense for “drug dealing” (Sense 5) and “enforcement” (Sense 8).</S>\n    <S sid=\"182\" ssid=\"28\">Because the BNC has a broader focus, finer distinctions are needed to cover as many senses as possible that are relevant to the target domain (WSJ).</S>\n    <S sid=\"183\" ssid=\"29\">Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.</S>\n    <S sid=\"184\" ssid=\"30\">We also investigate whether their combination, through our layered model (see Figure 2), yields performance improvements.</S>\n    <S sid=\"185\" ssid=\"31\">We used 4 senses for the system trained on WSJ and 8 for the system trained on the BNC (a was set to 0.02 and b to 0.1) Table 2 (left side) shows the performance of our model when using only one layer.</S>\n    <S sid=\"186\" ssid=\"32\">The layer composed of words co-occurring within a ±10-word window (10w), and representing wider, topical, information gives the highest scores on its own.</S>\n    <S sid=\"187\" ssid=\"33\">It is followed by the ±5 (5w) and ±1 (1w) word windows, which represent more immediate, local context.</S>\n    <S sid=\"188\" ssid=\"34\">Part-of-speech n-grams (pg) and word ngrams (ng), on their own, achieve lower scores, largely due to over-generalization and data sparseness, respectively.</S>\n    <S sid=\"189\" ssid=\"35\">The lowest-scoring single layer is the dependency layer (dp), with performance only slightly above the most-frequent-sense baseline (MFS).</S>\n    <S sid=\"190\" ssid=\"36\">Dependency information is very informative when present, but extremely sparse.</S>\n    <S sid=\"191\" ssid=\"37\">Table 2 (middle) also shows the results obtained when running the layered model with all but one of the layers as input.</S>\n    <S sid=\"192\" ssid=\"38\">We can use this information to determine the contribution of each layer by comparing to the combined model with all layers (all).</S>\n    <S sid=\"193\" ssid=\"39\">Because we are dealing with multiple layers, there is an element of overlap involved.</S>\n    <S sid=\"194\" ssid=\"40\">Therefore, each of the word-window layers, despite relatively high informativeness on its own, does not cause as much damage when it is absent, since the other layers compensate for the topical and local information.</S>\n    <S sid=\"195\" ssid=\"41\">The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.</S>\n    <S sid=\"196\" ssid=\"42\">Finally, we can see that the extremely sparse dependency layer is detrimental to the multi-layer model as a whole, and its removal increases performance.</S>\n    <S sid=\"197\" ssid=\"43\">The sparsity of the data in this layer means that there is often little information on which to base a decision.</S>\n    <S sid=\"198\" ssid=\"44\">In these cases, the layer contributes a close-to-uniform estimation of the sense distribution, which confuses the combined model.</S>\n    <S sid=\"199\" ssid=\"45\">Other layer combinations obtained similar results.</S>\n    <S sid=\"200\" ssid=\"46\">Table 2 (right side) shows the most informative two and three layer combinations.</S>\n    <S sid=\"201\" ssid=\"47\">Again, dependencies tend to decrease performance.</S>\n    <S sid=\"202\" ssid=\"48\">On the other hand, combining features that have similar performance on their own is beneficial.</S>\n    <S sid=\"203\" ssid=\"49\">We obtain the best performance overall with a two layered model combining topical (+10w) and local (+5w) contexts.</S>\n    <S sid=\"204\" ssid=\"50\">Table 3 replicates the same suite of experiments on the BNC corpus.</S>\n    <S sid=\"205\" ssid=\"51\">The general trends are similar.</S>\n    <S sid=\"206\" ssid=\"52\">Some interesting differences are apparent, however.</S>\n    <S sid=\"207\" ssid=\"53\">The sparser layers, notably word n-grams and dependencies, fare comparatively worse.</S>\n    <S sid=\"208\" ssid=\"54\">This is expected, since the more precise, local, information is likely to vary strongly across domains.</S>\n    <S sid=\"209\" ssid=\"55\">Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.</S>\n    <S sid=\"210\" ssid=\"56\">Another observable difference is that the combined model without the dependency layer does slightly better than each of the single layers.</S>\n    <S sid=\"211\" ssid=\"57\">The 1w+pg combination improves over its components, which have similar individual performance.</S>\n    <S sid=\"212\" ssid=\"58\">Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side).</S>\n    <S sid=\"213\" ssid=\"59\">Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition.</S>\n    <S sid=\"214\" ssid=\"60\">IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.</S>\n    <S sid=\"215\" ssid=\"61\">These models and our own model significantly outperform the most-frequent-sense baseline (p &lt; 0.01 using a x2 test).</S>\n    <S sid=\"216\" ssid=\"62\">Our best system (10w+5w on WSJ) is significantly better than UMND2 (p &lt; 0.01) and quantitatively better than IR2, although the difference is not statistically significant.</S>\n  </SECTION>\n  <SECTION title=\"7 Discussion\" number=\"7\">\n    <S sid=\"217\" ssid=\"1\">This paper presents a novel Bayesian approach to sense induction.</S>\n    <S sid=\"218\" ssid=\"2\">We formulated sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on the basis of latent variables.</S>\n    <S sid=\"219\" ssid=\"3\">Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.</S>\n    <S sid=\"220\" ssid=\"4\">Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account.</S>\n    <S sid=\"221\" ssid=\"5\">For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text.</S>\n    <S sid=\"222\" ssid=\"6\">In the future, we hope to explore more rigorous parameter estimation techniques.</S>\n    <S sid=\"223\" ssid=\"7\">Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.</S>\n    <S sid=\"224\" ssid=\"8\">Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.</S>\n    <S sid=\"225\" ssid=\"9\">In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal.</S>\n    <S sid=\"226\" ssid=\"10\">This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.</S>\n    <S sid=\"227\" ssid=\"11\">Acknowledgments The authors acknowledge the support of EPSRC (grant EP/C538447/1).</S>\n    <S sid=\"228\" ssid=\"12\">We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "e4c31afd66e83eb0ed6323a7fd3ef86e",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Bayesian Word Sense Induction"
                }
            }
        },
        {
            "index": 10,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "In this paper we explore the computational modelling of compositionality in distributional models of semantics. In particular, we model the semantic composition of pairs of adjacent English Adjecand Nouns from the National We build a vector-based semantic space from a lemmatised version of the BNC, where the most frequent A-N lemma pairs are treated as single tokens. We then extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and a Partial Least Squares Regression (PLSR) model. We propose two evaluation methods for the implemented models. Our study leads to the conclusion that regression-based models of compositionality generally out-perform additive and multiplicative approaches, and also show a number of advantages that make them very promising for future research.",
                    "filename": "A_Regression_Model_of_Adjective-Noun_Compositionality_in_Distributional_Semantics",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W10-2805/Documents_xml/W10-2805.xml",
                    "title": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a novel regression model for understanding adjective-noun (A-N) compositionality within distributional semantics models (DSMs). The authors focus on Partial Least Squares Regression (PLSR), a technique well-established in other fields but not previously applied to computational semantics, to address multivariate regression issues where variables exceed observations. Three models were compared: additive, multiplicative, and PLSR. The study utilized a dataset from the British National Corpus, constructing a vector space for A-N pairs. Evaluation methods relied on Euclidean distance measures and nearest neighbor comparisons. Results indicated that PLSR outperformed other models in the first evaluation setting and showed promising results overall, pointing to its effectiveness in generating neighbors for real vectors and suggesting potential applications in identifying distributional synonyms for unseen A-N pairs. The paper concludes by highlighting the importance of developing better evaluation methods and considering lexical associations in future research on compositionality in natural language.",
                        "This study presents a novel application of Partial Least Squares Regression (PLSR) to model the compositionality of meaning in distributional semantic models (DSMs). The research focuses on the semantic composition of English adjective-noun (A-N) pairs, utilizing a vector space derived from the British National Corpus. Three models of compositionality are compared: an additive model, a multiplicative model, and a PLSR model. The evaluation of these models against a dataset of A-N pairs shows that the PLSR model provides better approximations of compositional vectors compared to the additive and multiplicative models. Despite mixed results in a second evaluation test, PLSR demonstrates consistent performance, suggesting its potential for capturing the complexity of semantic relations in DSMs. The paper concludes with insights on the implications of modeling semantic compositionality as a machine learning task and the possibility of learning various types of compositional functions from natural language data.",
                        "This paper explores a regression model for adjective-noun (A-N) compositionality in distributional semantic models (DSMs), addressing a gap in DSMs' treatment of semantic compositionality. Three models—additive, multiplicative, and Partial Least Squares Regression (PLSR)—are tested for their ability to approximate the composed meaning of A-N pairs from the British National Corpus. The study finds that PLSR outperforms the other models in constructing vectors closer to the observed A-N pairs, suggesting it may be an effective tool for capturing semantic relations in DSMs. The evaluations also reveal that the models produce specialized versions of the original semantic space, each with different utilities depending on the task. The paper concludes that PLSR shows promise for future development in computational distributional semantics and highlights the need for better evaluation methods and further exploration of lexical association in compositionality modeling.",
                        "This academic paper presents a novel approach to modeling the compositionality of meaning in distributional semantics, particularly for adjective-noun (A-N) pairs, using Partial Least Squares Regression (PLSR). The authors propose that semantic composition should be viewed as a relation-specific phenomenon and not just the application of simple geometric operations to vector representations. The study utilizes a data set from the British National Corpus to build and evaluate three models: an additive model, a pointwise-multiplicative model, and a PLSR model. The evaluation, based on Euclidean distance measures, indicates that the PLSR model produces the most consistent and closest approximations to observed A-N compositional vectors. Despite mixed results in different tests, PLSR demonstrates potential as a method for predicting distributional synonyms of unseen A-N pairs and for improving computational models of semantic compositionality. This work is ongoing, and future research is to focus on developing better evaluation methods and exploring the role of lexical association in compositionality."
                    ],
                    "filename": "a_regression_model_of_adjectiven-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 121 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Regression Model of Adjective-Noun Compositionality in Distributional Semantics</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Word-space vector models or distributional models of semantics (henceforth DSMs), are computational models that build contextual semantic representations for lexical items from corpus data.</S>\n    <S sid=\"7\" ssid=\"2\">DSMs have been successfully used in the recent years for a number of different computational tasks involving semantic relations between words (e.g. synonym identification, computation of semantic similarity, modelling selectional preferences, etc., for a thorough discussion of the field, cf.</S>\n    <S sid=\"8\" ssid=\"3\">Sahlgren, 2006).</S>\n    <S sid=\"9\" ssid=\"4\">The theoretical foundation of DSMs is to be found in the “distributional hypothesis of meaning”, attributed to Z. Harris, which maintains that meaning is susceptible to distributional analysis and, in particular, that differences in meaning between words or morphemes in a language correlate with differences in their distribution (Harris 1970, pp.</S>\n    <S sid=\"10\" ssid=\"5\">784–787).</S>\n    <S sid=\"11\" ssid=\"6\">While the vector-based representation of word meaning has been used for a long time in computational linguistics, the techniques that are currently used have not seen much development with regards to one of the main aspects of semantics in natural language: compositionality.</S>\n    <S sid=\"12\" ssid=\"7\">To be fair, the study of semantic compositionality in DSMs has seen a slight revival in the recent times, cf.</S>\n    <S sid=\"13\" ssid=\"8\">Widdows (2008), Mitchell &amp; Lapata (2008), Giesbrecht (2009), Baroni &amp; Lenci (2009), who propose various DSM approaches to represent argument structure, subject-verb and verb-object co-selection.</S>\n    <S sid=\"14\" ssid=\"9\">Current approaches to compositionality in DSMs are based on the application of a simple geometric operation on the basis of individual vectors (vector addition, pointwisemultiplication of corresponding dimensions, tensor product) which should in principle approximate the composition of any two given vectors.</S>\n    <S sid=\"15\" ssid=\"10\">On the contrary, since the the very nature of compositionality depends on the semantic relation being instantiated in a syntactic structure, we propose that the composition of vector representations must be modelled as a relation-specific phenomenon.</S>\n    <S sid=\"16\" ssid=\"11\">In particular, we propose that the usual procedures from machine learning tasks must be implemented also in the search for semantic compositionality in DSM.</S>\n    <S sid=\"17\" ssid=\"12\">In this paper we present work in progress on the computational modelling of compositionality in a data-set of English Adjective-Noun pairs extracted from the BNC.</S>\n    <S sid=\"18\" ssid=\"13\">We extrapolate three different models of compositionality: a simple additive model, a pointwise-multiplicative model and, finally, a multinomial multiple regression model by Partial Least Squares Regression (PLSR).</S>\n  </SECTION>\n  <SECTION title=\"2 Compositionality of meaning in DSMs\" number=\"2\">\n    <S sid=\"19\" ssid=\"1\">Previous work in the field has produced a small number of operations to represent the composition of vectorial representations of word meaning.</S>\n    <S sid=\"20\" ssid=\"2\">In particular, given two independent vectors v1 and v2, the semantically compositional result v3 is modelled by: In the DSM literature, the additive model has become a de facto standard approach to approximate the composed meaning of a group of words (or a document) as the sum of their vectors (which results in the centroid of the starting vectors).</S>\n    <S sid=\"21\" ssid=\"3\">This has been successfully applied to document-based applications such as the computation of document similarity in information retrieval.</S>\n    <S sid=\"22\" ssid=\"4\">Mitchell &amp; Lapata (2008) indicate that the various variations of the pointwise-multiplication model perform better than simple additive models in term similarity tasks (variations included combination with simple addition and adding weights to individual vector components).</S>\n    <S sid=\"23\" ssid=\"5\">Widdows (2008) Obtain results indicating that both the tensor product and the convolution product perform better than the simple additive model.</S>\n    <S sid=\"24\" ssid=\"6\">For the sake of simplifying the implementation of evaluation methods, in this paper we will compare the first two approaches, vector addition and vector pointwise-multiplication, with regression modelling by partial least squares.</S>\n  </SECTION>\n  <SECTION title=\"3 Partial least squares regression of compositionality\" number=\"3\">\n    <S sid=\"25\" ssid=\"1\">We assume that the composition of meaning in DSMs is a function mapping two or more independent vectors in a multidimensional space to a newly composed vector the same space and, further, we assume that semantic composition is dependent on the syntactic structure being instantiated in natural language.1 Assuming that each dimension in the starting vectors v1 and v2 is a candidate predictor, and that each dimension in the composed vector v3 is a dependent variable, vector-based semantic compositionality can be formulated as a problem of multivariate multiple regression.</S>\n    <S sid=\"26\" ssid=\"2\">This is, in principle, a tractable problem that can be solved by standard machine learning techniques such as multilayer perceptrons or support vector machines.</S>\n    <S sid=\"27\" ssid=\"3\">However, given that sequences of words tend to be of very low frequency (and thus difficult to represent in a DSM), suitable data sets will inevitably suffer the curse of dimensionality: we will often have many more variables (dimensions) than observations.</S>\n    <S sid=\"28\" ssid=\"4\">Partial Least Squares Regression (PLSR) is a multivariate regression technique that has been designed specifically to tackle such situations with high dimensionality and limited data.</S>\n    <S sid=\"29\" ssid=\"5\">PLSR is widely used in in unrelated fields such as spectroscopy, medical chemistry, brain-imaging and marketing (Mevik &amp; Wehrens, 2007).</S>\n  </SECTION>\n  <SECTION title=\"4 Materials and tools\" number=\"4\">\n    <S sid=\"30\" ssid=\"1\">We use a general-purpose vector space extracted from the British National Corpus.</S>\n    <S sid=\"31\" ssid=\"2\">We used the Infomap software to collect co-occurrence statistics for lemmas within a rectangular 5L–5R window.</S>\n    <S sid=\"32\" ssid=\"3\">The corpus was pre-processed to represent frequent Adjective-Noun lemma pairs as a single token (e.g. while in the original corpus the A-N phrase nice house consists in two separate lemmas (nice and house), in the processed corpus it appears as a single entry nice_house).</S>\n    <S sid=\"33\" ssid=\"4\">The corpus was also processed by stop-word removal.</S>\n    <S sid=\"34\" ssid=\"5\">We extracted a list of A-N candidate pairs with simple regex-based queries targeting adjacent sequences composed of [Det/Art–A–N] (e.g. that little house).</S>\n    <S sid=\"35\" ssid=\"6\">We filtered the candidate list by frequency (&gt; 400) obtaining 1,380 different A-N pairs.</S>\n    <S sid=\"36\" ssid=\"7\">The vector space was built with the 40,000 most frequent tokens in the corpus (a cut-off point that included all the extracted A-N pairs).</S>\n    <S sid=\"37\" ssid=\"8\">The original dimensions were the 3,000 most frequent content words in the BNC.</S>\n    <S sid=\"38\" ssid=\"9\">The vector space was reduced to the first 500 “latent” dimensions by SVD as implemented by the Infomap software.</S>\n    <S sid=\"39\" ssid=\"10\">Thus, the resulting space consists in a matrix with 40, 000 x 500 dimensions.</S>\n    <S sid=\"40\" ssid=\"11\">We then extracted the vector representation for each A-N candidate as well as for each independent constituent, e.g. vectors for nice_house (v3), as well as for nice (v1) and house (v2) were saved.</S>\n    <S sid=\"41\" ssid=\"12\">The resulting vector subspace was imported into the R statistical computing environment for the subsequent model building and evaluation.</S>\n    <S sid=\"42\" ssid=\"13\">In particular, we produced our regression analysis with the pls package (Mevik &amp; Wehrens, 2007), which implements PLSR and a number of very useful functions for cross-validation, prediction, error analysis, etc.</S>\n    <S sid=\"43\" ssid=\"14\">By simply combining the vector representations of the independent Adjectives and Nouns in our data-set (v1 and v2) we built an additive prediction model (v1 + v2) and a simplified pointwise multiplicative prediction model (v1 x v2) for each candidate pair.</S>\n    <S sid=\"44\" ssid=\"15\">We also fitted a PLSR model using v1 and v2 as predictors and the corresponding observed pair v3 as dependent variable.</S>\n    <S sid=\"45\" ssid=\"16\">The data were divided into a training set (1,000 A-N pairs) and a testing set (the remaining 380 A-N pairs).</S>\n    <S sid=\"46\" ssid=\"17\">The model’s parameters were estimated by performing 10-fold cross-validation during the training phase.</S>\n    <S sid=\"47\" ssid=\"18\">In what follows we briefly evaluate the three resulting models of compositionality.</S>\n  </SECTION>\n  <SECTION title=\"5 Evaluation\" number=\"5\">\n    <S sid=\"48\" ssid=\"1\">In order to evaluate the three models of compositionality that were built, we devised two different procedures based on the Euclidean measure of geometric distance.</S>\n    <S sid=\"49\" ssid=\"2\">The first method draws a direct comparison of the different predicted vectors for each candidate A-N pair by computing the Euclidean distance between the observed vector and the modelled predictions.</S>\n    <S sid=\"50\" ssid=\"3\">We also inspect a general distance matrix for the whole compositionality subspace, i.e. all the observed vectors and all the predicted vectors.</S>\n    <S sid=\"51\" ssid=\"4\">We extract the 10 nearest neighbours for the 380 Adjective-Noun pairs in the test set and look for the intended predicted vectors in each case.</S>\n    <S sid=\"52\" ssid=\"5\">The idea here is that the best models should produce predictions that are as close as possible to the originally observed A-N vector.</S>\n    <S sid=\"53\" ssid=\"6\">Our second evaluation method uses the 10 nearest neighbours of each of the observed A-N pairs in the test set as gold-standard (excluding any modelled predictions), and compares them with the 10 nearest neighbours of each of the corresponding predictions as generated by the models.</S>\n    <S sid=\"54\" ssid=\"7\">The aim is to assess if the predictions made by each model share any top-10 neighbours with their corresponding gold-standard.</S>\n    <S sid=\"55\" ssid=\"8\">We award 1 point for every shared neighbour.</S>\n    <S sid=\"56\" ssid=\"9\">We calculated the Euclidean distance between each observed A-N pair and the corresponding prediction made by each model.</S>\n    <S sid=\"57\" ssid=\"10\">On general inspection, it is clear that the approximation of A-N compositional vectors made by PLSR is considerably closer than those produced by the additive and multiplicative models, cf.</S>\n    <S sid=\"58\" ssid=\"11\">Table 1.</S>\n    <S sid=\"59\" ssid=\"12\">We also computed in detail which of the three predicted composed vectors was closest to the corresponding observation.</S>\n    <S sid=\"60\" ssid=\"13\">To this effect we extracted the 10 nearest neighbours for each A-N pair in the test set using the whole compositionality subspace (all the predicted and the original vectors).</S>\n    <S sid=\"61\" ssid=\"14\">In 94 cases out of 380, the PLSR intended prediction was the nearest neighbour.</S>\n    <S sid=\"62\" ssid=\"15\">Cumulatively, PLSR’s predictions were in the top-10 nearest neighbour list in 219 out of 380 cases (57.6%).</S>\n    <S sid=\"63\" ssid=\"16\">The other models’ performance in this test was negligible, cf.</S>\n    <S sid=\"64\" ssid=\"17\">Table 2.</S>\n    <S sid=\"65\" ssid=\"18\">Overall, 223 items in the test set had at least one predicted vector in the top-10 list; of these, 219 (98%) were generated by PLSR and the remaining 4 (1%) by the multiplicative model.</S>\n    <S sid=\"66\" ssid=\"19\">Since the main use of DSMs is to extract similar vectors from a multidimensional space (representing related documents, distributional synonyms, etc.</S>\n    <S sid=\"67\" ssid=\"20\">), we would like to test if the modelling of semantic compositionality is able to produce predictions that are as similar as possible to the originally observed data.</S>\n    <S sid=\"68\" ssid=\"21\">A very desirable result would be if any predicted compositional A-N vector could be reliably used instead of the extracted bigram.</S>\n    <S sid=\"69\" ssid=\"22\">This could only be achieved if a model’s predictions show a similar distributional behaviour with respect to the observed vector.</S>\n    <S sid=\"70\" ssid=\"23\">To test this idea using our data, we took the 10 nearest neighbours of each of the observed AN pairs in the test set as gold standard.</S>\n    <S sid=\"71\" ssid=\"24\">These gold neighbours were extracted from the observation testing subspace, thus excluding any modelled predictions.</S>\n    <S sid=\"72\" ssid=\"25\">This is a very restrictive setting: it means that the gold standard for each of the 380 test items is composed of the 10 nearest neighbours from the same 380 items (which may turn out to be not very close at all).</S>\n    <S sid=\"73\" ssid=\"26\">We then extracted the 10 nearest neighbours for each of the three modelled predictions, but this time the subspace included all predictions, as well as all the original observations (380 x 4 = 1520 items).</S>\n    <S sid=\"74\" ssid=\"27\">Finally, we tested if the predictions made by each model shared any top-10 neighbours with their corresponding gold-standard.</S>\n    <S sid=\"75\" ssid=\"28\">We awarded 1 point for every shared neighbour.</S>\n    <S sid=\"76\" ssid=\"29\">The results obtained with these evaluation settings were very poor.</S>\n    <S sid=\"77\" ssid=\"30\">Only the additive model scored points (48), although the performance was rather disappointing (maximum potential score for the test was 3,800 points).</S>\n    <S sid=\"78\" ssid=\"31\">Both the pointwise multiplicative model and the PLSR model failed to retrieve any of the gold standard neighbours.</S>\n    <S sid=\"79\" ssid=\"32\">This poor results can be attributed to the very restrictive nature of our gold standard and, also, to the asymmetrical composition of the compared data (gold standard: 3,800 neighbours from a pool of just 380 different items; prediction space: 11,400 neighbours from a pool of 1,520 items).</S>\n    <S sid=\"80\" ssid=\"33\">However, given the that DSMs are known for their ability to extract similar items from the same space, we decided to relax our test settings by awarding points not only to shared neighbours, but also to the same model’s predictions of those neighbours.</S>\n    <S sid=\"81\" ssid=\"34\">Thus, given a target neighbour such as good_deal, in our second setting we awarded points not only to the gold standard good_deal, but also to the predictions good_deal_ADD, good_deal_MUL and good_deal_PLSR when evaluating each corresponding model.</S>\n    <S sid=\"82\" ssid=\"35\">With these settings the compared spaces become less asymmetrical (gold standard: 7,600 neighbours from a pool of just 380 different items plus predictions; prediction space: 11,400 neighbours from a pool of 1,520 items).</S>\n    <S sid=\"83\" ssid=\"36\">The obtained results show a great improvement (max. potential score 7,600 points): Once again, the additive model showed the best performance, followed by PLSR.</S>\n    <S sid=\"84\" ssid=\"37\">The multiplicative model’s performance was negligible.</S>\n    <S sid=\"85\" ssid=\"38\">While carrying out these experiments, an unexpected fact became evident.</S>\n    <S sid=\"86\" ssid=\"39\">Each of the models in turn produces predictions that are relatively close to each other, regardless of the independent words that were used to calculate the compositional vectors.</S>\n    <S sid=\"87\" ssid=\"40\">This has the consequence that the nearest neighbour lists for each model’s predictions are, by and large, populated by items generated in the same model, as shown in Table 4.</S>\n    <S sid=\"88\" ssid=\"41\">Neighbours of predictions from the multiplicative model are all multiplicative.</S>\n    <S sid=\"89\" ssid=\"42\">The additive model has the most varied set of neighbours, but the majority of them are additive-neighbours.</S>\n    <S sid=\"90\" ssid=\"43\">PLSR shows a mixed behaviour.</S>\n    <S sid=\"91\" ssid=\"44\">However, PLSR produced neighbours that find their way into the neighbour sets of both the additive model and the observations.</S>\n    <S sid=\"92\" ssid=\"45\">These remarks point in the same direction: every model is a simplified and specialised version of the original space, somewhat more orderly than the observed data, and may give different results depending on the task at stake.</S>\n    <S sid=\"93\" ssid=\"46\">PLSR (and to a lesser extent also the multiplicative model) is particularly efficient as generator of neighbours for real vectors, a characteristic that could be applied to guess distributional synonyms of unseen A-N pairs.</S>\n    <S sid=\"94\" ssid=\"47\">On the other hand, the additive model (and to a lesser extent PLSR) is especially successful in attracting gold standard neighbours.</S>\n    <S sid=\"95\" ssid=\"48\">Overall, even at this experimental stage, PLSR is clearly the model that produces the most consistent results.</S>\n  </SECTION>\n  <SECTION title=\"6 Concluding remarks\" number=\"6\">\n    <S sid=\"96\" ssid=\"1\">This paper proposed a novel method to model the compositionality of meaning in distributional models of semantics.</S>\n    <S sid=\"97\" ssid=\"2\">The method, Partial Least Squares Regression, is well known in other dataintensive fields of research, but to our knowledge had never been put to work in computational distributional semantics.</S>\n    <S sid=\"98\" ssid=\"3\">Its main advantage is the fact that it is designed to approximate functions in problems of multivariate multiple regression where the number of observations is relatively small if compared to the number of variables (dimensions).</S>\n    <S sid=\"99\" ssid=\"4\">We built a DSM targeting a type of semantic composition that has not been treated extensively in the literature before, adjacent A-N pairs.</S>\n    <S sid=\"100\" ssid=\"5\">The model built by PLSR performed better than both a simple additive model and a multiplicative model in the first proposed evaluation method.</S>\n    <S sid=\"101\" ssid=\"6\">Our second evaluation test (using comparison to a gold standard) gave mixed results: the best performance was obtained by the simple additive model, with PLSR coming in second place.</S>\n    <S sid=\"102\" ssid=\"7\">This is work in progress, but the results look very promising.</S>\n    <S sid=\"103\" ssid=\"8\">Future developments will certainly focus on the creation of better evaluation methods, as well as on extending the experiments to other techniques (e.g. convolution product as discussed by Widdows, 2008 and Giesbrecht, 2009).</S>\n    <S sid=\"104\" ssid=\"9\">Another important issue that we still have not touched is the role played by lexical association (collocations) in the prediction models.</S>\n    <S sid=\"105\" ssid=\"10\">We would like to make sure that we are not modelling the compositionality of noncompositional examples.</S>\n    <S sid=\"106\" ssid=\"11\">A last word on the view of semantic compositionality suggested by our approach.</S>\n    <S sid=\"107\" ssid=\"12\">Modelling compositionality as a machine learning task implies that a great number of different “types” of composition (functions combining vectors) may be learned from natural language samples.</S>\n    <S sid=\"108\" ssid=\"13\">In principle, any semantic relation instantiated by any syntactic structure could be learned if sufficient data is provided.</S>\n    <S sid=\"109\" ssid=\"14\">This approach must be confronted with other linguistic phenomena, also of greater complexity than just a set of bigrams.</S>\n    <S sid=\"110\" ssid=\"15\">Finally, we might wonder if there is an upper limit to the number of compositionality functions that we need to learn in natural language, or if there are types of functions that are more difficult, or even impossible, to learn.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgements\" number=\"7\">\n    <S sid=\"111\" ssid=\"1\">Thanks are due to Marco Baroni, Stefan Evert, Roberto Zamparelli and the three anonymous reviewers for their assistance and helpful comments.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "45cb7ae167b4c102957a16dd12a61910",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "A Regression Model of Adjective-Noun Compositionality in Distributional Semantics"
                }
            }
        },
        {
            "index": 11,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly outperforms previous two dependency tree kernels for relation extraction.",
                    "filename": "A_Composite_Kernel_To_Extract_Relations_Between_Entities_With_Both_Flat_And_Structured_Features",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P06-1104/Documents_xml/P06-1104.xml",
                    "title": "A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a novel composite kernel for relation extraction that combines flat entity features with structured syntactic features from parse trees without extensive feature engineering. The composite kernel, integrating an entity kernel and a convolution parse tree kernel, demonstrates superior performance on the ACE corpus compared to previous methods, effectively capturing both flat and syntactic structure features. The kernel's design allows for the addition of more individual kernels to cover broader knowledge, potentially improving relation detection accuracy. The study also highlights the importance of selecting effective portions of parse trees for kernel calculations in relation extraction tasks. Future work will focus on designing a more flexible tree kernel and addressing data imbalance and sparseness issues.",
                        "This paper presents a novel composite kernel for relation extraction from text that combines flat entity features with structured syntactic features. The composite kernel, which includes an entity kernel and a convolution parse tree kernel, demonstrates superior performance over previous methods on the ACE corpus without requiring extensive feature engineering. The entity kernel captures strong constraints imposed by entity features, while the convolution parse tree kernel accounts for syntactic structure. The approach outperforms previous best-reported methods, effectively handling both flat and structured features and enabling the inclusion of additional kernels to cover more knowledge. The study suggests that the structured information embedded in parse trees is critical for relation extraction and can be effectively exploited using the proposed kernel methods. Future work will focus on improving relation detection accuracy and designing a more flexible tree kernel for a more accurate similarity measure.",
                        "This paper introduces a novel composite kernel approach for relation extraction that combines an entity kernel with a convolution parse tree kernel to capture both flat and structured features of entity relationships within text. The approach is advantageous because it can implicitly explore large amounts of structured features without extensive feature engineering, and it easily integrates additional kernels to expand knowledge coverage. Evaluation on the ACE corpus demonstrates that the composite kernel outperforms previous methods, particularly by effectively exploiting syntactic structure features. The paper suggests the potential for further improvements in relation detection accuracy through the inclusion of more kernels and learning algorithm adjustments. Future work will focus on developing a more flexible tree kernel for accurate similarity measurement.",
                        "This paper introduces a novel composite kernel method that combines entity features with the syntactic structure of parse trees for relation extraction. The composite kernel, which incorporates both flat and structured features, demonstrates significant performance improvements over previous methods on the ACE corpus without extensive feature engineering. The kernel method leverages the mathematical properties of kernels to implicitly explore high-dimensional feature spaces, thereby effectively capturing the syntactic features that are critical for relation extraction. The paper also discusses the potential for further improvement and scalability by incorporating additional kernels and features, and highlights the importance of selecting effective portions of parse trees for kernel calculations. Future work will focus on enhancing the accuracy of relation detection and designing a more flexible tree kernel for a more accurate similarity measure."
                    ],
                    "filename": "a_composite_kernel_to_extract_re-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 117 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">The goal of relation extraction is to find various predefined semantic relations between pairs of entities in text.</S>\n    <S sid=\"7\" ssid=\"2\">The research on relation extraction has been promoted by the Message Understanding Conferences (MUCs) (MUC, 19871998) and Automatic Content Extraction (ACE) program (ACE, 2002-2005).</S>\n    <S sid=\"8\" ssid=\"3\">According to the ACE Program, an entity is an object or set of objects in the world and a relation is an explicitly or implicitly stated relationship among entities.</S>\n    <S sid=\"9\" ssid=\"4\">For example, the sentence “Bill Gates is chairman and chief software architect of Microsoft Corporation.” conveys the ACE-style relation “EMPLOYMENT.exec” between the entities “Bill Gates” (PERSON.Name) and “Microsoft Corporation” (ORGANIZATION.</S>\n    <S sid=\"10\" ssid=\"5\">Commercial).</S>\n    <S sid=\"11\" ssid=\"6\">In this paper, we address the problem of relation extraction using kernel methods (Schölkopf and Smola, 2001).</S>\n    <S sid=\"12\" ssid=\"7\">Many feature-based learning algorithms involve only the dot-product between feature vectors.</S>\n    <S sid=\"13\" ssid=\"8\">Kernel methods can be regarded as a generalization of the feature-based methods by replacing the dot-product with a kernel function between two vectors, or even between two objects.</S>\n    <S sid=\"14\" ssid=\"9\">A kernel function is a similarity function satisfying the properties of being symmetric and positive-definite.</S>\n    <S sid=\"15\" ssid=\"10\">Recently, kernel methods are attracting more interests in the NLP study due to their ability of implicitly exploring huge amounts of structured features using the original representation of objects.</S>\n    <S sid=\"16\" ssid=\"11\">For example, the kernels for structured natural language data, such as parse tree kernel (Collins and Duffy, 2001), string kernel (Lodhi et al., 2002) and graph kernel (Suzuki et al., 2003) are example instances of the wellknown convolution kernels1 in NLP.</S>\n    <S sid=\"17\" ssid=\"12\">In relation extraction, typical work on kernel methods includes: Zelenko et al. (2003), Culotta and Sorensen (2004) and Bunescu and Mooney (2005).</S>\n    <S sid=\"18\" ssid=\"13\">This paper presents a novel composite kernel to explore diverse knowledge for relation extraction.</S>\n    <S sid=\"19\" ssid=\"14\">The composite kernel consists of an entity kernel and a convolution parse tree kernel.</S>\n    <S sid=\"20\" ssid=\"15\">Our study demonstrates that the composite kernel is very effective for relation extraction.</S>\n    <S sid=\"21\" ssid=\"16\">It also shows without the need for extensive feature engineering the composite kernel can not only capture most of the flat features used in the previous work but also exploit the useful syntactic structure features effectively.</S>\n    <S sid=\"22\" ssid=\"17\">An advantage of our method is that the composite kernel can easily cover more knowledge by introducing more kernels.</S>\n    <S sid=\"23\" ssid=\"18\">Evaluation on the ACE corpus shows that our method outperforms the previous bestreported methods and significantly outperforms the previous kernel methods due to its effective exploration of various syntactic features.</S>\n    <S sid=\"24\" ssid=\"19\">The rest of the paper is organized as follows.</S>\n    <S sid=\"25\" ssid=\"20\">In Section 2, we review the previous work.</S>\n    <S sid=\"26\" ssid=\"21\">Section 3 discusses our composite kernel.</S>\n    <S sid=\"27\" ssid=\"22\">Section 4 reports the experimental results and our observations.</S>\n    <S sid=\"28\" ssid=\"23\">Section 5 compares our method with the 1 Convolution kernels were proposed for a discrete structure by Haussler (1999) in the machine learning field.</S>\n    <S sid=\"29\" ssid=\"24\">This framework defines a kernel between input objects by applying convolution “sub-kernels” that are the kernels for the decompositions (parts) of the objects. previous work from the viewpoint of feature exploration.</S>\n    <S sid=\"30\" ssid=\"25\">We conclude our work and indicate the future work in Section 6.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"2\">\n    <S sid=\"31\" ssid=\"1\">Many techniques on relation extraction, such as rule-based (MUC, 1987-1998; Miller et al., 2000), feature-based (Kambhatla 2004; Zhou et al., 2005) and kernel-based (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005), have been proposed in the literature.</S>\n    <S sid=\"32\" ssid=\"2\">Rule-based methods for this task employ a number of linguistic rules to capture various relation patterns.</S>\n    <S sid=\"33\" ssid=\"3\">Miller et al. (2000) addressed the task from the syntactic parsing viewpoint and integrated various tasks such as POS tagging, NE tagging, syntactic parsing, template extraction and relation extraction using a generative model.</S>\n    <S sid=\"34\" ssid=\"4\">Feature-based methods (Kambhatla, 2004; Zhou et al., 2005; Zhao and Grishman, 20052) for this task employ a large amount of diverse linguistic features, such as lexical, syntactic and semantic features.</S>\n    <S sid=\"35\" ssid=\"5\">These methods are very effective for relation extraction and show the bestreported performance on the ACE corpus.</S>\n    <S sid=\"36\" ssid=\"6\">However, the problems are that these diverse features have to be manually calibrated and the hierarchical structured information in a parse tree is not well preserved in their parse tree-related features, which only represent simple flat path information connecting two entities in the parse tree through a path of non-terminals and a list of base phrase chunks.</S>\n    <S sid=\"37\" ssid=\"7\">Prior kernel-based methods for this task focus on using individual tree kernels to exploit tree structure-related features.</S>\n    <S sid=\"38\" ssid=\"8\">Zelenko et al. (2003) developed a kernel over parse trees for relation extraction.</S>\n    <S sid=\"39\" ssid=\"9\">The kernel matches nodes from roots to leaf nodes recursively layer by layer in a topdown manner.</S>\n    <S sid=\"40\" ssid=\"10\">Culotta and Sorensen (2004) generalized it to estimate similarity between dependency trees.</S>\n    <S sid=\"41\" ssid=\"11\">Their tree kernels require the matchable nodes to be at the same layer counting from the root and to have an identical path of ascending nodes from the roots to the current nodes.</S>\n    <S sid=\"42\" ssid=\"12\">The two constraints make their kernel high precision but very low recall on the ACE 2003 corpus.</S>\n    <S sid=\"43\" ssid=\"13\">Bunescu and Mooney (2005) proposed another dependency tree kernel for relation extraction.</S>\n    <S sid=\"44\" ssid=\"14\">Their kernel simply counts the number of common word classes at each position in the shortest paths between two entities in dependency trees.</S>\n    <S sid=\"45\" ssid=\"15\">The kernel requires the two paths to have the same length; otherwise the kernel value is zero.</S>\n    <S sid=\"46\" ssid=\"16\">Therefore, although this kernel shows performance improvement over the previous one (Culotta and Sorensen, 2004), the constraint makes the two dependency kernels share the similar behavior: good precision but much lower recall on the ACE corpus.</S>\n    <S sid=\"47\" ssid=\"17\">The above discussion shows that, although kernel methods can explore the huge amounts of implicit (structured) features, until now the feature-based methods enjoy more success.</S>\n    <S sid=\"48\" ssid=\"18\">One may ask: how can we make full use of the nice properties of kernel methods and define an effective kernel for relation extraction?</S>\n    <S sid=\"49\" ssid=\"19\">In this paper, we study how relation extraction can benefit from the elegant properties of kernel methods: 1) implicitly exploring (structured) features in a high dimensional space; and 2) the nice mathematical properties, for example, the sum, product, normalization and polynomial expansion of existing kernels is a valid kernel (Schölkopf and Smola, 2001).</S>\n    <S sid=\"50\" ssid=\"20\">We also demonstrate how our composite kernel effectively captures the diverse knowledge for relation extraction.</S>\n  </SECTION>\n  <SECTION title=\"3 Composite Kernel for Relation Extraction\" number=\"3\">\n    <S sid=\"51\" ssid=\"1\">In this section, we define the composite kernel and study the effective representation of a relation instance.</S>\n    <S sid=\"52\" ssid=\"2\">Our composite kernel consists of an entity kernel and a convolution parse tree kernel.</S>\n    <S sid=\"53\" ssid=\"3\">To our knowledge, convolution kernels have not been explored for relation extraction.</S>\n    <S sid=\"54\" ssid=\"4\">(1) Entity Kernel: The ACE 2003 data defines four entity features: entity headword, entity type and subtype (only for GPE), and mention type while the ACE 2004 data makes some modifications and introduces a new feature “LDC mention type”.</S>\n    <S sid=\"55\" ssid=\"5\">Our statistics on the ACE data reveals that the entity features impose a strong constraint on relation types.</S>\n    <S sid=\"56\" ssid=\"6\">Therefore, we design a linear kernel to explicitly capture such features: where R1 and R2 stands for two relation instances, Ei means the ith entity of a relation instance, and KE(•,•) is a simple kernel function over the features of entities: where fi represents the ith entity feature, and the function C(•,•) returns 1 if the two feature values are identical and 0 otherwise.</S>\n    <S sid=\"57\" ssid=\"7\">KE(•,•) returns the number of feature values in common of two entities.</S>\n    <S sid=\"58\" ssid=\"8\">(2) Convolution Parse Tree Kernel: A convolution kernel aims to capture structured information in terms of substructures.</S>\n    <S sid=\"59\" ssid=\"9\">Here we use the same convolution parse tree kernel as described in Collins and Duffy (2001) for syntactic parsing and Moschitti (2004) for semantic role labeling.</S>\n    <S sid=\"60\" ssid=\"10\">Generally, we can represent a parse tree T by a vector of integer counts of each sub-tree type (regardless of its ancestors): φ(T) = (# subtree1(T), ..., # subtreei(T), ..., # subtreen(T) ) where # subtreei(T) is the occurrence number of the ith sub-tree type (subtreei) in T. Since the number of different sub-trees is exponential with the parse tree size, it is computationally infeasible to directly use the feature vectorφ(T) .</S>\n    <S sid=\"61\" ssid=\"11\">To solve this computational issue, Collins and Duffy (2001) proposed the following parse tree kernel to calculate the dot product between the above high dimensional vectors implicitly. where N1 and N2 are the sets of nodes in trees T1 and T2, respectively, and ( ) I subtree i n is a function that is 1 iff the subtreei occurs with root at node n and zero otherwise, and ∆(n1,n2) is the number of the common subtrees rooted at n1 and n2, i.e.</S>\n    <S sid=\"62\" ssid=\"12\">∆(n1, n2) = ∑i Isubtreei (n1 ) ⋅ Isubtreei (n2 ) ∆(n1 , n2) can be computed by the following recursive rules: where nc(n1) is the child number of n1, ch(n,j) is the jth child of node n andλ (0&lt;λ &lt;1) is the decay factor in order to make the kernel value less variable with respect to the subtree sizes.</S>\n    <S sid=\"63\" ssid=\"13\">In addition, the recursive rule (3) holds because given two nodes with the same children, one can construct common sub-trees using these children and common sub-trees of further offspring.</S>\n    <S sid=\"64\" ssid=\"14\">The parse tree kernel counts the number of common sub-trees as the syntactic similarity measure between two relation instances.</S>\n    <S sid=\"65\" ssid=\"15\">The time complexity for computing this kernel is O( |N1  |⋅  |N2|) .</S>\n    <S sid=\"66\" ssid=\"16\">In this paper, two composite kernels are defined by combing the above two individual kernels in the following ways: is the coefficient.</S>\n    <S sid=\"67\" ssid=\"17\">Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.4.</S>\n    <S sid=\"68\" ssid=\"18\">Here, Kˆ(•,•) is the normalizedK(•,•), Kp(•,•) is the polynomial expansion of K(•,•) with degree d=2, i.e.</S>\n    <S sid=\"69\" ssid=\"19\">Kp(•,•) = (K(•,•)+1)2 , and α is the coefficient.</S>\n    <S sid=\"70\" ssid=\"20\">Evaluation on the development set shows that this composite kernel yields the best performance when α is set to 0.23.</S>\n    <S sid=\"71\" ssid=\"21\">The polynomial expansion aims to explore the entity bi-gram features, esp. the combined features from the first and second entities, respectively.</S>\n    <S sid=\"72\" ssid=\"22\">In addition, due to the different scales of the values of the two individual kernels, they are normalized before combination.</S>\n    <S sid=\"73\" ssid=\"23\">This can avoid one kernel value being overwhelmed by that of another one.</S>\n    <S sid=\"74\" ssid=\"24\">The entity kernel formulated by eqn.</S>\n    <S sid=\"75\" ssid=\"25\">(1) is a proper kernel since it simply calculates the dot product of the entity feature vectors.</S>\n    <S sid=\"76\" ssid=\"26\">The tree kernel formulated by eqn.</S>\n    <S sid=\"77\" ssid=\"27\">(3) is proven to be a proper kernel (Collins and Duffy, 2001).</S>\n    <S sid=\"78\" ssid=\"28\">Since kernel function set is closed under normalization, polynomial expansion and linear combination (Schölkopf and Smola, 2001), the two composite kernels are also proper kernels.</S>\n    <S sid=\"79\" ssid=\"29\">A relation instance is encapsulated by a parse tree.</S>\n    <S sid=\"80\" ssid=\"30\">Thus, it is critical to understand which portion of a parse tree is important in the kernel calculation.</S>\n    <S sid=\"81\" ssid=\"31\">We study five cases as shown in Fig.1. mon sub-tree including the two entities.</S>\n    <S sid=\"82\" ssid=\"32\">In other words, the sub-tree is enclosed by the shortest path linking the two entities in the parse tree (this path is also commonly-used as the path tree feature in the feature-based methods).</S>\n    <S sid=\"83\" ssid=\"33\">Fig.</S>\n    <S sid=\"84\" ssid=\"34\">1 illustrates different representations of an example relation instance.</S>\n    <S sid=\"85\" ssid=\"35\">T1 is MCT for the relation instance, where the sub-tree circled by a dashed line is PT, which is also shown in T2 for clarity.</S>\n    <S sid=\"86\" ssid=\"36\">The only difference between MCT and PT lies in that MCT does not allow partial production rules (for example, NP4PP is a partial production rule while NP4NP+PP is an entire production rule in the top of T2).</S>\n    <S sid=\"87\" ssid=\"37\">For instance, only the most-right child in the most-left sub-tree [NP [CD 200] [JJ domestic] [E1-PER ...]] of T1 is kept in T2.</S>\n    <S sid=\"88\" ssid=\"38\">By comparing the performance of T1 and T2, we can evaluate the effect of sub-trees with partial production rules as shown in T2 and the necessity of keeping the whole left and right context sub-trees as shown in T1 in relation extraction.</S>\n    <S sid=\"89\" ssid=\"39\">T3 is CPT, where the two sub-trees circled by dashed lines are included as the context to T2 and make T3 context-sensitive.</S>\n    <S sid=\"90\" ssid=\"40\">This is to evaluate whether the limited context information in CPT can boost performance.</S>\n    <S sid=\"91\" ssid=\"41\">FPT in T4 is formed by removing the two circled nodes in T2.</S>\n    <S sid=\"92\" ssid=\"42\">This is to study whether and how the elimination of single non-terminal nodes affects the performance of relation extraction. fits to 200 domestic partners of their own workers in New York”, where the phrase type “E1-PER” denotes that the current node is the 1st entity with type “PERSON”, and likewise for the others.</S>\n    <S sid=\"93\" ssid=\"43\">The relation instance is excerpted from the ACE 2003 corpus, where a relation “SOCIAL.Other-Personal” exists between entities “partners” (PER) and “workers” (PER).</S>\n    <S sid=\"94\" ssid=\"44\">We use Charniak’s parser (Charniak, 2001) to parse the example sentence.</S>\n    <S sid=\"95\" ssid=\"45\">To save space, the FCPT is not shown here.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiments\" number=\"4\">\n    <S sid=\"96\" ssid=\"1\">Data: We use the English portion of both the ACE 2003 and 2004 corpora from LDC in our experiments.</S>\n    <S sid=\"97\" ssid=\"2\">In the ACE 2003 data, the training set consists of 674 documents and 9683 relation instances while the test set consists of 97 documents and 1386 relation instances.</S>\n    <S sid=\"98\" ssid=\"3\">The ACE 2003 data defines 5 entity types, 5 major relation types and 24 relation subtypes.</S>\n    <S sid=\"99\" ssid=\"4\">The ACE 2004 data contains 451 documents and 5702 relation instances.</S>\n    <S sid=\"100\" ssid=\"5\">It redefines 7 entity types, 7 major relation types and 23 subtypes.</S>\n    <S sid=\"101\" ssid=\"6\">Since Zhao and Grishman (2005) use a 5-fold cross-validation on a subset of the 2004 data (newswire and broadcast news domains, containing 348 documents and 4400 relation instances), for comparison, we use the same setting (5-fold cross-validation on the same subset of the 2004 data, but the 5 partitions may not be the same) for the ACE 2004 data.</S>\n    <S sid=\"102\" ssid=\"7\">Both corpora are parsed using Charniak’s parser (Charniak, 2001).</S>\n    <S sid=\"103\" ssid=\"8\">We iterate over all pairs of entity mentions occurring in the same sentence to generate potential relation instances.</S>\n    <S sid=\"104\" ssid=\"9\">In this paper, we only measure the performance of relation extraction models on “true” mentions with “true” chaining of coreference (i.e. as annotated by LDC annotators).</S>\n    <S sid=\"105\" ssid=\"10\">Implementation: We formalize relation extraction as a multi-class classification problem.</S>\n    <S sid=\"106\" ssid=\"11\">SVM is selected as our classifier.</S>\n    <S sid=\"107\" ssid=\"12\">We adopt the one vs. others strategy and select the one with the largest margin as the final answer.</S>\n    <S sid=\"108\" ssid=\"13\">The training parameters are chosen using cross-validation (C=2.4 (SVM); λ =0.4(tree kernel)).</S>\n    <S sid=\"109\" ssid=\"14\">In our implementation, we use the binary SVMLight (Joachims, 1998) and Tree Kernel Tools (Moschitti, 2004).</S>\n    <S sid=\"110\" ssid=\"15\">Precision (P), Recall (R) and F-measure (F) are adopted to measure the performance.</S>\n    <S sid=\"111\" ssid=\"16\">In this subsection, we report the experiments of different kernel setups for different purposes. parse tree on relation extraction, we remove the entity-related information from parse trees by replacing the entity-related phrase types (“E1PER” and so on as shown in Fig.</S>\n    <S sid=\"112\" ssid=\"17\">1) with “NP”.</S>\n    <S sid=\"113\" ssid=\"18\">Table 1 compares the performance of 5 tree kernel setups on the ACE 2003 data using the tree structure information only.</S>\n    <S sid=\"114\" ssid=\"19\">It shows that: ACE 2003 five major types using the parse tree structure information only (regardless of any entity-related information) kernel setups over the ACE major types of both the 2003 data (the numbers in parentheses) and the 2004 data (the numbers outside parentheses) the 2003 data although the ACE 2003 data is two times larger than the ACE 2004 data.</S>\n    <S sid=\"115\" ssid=\"20\">This may be due to two reasons: 1) The ACE 2004 data defines two new entity types and re-defines the relation types and subtypes in order to reduce the inconsistency between LDC annotators.</S>\n    <S sid=\"116\" ssid=\"21\">2) More importantly, the ACE 2004 data defines 43 entity subtypes while there are only 3 subtypes in the 2003 data.</S>\n    <S sid=\"117\" ssid=\"22\">The detailed classification in the 2004 data leads to significant performance improvement of 6.2 (54.4-48.2) in Fmeasure over that on the 2003 data.</S>\n    <S sid=\"118\" ssid=\"23\">Our composite kernel can achieve 77.3/65.6/70.9 and 76.1/68.4/72.1 in P/R/F over the ACE 2003/2004 major types, respectively. compare our method with previous work on the ACE 2002/2003/2004 data, respectively.</S>\n    <S sid=\"119\" ssid=\"24\">They show that our method outperforms the previous methods and significantly outperforms the previous two dependency kernels4.</S>\n    <S sid=\"120\" ssid=\"25\">This may be due to two reasons: 1) the dependency tree (Culotta and Sorensen, 2004) and the shortest path (Bunescu and Mooney, 2005) lack the internal hierarchical phrase structure information, so their corresponding kernels can only carry out node-matching directly over the nodes with word tokens; 2) the parse tree kernel has less constraints.</S>\n    <S sid=\"121\" ssid=\"26\">That is, it is 4 Bunescu and Mooney (2005) used the ACE 2002 corpus, including 422 documents, which is known to have many inconsistencies than the 2003 version.</S>\n    <S sid=\"122\" ssid=\"27\">Culotta and Sorensen (2004) used a generic ACE corpus including about 800 documents (no corpus version is specified).</S>\n    <S sid=\"123\" ssid=\"28\">Since the testing corpora are in different sizes and versions, strictly speaking, it is not ready to compare these methods exactly and fairly.</S>\n    <S sid=\"124\" ssid=\"29\">Therefore Table 3 is only for reference purpose.</S>\n    <S sid=\"125\" ssid=\"30\">We just hope that we can get a few clues from this table. not restricted by the two constraints of the two dependency kernels (identical layer and ancestors for the matchable nodes and identical length of two shortest paths, as discussed in Section 2).</S>\n    <S sid=\"126\" ssid=\"31\">The above experiments verify the effectiveness of our composite kernels for relation extraction.</S>\n    <S sid=\"127\" ssid=\"32\">They suggest that the parse tree kernel can effectively explore the syntactic features which are critical for relation extraction. both the 2003 and 2004 data for the composite kernel by polynomial expansion (4) Error Analysis: Table 5 reports the error distribution of the polynomial composite kernel over the major types on the ACE data.</S>\n    <S sid=\"128\" ssid=\"33\">It shows that 83.5%(198+115/198+115+62) / 85.8%(416 +171/416+171+96) of the errors result from relation detection and only 16.5%/14.2% of the errors result from relation characterization.</S>\n    <S sid=\"129\" ssid=\"34\">This may be due to data imbalance and sparseness issues since we find that the negative samples are 8 times more than the positive samples in the training set.</S>\n    <S sid=\"130\" ssid=\"35\">Nevertheless, it clearly directs our future work.</S>\n  </SECTION>\n  <SECTION title=\"5 Discussion\" number=\"5\">\n    <S sid=\"131\" ssid=\"1\">In this section, we compare our method with the previous work from the feature engineering viewpoint and report some other observations and issues in our experiments.</S>\n    <S sid=\"132\" ssid=\"2\">This is to explain more about why our method performs better and significantly outperforms the previous two dependency tree kernels from the theoretical viewpoint.</S>\n    <S sid=\"133\" ssid=\"3\">(1) Compared with Feature-based Methods: The basic difference lies in the relation instance representation (parse tree vs. feature vector) and the similarity calculation mechanism (kernel function vs. dot-product).</S>\n    <S sid=\"134\" ssid=\"4\">The main difference is the different feature spaces.</S>\n    <S sid=\"135\" ssid=\"5\">Regarding the parse tree features, our method implicitly represents a parse tree by a vector of integer counts of each sub-tree type, i.e., we consider the entire sub-tree types and their occurring frequencies.</S>\n    <S sid=\"136\" ssid=\"6\">In this way, the parse tree-related features (the path features and the chunking features) used in the featurebased methods are embedded (as a subset) in our feature space.</S>\n    <S sid=\"137\" ssid=\"7\">Moreover, the in-between word features and the entity-related features used in the feature-based methods are also captured by the tree kernel and the entity kernel, respectively.</S>\n    <S sid=\"138\" ssid=\"8\">Therefore our method has the potential of effectively capturing not only most of the previous flat features but also the useful syntactic structure features.</S>\n    <S sid=\"139\" ssid=\"9\">(2) Compared with Previous Kernels: Since our method only counts the occurrence of each sub-tree without considering the layer and the ancestors of the root node of the sub-tree, our method is not limited by the constraints (identical layer and ancestors for the matchable nodes, as discussed in Section 2) in Culotta and Sorensen (2004).</S>\n    <S sid=\"140\" ssid=\"10\">Moreover, the difference between our method and Bunescu and Mooney (2005) is that their kernel is defined on the shortest path between two entities instead of the entire subtrees.</S>\n    <S sid=\"141\" ssid=\"11\">However, the path does not maintain the tree structure information.</S>\n    <S sid=\"142\" ssid=\"12\">In addition, their kernel requires the two paths to have the same length.</S>\n    <S sid=\"143\" ssid=\"13\">Such constraint is too strict.</S>\n    <S sid=\"144\" ssid=\"14\">(1) Speed Issue: The recursively-defined convolution kernel is much slower compared to feature-based classifiers.</S>\n    <S sid=\"145\" ssid=\"15\">In this paper, the speed issue is solved in three ways.</S>\n    <S sid=\"146\" ssid=\"16\">First, the inclusion of the entity kernel makes the composite kernel converge fast.</S>\n    <S sid=\"147\" ssid=\"17\">Furthermore, we find that the small portion (PT) of a full parse tree can effectively represent a relation instance.</S>\n    <S sid=\"148\" ssid=\"18\">This significantly improves the speed.</S>\n    <S sid=\"149\" ssid=\"19\">Finally, the parse tree kernel requires exact match between two subtrees, which normally does not occur very frequently.</S>\n    <S sid=\"150\" ssid=\"20\">Collins and Duffy (2001) report that in practice, running time for the parse tree kernel is more close to linear (O(JN1J+JN2J), rather than O(JN1J*JN2J ).</S>\n    <S sid=\"151\" ssid=\"21\">As a result, using the PC with Intel P4 3.0G CPU and 2G RAM, our system only takes about 110 minutes and 30 minutes to do training on the ACE 2003 (~77k training instances) and 2004 (~33k training instances) data, respectively.</S>\n    <S sid=\"152\" ssid=\"22\">(2) Further Improvement: One of the potential problems in the parse tree kernel is that it carries out exact matches between sub-trees, so that this kernel fails to handle sparse phrases (i.e.</S>\n    <S sid=\"153\" ssid=\"23\">“a car” vs. “a red car”) and near-synonymic grammar tags (for example, the variations of a verb (i.e. go, went, gone)).</S>\n    <S sid=\"154\" ssid=\"24\">To some degree, it could possibly lead to over-fitting and compromise the performance.</S>\n    <S sid=\"155\" ssid=\"25\">However, the above issues can be handled by allowing grammar-driven partial rule matching and other approximate matching mechanisms in the parse tree kernel calculation.</S>\n    <S sid=\"156\" ssid=\"26\">Finally, it is worth noting that by introducing more individual kernels our method can easily scale to cover more features from a multitude of sources (e.g.</S>\n    <S sid=\"157\" ssid=\"27\">Wordnet, gazetteers, etc) that can be brought to bear on the task of relation extraction.</S>\n    <S sid=\"158\" ssid=\"28\">In addition, we can also easily implement the feature weighting scheme by adjusting the eqn.</S>\n    <S sid=\"159\" ssid=\"29\">(2) and the rule (2) in calculating ∆(n1,n2) (see subsection 3.1).</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion and Future Work\" number=\"6\">\n    <S sid=\"160\" ssid=\"1\">Kernel functions have nice properties.</S>\n    <S sid=\"161\" ssid=\"2\">In this paper, we have designed a composite kernel for relation extraction.</S>\n    <S sid=\"162\" ssid=\"3\">Benefiting from the nice properties of the kernel methods, the composite kernel could well explore and combine the flat entity features and the structured syntactic features, and therefore outperforms previous bestreported feature-based methods on the ACE corpus.</S>\n    <S sid=\"163\" ssid=\"4\">To our knowledge, this is the first research to demonstrate that, without the need for extensive feature engineering, an individual tree kernel achieves comparable performance with the feature-based methods.</S>\n    <S sid=\"164\" ssid=\"5\">This shows that the syntactic features embedded in a parse tree are particularly useful for relation extraction and which can be well captured by the parse tree kernel.</S>\n    <S sid=\"165\" ssid=\"6\">In addition, we find that the relation instance representation (selecting effective portions of parse trees for kernel calculations) is very important for relation extraction.</S>\n    <S sid=\"166\" ssid=\"7\">The most immediate extension of our work is to improve the accuracy of relation detection.</S>\n    <S sid=\"167\" ssid=\"8\">This can be done by capturing more features by including more individual kernels, such as the WordNet-based semantic kernel (Basili et al., 2005) and other feature-based kernels.</S>\n    <S sid=\"168\" ssid=\"9\">We can also benefit from machine learning algorithms to study how to solve the data imbalance and sparseness issues from the learning algorithm viewpoint.</S>\n    <S sid=\"169\" ssid=\"10\">In the future work, we will design a more flexible tree kernel for more accurate similarity measure.</S>\n    <S sid=\"170\" ssid=\"11\">Acknowledgements: We would like to thank Dr. Alessandro Moschitti for his great help in using his Tree Kernel Toolkits and fine-tuning the system.</S>\n    <S sid=\"171\" ssid=\"12\">We also would like to thank the three anonymous reviewers for their invaluable suggestions.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "1b16241979cebafc77e92c6cd31ed7bf",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "A Composite Kernel To Extract Relations Between Entities With Both Flat And Structured Features"
                }
            }
        },
        {
            "index": 12,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Semantic knowledge can be a great asset to natural language processing systems, but it is usually hand-coded for each application. Although some semantic information is available in general-purpose knowledge bases such as WordNet and Cyc, many applications require domain-specific lexicons that represent words and categories for a particular topic. In this paper, we present a corpus-based method that can be used to build semantic lexicons for specific categories. The input to the system is a small set of seed words for a category and a representative text corpus. The output is a ranked list of words that are associated with the category. A user then reviews the top-ranked words and decides which ones should be entered in the semantic lexicon. In experiments with five categories, users typically found about 60 words per category in 10-15 minutes to build a core semantic lexicon.",
                    "filename": "A_Corpus-Based_Approach_For_Building_Semantic_Lexicons",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W97-0313/Documents_xml/W97-0313.xml",
                    "title": "A Corpus-Based Approach For Building Semantic Lexicons"
                },
                "llm": {
                    "descriptions": [
                        "This study presents a corpus-based method for semi-automatically constructing semantic lexicons. By utilizing a representative text corpus and a set of seed words for a given category, the system identifies and ranks potential category words through a bootstrapping algorithm. The method requires minimal human intervention, with a user reviewing the output list to finalize the lexicon. Experiments with various categories demonstrate the method's effectiveness, showing how it can quickly generate a core semantic lexicon with just 10-15 minutes of human review. The approach has potential for semantic knowledge acquisition and is especially adept at identifying domain-specific terms not typically considered by users. Further work is needed to refine the method and determine its applicability to different types of categories.",
                        "This paper presents a corpus-based method for semiautomatically building semantic lexicons, which are crucial for enhancing natural language processing systems. The approach utilizes a small set of seed words and a representative text corpus to identify and rank potential category words through a statistical bootstrapping algorithm. The process is rapid, requiring minimal human intervention for the initial selection of seed words and final review of the ranked list to establish a semantic lexicon. Experiments with five distinct categories demonstrate the method's effectiveness, with the potential to expand category words by reviewing more than the initial 200 words. The approach shows promise for fast semantic knowledge acquisition, although further refinement and exploration into category and seed word selection are needed.",
                        "This paper presents a corpus-based method for semi-automatically constructing semantic lexicons. By analyzing text corpora with a set of seed words for each category, the authors employ a statistical bootstrapping algorithm to identify and rank potential category words. This iterative process generates a list of nouns, which are then manually reviewed to produce a semantic lexicon. Experiments with five categories—energy, financial, military, vehicles, and weapons—demonstrated the effectiveness of this approach, enabling the creation of a core semantic lexicon with minimal human intervention. The success of the method relies on category representation within the corpus and the quality of seed words. While the approach is promising for rapid lexicon construction, further refinements and exploration into category type suitability are necessary.",
                        "This paper introduces a corpus-based method for semi-automatically building semantic lexicons, which are crucial for various natural language processing tasks. The proposed system uses a text corpus and a set of seed words to identify and rank potential category members. The process is bootstrapped with simple statistics to expand the seed list and refine the lexicon. Experiments with five categories demonstrate the method's effectiveness, showing that a core semantic lexicon can be constructed in minutes with minimal human intervention. The approach is particularly adept at identifying domain-specific terms that may not be obvious to lexicon creators. Further work is required to refine the method and understand its applicability across various categories."
                    ],
                    "filename": "a_corpusbased_approach_for_build-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 142 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Corpus-Based Approach For Building Semantic Lexicons</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"8\" ssid=\"1\">Semantic information can be helpful in almost all aspects of natural language understanding, including word sense disambiguation, selectional restrictions, attachment decisions, and discourse processing.</S>\n    <S sid=\"9\" ssid=\"2\">Semantic knowledge can add a great deal of power and accuracy to natural language processing systems.</S>\n    <S sid=\"10\" ssid=\"3\">But semantic information is difficult to obtain.</S>\n    <S sid=\"11\" ssid=\"4\">In most cases, semantic knowledge is encoded manually for each application.</S>\n    <S sid=\"12\" ssid=\"5\">There have been a few large-scale efforts to create broad semantic knowledge bases, such as WordNet (Miller, 1990) and Cyc (Lenat, Prakash, and Shepherd, 1986).</S>\n    <S sid=\"13\" ssid=\"6\">While these efforts may be useful for some applications, we believe that they will never fully satisfy the need for semantic knowledge.</S>\n    <S sid=\"14\" ssid=\"7\">Many domains are characterized by their own sublanguage containing terms and jargon specific to the field.</S>\n    <S sid=\"15\" ssid=\"8\">Representing all sublanguages in a single knowledge base would be nearly impossible.</S>\n    <S sid=\"16\" ssid=\"9\">Furthermore, domain-specific semantic lexicons are useful for minimizing ambiguity problems.</S>\n    <S sid=\"17\" ssid=\"10\">Within the context of a restricted domain, many polysemous words have a strong preference for one word sense, so knowing the most probable word sense in a domain can strongly constrain the ambiguity.</S>\n    <S sid=\"18\" ssid=\"11\">We have been experimenting with a corpusbased method for building semantic lexicons semiautomatically.</S>\n    <S sid=\"19\" ssid=\"12\">Our system uses a text corpus and a small set of seed words for a category to identify other words that also belong to the category.</S>\n    <S sid=\"20\" ssid=\"13\">The algorithm uses simple statistics and a bootstrapping mechanism to generate a ranked list of potential category words.</S>\n    <S sid=\"21\" ssid=\"14\">A human then reviews the top words and selects the best ones for the dictionary.</S>\n    <S sid=\"22\" ssid=\"15\">Our approach is geared toward fast semantic lexicon construction: given a handful of seed words for a category and a representative text corpus, one can build a semantic lexicon for a category in just a few minutes.</S>\n    <S sid=\"23\" ssid=\"16\">In the first section, we describe the statistical bootstrapping algorithm for identifying candidate category words and ranking them.</S>\n    <S sid=\"24\" ssid=\"17\">Next, we describe experimental results for five categories.</S>\n    <S sid=\"25\" ssid=\"18\">Finally, we discuss our experiences with additional categories and seed word lists, and summarize our results.</S>\n  </SECTION>\n  <SECTION title=\"2 Generating a Semantic Lexicon\" number=\"2\">\n    <S sid=\"26\" ssid=\"1\">Our work is based on the observation that category members are often surrounded by other category members in text, for example in conjunctions (lions and tigers and bears), lists (lions, tigers, bears...), appositives (the stallion, a white Arabian), and nominal compounds (Arabian stallion; tuna fish).</S>\n    <S sid=\"27\" ssid=\"2\">Given a few category members, we wondered whether it would be possible to collect surrounding contexts and use statistics to identify other words that also belong to the category.</S>\n    <S sid=\"28\" ssid=\"3\">Our approach was motivated by Yarowsky's word sense disambiguation algorithm (Yarowsky, 1992) and the notion of statistical salience, although our system uses somewhat different statistical measures and techniques.</S>\n    <S sid=\"29\" ssid=\"4\">We begin with a small set of seed words for a category.</S>\n    <S sid=\"30\" ssid=\"5\">We experimented with different numbers of seed words, but were surprised to find that only 5 seed words per category worked quite well.</S>\n    <S sid=\"31\" ssid=\"6\">As an example, the seed word lists used in our experiments are shown below.</S>\n    <S sid=\"32\" ssid=\"7\">Energy: fuel gas gasoline oil power Financial: bank banking currency dollar money Military: army commander infantry soldier troop Vehicle: airplane car jeep plane truck Weapon: bomb dynamite explosives gun rifle The input to our system is a text corpus and an initial set of seed words for each category.</S>\n    <S sid=\"33\" ssid=\"8\">Ideally, the text corpus should contain many references to the category.</S>\n    <S sid=\"34\" ssid=\"9\">Our approach is designed for domainspecific text processing, so the text corpus should be a representative sample of texts for the domain and the categories should be semantic classes associated with the domain.</S>\n    <S sid=\"35\" ssid=\"10\">Given a text corpus and an initial seed word list for a category C, the algorithm for building a semantic lexicon is as follows: The context windows do not cut across sentence boundaries.</S>\n    <S sid=\"36\" ssid=\"11\">Note that our context window is much narrower than those used by other researchers (Yarowsky, 1992).</S>\n    <S sid=\"37\" ssid=\"12\">We experimented with larger window sizes and found that the narrow windows more consistently included words related to the target category.</S>\n    <S sid=\"38\" ssid=\"13\">Note that this is not exactly a conditional probability because a single word occurrence can belong to more than one context window.</S>\n    <S sid=\"39\" ssid=\"14\">For example, consider the sentence: I bought an AK-47 gun and an M-16 rifle.</S>\n    <S sid=\"40\" ssid=\"15\">The word M-16 would be in the context windows for both gun and rifle even though there was just one occurrence of it in the sentence.</S>\n    <S sid=\"41\" ssid=\"16\">Consequently, the category score for a word can be greater than 1.</S>\n    <S sid=\"42\" ssid=\"17\">4.</S>\n    <S sid=\"43\" ssid=\"18\">Next, we remove stopwords, numbers, and any words with a corpus frequency &lt; 5.</S>\n    <S sid=\"44\" ssid=\"19\">We used a stopword list containing about 30 general nouns, mostly pronouns (e.g., I, he, she, they) and determiners (e.g., this, that, those).</S>\n    <S sid=\"45\" ssid=\"20\">The stopwords and numbers are not specific to any category and are common across many domains, so we felt it was safe to remove them.</S>\n    <S sid=\"46\" ssid=\"21\">The remaining nouns are sorted by category score and ranked so that the nouns most strongly associated with the category appear at the top.</S>\n    <S sid=\"47\" ssid=\"22\">5.</S>\n    <S sid=\"48\" ssid=\"23\">The top five nouns that are not already seed words are added to the seed word list dynamically.</S>\n    <S sid=\"49\" ssid=\"24\">We then go back to Step 1 and repeat the process.</S>\n    <S sid=\"50\" ssid=\"25\">This bootstrapping mechanism dynamically grows the seed word list so that each iteration produces a larger category context.</S>\n    <S sid=\"51\" ssid=\"26\">In our experiments, the top five nouns were added automatically without any human intervention, but this sometimes allows non-category words to dilute the growing seed word list.</S>\n    <S sid=\"52\" ssid=\"27\">A few inappropriate words are not likely to have much impact, but many inappropriate words or a few highly frequent words can weaken the feedback process.</S>\n    <S sid=\"53\" ssid=\"28\">One could have a person verify that each word belongs to the target category before adding it to the seed word list, but this would require human interaction at each iteration of the feedback cycle.</S>\n    <S sid=\"54\" ssid=\"29\">We decided to see how well the technique could work without this additional human interaction, but the potential benefits of human feedback still need to be investigated.</S>\n    <S sid=\"55\" ssid=\"30\">After several iterations, the seed word list typically contains many relevant category words.</S>\n    <S sid=\"56\" ssid=\"31\">But more importantly, the ranked list contains many additional category words, especially near the top.</S>\n    <S sid=\"57\" ssid=\"32\">The number of iterations can make a big difference in the quality of the ranked list.</S>\n    <S sid=\"58\" ssid=\"33\">Since new seed words are generated dynamically without manual review, the quality of the ranked list can deteriorate rapidly when too many non-category words become seed words.</S>\n    <S sid=\"59\" ssid=\"34\">In our experiments, we found that about eight iterations usually worked well.</S>\n    <S sid=\"60\" ssid=\"35\">The output of the system is the ranked list of nouns after the final iteration.</S>\n    <S sid=\"61\" ssid=\"36\">The seed word list is thrown away.</S>\n    <S sid=\"62\" ssid=\"37\">Note that the original seed words were already known to be category members, and the new seed words are already in the ranked list because that is how they were selected.2 Finally, a user must review the ranked list and identify the words that are true category members.</S>\n    <S sid=\"63\" ssid=\"38\">How one defines a &amp;quot;true&amp;quot; category member is subjective and may depend on the specific application, so we leave this exercise to a person.</S>\n    <S sid=\"64\" ssid=\"39\">Typically, the words near the top of the ranked list are highly associated with the category but the density of category words decreases as one proceeds down the list.</S>\n    <S sid=\"65\" ssid=\"40\">The user may scan down the list until a sufficient number of category words is found, or as long as time permits.</S>\n    <S sid=\"66\" ssid=\"41\">The words selected by the user are added to a permanent semantic lexicon with the appropriate category label.</S>\n    <S sid=\"67\" ssid=\"42\">Our goal is to allow a user to build a semantic lexicon for one or more categories using only a small set of known category members as seed words and a text corpus.</S>\n    <S sid=\"68\" ssid=\"43\">The output is a ranked list of potential category words that a user can review to create a semantic lexicon quickly.</S>\n    <S sid=\"69\" ssid=\"44\">The success of this approach depends on the quality of the ranked list, especially the density of category members near the top.</S>\n    <S sid=\"70\" ssid=\"45\">In the next section, we describe experiments to evaluate our system.</S>\n    <S sid=\"71\" ssid=\"46\">2It is possible that a word may be near the top of the ranked list during one iteration (and subsequently become a seed word) but become buried at the bottom of the ranked list during later iterations.</S>\n    <S sid=\"72\" ssid=\"47\">However, we have not observed this to be a problem so far.</S>\n  </SECTION>\n  <SECTION title=\"3 Experimental Results\" number=\"3\">\n    <S sid=\"73\" ssid=\"1\">We performed experiments with five categories to evaluate the effectiveness and generality of our approach: energy, financial, military, vehicles, and weapons.</S>\n    <S sid=\"74\" ssid=\"2\">The MUC-4 development corpus (1700 texts) was used as the text corpus (MUC-4 Proceedings, 1992).</S>\n    <S sid=\"75\" ssid=\"3\">We chose these five categories because they represented relatively different semantic classes, they were prevalent in the MUC-4 corpus, and they seemed to be useful categories.</S>\n    <S sid=\"76\" ssid=\"4\">For each category, we began with the seed word lists shown in Figure 1.</S>\n    <S sid=\"77\" ssid=\"5\">We ran the bootstrapping algorithm for eight iterations, adding five new words to the seed word list after each cycle.</S>\n    <S sid=\"78\" ssid=\"6\">After the final iteration, we had ranked lists of potential category words for each of the five categories.</S>\n    <S sid=\"79\" ssid=\"7\">The top 45 words3 from each ranked list are shown in Figure 2.</S>\n    <S sid=\"80\" ssid=\"8\">While the ranked lists are far from perfect, one can see that there are many category members near the top of each list.</S>\n    <S sid=\"81\" ssid=\"9\">It is also apparent that a few additional heuristics could be used to remove many of the extraneous words.</S>\n    <S sid=\"82\" ssid=\"10\">For example, our number processor failed to remove numbers with commas (e.g., 2,000).</S>\n    <S sid=\"83\" ssid=\"11\">And the military category contains several ordinal numbers (e.g., 10th 3rd 1st) that could be easily identified and removed.</S>\n    <S sid=\"84\" ssid=\"12\">But the key question is whether the ranked list contains many true category members.</S>\n    <S sid=\"85\" ssid=\"13\">Since this is a subjective question, we set up an experiment involving human judges.</S>\n    <S sid=\"86\" ssid=\"14\">For each category, we selected the top 200 words from its ranked list and presented them to a user.</S>\n    <S sid=\"87\" ssid=\"15\">We presented the words in random order so that the user had no idea how our system had ranked the words.</S>\n    <S sid=\"88\" ssid=\"16\">This was done to minimize contextual effects (e.g., seeing five category members in a row might make someone more inclined to judge the next word as relevant).</S>\n    <S sid=\"89\" ssid=\"17\">Each category was judged by two people independently.4 The judges were asked to rate each word on a scale from 1 to 5 indicating how strongly it was associated with the category.</S>\n    <S sid=\"90\" ssid=\"18\">Since category judgements can be highly subjective, we gave them guidelines to help establish uniform criteria.</S>\n    <S sid=\"91\" ssid=\"19\">The instructions that were given to the judges are shown in Figure 3.</S>\n    <S sid=\"92\" ssid=\"20\">We asked the judges to rate the words on a scale from 1 to 5 because different degrees of category membership might be acceptable for different applications.</S>\n    <S sid=\"93\" ssid=\"21\">Some applications might require strict cat3 Note that some of these words are not nouns, such as boarded and U.S.-made.</S>\n    <S sid=\"94\" ssid=\"22\">Our parser tags unknown words as nouns, so sometimes unknown words are mistakenly selected for context windows.</S>\n    <S sid=\"95\" ssid=\"23\">'The judges were members of our research group but not the authors. aLimon-Covenas refers to an oil pipeline. aLa_Aurora refers to an airport.</S>\n    <S sid=\"96\" ssid=\"24\">CRITERIA: On a scale of 0 to 5, rate each word's strength of association with the given category using the following criteria.</S>\n    <S sid=\"97\" ssid=\"25\">We'll use the category ANIMAL as an example.</S>\n    <S sid=\"98\" ssid=\"26\">5: CORE MEMBER OF THE CATEGORY: If a word is clearly a member of the category, then it deserves a 5.</S>\n    <S sid=\"99\" ssid=\"27\">For example, dogs and sparrows are members of the ANIMAL category.</S>\n  </SECTION>\n  <SECTION title=\"4: SUBPART OF MEMBER OF THE CATEGORY:\" number=\"4\">\n    <S sid=\"100\" ssid=\"1\">If a word refers to a part of something that is a member of the category, then it deserves a 4.</S>\n    <S sid=\"101\" ssid=\"2\">For example, feathers and tails are parts of ANIMALS.</S>\n  </SECTION>\n  <SECTION title=\"3: STRONGLY ASSOCIATED WITH THE CATEGORY:\" number=\"5\">\n    <S sid=\"102\" ssid=\"1\">If a word refers to something that is strongly associated with members of the category, but is not actually a member of the category itself, then it deserves a 3.</S>\n    <S sid=\"103\" ssid=\"2\">For example, zoos and nests are strongly associated with ANIMALS.</S>\n  </SECTION>\n  <SECTION title=\"2: WEAKLY ASSOCIATED WITH THE CATEGORY:\" number=\"6\">\n    <S sid=\"104\" ssid=\"1\">If a word refers to something that can be associated with members of the category, but is also associated with many other types of things, then it deserves a 2.</S>\n    <S sid=\"105\" ssid=\"2\">For example, bowls and parks are weakly associated with ANIMALS.</S>\n    <S sid=\"106\" ssid=\"3\">1: NO ASSOCIATION WITH THE CATEGORY: If a word has virtually no association with the category, then it deserves a 1.</S>\n    <S sid=\"107\" ssid=\"4\">For example, tables and moons have virtually no association with ANIMALS.</S>\n    <S sid=\"108\" ssid=\"5\">0: UNKNOWN WORD: If you do not know what a word means, then it should be labeled with a 0.</S>\n    <S sid=\"109\" ssid=\"6\">IMPORTANT!</S>\n    <S sid=\"110\" ssid=\"7\">Many words have several distinct meanings.</S>\n    <S sid=\"111\" ssid=\"8\">For example, the word &amp;quot;horse&amp;quot; can refer to an animal, a piece of gymnastics equipment, or it can mean to fool around (e.g., &amp;quot;Don't horse around!&amp;quot;).</S>\n    <S sid=\"112\" ssid=\"9\">If a word has ANY meaning associated with the given category, then only consider that meaning when assigning numbers.</S>\n    <S sid=\"113\" ssid=\"10\">For example, the word &amp;quot;horse&amp;quot; would be a 5 because one of its meanings refers to an ANIMAL. egory membership, for example only words like gun, rifle, and bomb should be labeled as weapons.</S>\n    <S sid=\"114\" ssid=\"11\">But from a practical perspective, subparts of category members might also be acceptable.</S>\n    <S sid=\"115\" ssid=\"12\">For example, if a cartridge or trigger is mentioned in the context of an event, then one can infer that a gun was used.</S>\n    <S sid=\"116\" ssid=\"13\">And for some applications, any word that is strongly associated with a category might be useful to include in the semantic lexicon.</S>\n    <S sid=\"117\" ssid=\"14\">For example, words like ammunition or bullets are highly suggestive of a weapon.</S>\n    <S sid=\"118\" ssid=\"15\">In the UMass/MUC-4 information extraction system (Lehnert et al., 1992), the words ammunition and bullets were defined as weapons, mainly for the purpose of selectional restrictions.</S>\n    <S sid=\"119\" ssid=\"16\">The human judges estimated that it took them approximately 10-15 minutes, on average, to judge the 200 words for each category.</S>\n    <S sid=\"120\" ssid=\"17\">Since the instructions allowed the users to assign a zero to a word if they did not know what it meant, we manually removed the zeros and assigned ratings that we thought were appropriate.</S>\n    <S sid=\"121\" ssid=\"18\">We considered ignoring the zeros, but some of the categories would have been severely impacted.</S>\n    <S sid=\"122\" ssid=\"19\">For example, many of the legitimate weapons (e.g., M-16 and AR-15) were not known to the judges.</S>\n    <S sid=\"123\" ssid=\"20\">Fortunately, most of the unknown words were proper nouns with relatively unambiguous semantics, so we do not believe that this process compromised the integrity of the experiment.</S>\n    <S sid=\"124\" ssid=\"21\">Finally, we graphed the results from the human judges.</S>\n    <S sid=\"125\" ssid=\"22\">We counted the number of words judged as 5's by either judge, the number of words judged as 5's or 4's by either judge, the number of words judged as 5's, 4's, or 3's by either judge, and the number of words judged as either 5's, 4's, 3's, or 2's.</S>\n    <S sid=\"126\" ssid=\"23\">We plotted the results after each 20 words, stepping down the ranked list, to see whether the words near the top of the list were more highly associated with the category than words farther down.</S>\n    <S sid=\"127\" ssid=\"24\">We also wanted to see whether the number of category words leveled off or whether it continued to grow.</S>\n    <S sid=\"128\" ssid=\"25\">The results from this experiment are shown in Figures 4-8.</S>\n    <S sid=\"129\" ssid=\"26\">With the exception of the Energy category, we were able to find 25-45 words that were judged as 4's or 5's for each category.</S>\n    <S sid=\"130\" ssid=\"27\">This was our strictest test because only true category members (or subparts of true category members) earned this rating.</S>\n    <S sid=\"131\" ssid=\"28\">Although this might not seem like a lot of category words, 25-45 words is enough to produce a reasonable core semantic lexicon.</S>\n    <S sid=\"132\" ssid=\"29\">For example, the words judged as 5's for each category are shown in Figure 9.</S>\n    <S sid=\"133\" ssid=\"30\">Figure 9 illustrates an important benefit of the corpus-based approach.</S>\n    <S sid=\"134\" ssid=\"31\">By sifting through a large text corpus, the algorithm can find many relevant category words that a user would probably not enter in a semantic lexicon on their own.</S>\n    <S sid=\"135\" ssid=\"32\">For example, suppose a user wanted to build a dictionary of Vehicle words.</S>\n    <S sid=\"136\" ssid=\"33\">Most people would probably define words such as car, truck, plane, and automobile.</S>\n    <S sid=\"137\" ssid=\"34\">But it is doubtful that most people would think of words like gunships, fighter, carrier, and ambulances.</S>\n    <S sid=\"138\" ssid=\"35\">The corpus-based algorithm is especially good at identifying words that are common in the text corpus even though they might not be commonly used in general.</S>\n    <S sid=\"139\" ssid=\"36\">As another example, specific types of weapons (e.g., M-16, AR-15, M-60, or M-79) might not even be known to most users, but they are abundant in the MUC-4 corpus.</S>\n    <S sid=\"140\" ssid=\"37\">If we consider all the words rated as 3's, 4's, or 5's, then we were able to find about 50-65 words for every category except Energy.</S>\n    <S sid=\"141\" ssid=\"38\">Many of these words would be useful in a semantic dictionary for the category.</S>\n    <S sid=\"142\" ssid=\"39\">For example, some of the words rated as 3's for the Vehicle category include: flight, flights, aviation, pilot, airport, and highways.</S>\n    <S sid=\"143\" ssid=\"40\">Most of the words rated as 2's are not specific to the target category, but some of them might be useful for certain tasks.</S>\n    <S sid=\"144\" ssid=\"41\">For example, some words judged as 2's for the Energy category are: spill, pole, tower, and fields.</S>\n    <S sid=\"145\" ssid=\"42\">These words may appear in many different contexts, but in texts about Energy topics these words are likely to be relevant and probably should be defined in the dictionary.</S>\n    <S sid=\"146\" ssid=\"43\">Therefore we expect that a user would likely keep some of these words in the semantic lexicon but would probably be very selective.</S>\n    <S sid=\"147\" ssid=\"44\">Finally, the graphs show that most of the acquisition curves displayed positive slopes even at the end of the 200 words.</S>\n    <S sid=\"148\" ssid=\"45\">This implies that more category words would likely have been found if the users had reviewed more than 200 words.</S>\n    <S sid=\"149\" ssid=\"46\">The one exception, again, was the Energy category, which we will discuss in the next section.</S>\n    <S sid=\"150\" ssid=\"47\">The size of the ranked lists ranged from 442 for the financial category to 919 for the military category, so it would be interesting to know how many category members would have been found if we had given the entire lists to our judges.</S>\n  </SECTION>\n  <SECTION title=\"4 Selecting Categories and Seed Words\" number=\"7\">\n    <S sid=\"151\" ssid=\"1\">When we first began this work, we were unsure about what types of categories would be amenable to this approach.</S>\n    <S sid=\"152\" ssid=\"2\">So we experimented with a number of different categories.</S>\n    <S sid=\"153\" ssid=\"3\">Fortunately, most of them worked fairly well, but some of them did not.</S>\n    <S sid=\"154\" ssid=\"4\">We do not claim to understand exactly what types of categories will work well and which ones will not, but our early experiences did shed some light on the strengths and weaknesses of this approach.</S>\n    <S sid=\"155\" ssid=\"5\">In addition to the previous five categories, we also experimented with categories for Location, Commercial, and Person.</S>\n    <S sid=\"156\" ssid=\"6\">The Location category performed very well using seed words such as city, town, and province.</S>\n    <S sid=\"157\" ssid=\"7\">We didn't formally evaluate this category because most of the category words were proper nouns and we did not expect that our judges would know what they were.</S>\n    <S sid=\"158\" ssid=\"8\">But it is worth noting that this category achieved good results, presumably because location names often cluster together in appositives, conjunctions, and nominal compounds.</S>\n    <S sid=\"159\" ssid=\"9\">For the Commercial category, we chose seed words such as store, shop, and market.</S>\n    <S sid=\"160\" ssid=\"10\">Only a few new commercial words were identified, such as hotel and restaurant.</S>\n    <S sid=\"161\" ssid=\"11\">In retrospect, we realized that there were probably few words in the MUC-4 corpus that referred to commercial establishments.</S>\n    <S sid=\"162\" ssid=\"12\">(The MUC-4 corpus mainly contains reports of terrorist and military events.)</S>\n    <S sid=\"163\" ssid=\"13\">The relatively poor performance of the Energy category was probably due to the same problem.</S>\n    <S sid=\"164\" ssid=\"14\">If a category is not well-represented in the corpus then it is doomed because inappropriate words become seed words in the early iterations and quickly derail the feedback loop.</S>\n    <S sid=\"165\" ssid=\"15\">The Person category produced mixed results.</S>\n    <S sid=\"166\" ssid=\"16\">Some good category words were found, such as rebel, advisers, criminal, and citizen.</S>\n    <S sid=\"167\" ssid=\"17\">But many of the words referred to organizations (e.g., FMLN), groups (e.g., forces), and actions (e.g., attacks).</S>\n    <S sid=\"168\" ssid=\"18\">Some of these words seemed reasonable, but it was hard to draw a line between specific references to people and concepts like organizations and groups that may or may not consist entirely of people.</S>\n    <S sid=\"169\" ssid=\"19\">The large proportion of action words also diluted the list.</S>\n    <S sid=\"170\" ssid=\"20\">More experiments are needed to better understand whether this category is inherently difficult or whether a more carefully chosen set of seed words would improve performance.</S>\n    <S sid=\"171\" ssid=\"21\">More experiments are also needed to evaluate different seed word lists.</S>\n    <S sid=\"172\" ssid=\"22\">The algorithm is clearly sensitive to the initial seed words, but the degree of sensitivity is unknown.</S>\n    <S sid=\"173\" ssid=\"23\">For the five categories reported in this paper, we arbitrarily chose a few words that were central members of the category.</S>\n    <S sid=\"174\" ssid=\"24\">Our initial seed words worked well enough that we did not experiment with them very much.</S>\n    <S sid=\"175\" ssid=\"25\">But we did perform a few experiments varying the number of seed words.</S>\n    <S sid=\"176\" ssid=\"26\">In general, we found that additional seed words tend to improve performance, but the results were not substantially different using five seed words or using ten.</S>\n    <S sid=\"177\" ssid=\"27\">Of course, there is also a law of diminishing returns: using a seed word list containing 60 category words is almost like creating a semantic lexicon for the category by hand!</S>\n  </SECTION>\n  <SECTION title=\"5 Conclusions\" number=\"8\">\n    <S sid=\"178\" ssid=\"1\">Building semantic lexicons will always be a subjective process, and the quality of a semantic lexicon is highly dependent on the task for which it will be used.</S>\n    <S sid=\"179\" ssid=\"2\">But there is no question that semantic knowledge is essential for many problems in natural language processing.</S>\n    <S sid=\"180\" ssid=\"3\">Most of the time semantic knowledge is defined manually for the target application, but several techniques have been developed for generating semantic knowledge automatically.</S>\n    <S sid=\"181\" ssid=\"4\">Some systems learn the meanings of unknown words using expectations derived from other word definitions in the surrounding context (e.g., (Granger, 1977; Carbonell, 1979; Jacobs and Zernik, 1988; Hastings and Lytinen, 1994)).</S>\n    <S sid=\"182\" ssid=\"5\">Other approaches use example or case-based methods to match unknown word contexts against previously seen word contexts (e.g., (Berwick, 1989; Cardie, 1993)).</S>\n    <S sid=\"183\" ssid=\"6\">Our task orientation is a bit different because we are trying to construct a semantic lexicon for a target category, instead of classifying unknown or polysemous words in context.</S>\n    <S sid=\"184\" ssid=\"7\">To our knowledge, our system is the first one aimed at building semantic lexicons from raw text without using any additional semantic knowledge.</S>\n    <S sid=\"185\" ssid=\"8\">The only lexical knowledge used by our parser is a part-of-speech dictionary for syntactic processing.</S>\n    <S sid=\"186\" ssid=\"9\">Although we used a hand-crafted part-of-speech dictionary for these experiments, statistical and corpusbased taggers are readily available (e.g., (Brill, 1994; Church, 1989; Weischedel et al., 1993)).</S>\n    <S sid=\"187\" ssid=\"10\">Our corpus-based approach is designed to support fast semantic lexicon construction.</S>\n    <S sid=\"188\" ssid=\"11\">A user only needs to supply a representative text corpus and a small set of seed words for each target category.</S>\n    <S sid=\"189\" ssid=\"12\">Our experiments suggest that a core semantic lexicon can be built for each category with only 10-15 minutes of human interaction.</S>\n    <S sid=\"190\" ssid=\"13\">While more work needs to be done to refine this procedure and characterize the types of categories it can handle, we believe that this is a promising approach for corpus-based semantic knowledge acquisition.</S>\n  </SECTION>\n  <SECTION title=\"6 Acknowledgments\" number=\"9\">\n    <S sid=\"191\" ssid=\"1\">This research was funded by NSF grant IRI-9509820 and the University of Utah Research Committee.</S>\n    <S sid=\"192\" ssid=\"2\">We would like to thank David Bean, Jeff Lorenzen, and Kiri Wagstaff for their help in judging our category lists.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "ed324d9a3c35296183853f41afc9b909",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "A Corpus-Based Approach For Building Semantic Lexicons"
                }
            }
        },
        {
            "index": 13,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.",
                    "filename": "Generating_Natural_Language_Summaries_From_Multiple_On-Line_Sources",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J98-3005/Documents_xml/J98-3005.xml",
                    "title": "Generating Natural Language Summaries From Multiple On-Line Sources"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents SUMMONS, a system designed to automatically generate natural language summaries of news events by integrating information from multiple online sources. SUMMONS stands out by synthesizing similarities, differences, and contradictions across various reports, and by providing historical context for the events described. The system incorporates outputs from DARPA Message Understanding Conferences, creating summaries that are not only informative but also fluent and coherent in English. A major challenge addressed is the overwhelming volume of information available on the Internet, which makes it difficult for users to manually process all relevant documents on a topic. SUMMONS aims to alleviate this by providing search, selection, and summarization services. The paper details the components of SUMMONS, the methodology behind it, and the use of extracted entity descriptions to enhance briefings. The system focuses on the domain of news on terrorism and has the potential to be adapted to other domains with further research.",
                        "This paper introduces SUMMONS, a system designed to generate natural language summaries of news articles from multiple online sources. The authors discuss challenges posed by the information overload on the internet, particularly in the context of current events, where users face difficulties in selecting and reading relevant news articles from the vast amount available. SUMMONS leverages the output from systems developed for the DARPA Message Understanding Conferences to create briefings that synthesize information across documents, highlighting similarities, differences, and contradictions. The system provides briefings that include background information on entities such as people and places, enhancing user understanding. By combining information extraction techniques and language generation, SUMMONS presents a significant step towards addressing the need for efficient summarization tools in the domain of news on terrorism, with potential applications across various domains and ongoing developments for real-time updates and improved user profiling.",
                        "This paper introduces SUMMONS, a system that generates natural language summaries by synthesizing information from multiple online news sources on current events. Utilizing outputs from systems developed for the DARPA Message Understanding Conferences, SUMMONS identifies similarities, differences, and contradictions in various reports. The system is particularly adept at providing briefings that highlight changes over time and across different information sources, incorporating background information on entities such as people and places. The methodology for content summarization, the system architecture, and the novel techniques employed by SUMMONS are discussed. The paper also explores the potential for future work in symbolic summarization and the challenges of evaluating the summaries generated by such systems.",
                        "This paper presents SUMMONS, a system developed to generate natural language summaries from multiple online news sources about current events, incorporating background information. The system leverages the output from DARPA Message Understanding Conferences to summarize documents on similar events by showing agreements, contradictions, and generalizations across different information sources. The authors describe the system's components, focusing on how information from various articles is amalgamated and structured into paragraphs, and then realized as English sentences. They discuss the challenges posed by the vast amount of information on the Internet and the need for summarization facilities. SUMMONS is particularly applied to the domain of news on terrorism, and the paper explores how entity descriptions such as people and places are extracted and reused to enhance briefings. The work showcases the potential of knowledge-based summarization systems in restricted domains and suggests directions for future research in symbolic summarization of heterogeneous sources."
                    ],
                    "filename": "generating_natural_language_summ-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 112 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Generating Natural Language Summaries From Multiple On-Line Sources</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"5\" ssid=\"1\">We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information.</S>\n    <S sid=\"6\" ssid=\"2\">The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information.</S>\n    <S sid=\"7\" ssid=\"3\">We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences.</S>\n    <S sid=\"8\" ssid=\"4\">A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"9\" ssid=\"1\">One of the major problems with the Internet is the abundance of information and the resulting difficulty for a typical computer user to read all existing documents on a specific topic.</S>\n    <S sid=\"10\" ssid=\"2\">Even within the domain of current news, the user's task is infeasible.</S>\n    <S sid=\"11\" ssid=\"3\">There exist now more than 100 sources of live newswire on the Internet, mostly accessible through the World-Wide Web (Berners-Lee 1992).</S>\n    <S sid=\"12\" ssid=\"4\">Some of the most popular sites include news agencies and television stations like Reuters News (Reuters 1996), CNN's Web (CNN 1996), and ClariNet's e.News on-line newspaper (ClariNet 1996), as well as on-line versions of print media such as the New York Times on the Web edition (NYT 1996).</S>\n    <S sid=\"13\" ssid=\"5\">For the typical user, it is nearly impossible to go through megabytes of news every day to select articles he wishes to read.</S>\n    <S sid=\"14\" ssid=\"6\">Even when the user can actually select all news relevant to the topic of interest, he will still be faced with the problem of selecting a small subset that he can actually read in a limited time from the immense corpus of news available.</S>\n    <S sid=\"15\" ssid=\"7\">Hence, there is a need for search and selection services, as well as for summarization facilities.</S>\n    <S sid=\"16\" ssid=\"8\">, There currently exist more than 40 search and selection services on the WorldWide Web, such as DEC's Altavista (Altavista 1996), Lycos (Lycos 1996), and DejaNews (DejaNews 1997), all of which allow keyword searches for recent news.</S>\n    <S sid=\"17\" ssid=\"9\">However, only recently have there been practical results in the area of summarization.</S>\n    <S sid=\"18\" ssid=\"10\">Summaries can be used to determine if any of the retrieved articles are relevant (thereby allowing the user to avoid reading those that are not) or can be read in place of the articles to learn about information of interest to the user.</S>\n    <S sid=\"19\" ssid=\"11\">Existing summarization systems (e.g., Preston and Williams 1994; Cuts 1994; NetSumm 1996; Kupiec, Pedersen, and Chen 1995; Rau, Brandow, and Mitze, 1994) typically use statistical techniques to extract relevant sentences from a news article.</S>\n    <S sid=\"20\" ssid=\"12\">This domain-independent approach produces a summary of a single article at a time, which can indicate to the user what the article is about.</S>\n    <S sid=\"21\" ssid=\"13\">In contrast, our work focuses on generation of a summary that briefs the user on information in which he has indicated interest.</S>\n    <S sid=\"22\" ssid=\"14\">Such briefings pull together information of interest from multiple sources, aggregating information to provide generalizations, similarities, and differences across articles, and changes in perspective across time.</S>\n    <S sid=\"23\" ssid=\"15\">Briefings do not necessarily fully summarize the articles retrieved, but they update the user on information he has specified is of interest.</S>\n    <S sid=\"24\" ssid=\"16\">We present a system, called SUMMONS' (McKeown and Radev 1995; Radev 1996; Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques in the following areas: As can be expected from a knowledge-based summarization system, SUMMONS works in a restricted domain.</S>\n    <S sid=\"25\" ssid=\"17\">We have chosen the domain of news on terrorism for several reasons.</S>\n    <S sid=\"26\" ssid=\"18\">First, there is already a large body of related research projects in information extraction, knowledge representation, and text planning in the domain of terrorism.</S>\n    <S sid=\"27\" ssid=\"19\">For example, earlier systems developed under the DARPA Message Understanding Conference (MUC) were in the terrorist domain, and thus, we can build on these systems without having to start from scratch.</S>\n    <S sid=\"28\" ssid=\"20\">The domain is important to a variety of users, including casual news readers, journalists, and security analysts.</S>\n    <S sid=\"29\" ssid=\"21\">Finally, SUMMONS is being developed as part of a general environment for illustrated briefing over live multimedia information (Aho et al. 1997).</S>\n    <S sid=\"30\" ssid=\"22\">Of all MUC system domains, terrorism is more likely to have a variety of related images than other domains that were explored, such as mergers and acquisitions or management succession.</S>\n    <S sid=\"31\" ssid=\"23\">In order to extract information of interest to the user, SUMMONS makes use of components from several MUC systems.</S>\n    <S sid=\"32\" ssid=\"24\">The output of such modules is in the form of SUMMONS architecture. templates that represent certain pieces of information found in the source news articles, such as victims, perpetrators, or type of event.</S>\n    <S sid=\"33\" ssid=\"25\">By relying on these systems, the task we have addressed to date is happily more restricted than direct summarization of full text.</S>\n    <S sid=\"34\" ssid=\"26\">This has allowed us to focus on issues related to the combination of information in the templates and the generation of text to express them.</S>\n    <S sid=\"35\" ssid=\"27\">In order to port our system to other domains, we would need to develop new templates and the information extraction rules required for them.</S>\n    <S sid=\"36\" ssid=\"28\">While this is a task we leave to those working in the information extraction field, we note that there do exist tools for semi-automatically acquiring such rules (Lehnert et al. 1993; Fisher et al.</S>\n    <S sid=\"37\" ssid=\"29\">1995).</S>\n    <S sid=\"38\" ssid=\"30\">This helps to alleviate the otherwise knowledge-intensive nature of the task.</S>\n    <S sid=\"39\" ssid=\"31\">We are working on the development of tools for domain-independent types of information extraction.</S>\n    <S sid=\"40\" ssid=\"32\">For example, our work on extracting descriptions of individuals and organizations and representing them in a formalism that facilitates reuse of the descriptions in summaries can be used in any domain.</S>\n    <S sid=\"41\" ssid=\"33\">In the remainder of this section, we highlight the novel techniques of SUMMONS and explain why they are important for our work.</S>\n    <S sid=\"42\" ssid=\"34\">With a few exceptions (cf.</S>\n    <S sid=\"43\" ssid=\"35\">Section 2), all existing summarizers provide summaries of single articles by extracting sentences from them.</S>\n    <S sid=\"44\" ssid=\"36\">If such systems were applied to a series of articles, they might be able to extract sentences that have words in common with the other articles, but they would be unable to indicate how sentences that were extracted from different articles were similar.</S>\n    <S sid=\"45\" ssid=\"37\">Moreover, they would certainly not be able to indicate significant differences between articles.</S>\n    <S sid=\"46\" ssid=\"38\">In contrast, our work focuses on processing of information from multiple sources to highlight agreements and contradictions as part of the summary.</S>\n    <S sid=\"47\" ssid=\"39\">Given the omnipresence of on-line news services, one can expect that any interesting news event will be covered by several, if not most of them.</S>\n    <S sid=\"48\" ssid=\"40\">If different sources present the same information, the user clearly only needs to have access to one of them.</S>\n    <S sid=\"49\" ssid=\"41\">Practically, this assumption doesn't hold, as different sources provide updates from a different perspective and at different times.</S>\n    <S sid=\"50\" ssid=\"42\">An intelligent summarizer's task, therefore, is to attain as much information from the multiple sources as possible, combine it, and present it in a concise form to the user.</S>\n    <S sid=\"51\" ssid=\"43\">For example, if two sources of information report a different number of casualties in a particular incident, SUMMONS will report the contradiction and attribute the contradictory information to its sources, rather than select one of the contradictory pieces without the other.</S>\n    <S sid=\"52\" ssid=\"44\">An inherent problem to summarizers based on sentence extraction is the lack of discourse-level fluency in the output.</S>\n    <S sid=\"53\" ssid=\"45\">The extracted sentences fit together only in the case they are adjacent in the source document.</S>\n    <S sid=\"54\" ssid=\"46\">Because SUMMONS uses language generation techniques to determine the content and wording of the summary based on information extracted from input articles, it has all necessary information to produce a fluent surface summary.</S>\n    <S sid=\"55\" ssid=\"47\">We show how the summary generated using symbolic techniques can be enhanced so that it includes descriptions of entities (such as people, places, or organizations) it contains.</S>\n    <S sid=\"56\" ssid=\"48\">If a user tunes in to news on a given event several days after the first report, references to and descriptions of the event, people, and organizations involved may not be adequate.</S>\n    <S sid=\"57\" ssid=\"49\">We collect such descriptions from on-line sources of past news and represent them using our generation formalism for reuse in later generation of summaries.</S>\n    <S sid=\"58\" ssid=\"50\">The following section positions our research in the context of prior work in the area.</S>\n    <S sid=\"59\" ssid=\"51\">Section 3 describes the system architecture that we have developed for the summarization task.</S>\n    <S sid=\"60\" ssid=\"52\">The next two sections describe in more detail how a base summary is generated from multiple source articles and how the base summary is extended using descriptions extracted from on-line sources.</S>\n    <S sid=\"61\" ssid=\"53\">Section 6 describes the current status of our system.</S>\n    <S sid=\"62\" ssid=\"54\">We conclude this article in Sections 7 and 8 by describing some directions for future work in symbolic summarization of heterogeneous sources.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"63\" ssid=\"1\">Previous work related to summarization falls into three main categories.</S>\n    <S sid=\"64\" ssid=\"2\">In the first, full text is accepted as input and some percentage of the text is produced as output.</S>\n    <S sid=\"65\" ssid=\"3\">Typically, statistical approaches, augmented with keyword or phrase matching, are used to lift from the article full sentences that can serve as a summary.</S>\n    <S sid=\"66\" ssid=\"4\">Most of the work in this category produces a summary for a single article, although there are a few exceptions.</S>\n    <S sid=\"67\" ssid=\"5\">The other two categories correspond to the two stages of processing that would have to be carried out if sentence extraction were not used: analysis of the input document to identify information that should appear in a summary and generation of a textual summary from a set of facts that are to be included.</S>\n    <S sid=\"68\" ssid=\"6\">In this section, we first present work on sentence extraction, next turn to work on identifying information in an article that should appear in a summary, and conclude with work on generation of summaries from data, showing how this task differs from the more general language generation task.</S>\n    <S sid=\"69\" ssid=\"7\">This is a systems-oriented perspective on summarization-related work, focusing on techniques that have been implemented for the task.</S>\n    <S sid=\"70\" ssid=\"8\">There is also a large body of work on the nature of abstracting from a library science point of view (Borko 1975).</S>\n    <S sid=\"71\" ssid=\"9\">This work distinguishes between different types of abstracts, most notably, indicative abstracts that tell what an article is about, and informative abstracts, that include major results from the article and can be read in place of it.</S>\n    <S sid=\"72\" ssid=\"10\">SUMMONS generates summaries that are informative in nature.</S>\n    <S sid=\"73\" ssid=\"11\">Research in psychology and education also focuses on how to teach people to write summaries (e.g., Endres-Niggemeyer 1993; Rothkegel 1993).</S>\n    <S sid=\"74\" ssid=\"12\">This type of work can aid the development of summarization systems by providing insights into the human process of summarization that could be simulated in systems.</S>\n    <S sid=\"75\" ssid=\"13\">To allow summarization in arbitrary domains, researchers have traditionally applied statistical techniques (Luhn 1958; Paice 1990; Preston and Williams 1994; Rau, Brandow, and Mitze 1994).</S>\n    <S sid=\"76\" ssid=\"14\">This approach can be better termed extraction rather than summarization, since it attempts to identify and extract key sentences from an article using statistical techniques that locate important phrases using various statistical measures.</S>\n    <S sid=\"77\" ssid=\"15\">This has been successful in different domains (Preston and Williams 1994) and is, in fact, the approach used in recent commercial summarizers (Apple [Boguraev and Kennedy 1997], Microsoft, and in)(ight).</S>\n    <S sid=\"78\" ssid=\"16\">Rau, Brandow, and Mitze (1994) report that statistical summaries of individual news articles were rated lower by evaluators than summaries formed by simply using the lead sentence or two from the article.</S>\n    <S sid=\"79\" ssid=\"17\">This follows the principle of the &amp;quot;inverted pyramid&amp;quot; in news writing, which puts the most salient information in the beginning of the article and leaves elaborations for later paragraphs, allowing editors to cut from the end of the text without compromising the readability of the remaining text.</S>\n    <S sid=\"80\" ssid=\"18\">Paice (1990) also notes that problems for this approach center around the fluency of the resulting summary.</S>\n    <S sid=\"81\" ssid=\"19\">For example, extracted sentences may accidentally include pronouns that have no previous reference in the extracted text or, in the case of extracting several sentences, may result in incoherent text when the extracted sentences are not consecutive in the original text and do not naturally follow one another.</S>\n    <S sid=\"82\" ssid=\"20\">Paice describes techniques for modifying the extracted text to replace unresolved references.</S>\n    <S sid=\"83\" ssid=\"21\">Summaries that consist of sentences plucked from texts have been shown to be useful indicators of content, but they are often judged to be highly unreadable (Brandow, Mitze, and Rau 1990).</S>\n    <S sid=\"84\" ssid=\"22\">A more recent approach (Kupiec, Pedersen, and Chen 1995) uses a corpus of articles with summaries to train a statistical summarization system.</S>\n    <S sid=\"85\" ssid=\"23\">During training, the system uses abstracts of existing articles to identify the features of sentences that are typically included in abstracts.</S>\n    <S sid=\"86\" ssid=\"24\">In order to avoid problems noted by Paice, the system produces an itemized list of sentences from the article thus eliminating the implication that these sentences function together coherently as a full paragraph.</S>\n    <S sid=\"87\" ssid=\"25\">As with the other statistical approaches, this work is aimed at summarization of single articles.</S>\n    <S sid=\"88\" ssid=\"26\">Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summarization primarily focused on the use of sentence extraction.</S>\n    <S sid=\"89\" ssid=\"27\">Alternatives to the use of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997).</S>\n    <S sid=\"90\" ssid=\"28\">While most of the work in this category focuses on summarization of single articles, early work is beginning to emerge on summarization across multiple documents.</S>\n    <S sid=\"91\" ssid=\"29\">In ongoing work at Carnegie Mellon, Carbonell (personal communication) is developing statistical techniques to identify similar sentences and phrases across articles.</S>\n    <S sid=\"92\" ssid=\"30\">The aim is to identify sentences that are representative of more than one article.</S>\n    <S sid=\"93\" ssid=\"31\">Mani and Bloedorn (1997) link similar words and phrases from a pair of articles using WordNet (Miller et al. 1990) semantic relations.</S>\n    <S sid=\"94\" ssid=\"32\">They show extracted sentences from the two articles side by side in the output.</S>\n    <S sid=\"95\" ssid=\"33\">While useful in general, sentence extraction approaches cannot handle the task that we address, aggregate summarization across multiple documents, since this requires reasoning about similarities and differences across documents to produce generalizations or contradictions at a conceptual level.</S>\n    <S sid=\"96\" ssid=\"34\">Work in summarization using symbolic techniques has tended to focus more on identifying information in text that can serve as a summary (Young and Hayes 1985; Rau 1988; Hahn 1990) than on generating the summary, and often relies heavily on domain-dependent scripts (DeJong 1979; Tait 1983).</S>\n    <S sid=\"97\" ssid=\"35\">The DARPA message understanding systems (MUC 1992), which process news articles in specific domains to extract specified types of information, also fall within this category.</S>\n    <S sid=\"98\" ssid=\"36\">As output, work of this type produces templates that identify important pieces of information in the text, representing them as attribute-value pairs that could be part of a database entry.</S>\n    <S sid=\"99\" ssid=\"37\">The message understanding systems, in particular, have been developed over a long period, have undergone repeated evaluation and development, including moves to new domains, and as a result, are quite robust.</S>\n    <S sid=\"100\" ssid=\"38\">They are impressive in their ability to handle large quantities of free-form text as input.</S>\n    <S sid=\"101\" ssid=\"39\">As stand-alone systems, however, they do not address the task of summarization since they do not combine and rephrase extracted information as part of a textual summary.</S>\n    <S sid=\"102\" ssid=\"40\">A recent approach to symbolic summarization is being carried out at Cambridge University on identifying strategies for summarization (Sparck Jones 1993).</S>\n    <S sid=\"103\" ssid=\"41\">This work studies how various discourse processing techniques (e.g., rhetorical structure relations) can be used to both identify important information and form the actual summary.</S>\n    <S sid=\"104\" ssid=\"42\">While promising, this work does not involve an implementation as of yet, but provides a framework and strategies for future work.</S>\n    <S sid=\"105\" ssid=\"43\">Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree.</S>\n    <S sid=\"106\" ssid=\"44\">In addition to domain-specific information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction.</S>\n    <S sid=\"107\" ssid=\"45\">These are domain-independent techniques that can also be used to extract information for a summary.</S>\n    <S sid=\"108\" ssid=\"46\">Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et al.</S>\n    <S sid=\"109\" ssid=\"47\">1994), the use of extensive name lists, place names, titles and &amp;quot;gazetteers&amp;quot; in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al.</S>\n    <S sid=\"110\" ssid=\"48\">1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (VVeischedel et al.</S>\n    <S sid=\"111\" ssid=\"49\">1993; Lehnert et al. 1993).</S>\n    <S sid=\"112\" ssid=\"50\">In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald 1993; Coates-Stephens 1991) and the larger discourse context (Mani et al. 1993) to improve the accuracy of proper noun extraction when large known-word lists are not available.</S>\n    <S sid=\"113\" ssid=\"51\">In a way similar to this research, our work also aims at extracting proper nouns without the aid of large word lists.</S>\n    <S sid=\"114\" ssid=\"52\">We use a regular grammar encoding part-of-speech categories to extract certain text patterns (descriptions) and we use WordNet (Miller et al. 1990) to provide semantic filtering.</S>\n    <S sid=\"115\" ssid=\"53\">Another system, called MURAX (Kupiec 1993), is similar to ours from a different perspective.</S>\n    <S sid=\"116\" ssid=\"54\">MURAX also extracts information from a text to serve directly in response to a user question.</S>\n    <S sid=\"117\" ssid=\"55\">MURAX uses lexicosyntactic patterns, collocational analysis, along with information retrieval statistics, to find the string of words in a text that is most likely to serve as an answer to a user's wh-query Ultimately, this approach could be used to extract information on items of interest in a user profile, where each question may represent a different point of interest.</S>\n    <S sid=\"118\" ssid=\"56\">In our work, we also reuse strings (i.e., descriptions) as part of the summary, but the string that is extracted may be merged, or regenerated, as part of a larger textual summary.</S>\n    <S sid=\"119\" ssid=\"57\">Summarization of data using symbolic techniques has met with more success than summarization of text.</S>\n    <S sid=\"120\" ssid=\"58\">Summary generation is distinguished from the more traditional language generation problem by the fact that summarization is concerned with conveying the maximal amount of information within minimal space.</S>\n    <S sid=\"121\" ssid=\"59\">This goal is achieved through two distinct subprocesses, conceptual and linguistic summarization.</S>\n    <S sid=\"122\" ssid=\"60\">Conceptual summarization is a form of content selection.</S>\n    <S sid=\"123\" ssid=\"61\">It must determine which concepts out of a large number of concepts in the input should be included in the summary.</S>\n    <S sid=\"124\" ssid=\"62\">Linguistic summarization is concerned with expressing that information in the most concise way possible.</S>\n    <S sid=\"125\" ssid=\"63\">We have worked on the problem of summarization of data within the context of three separate systems.</S>\n    <S sid=\"126\" ssid=\"64\">STREAK (Robin and McKeown 1993; Robin 1994; Robin and McKeown 1995) generates summaries of basketball games, using a revision-based approach to summarization.</S>\n    <S sid=\"127\" ssid=\"65\">It builds a first draft using fixed information that must appear in the summary (e.g., in basketball summaries, the score and who won and lost is always present).</S>\n    <S sid=\"128\" ssid=\"66\">In a second pass, it uses revision rules to opportunistically add in information, as allowed by the form of the existing text.</S>\n    <S sid=\"129\" ssid=\"67\">Using this approach, information that might otherwise appear as separate sentences gets added in as modifiers of the existing sentences, or new words that can simultaneously convey both pieces of information are selected.</S>\n    <S sid=\"130\" ssid=\"68\">PLANDoc (McKeown, Kukich, and Shaw 1994a; McKeown, Robin, and Kukich 1995; Shaw 1995) generates summaries of the activities of telephone planning engineers, using linguistic summarization both to order its input messages and to combine them into single sentences.</S>\n    <S sid=\"131\" ssid=\"69\">Focus has been on the combined use of conjunction, ellipsis, and paraphrase to result in concise, yet fluent reports (Shaw 1995).</S>\n    <S sid=\"132\" ssid=\"70\">ZEDDoc (Passormeau et al. 1997; Kukich et al.</S>\n    <S sid=\"133\" ssid=\"71\">1997) generates Web traffic summaries for advertisement management software.</S>\n    <S sid=\"134\" ssid=\"72\">It makes use of an ontology over the domain to combine information at the conceptual level.</S>\n    <S sid=\"135\" ssid=\"73\">All of these systems take tabular data as input.</S>\n    <S sid=\"136\" ssid=\"74\">The research focus has been on linguistic summarization.</S>\n    <S sid=\"137\" ssid=\"75\">SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data.</S>\n    <S sid=\"138\" ssid=\"76\">At least four previous systems developed elsewhere use natural language to summarize quantitative data, including ANA (Kukich 1983), SEMTEX (Rosner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al.</S>\n    <S sid=\"139\" ssid=\"77\">1994).</S>\n    <S sid=\"140\" ssid=\"78\">All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles.</S>\n    <S sid=\"141\" ssid=\"79\">In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"142\" ssid=\"1\">The overall architecture of our summarization system given earlier in Figure 1 draws on research in software agents (Genesereth and Ketchpel 1994) to allow connections to a variety of different types of data sources.</S>\n    <S sid=\"143\" ssid=\"2\">Facilities are used to provide a transparent interface to heterogeneous data sources that run on several machines and may be written in different programming languages.</S>\n    <S sid=\"144\" ssid=\"3\">Currently, we have incorporated facilities to various live news streams, the CIA World Factbook, and past newspaper archives.</S>\n    <S sid=\"145\" ssid=\"4\">The architecture allows for the incorporation of additional facilitators and data sources as our work progresses.</S>\n    <S sid=\"146\" ssid=\"5\">The system extracts data from the different sources and then combines it into a conceptual representation of the summary.</S>\n    <S sid=\"147\" ssid=\"6\">The summarization component, shown on the left side of the figure, consists of a base summary generator, which combines information from multiple input articles and organizes that information using a paragraph planner.</S>\n    <S sid=\"148\" ssid=\"7\">The structured conceptual representation of the summary is passed to the lexical chooser, shown at the bottom of the diagram.</S>\n    <S sid=\"149\" ssid=\"8\">The lexical chooser also receives input from the World Factbook and possible descriptions of people or organizations to augment the base summary The full content is then passed through a sentence generator, implemented using the FUF / SURGE language generation system (Elhadad 1993; Robin 1994).</S>\n    <S sid=\"150\" ssid=\"9\">FUF is a functional unification formalism that uses a large systemic grammar of English, called SURGE, to fill in syntactic constraints, build a syntactic tree, choose closed class words, and eventually linearize the tree as a sentence.</S>\n    <S sid=\"151\" ssid=\"10\">The right side of the figure shows how proper nouns and their descriptions are extracted from past news.</S>\n    <S sid=\"152\" ssid=\"11\">An entity extractor identifies proper nouns in the past newswire archives, along with descriptions.</S>\n    <S sid=\"153\" ssid=\"12\">Descriptions are then categorized using the WordNet hierarchy.</S>\n    <S sid=\"154\" ssid=\"13\">Finally, an FD or functional description (Elhadad 1993) for the description is generated so that it can be reused in fluent ways in the final summary FDs mix functional, semantic, syntactic, and lexical information in a recursive attribute-value format that serves as the basic data structure for all information within FUF /SURGE.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"155\" ssid=\"1\">SUMMONS produces a summary from sets of templates that contain the salient facts reported in the input articles and that are produced by the message understanding systems.</S>\n    <S sid=\"156\" ssid=\"2\">These systems extract specific pieces of information from a given news article.</S>\n    <S sid=\"157\" ssid=\"3\">An example of a template produced by MUC systems and used in our system is shown in Figures 2 and 3.</S>\n    <S sid=\"158\" ssid=\"4\">To test our system, we used the templates produced by systems participating in MUC-4 (MUC 1992) as input.</S>\n    <S sid=\"159\" ssid=\"5\">MUC-4 systems operate on the terrorist domain and extract information by filling fields such as perpetrator, victim, and type of event, for a total number of 25 fields per template.</S>\n    <S sid=\"160\" ssid=\"6\">In addition, we filled the same template forms by hand from current news articles for further testing.</S>\n    <S sid=\"161\" ssid=\"7\">Currently, work is under way in our group on the building of an information extraction module similar to the ones used in the MUC conferences, which we will later use as an input to SUMMONS.</S>\n    <S sid=\"162\" ssid=\"8\">We are basing our implementation on the tools developed at the University of Massachusetts (Fisher et al. 1995).</S>\n    <S sid=\"163\" ssid=\"9\">The resulting system will not only be able to generate summaries from preparsed templates but will also produce summaries directly from raw text by merging the message understanding component with the current version of SUMMONS.</S>\n    <S sid=\"164\" ssid=\"10\">Our work provides a methodology for developing summarization systems, identifies planning operators for combining information in a concise summary, and uses empirically collected phrases to mark summarized material.</S>\n    <S sid=\"165\" ssid=\"11\">We have collected a corpus of newswire summaries that we used as data for developing the planning operators and for gathering a large set of lexical constructions used in summarization.</S>\n    <S sid=\"166\" ssid=\"12\">This Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.</S>\n    <S sid=\"167\" ssid=\"13\">The next day, a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israel radio.</S>\n    <S sid=\"168\" ssid=\"14\">Reuters reported that at least 12 people were killed and 105 wounded.</S>\n    <S sid=\"169\" ssid=\"15\">Later the same day, Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act. corpus will eventually aid in a full system evaluation.</S>\n    <S sid=\"170\" ssid=\"16\">Since news articles often summarize previous reports of the same event, our corpus also includes short summaries of previous articles.</S>\n    <S sid=\"171\" ssid=\"17\">We used this corpus to develop both the content planner (i.e., the module that determines what information to include in the summary) and the linguistic component (i.e., the module that determines the words and surface syntactic form of the summary) of our system.</S>\n    <S sid=\"172\" ssid=\"18\">We used the corpus to identify planning operators that are used to combine information; this includes techniques for linking information together in a related way (e.g., identifying changes, similarities, trends) as well as making generalizations.</S>\n    <S sid=\"173\" ssid=\"19\">We also identified phrases that are used to mark summaries and used these to build the system lexicon.</S>\n    <S sid=\"174\" ssid=\"20\">An example summary produced by the system is shown in Figure 4.</S>\n    <S sid=\"175\" ssid=\"21\">This paragraph summarizes four articles about two separate terrorist acts that took place in Israel in March of 1996 using two different planning operators.</S>\n    <S sid=\"176\" ssid=\"22\">While the system we report on is fully implemented, our work is undergoing continuous development.</S>\n    <S sid=\"177\" ssid=\"23\">Currently, the system includes eight different planning operators, a testbed of 200 input templates grouped into sets on the same event, and can produce fully lexicalized summaries for approximately half of the cases (the rest of the templates were either not complete or the information extracted in them was irrelevant to the task).</S>\n    <S sid=\"178\" ssid=\"24\">We haven't performed an evaluation beyond the testbed.</S>\n    <S sid=\"179\" ssid=\"25\">Our work provides a methodology for increasing the vocabulary size and the robustness of the system using a collected corpus, and moreover, it shows how summarization can be used to evaluate the message understanding systems, identifying future research directions that would not be pursued under the current MUC evaluation cycle.2 Due to inherent difficulties in the summarization task, our work is a substantial first step and provides the framework for a number of different research directions.</S>\n    <S sid=\"180\" ssid=\"26\">The rest of this section describes the summarizer, specifying the planning operators used for summarization as well as a detailed discussion of the summarization algorithm showing how summaries of different length are generated.</S>\n    <S sid=\"181\" ssid=\"27\">We provide examples of the summarization markers we collected for the lexicon and show the demands that summarization creates for interpretation.</S>\n    <S sid=\"182\" ssid=\"28\">The summarization component of SUMMONS is based on the traditional language generation system architecture (McKeown 1985; McDonald and Pustejovsky 1986; Hovy 1988).</S>\n    <S sid=\"183\" ssid=\"29\">A typical language generator is divided into two main components, a 2 Participating systems in the DARPA message understanding program are evaluated on a regular basis.</S>\n    <S sid=\"184\" ssid=\"30\">Participants are given a set of training text to tune their systems over a period of time and their systems are tested on unseen text at follow-up conferences. content planner, which selects information from an underlying knowledge base to include in a text, and a linguistic component, which selects words to refer to concepts contained in the selected information and arranges those words, appropriately inflecting them, to form an English sentence.</S>\n    <S sid=\"185\" ssid=\"31\">The content planner produces a conceptual representation of text meaning (e.g., a frame, a logical form, or an internal representation of text) and typically does not include any linguistic information.</S>\n    <S sid=\"186\" ssid=\"32\">The linguistic component uses a lexicon and a grammar of English to realize the conceptual representation into a sentence.</S>\n    <S sid=\"187\" ssid=\"33\">The lexicon contains the vocabulary for the system and encodes constraints about when each word can be used.</S>\n    <S sid=\"188\" ssid=\"34\">As shown in Figure 1, the content planner used by SUMMONS determines what information from the input MUC templates should be included in the summary using a set of planning operators that are specific to summarization and, to some extent, to the terrorist domain.</S>\n    <S sid=\"189\" ssid=\"35\">Its linguistic component determines the phrases and surface syntactic form of the summary.</S>\n    <S sid=\"190\" ssid=\"36\">The linguistic component consists of a lexical chooser, which determines the high-level sentence structure of each sentence and the words that realize each semantic role, and the FUF /SURGE (Elhadad 1991; Elhadad 1993) sentence generator.</S>\n    <S sid=\"191\" ssid=\"37\">Input to SUMMONS is a set of templates, where each template represents the information extracted from one or more articles by a message understanding system.</S>\n    <S sid=\"192\" ssid=\"38\">However, we constructed by hand an additional set of templates that include also terrorist events that have taken place after the period of time covered in MUC-4, such as the World Trade Center bombing, the Hebron Mosque massacre and more recent incidents in Israel, as well as the disaster in Oklahoma City.</S>\n    <S sid=\"193\" ssid=\"39\">These incidents were not handled by the original message understanding systems.</S>\n    <S sid=\"194\" ssid=\"40\">We also created by hand a set of templates unrelated to real newswire articles, which we used for testing some techniques of our system.</S>\n    <S sid=\"195\" ssid=\"41\">We enriched the templates for all these cases by adding four slots: the primary source, the secondary source, and the times at which both sources made their reports.'</S>\n    <S sid=\"196\" ssid=\"42\">We found having the source of the report immensely useful for discovering and reporting contradictions and generalizations, because often different reports of an event are in conflict.</S>\n    <S sid=\"197\" ssid=\"43\">Also, source information can indicate the level of confidence of the report, particularly when reported information changes over time.</S>\n    <S sid=\"198\" ssid=\"44\">For example, if several secondary sources all report the same facts for a single event, citing multiple primary sources, it is more likely that this is the way the event really happened, while if there are many contradictions between reports, it is likely that the facts are not yet fully known.</S>\n    <S sid=\"199\" ssid=\"45\">Members of our research group are currently working on event tracking (Aho et al. 1997).</S>\n    <S sid=\"200\" ssid=\"46\">Their prototype uses pattern-matching techniques to track changes to on-line news sources and provide a live feed of articles that relate to a changing event.</S>\n    <S sid=\"201\" ssid=\"47\">SUMMONS's summarization component generates a base summary, which contains facts extracted from the input set of articles.</S>\n    <S sid=\"202\" ssid=\"48\">The base summary is later enhanced with additional facts from on-line structured databases with descriptions of individuals extracted from previous news to produce the extended summary.</S>\n    <S sid=\"203\" ssid=\"49\">The base summary is a paragraph consisting of one or more sentences, where the length of the summary is controlled by a variable input parameter.</S>\n    <S sid=\"204\" ssid=\"50\">In the absence of a specific user model, the base summary is produced.</S>\n    <S sid=\"205\" ssid=\"51\">Otherwise, the extended summary (base summary with added descriptions of entities) is generated instead.</S>\n    <S sid=\"206\" ssid=\"52\">Similarly, the default is that the summary contains references to contradictory and updated information.</S>\n    <S sid=\"207\" ssid=\"53\">However, if the user profile makes it explicit, only the latest and the most trusted (as per the user's preference of sources) facts are included.</S>\n    <S sid=\"208\" ssid=\"54\">SUMMONS rates information in terms of importance, where information that appears in only one article is given a lower rating and information that is synthesized from multiple articles is rated more highly.</S>\n    <S sid=\"209\" ssid=\"55\">Development of the text generation component of SUMMONS was made easier because of the language generation tools and framework available at Columbia University.</S>\n    <S sid=\"210\" ssid=\"56\">No changes in the FUF sentence generator were needed.</S>\n    <S sid=\"211\" ssid=\"57\">In addition, the lexical chooser and content planner were based on the design used in the PLANDoc automated documentation system described in Section 2.3.</S>\n    <S sid=\"212\" ssid=\"58\">In particular, we used FUF to implement the lexical chooser, representing the lexicon as a grammar as we have done in many previous systems (Elhadad 1993; Robin 1994; McKeown, Robin, and Tanenblatt 1993; Feiner and McKeown 1991).</S>\n    <S sid=\"213\" ssid=\"59\">The main effort in porting the approach to SUMMONS was in identifying the words and phrases needed for the domain.</S>\n    <S sid=\"214\" ssid=\"60\">The content planner features several stages.</S>\n    <S sid=\"215\" ssid=\"61\">It first groups news articles together, identifies commonalities between them, and notes how the discourse influences wording by setting realization flags, which denote such discourse features as &amp;quot;similarity&amp;quot; and &amp;quot;contradiction.&amp;quot; Realization flags (McKeown, Kukich, and Shaw 1994b) guide the choice of connectives in the generation stage.</S>\n    <S sid=\"216\" ssid=\"62\">Before lexical choice, SUMMONS maps the templates into FDs that are expected as input to FUF and uses a domain ontology (derived from the ontologies represented in the message understanding systems) to enrich the input.</S>\n    <S sid=\"217\" ssid=\"63\">For example, grenades and bombs are both explosives, while diplomats and civilians are both considered to be human targets.</S>\n    <S sid=\"218\" ssid=\"64\">In order to produce plausible and understandable summaries, we used available online corpora as models, including the Wall Street Journal and current newswire from Reuters and the Associated Press.</S>\n    <S sid=\"219\" ssid=\"65\">The corpus of summaries is 2.5 MB in size.</S>\n    <S sid=\"220\" ssid=\"66\">We have manually grouped 300 articles in threads related to single events or series of similar events.</S>\n    <S sid=\"221\" ssid=\"67\">From the corpora collected in this way, we extracted manually, and after careful investigation, several hundred language constructions that we found relevant to the types of summaries we want to produce.</S>\n    <S sid=\"222\" ssid=\"68\">In addition to the summary cue phrases collected from the corpus, we also tried to incorporate as many phrases as possible that have relevance to the message understanding conference domain.</S>\n    <S sid=\"223\" ssid=\"69\">Due to domain variety, such phrases were essentially scarce in the newswire corpora and we needed to collect them from other sources (e.g., modifying templates that we acquired from the summary corpora to provide a wider coverage).</S>\n    <S sid=\"224\" ssid=\"70\">Since one of the features of a briefing is conciseness, we have tried to assemble small paragraph summaries that, in essence, describe a single event and the change of perception of the event over time, or a series of related events with no more than a few sentences.</S>\n    <S sid=\"225\" ssid=\"71\">The main point of departure for SUMMONS from previous work is in the stage of identifying what information to include and how to group it together, as well as the use of a corpus to guide this and later processes.</S>\n    <S sid=\"226\" ssid=\"72\">In PLANDoc, successive items to summarize are very similar and the problem is to form a grouping that puts the most similar items together, allowing the use of conjunction and ellipsis to delete repetitive material.</S>\n    <S sid=\"227\" ssid=\"73\">For summarizing multiple news articles, the task is almost the opposite; we need to find the differences from one article to the next, identifying how the reported facts have changed.</S>\n    <S sid=\"228\" ssid=\"74\">Thus, the main problem was the identification of summarization strategies, which indicate how information is linked together to form a concise and cohesive summary.</S>\n    <S sid=\"229\" ssid=\"75\">As we have found in other work (Robin 1994), what information is included is often dependent on the language available to make concise additions.</S>\n    <S sid=\"230\" ssid=\"76\">Thus, using a corpus summary was critical to identifying the different summaries possible.</S>\n    <S sid=\"231\" ssid=\"77\">We have developed a set of heuristics derived from the corpora that decide what types of simple sentences constitute a summary, in what order they need to be listed, as well as the ways in which simple sentences are combined into more complex ones.</S>\n    <S sid=\"232\" ssid=\"78\">In addition, we have specified which summarization-specific phrases are to be included in different types of summaries.</S>\n    <S sid=\"233\" ssid=\"79\">The system identifies a preeminent set of templates from the input to the MUC system.</S>\n    <S sid=\"234\" ssid=\"80\">This set needs to contain a large number of similar fields.</S>\n    <S sid=\"235\" ssid=\"81\">If this holds, we can merge the set into a simpler structure, keeping the common features and marking the distinct features as Elhadad (1993) and McKeown, Kukich, and Shaw (1994b) suggest.</S>\n    <S sid=\"236\" ssid=\"82\">At each step, a summary operator is selected based on existing similarities between articles in the database.</S>\n    <S sid=\"237\" ssid=\"83\">This operator is then applied to the input templates, resulting in a new template that combines, or synthesizes, information from the old.</S>\n    <S sid=\"238\" ssid=\"84\">Each operator is independent of the others and several can be applied in succession to the input templates.</S>\n    <S sid=\"239\" ssid=\"85\">Each of the seven major operators is further subdivided to cover various modifications to its input.</S>\n    <S sid=\"240\" ssid=\"86\">Figure 5 shows part of the rules for the Contradiction operator.</S>\n    <S sid=\"241\" ssid=\"87\">Given two templates, if INCIDENT.LOCATION is the same, the time of first report is before time of second report, the report sources are different, and at least one other slot differs in value, apply the contradiction operator to combine the templates.</S>\n    <S sid=\"242\" ssid=\"88\">A summary operator encodes a means for linking information in two different templates.</S>\n    <S sid=\"243\" ssid=\"89\">Often it results in the synthesis of new information.</S>\n    <S sid=\"244\" ssid=\"90\">For example, a generalization may be formed from two independent facts.</S>\n    <S sid=\"245\" ssid=\"91\">Alternatively, since we are summarizing reports written over time, highlighting how knowledge of the event changed is important and, therefore, summaries sometimes must identify differences between reports.</S>\n    <S sid=\"246\" ssid=\"92\">A description of the operators we identified in our corpus follows, accompanied by an example of system output for each operator.</S>\n    <S sid=\"247\" ssid=\"93\">Each example primarily summarizes two or three input templates, as this is the result of applying a single operator once.</S>\n    <S sid=\"248\" ssid=\"94\">More complex summaries can be produced by applying multiple operators on the same input, as shown in the examples; see Figures 6 to 11 in Section 4.5.</S>\n    <S sid=\"249\" ssid=\"95\">4.3.1 Change of Perspective.</S>\n    <S sid=\"250\" ssid=\"96\">When an initial report gets a fact wrong or has incomplete information, the change is usually included in the summary.</S>\n    <S sid=\"251\" ssid=\"97\">In order for the &amp;quot;change of perspective&amp;quot; operator to apply, the SOURCE field must be the same, while the value of another field changes so that it is not compatible with the original value.</S>\n    <S sid=\"252\" ssid=\"98\">For example, if the number of victims changes, we know that the first report was wrong if the number goes down, while the source had incomplete information (or additional people died) if the number goes up.</S>\n    <S sid=\"253\" ssid=\"99\">The first two sentences from the following example were generated using the change of perspective operator.</S>\n    <S sid=\"254\" ssid=\"100\">The initial estimate of &amp;quot;at least 10 people&amp;quot; killed in the incident becomes &amp;quot;at least 12 people.&amp;quot; Similarly, the change in the number of wounded people is also reported.</S>\n    <S sid=\"255\" ssid=\"101\">March 4th, Reuters reported that a bomb in Tel Aviv killed at least 10 people and wounded 30.</S>\n    <S sid=\"256\" ssid=\"102\">Later the same day, Reuters reported that at least 12 people were killed and 105 wounded.</S>\n    <S sid=\"257\" ssid=\"103\">4.3.2 Contradiction.</S>\n    <S sid=\"258\" ssid=\"104\">When two sources report conflicting information about the same event, a contradiction arises.</S>\n    <S sid=\"259\" ssid=\"105\">In the absence of values indicating the reliability of the sources, a summary cannot report either of them as true, but can indicate that the facts are not clear.</S>\n    <S sid=\"260\" ssid=\"106\">The number of sources that contradict each other can indicate the level of confusion about the event.</S>\n    <S sid=\"261\" ssid=\"107\">Note that the current output of the message understanding systems does not include sources.</S>\n    <S sid=\"262\" ssid=\"108\">However, SUMMONS uses this feature to report disagreement between output by different systems.</S>\n    <S sid=\"263\" ssid=\"109\">A summary might indicate that one of the sources determined that 20 people were killed, while the other source determined that only 5 were indeed killed.</S>\n    <S sid=\"264\" ssid=\"110\">The difference between this example and the previous one on change of perspective is the source of the update.</S>\n    <S sid=\"265\" ssid=\"111\">If the same source announces a change, then we know that it is reporting a change in the facts.</S>\n    <S sid=\"266\" ssid=\"112\">Otherwise, an additional source presents information that is not necessarily more correct than the information presented by the earlier source and we can therefore conclude that we have a contradiction.</S>\n    <S sid=\"267\" ssid=\"113\">The afternoon of February 26, 1993, Reuters reported that a suspected bomb killed at least six people in the World Trade Center.</S>\n    <S sid=\"268\" ssid=\"114\">However, Associated Press announced that exactly five people were killed in the blast.</S>\n    <S sid=\"269\" ssid=\"115\">4.3.3 Addition.</S>\n    <S sid=\"270\" ssid=\"116\">When a subsequent report indicates that additional facts are known, this is reported in a summary.</S>\n    <S sid=\"271\" ssid=\"117\">Additional results of the event may occur after the initial report or additional information may become known.</S>\n    <S sid=\"272\" ssid=\"118\">The operator determines this by the way the value of a template slot changes.</S>\n    <S sid=\"273\" ssid=\"119\">Since the former template doesn't contain a value for the perpetrator slot and the latter contains information about claimed responsibility, we can apply the addition operator.</S>\n    <S sid=\"274\" ssid=\"120\">On Monday, a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israel radio.</S>\n    <S sid=\"275\" ssid=\"121\">Later the same day, Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act.</S>\n    <S sid=\"276\" ssid=\"122\">4.3.4 Refinement.</S>\n    <S sid=\"277\" ssid=\"123\">In subsequent reports a more general piece of information may be refined.</S>\n    <S sid=\"278\" ssid=\"124\">Thus, if an event is originally reported to have occurred in New York City, the location might later be specified as a particular borough of the city.</S>\n    <S sid=\"279\" ssid=\"125\">Similarly, if a terrorist group is identified as Palestinian, later the exact name of the terrorist group may be determined.</S>\n    <S sid=\"280\" ssid=\"126\">Since the update is assigned a higher value of &amp;quot;importance,&amp;quot; it will be favored over the original article in a shorter summary.</S>\n    <S sid=\"281\" ssid=\"127\">Unlike the previous example, there was a value for the perpetrator slot in the first template, while the second one further elaborates on it, identifying the perpetrator more specifically.</S>\n    <S sid=\"282\" ssid=\"128\">Example 4 On Monday, Reuters announced that a suicide bomber killed at least 10 people in Tel Aviv.</S>\n    <S sid=\"283\" ssid=\"129\">Later the same day, Reuters reported that the Islamic fundamentalist group Hamas claimed responsibility.</S>\n    <S sid=\"284\" ssid=\"130\">4.3.5 Agreement.</S>\n    <S sid=\"285\" ssid=\"131\">If two sources have the same values for a specific slot, this will heighten the reader's confidence in their veracity and thus, agreement between sources is usually reported.</S>\n    <S sid=\"286\" ssid=\"132\">Example 5 The morning of March 1st 1994, UPI reported that a man was kidnapped in the Bronx.</S>\n    <S sid=\"287\" ssid=\"133\">Later, this was confirmed by Reuters.</S>\n    <S sid=\"288\" ssid=\"134\">4.3.6 Superset/Generalization.</S>\n    <S sid=\"289\" ssid=\"135\">If the same event is reported from different sources and all of them have incomplete information, it is possible to combine information from them to produce a more complete summary.</S>\n    <S sid=\"290\" ssid=\"136\">This operator is also used to aggregate multiple events as shown in the example.</S>\n    <S sid=\"291\" ssid=\"137\">Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.</S>\n    <S sid=\"292\" ssid=\"138\">The next day, a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israel radio.</S>\n    <S sid=\"293\" ssid=\"139\">A total of at least 28 people were killed in the two terrorist acts in Israel over the last two days.</S>\n    <S sid=\"294\" ssid=\"140\">It should be noted that in this example, the third sentence will not be generated if there is a restriction on the length of the summary.</S>\n    <S sid=\"295\" ssid=\"141\">4.3.7 Trend.</S>\n    <S sid=\"296\" ssid=\"142\">There is a trend if two or more articles reflect similar patterns over time.</S>\n    <S sid=\"297\" ssid=\"143\">Thus, we might notice that three consecutive bombings occurred at the same location and summarize them into a single sentence.</S>\n    <S sid=\"298\" ssid=\"144\">This is the third terrorist act committed by Hamas in four weeks.</S>\n    <S sid=\"299\" ssid=\"145\">4.3.8 No Information.</S>\n    <S sid=\"300\" ssid=\"146\">Since we are interested in conveying information about the primary and secondary sources of a certain piece of news, and since these are generally trusted sources of information, we ought also to pay attention to the lack of information from a certain source when such is expected to be present.</S>\n    <S sid=\"301\" ssid=\"147\">For example, it might be the case that a certain news agency reports a terrorist act in a given country, but the authorities of that country don't give out any information.</S>\n    <S sid=\"302\" ssid=\"148\">Since there is an infinite number of sources that might not confirm a given fact (or the system will not have access to the appropriate templates), we have included this operator only as an illustration of a concept that further highlights the domain-specificity of the system.</S>\n    <S sid=\"303\" ssid=\"149\">Two bombs exploded in Baghdad, Iraqi dissidents reported Friday.</S>\n    <S sid=\"304\" ssid=\"150\">There was no confirmation of the incidents by the Iraqi National Congress.</S>\n    <S sid=\"305\" ssid=\"151\">The algorithm used in the system to sort, combine, and generalize the input templates is described in the following subsections.</S>\n    <S sid=\"306\" ssid=\"152\">4.4.1 Input.</S>\n    <S sid=\"307\" ssid=\"153\">At this stage, the system receives a set of templates from the message understanding conferences or a similar set of templates from a related domain.</S>\n    <S sid=\"308\" ssid=\"154\">All templates are described as lists of attribute! value pairs (as shown later in Figure 7).</S>\n    <S sid=\"309\" ssid=\"155\">These pairs (with the exception of the source information) are defined in the MUC-4 guidelines. message understanding system for the site submitting the template if it is not present in the input template.</S>\n    <S sid=\"310\" ssid=\"156\">Note that since the current message understanding systems do not extract the source, this is the most specific we can be for such cases.</S>\n    <S sid=\"311\" ssid=\"157\">We are experimenting with some techniques to automate the preprocessing stage.</S>\n    <S sid=\"312\" ssid=\"158\">Our preliminary impressions show that by restricting SUMMONS to templates in which at least five or six slots are filled, we can eliminate most of the irrelevant templates. tween templates, which will trigger certain operators.</S>\n    <S sid=\"313\" ssid=\"159\">Since slots are matched among templates in chronological order, there is only one sequence in which they can be applied.</S>\n    <S sid=\"314\" ssid=\"160\">Such patterns trigger reordering of the templates and modification of their individual importance values.</S>\n    <S sid=\"315\" ssid=\"161\">As an example, if two templates are combined with the refinement operator, the importance value of the combined template will be greater than the sum of the individual importance of the constituent templates.</S>\n    <S sid=\"316\" ssid=\"162\">At the same time, the values of these two templates are lowered (still keeping a higher value on the later one, which is assumed to be the more correct of the two).</S>\n    <S sid=\"317\" ssid=\"163\">All templates directly extracted from the MUC output are assigned an initial importance value of 100.</S>\n    <S sid=\"318\" ssid=\"164\">Currently, with each application of an operator, we lower the value of a contributing individual template by 20 points and give any newly produced template that combines information from already existing contributing templates a value greater than the sum of the values of the contributing templates after those values have been updated.</S>\n    <S sid=\"319\" ssid=\"165\">Furthermore, some operators reduce the importance values of existing templates even further (e.g., the refinement operator reduces the importance of chronologically earlier templates by additional increments of 20 points because they contain outdated information).</S>\n    <S sid=\"320\" ssid=\"166\">Thus, the final summary will contain only the combined template if there are restrictions on length.</S>\n    <S sid=\"321\" ssid=\"167\">Otherwise, text corresponding to the constituent templates will also be generated.</S>\n    <S sid=\"322\" ssid=\"168\">The value of the importance of the template corresponds also to the position in the summary paragraph, as more important templates will be generated first.</S>\n    <S sid=\"323\" ssid=\"169\">Each new template contains information indicating whether its constituent templates are obsolete and thus no longer needed.</S>\n    <S sid=\"324\" ssid=\"170\">Also, at this stage the coverage vector (a data structure that keeps track of which templates have already been combined and which ones are still to be considered in applying operators) is updated to point to the templates that are still active and can be further combined.</S>\n    <S sid=\"325\" ssid=\"171\">This way we make sure that all templates still have a chance of participating in the actual summary.</S>\n    <S sid=\"326\" ssid=\"172\">The resulting templates are combined into small paragraphs according to the event or series of events that they describe.</S>\n    <S sid=\"327\" ssid=\"173\">Each paragraph is then realized by the linguistic component.</S>\n    <S sid=\"328\" ssid=\"174\">Each set of templates produces a single paragraph.</S>\n    <S sid=\"329\" ssid=\"175\">4.4.4 Discourse Planning.</S>\n    <S sid=\"330\" ssid=\"176\">Given the relative importance of the templates included in the database after the heuristic combination stage, the content planner organizes the presentation of information within a paragraph.</S>\n    <S sid=\"331\" ssid=\"177\">It looks at consecutive templates in the database, marked as separate paragraphs from the previous stage, and assigns values to &amp;quot;realization switches&amp;quot; that control local choices such as tense and voice.</S>\n    <S sid=\"332\" ssid=\"178\">They also govern the presence or absence of certain constituents to avoid repetition of constituents and to satisfy anaphora constraints.</S>\n    <S sid=\"333\" ssid=\"179\">This subsection describes how the algorithm is applied to a set of four templates by tracing the computational process that transforms the raw source into a final natural language summary.</S>\n    <S sid=\"334\" ssid=\"180\">Excerpts from the four input news articles are shown in Figure 6.</S>\n    <S sid=\"335\" ssid=\"181\">The four news articles are transformed into four templates that correspond to four separate accounts of two related events and will be included in the set of templates from which the template combiner will work.</S>\n    <S sid=\"336\" ssid=\"182\">Only the relevant fields are shown.</S>\n    <S sid=\"337\" ssid=\"183\">Let's now consider the four templates in the order that they appear in the list of templates.</S>\n    <S sid=\"338\" ssid=\"184\">These templates are shown in Figures 7 to 10.</S>\n    <S sid=\"339\" ssid=\"185\">They are generated manually from the input newswire texts.</S>\n    <S sid=\"340\" ssid=\"186\">Information about the primary and secondary sources of information is added (PRIMSOURCE and SECSOURCE) .</S>\n    <S sid=\"341\" ssid=\"187\">The differences in the templates (which will trigger certain operators) are shown in bold face.</S>\n    <S sid=\"342\" ssid=\"188\">The summary generated by the system was shown earlier in Figure 4 and is repeated here in Figure 11.</S>\n    <S sid=\"343\" ssid=\"189\">The first two sentences are generated from template one.</S>\n    <S sid=\"344\" ssid=\"190\">The subsequent sentences are generated using different operators that are triggered according to changing values for certain attributes in the three remaining templates.</S>\n    <S sid=\"345\" ssid=\"191\">As previous templates didn't contain information about the perpetrator, SUMMONS applies the refinement operator to generate the fourth sentence.</S>\n    <S sid=\"346\" ssid=\"192\">Sentence three is generated using the change of perspective operator, as the number of victims reported in articles two and three is different.</S>\n    <S sid=\"347\" ssid=\"193\">The description for Hamas (&amp;quot;radical Muslim group&amp;quot;) was added by the extraction generator (see Section 5).</S>\n    <S sid=\"348\" ssid=\"194\">Typically, a description is included in the source text and should be extracted by the message understanding system.</S>\n    <S sid=\"349\" ssid=\"195\">In cases in which a description doesn't appear or is not extracted, SUMMONS generates a description from the database of extracted descriptions.</S>\n    <S sid=\"350\" ssid=\"196\">We are currently working on an algorithm that Template for article three. will select the best description based on such parameters as the user model (what information has already been presented to the user?</S>\n    <S sid=\"351\" ssid=\"197\">), the attitude towards the entity (is it favorable?</S>\n    <S sid=\"352\" ssid=\"198\">), or a historical model that describes the changes in the profile of a person over the period of time (what was the previous occupation of the person who is being described?).</S>\n    <S sid=\"353\" ssid=\"199\">Template for article four.</S>\n    <S sid=\"354\" ssid=\"200\">Reuters reported that 18 people were killed in a Jerusalem bombing Sunday.</S>\n    <S sid=\"355\" ssid=\"201\">The next day, a bomb in Tel Aviv killed at least 10 people and wounded 30 according to Israel radio.</S>\n    <S sid=\"356\" ssid=\"202\">Reuters reported that at least 12 people were killed and 105 wounded.</S>\n    <S sid=\"357\" ssid=\"203\">Later the same day, Reuters reported that the radical Muslim group Hamas had claimed responsibility for the act.</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"358\" ssid=\"1\">When a summary refers to an entity (person, place, or organization), it can make use of descriptions extracted by the MUC systems.</S>\n    <S sid=\"359\" ssid=\"2\">Problems arise when information needed for the summary is either missing from the input article(s) or not extracted by the information extraction system.</S>\n    <S sid=\"360\" ssid=\"3\">In such cases, the information may be readily available in other current news stories, in past news, or in on-line databases.</S>\n    <S sid=\"361\" ssid=\"4\">If the summarization system can find the needed information in other on-line sources, then it can produce an improved summary by merging information extracted from the input articles with information from the other sources (Radev and McKeown 1997).</S>\n    <S sid=\"362\" ssid=\"5\">In the news domain, a summary needs to refer to people, places, and organizations and provide descriptions that clearly identify the entity for the reader.</S>\n    <S sid=\"363\" ssid=\"6\">Such descriptions may not be present in the original text that is being summarized.</S>\n    <S sid=\"364\" ssid=\"7\">For example, the American pilot Scott O'Grady, downed in Bosnia in June of 1995, was unknown to the American public prior to the incident.</S>\n    <S sid=\"365\" ssid=\"8\">To a reader who tuned into news on this event days later, descriptions from the initial articles might be more useful.</S>\n    <S sid=\"366\" ssid=\"9\">A summarizer that has access to different descriptions will be able to select the description that best suits both the reader and the series of articles being summarized.</S>\n    <S sid=\"367\" ssid=\"10\">Similarly, in the example in Section 4, if the user hasn't been informed about what Hamas is and no description is available in the source template, older descriptions in the FD format can be retrieved and used.</S>\n    <S sid=\"368\" ssid=\"11\">In this section, we describe an enhancement to the base summarization system, called the profile manager, which tracks prior references to a given entity by extracting descriptions for later use in summarization.</S>\n    <S sid=\"369\" ssid=\"12\">The component includes the entity extractor and description extractor modules shown in Figure 1 and has the following features: As a result, SUMMONS will be able to combine descriptions from articles appearing only a few minutes before the ones being summarized with descriptions from past news in a permanent storage for future use.</S>\n    <S sid=\"370\" ssid=\"13\">Since the profile manager constructs a lexicalized, syntactic FD from the extracted description, the generator can reuse the description in new contexts, merging it with other descriptions, into a new grammatical sentence.</S>\n    <S sid=\"371\" ssid=\"14\">This would not be possible if only canned strings were used, with no information about their internal structure.</S>\n    <S sid=\"372\" ssid=\"15\">Thus, in addition to collecting a knowledge source that provides identifying features of individuals, the profile manager also provides a lexicon of domain-appropriate phrases that can be integrated with individual words from a generator's lexicon to produce summary wording in a flexible fashion.</S>\n    <S sid=\"373\" ssid=\"16\">We have extended the profile manager by semantically categorizing descriptions using WordNet, so that a generator can more easily determine which description is relevant in different contexts.</S>\n    <S sid=\"374\" ssid=\"17\">The profile manager can also be used in a real-time fashion to monitor entities and the changes of descriptions associated with them over the course of time.</S>\n    <S sid=\"375\" ssid=\"18\">The rest of this section discusses the stages involved in the collection and reuse of descriptions.</S>\n    <S sid=\"376\" ssid=\"19\">In this subsection, we describe the description management module of SUMMONS shown in Figure 1.</S>\n    <S sid=\"377\" ssid=\"20\">We explain how entity names and descriptions for them are extracted from old newswire and how these descriptions are converted to FDs for surface generation. an initial set of descriptions, we used a 1.7 MB corpus containing Reuters newswire from February to June of 1995.</S>\n    <S sid=\"378\" ssid=\"21\">Later, we used a Web-based interface that allowed anyone on the Internet to type in an entity name and force a robot to search for documents containing mentions of the entity and extract the relevant descriptions.</S>\n    <S sid=\"379\" ssid=\"22\">These descriptions are then also added to the database.</S>\n    <S sid=\"380\" ssid=\"23\">At this stage, search is limited to the database of retrieved descriptions only, thus reducing search time, as no connections will be made to external news sources at the time of the query.</S>\n    <S sid=\"381\" ssid=\"24\">Only when a suitable stored description cannot be found will the system initiate search of additional text. dictionary.</S>\n    <S sid=\"382\" ssid=\"25\">This resulted in a list of 421 unique entity names that we used for the automatic description extraction stage.</S>\n    <S sid=\"383\" ssid=\"26\">All 421 entity names retrieved by the system are indeed proper nouns.</S>\n    <S sid=\"384\" ssid=\"27\">5.1.2 Extraction of Descriptions.</S>\n    <S sid=\"385\" ssid=\"28\">There are two occasions on which we extract descriptions using finite-state techniques.</S>\n    <S sid=\"386\" ssid=\"29\">The first case is when the entity that we want to describe was already extracted automatically (see Section 5.1.1) and exists in the database of descriptions.</S>\n    <S sid=\"387\" ssid=\"30\">The second case is when we want a description to be retrieved in real time based on a request from the generation component.</S>\n    <S sid=\"388\" ssid=\"31\">In the first stage, the profile manager generates finite-state representations of the entities that need to be described.</S>\n    <S sid=\"389\" ssid=\"32\">These full expressions are used as input to the description extraction module, which uses them to find candidate sentences in the corpus for extracting descriptions.</S>\n    <S sid=\"390\" ssid=\"33\">Since the need for a description may arise at a later time than when the entity was found and may require searching new text, the description finder must first locate these expressions in the text.</S>\n    <S sid=\"391\" ssid=\"34\">These representations are fed to CREP, which extracts noun phrases on either side of the entity (either pre-modifiers or appositions) from the news corpus.</S>\n    <S sid=\"392\" ssid=\"35\">The finite-state grammar for noun phrases that we use represents a variety of different syntactic structures for both pre-modifiers and appositions.</S>\n    <S sid=\"393\" ssid=\"36\">Thus, they may range from a simple noun (e.g., &amp;quot;president Bill Clinton&amp;quot;) to a much longer expression (e.g., &amp;quot;Gilberto Rodriguez Orejuela, the head of the Cali cocaine cartel&amp;quot;).</S>\n    <S sid=\"394\" ssid=\"37\">Other forms of descriptions, such as relative clauses, are the focus of ongoing implementation.</S>\n    <S sid=\"395\" ssid=\"38\">Table 2 shows some of the different patterns retrieved.</S>\n    <S sid=\"396\" ssid=\"39\">For example, when the profile manager has retrieved the description the political arm of the Irish Republican Army for Sinn Fein, it looks at the head noun in the description NP (arm), which we manually added to the list of trigger words to be categorized as an organization (see next subsection).</S>\n    <S sid=\"397\" ssid=\"40\">It is important to notice that even though WordNet typically presents problems with disambiguation of words retrieved from arbitrary text, we don't have any trouble disambiguating arm in this case due to the constraints on the context in which it appears (as an apposition describing an entity).</S>\n    <S sid=\"398\" ssid=\"41\">5.1.3 Categorization of Descriptions.</S>\n    <S sid=\"399\" ssid=\"42\">We use WordNet to group extracted descriptions into categories.</S>\n    <S sid=\"400\" ssid=\"43\">For the head noun of the description NP, we try to find a WordNet hypernym that can restrict the semantics of the description.</S>\n    <S sid=\"401\" ssid=\"44\">Currently, we identify concepts such as &amp;quot;profession,&amp;quot; &amp;quot;nationality,&amp;quot; and &amp;quot;organization.&amp;quot; Each of these concepts is triggered by one or more words (which we call trigger terms) in the description.</S>\n    <S sid=\"402\" ssid=\"45\">Table 2 shows some examples of descriptions and the concepts under which they are classified based on the WordNet hypernyms for some trigger words.</S>\n    <S sid=\"403\" ssid=\"46\">For example, all of the following triggers in the list (minister, head, administrator, and commissioner) can be traced up to leader in the WordNet hierarchy.</S>\n    <S sid=\"404\" ssid=\"47\">We have currently a list of 75 such trigger words that we have compiled manually. we create a new profile in a database of profiles.</S>\n    <S sid=\"405\" ssid=\"48\">We keep information about the surface string that is used to describe the entity in newswire (e.g., Addis Ababa), the source of the description and the date that the entry has been made in the database (e.g., &amp;quot;reuters95_06_25&amp;quot;).</S>\n    <S sid=\"406\" ssid=\"49\">In addition to these pieces of metainformation, all retrieved descriptions and their frequencies are also stored.</S>\n    <S sid=\"407\" ssid=\"50\">Currently, our system doesn't have the capability of matching references to the same entity that use different wordings.</S>\n    <S sid=\"408\" ssid=\"51\">As a result, we keep separate profiles for each of the following: Robert Dole, Dole, and Bob Dole.</S>\n    <S sid=\"409\" ssid=\"52\">We use each of these strings as the key in the database of descriptions.</S>\n    <S sid=\"410\" ssid=\"53\">Figure 12 shows the profile associated with the key John Major.</S>\n    <S sid=\"411\" ssid=\"54\">It can be seen that four different descriptions have been used in the parsed corpus to describe John Major.</S>\n    <S sid=\"412\" ssid=\"55\">Two of the four are common and are used in SUMMONS, whereas the other two result from incorrect processing by POS and/ or CREP.</S>\n    <S sid=\"413\" ssid=\"56\">The database of profiles is updated every time a query retrieves new descriptions matching a certain key.</S>\n    <S sid=\"414\" ssid=\"57\">When presenting an entity to the user, the content planner of a language generation system may decide to include some background information about it if the user has Generated FD for Silvio Berlusconi. not previously seen the entity.</S>\n    <S sid=\"415\" ssid=\"58\">When the extracted information doesn't contain an appropriate description, the system can use some descriptions retrieved by the profile manager. the extracted descriptions in the generation of summaries, we have developed a module that converts finite-state descriptions retrieved by the description extractor into functional descriptions that we can use directly in generation.</S>\n    <S sid=\"416\" ssid=\"59\">A description retrieved by the system is shown in Figure 13.</S>\n    <S sid=\"417\" ssid=\"60\">The corresponding FD is shown in Figure 14. semantics, the profile manager can prefer to generate one over another based on semantic features.</S>\n    <S sid=\"418\" ssid=\"61\">This is useful if a summary discusses events related to one description associated with the entity more than the others.</S>\n    <S sid=\"419\" ssid=\"62\">For example, when an article concerns Bill Clinton on the campaign trail, then the description democratic presidential candidate is more appropriate.</S>\n    <S sid=\"420\" ssid=\"63\">On the other hand, when an article concerns an international summit of world leaders, then the description U.S. President is more appropriate.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"421\" ssid=\"1\">Currently, our system can produce simple summaries consisting of one- to three- sentence paragraphs, which are limited to the MUC domain and to a few additional events for which we have manually created MUC-like templates.</S>\n    <S sid=\"422\" ssid=\"2\">We have also implemented the modules to connect to the World Factbook.</S>\n    <S sid=\"423\" ssid=\"3\">We have converted all ontologies related to the MUC and the Factbook into FDs.</S>\n    <S sid=\"424\" ssid=\"4\">The user model, which would allow users to specify preferred sources of information, frequency of briefings, etc., hasn't been fully implemented yet.</S>\n    <S sid=\"425\" ssid=\"5\">A problem that we haven't addressed is related to the clustering of articles according to their relevance to a specific event.</S>\n    <S sid=\"426\" ssid=\"6\">This is an area that requires further research.</S>\n    <S sid=\"427\" ssid=\"7\">Another such area is the development of algorithms for grouping together articles that belong to the same topic.</S>\n    <S sid=\"428\" ssid=\"8\">Finally, one of our main topics for future work is the development of techniques that can generate summary updates.</S>\n    <S sid=\"429\" ssid=\"9\">To do this, we must make use of a discourse model that represents the content and wording of summaries that have already been presented to the user.</S>\n    <S sid=\"430\" ssid=\"10\">When generating an update, the summarizer must avoid repeating content and, at the same time, must be able to generate references to entities and events that were previously described.</S>\n    <S sid=\"431\" ssid=\"11\">At the current stage, the description generator has the following coverage: person's profile with the profile of the organization of which he is a member.</S>\n    <S sid=\"432\" ssid=\"12\">We should note that extensive research in this field exists and we plan to make use of one of the proposed methods (Wacholder, Ravin, and Choi 1997) to solve this problem.</S>\n    <S sid=\"433\" ssid=\"13\">An important issue is portability of SUMMONS to other domains.</S>\n    <S sid=\"434\" ssid=\"14\">There are no a priori restrictions in our approach that would limit SUMMONS to template-based inputs (and hence, shallow knowledge representation schemes without recursion).</S>\n    <S sid=\"435\" ssid=\"15\">It would be interesting to determine the actual number of different representation schemes for news in general.</S>\n    <S sid=\"436\" ssid=\"16\">Since there exist systems that can learn extraction rules for unrestricted domains (Lehnert et al. 1993), the information extraction doesn't seem to present any fundamental bottleneck either.</S>\n    <S sid=\"437\" ssid=\"17\">Rather the questions are: how many man-hours are required to convert to each new domain? and how many of the rules from one domain are applicable to each new domain?</S>\n    <S sid=\"438\" ssid=\"18\">There are no clear answers to these questions.</S>\n    <S sid=\"439\" ssid=\"19\">The library of planning operators used in SUMMONS is extensible and can be ported to other domains, although it is likely that new operators will be needed.</S>\n    <S sid=\"440\" ssid=\"20\">In addition, new vocabulary will also be needed.</S>\n    <S sid=\"441\" ssid=\"21\">The authors plan to perform a portability analysis and report on it in the future.</S>\n    <S sid=\"442\" ssid=\"22\">Given that no alternative approaches to conceptual summarization of multiple articles exist, we have found it very hard to perform an adequate evaluation of the summaries generated by SUMMONS.</S>\n    <S sid=\"443\" ssid=\"23\">We consider several potential evaluations: qualitative (user satisfaction and readability) and task-based.</S>\n    <S sid=\"444\" ssid=\"24\">In a task-based evaluation, one set of judges would have access to the full set of articles, while another set of evaluators would have the summaries generated by SUMMONS.</S>\n    <S sid=\"445\" ssid=\"25\">The task would involve decision making (e.g., deciding whether the same organization has been involved in multiple incidents).</S>\n    <S sid=\"446\" ssid=\"26\">The time for decision making will be plotted against the accuracy of the answers provided by the judges from the two sets.</S>\n    <S sid=\"447\" ssid=\"27\">A third set of judges might have access to summaries generated by summarizers based on sentence extraction from multiple documents.</S>\n    <S sid=\"448\" ssid=\"28\">Similar evaluation techniques have been proposed for singledocument summarizers (Jing et al. 1998).</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"449\" ssid=\"1\">The prototype system that we have developed serves as the springboard for research in a variety of directions.</S>\n    <S sid=\"450\" ssid=\"2\">First and foremost is the need to use statistical techniques to increase the robustness and vocabulary of the system.</S>\n    <S sid=\"451\" ssid=\"3\">Since we were looking for phrasings that mark summarization in a full article that includes other material as well, for a first pass we found it necessary to do a manual analysis in order to determine which phrases were used for summarization.</S>\n    <S sid=\"452\" ssid=\"4\">In other words, we knew of no automatic way of identifying summary phrases.</S>\n    <S sid=\"453\" ssid=\"5\">However, having an initial seed set of summary phrases might allow us to automate a second pass analysis of the corpus by looking for variant patterns of the ones we have found.</S>\n    <S sid=\"454\" ssid=\"6\">By using automated, statistical techniques to find additional phrases, we could increase the size of the lexicon and use the additional phrases to identify new summarization strategies to add to our stock of operators.</S>\n    <S sid=\"455\" ssid=\"7\">Our summary generator could be used both for evaluating message understanding systems by using the summaries to highlight differences between systems and for identifying weaknesses in the current systems.</S>\n    <S sid=\"456\" ssid=\"8\">We have already noted a number of drawbacks with the current output, which makes summarization more difficult, giving the generator less information to work with.</S>\n    <S sid=\"457\" ssid=\"9\">For example, it is only sometimes indicated in the output that a reference to a person, place, or event is identical to an earlier reference; there is no connection across articles; the source of the report is not included.</S>\n    <S sid=\"458\" ssid=\"10\">Finally, the structure of the template representation is somewhat shallow, being closer to a database record than a knowledge representation.</S>\n    <S sid=\"459\" ssid=\"11\">This means that the generator's knowledge of different features of the event and relations between them is somewhat shallow.</S>\n    <S sid=\"460\" ssid=\"12\">One of the more important current goals is to increase coverage of the system by providing interfaces to a large number of on-line sources of news.</S>\n    <S sid=\"461\" ssid=\"13\">We would ideally want to build a comprehensive and shareable database of profiles that can be queried over the World-Wide Web.</S>\n    <S sid=\"462\" ssid=\"14\">The database will have a defined interface that will allow for systems such as SUMMONS to connect to it.</S>\n    <S sid=\"463\" ssid=\"15\">Another goal of our research is the generation of evolving summaries that continuously update the user on a given topic of interest.</S>\n    <S sid=\"464\" ssid=\"16\">In that case, the system will have a model containing all prior interaction with the user.</S>\n    <S sid=\"465\" ssid=\"17\">To avoid repetitiveness, such a system will have to resort to using different descriptions (as well as referring expressions) to address a specific entity.'</S>\n    <S sid=\"466\" ssid=\"18\">We will be investigating an algorithm that will select a proper ordering of multiple descriptions referring to the same person within the same discourse.</S>\n    <S sid=\"467\" ssid=\"19\">After we collect a series of descriptions for each possible entity, we need to decide how to select among them.</S>\n    <S sid=\"468\" ssid=\"20\">There are two scenarios.</S>\n    <S sid=\"469\" ssid=\"21\">In the first one, we have to pick one single description from the database that best fits the summary we are generating.</S>\n    <S sid=\"470\" ssid=\"22\">In the second scenario, the evolving summary, we have to generate a sequence of descriptions, which might possibly view the entity from different perspectives.</S>\n    <S sid=\"471\" ssid=\"23\">We are investigating algorithms that will decide the order of generation of the different descriptions.</S>\n    <S sid=\"472\" ssid=\"24\">Among the factors that will influence the selection and ordering of descriptions, we can note the user's interests, his knowledge of the entity, and the focus of the summary (e.g., democratic presidential candidate for Bill Clinton, versus U.S. president).</S>\n    <S sid=\"473\" ssid=\"25\">We can also select one description over another based on how recently they have been included in the database, whether or not one of them has been used in a summary already, whether the summary is an update to an earlier summary, and whether another description from the same category has been used already.</S>\n    <S sid=\"474\" ssid=\"26\">We have yet to decide under what circumstances a description needs to be generated at all.</S>\n    <S sid=\"475\" ssid=\"27\">We are interested in implementing existing algorithms or designing our own that will match different instances of the same entity appearing in different syntactic forms, e.g., to establish that PLO is an alias for the Palestine Liberation Organization.</S>\n    <S sid=\"476\" ssid=\"28\">We will investigate using co-occurrence information to match acronyms to full organization names as well as alternative spellings of the same name.</S>\n    <S sid=\"477\" ssid=\"29\">We will also look into connecting the current interface with news available on the Internet and with an existing search engine such as Lycos, AltaVista, or Yahoo.</S>\n    <S sid=\"478\" ssid=\"30\">We can then use the existing indices of all Web documents mentioning a given entity as a news corpus on which to perform the extraction of descriptions.</S>\n  </SECTION>\n  <SECTION title=\"8.\" number=\"9\">\n    <S sid=\"479\" ssid=\"1\">Our prototype system demonstrates the feasibility of generating briefings of a series of domain-specific news articles on the same event, highlighting changes over time as well as similarities and differences among sources and including some historical information about the participants.</S>\n    <S sid=\"480\" ssid=\"2\">The ability to automatically provide summaries of heterogeneous material will critically help in the effective use of the Internet in order to avoid overload with information.</S>\n    <S sid=\"481\" ssid=\"3\">We show how planning operators can be used to synthesize summary content from a set of templates, each representing a single article.</S>\n    <S sid=\"482\" ssid=\"4\">These planning operators are empirically based, coming from analysis of existing summaries, and allow for the generation of concise briefings.</S>\n    <S sid=\"483\" ssid=\"5\">Our framework allows for experimentation with summaries of different lengths and for the combination of multiple, independent summary operators to produce more complex summaries with added descriptions.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"10\">\n    <S sid=\"484\" ssid=\"1\">This work was partially supported by NSF grants GER-90-24069, IRI-96-19124, IRI-96-18797, and CDA-96-25374, as well as a grant from Columbia University's Strategic Initiative Fund sponsored by the Provost's Office.</S>\n    <S sid=\"485\" ssid=\"2\">The authors are grateful to the following people for their invaluable comments during the writing of the paper and at presentations of work related to the content of the paper: Alfred Aho, Shih-Fu Chang, Eleazar Eskin, Vasileios Hatzivassiloglou, Alejandro Jaimes, Hongyan Jing, Judith Klavans, Min-Yen Kan, Carl Sable, Eric Siegel, John Smith, Nina Wacholder, Kazi Zaman as well as the anonymous reviewers and the editors of the special issue on natural language generation.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "7625ac3d158d96f50dc25f686762676d",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Generating Natural Language Summaries From Multiple On-Line Sources"
                }
            }
        },
        {
            "index": 14,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a novel method for discovering parallel sentences in comparable, non-parallel corpora. We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other. Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora. We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system. We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus. Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.",
                    "filename": "Improving_Machine_Translation_Performance_By_Exploiting_Non-Parallel_Corpora",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J05-4003/Documents_xml/J05-4003.xml",
                    "title": "Improving Machine Translation Performance By Exploiting Non-Parallel Corpora"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces a novel method for identifying parallel sentences in non-parallel, comparable corpora using a maximum entropy classifier. The technique involves training the classifier to detect whether pairs of sentences are mutual translations, independent of context. The authors apply this method to extract parallel data from large Chinese, Arabic, and English newspaper corpora and demonstrate improved performance of statistical machine translation (SMT) systems with the extracted data. The method is beneficial for language pairs with limited parallel resources, and the paper provides evidence that even starting with a small parallel corpus and a large non-parallel corpus, one can build a quality SMT system. The approach is robust, efficient, and scalable, potentially enabling the automatic acquisition of training data for SMT across various domains.",
                        "This paper introduces a novel method for extracting parallel sentences from non-parallel, comparable corpora to improve machine translation (MT) performance. The authors propose a maximum entropy classifier to identify translations among sentences in large non-parallel corpora in Chinese, Arabic, and English. The extracted data enhances the performance of statistical MT systems, particularly for language pairs with limited parallel resources. The paper demonstrates that even a small initial parallel corpus, when combined with a large non-parallel corpus, can yield a high-quality MT system. This approach benefits language pairs with scarce resources by effectively utilizing available non-parallel texts, thus addressing the challenge of limited parallel corpora in MT training.",
                        "This paper introduces a novel method for identifying parallel sentences in non-parallel, comparable corpora, improving machine translation (MT) performance. The authors propose a maximum entropy classifier to discern whether sentence pairs across languages are translations. The method is tested on Chinese, Arabic, and English newspaper data, demonstrating that it can enhance the quality of a statistical MT system. Additionally, the paper shows that a high-quality MT system can be developed from a small parallel corpus by utilizing a large non-parallel corpus, particularly benefiting language pairs with limited resources. The approach is robust against the noise inherent in comparable corpora and is computationally efficient, allowing for the extraction of useful data that rivals human-translated text in MT applications.",
                        "This paper introduces a novel method for extracting parallel sentences from large non-parallel corpora in different languages, specifically targeting Chinese, Arabic, and English. The authors have developed a maximum entropy classifier to identify parallel sentences within comparable corpora. The extracted parallel data, when used to supplement existing parallel corpora, significantly enhances the performance of statistical machine translation systems, especially for language pairs with limited resources. This approach is particularly beneficial for domains with scarce parallel texts, as it allows the construction of a competent machine translation system starting from a minimal parallel corpus by leveraging a more abundant non-parallel corpus. The effectiveness of the extracted data is demonstrated through machine translation performance improvements, showcasing the potential of this method in addressing the data scarcity problem in machine translation."
                    ],
                    "filename": "improving_machine_translation_pe-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 123 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Improving Machine Translation Performance By Exploiting Non-Parallel Corpora</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">We present a novel method for discovering parallel sentences in comparable, non-parallel corpora.</S>\n    <S sid=\"8\" ssid=\"2\">We train a maximum entropy classifier that, given a pair of sentences, can reliably determine whether or not they are translations of each other.</S>\n    <S sid=\"9\" ssid=\"3\">Using this approach, we extract parallel data from large Chinese, Arabic, and English non-parallel newspaper corpora.</S>\n    <S sid=\"10\" ssid=\"4\">We evaluate the quality of the extracted data by showing that it improves the performance of a state-of-the-art statistical machine translation system.</S>\n    <S sid=\"11\" ssid=\"5\">We also show that a good-quality MT system can be built from scratch by starting with a very small parallel corpus (100,000 words) and exploiting a large non-parallel corpus.</S>\n    <S sid=\"12\" ssid=\"6\">Thus, our method can be applied with great benefit to language pairs for which only scarce resources are available.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"13\" ssid=\"1\">Parallel texts—texts that are translations of each other—are an important resource in many NLP applications.</S>\n    <S sid=\"14\" ssid=\"2\">They provide indispensable training data for statistical machine translation (Brown et al. 1990; Och and Ney 2002) and have been found useful in research on automatic lexical acquisition (Gale and Church 1991; Melamed 1997), crosslanguage information retrieval (Davis and Dunning 1995; Oard 1997), and annotation projection (Diab and Resnik 2002; Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentowski 2001).</S>\n    <S sid=\"15\" ssid=\"3\">Unfortunately, parallel texts are also scarce resources: limited in size, language coverage, and language register.</S>\n    <S sid=\"16\" ssid=\"4\">There are relatively few language pairs for which parallel corpora of reasonable sizes are available; and even for those pairs, the corpora come mostly from one domain, that of political discourse (proceedings of the Canadian or European Parliament, or of the United Nations).</S>\n    <S sid=\"17\" ssid=\"5\">This is especially problematic for the field of statistical machine translation (SMT), because translation systems trained on data from a particular domain (e.g., parliamentary proceedings) will perform poorly when translating texts from a different domain (e.g., news articles).</S>\n    <S sid=\"18\" ssid=\"6\">One way to alleviate this lack of parallel data is to exploit a much more available and diverse resource: comparable non-parallel corpora.</S>\n    <S sid=\"19\" ssid=\"7\">Comparable corpora are texts that, while not parallel in the strict sense, are somewhat related and convey overlapping information.</S>\n    <S sid=\"20\" ssid=\"8\">Good examples are the multilingual news feeds produced by news agencies such as Agence France Presse, Xinhua News, Reuters, CNN, BBC, etc.</S>\n    <S sid=\"21\" ssid=\"9\">Such texts are widely available on the Web for many language pairs and domains.</S>\n    <S sid=\"22\" ssid=\"10\">They often contain many sentence pairs that are fairly good translations of each other.</S>\n    <S sid=\"23\" ssid=\"11\">The ability to reliably identify these pairs would enable the automatic creation of large and diverse parallel corpora.</S>\n    <S sid=\"24\" ssid=\"12\">However, identifying good translations in comparable corpora is hard.</S>\n    <S sid=\"25\" ssid=\"13\">Even texts that convey the same information will exhibit great differences at the sentence level.</S>\n    <S sid=\"26\" ssid=\"14\">Consider the two newspaper articles in Figure 1.</S>\n    <S sid=\"27\" ssid=\"15\">They have been published by the English and French editors of Agence France Presse, and report on the same event, an epidemic of cholera in Pyongyang.</S>\n    <S sid=\"28\" ssid=\"16\">The lines in the figure connect sentence pairs that are approximate translations of each other.</S>\n    <S sid=\"29\" ssid=\"17\">Discovering these links automatically is clearly non-trivial.</S>\n    <S sid=\"30\" ssid=\"18\">Traditional sentence alignment algorithms (Gale and Church 1991; Wu 1994; Fung and Church 1994; Melamed 1999; Moore 2002) are designed to align sentences in parallel corpora and operate on the assumption that there are no reorderings and only limited insertions and deletions between the two renderings of a parallel document.</S>\n    <S sid=\"31\" ssid=\"19\">Thus, they perform poorly on comparable, non-parallel texts.</S>\n    <S sid=\"32\" ssid=\"20\">What we need are methods able to judge sentence pairs in isolation, independent of the (potentially misleading) context.</S>\n    <S sid=\"33\" ssid=\"21\">This article describes a method for identifying parallel sentences in comparable corpora and builds on our earlier work on parallel sentence extraction (Munteanu, Fraser, and Marcu 2004).</S>\n    <S sid=\"34\" ssid=\"22\">We describe how to build a maximum entropy-based classifier that can reliably judge whether two sentences are translations of each other, without making use of any context.</S>\n    <S sid=\"35\" ssid=\"23\">Using this classifier, we extract parallel sentences from very large comparable corpora of newspaper articles.</S>\n    <S sid=\"36\" ssid=\"24\">We demonstrate the quality of our A pair of comparable texts. extracted sentences by showing that adding them to the training data of an SMT system improves the system’s performance.</S>\n    <S sid=\"37\" ssid=\"25\">We also show that language pairs for which very little parallel data is available are likely to benefit the most from our method; by running our extraction system on a large comparable corpus in a bootstrapping manner, we can obtain performance improvements of more than 50% over a baseline MT system trained only on existing parallel data.</S>\n    <S sid=\"38\" ssid=\"26\">Our main experimental framework is designed to address the commonly encountered situation that exists when the MT training and test data come from different domains.</S>\n    <S sid=\"39\" ssid=\"27\">In such a situation, the test data is in-domain, and the training data is out-of-domain.</S>\n    <S sid=\"40\" ssid=\"28\">The problem is that in such conditions, translation performance is quite poor; the out-of-domain data doesn’t really help the system to produce good translations.</S>\n    <S sid=\"41\" ssid=\"29\">What is needed is additional in-domain training data.</S>\n    <S sid=\"42\" ssid=\"30\">Our goal is to get such data from a large in-domain comparable corpus and use it to improve the performance of an out-of-domain MT system.</S>\n    <S sid=\"43\" ssid=\"31\">We work in the context of Arabic-English and Chinese-English statistical machine translation systems.</S>\n    <S sid=\"44\" ssid=\"32\">Our out-of-domain data comes from translated United Nations proceedings, and our indomain data consists of news articles.</S>\n    <S sid=\"45\" ssid=\"33\">In this experimental framework we have access to a variety of resources, all of which are available from the Linguistic Data Consortium:1 In summary, we call in-domain the domain of the test data that we wish to translate; in this article, that in-domain data consists of news articles.</S>\n    <S sid=\"46\" ssid=\"34\">Out-of-domain data is data that belongs to any other domain; in this article, the out-of-domain data is drawn from United Nations (UN) parliamentary proceedings.</S>\n    <S sid=\"47\" ssid=\"35\">We are interested in the situation that exists when we need to translate news data but only have UN data available for training.</S>\n    <S sid=\"48\" ssid=\"36\">The solution we propose is to get comparable news data, automatically extract parallel sentences from it, and use these sentences as additional training data; we will show that doing this improves translation performance on a news test set.</S>\n    <S sid=\"49\" ssid=\"37\">The Arabic-English and Chinese-English resources described in the previous paragraph enable us to simulate our conditions of interest and perform detailed measurements of the impact of our proposed solution.</S>\n    <S sid=\"50\" ssid=\"38\">We can train baseline systems on UN parallel data (using the data from the first bullet in the previous paragraph), extract additional news data from the large comparable corpora (the fourth bullet), accurately measure translation performance on news data against four reference translations (the third bullet), and compare the impact of the automatically extracted news data with that of similar amounts of human-translated news data (the second bullet).</S>\n    <S sid=\"51\" ssid=\"39\">In the next section, we give a high-level overview of our parallel sentence extraction system.</S>\n    <S sid=\"52\" ssid=\"40\">In Section 3, we describe in detail the core of the system, the parallel sentence classifier.</S>\n    <S sid=\"53\" ssid=\"41\">In Section 4, we discuss several data extraction experiments.</S>\n    <S sid=\"54\" ssid=\"42\">In Section 5, we evaluate the extracted data by showing that adding it to out-of-domain parallel data improves the in-domain performance of an out-of-domain MT system, and in Section 6, we show that in certain cases, even larger improvements can be obtained by using bootstrapping.</S>\n    <S sid=\"55\" ssid=\"43\">In Section 7, we present examples of sentence pairs extracted by our method and discuss some of its weaknesses.</S>\n    <S sid=\"56\" ssid=\"44\">Before concluding, we discuss related work.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"57\" ssid=\"1\">The general architecture of our extraction system is presented in Figure 2.</S>\n    <S sid=\"58\" ssid=\"2\">Starting with two large monolingual corpora (a non-parallel corpus) divided into documents, we begin by selecting pairs of similar documents (Section 2.1).</S>\n    <S sid=\"59\" ssid=\"3\">From each such pair, we generate all possible sentence pairs and pass them through a simple word-overlapbased filter (Section 2.2), thus obtaining candidate sentence pairs.</S>\n    <S sid=\"60\" ssid=\"4\">The candidates are presented to a maximum entropy (ME) classifier (Section 2.3) that decides whether the sentences in each pair are mutual translations of each other.</S>\n    <S sid=\"61\" ssid=\"5\">The resources required by the system are minimal: a bilingual dictionary and a small amount of parallel data (used for training the ME classifier).</S>\n    <S sid=\"62\" ssid=\"6\">The dictionaries used in our experiments are learned automatically from (out-of-domain) parallel corpora;2 thus, the only resource used by our system consists of parallel sentences.</S>\n    <S sid=\"63\" ssid=\"7\">2 If such a resource is unavailable, other dictionaries can be used.</S>\n    <S sid=\"64\" ssid=\"8\">Our comparable corpus consists of two large, non-parallel, news corpora, one in English and the other in the foreign language of interest (in our case, Chinese or Arabic).</S>\n    <S sid=\"65\" ssid=\"9\">The parallel sentence extraction process begins by selecting, for each foreign article, English articles that are likely to contain sentences that are parallel to those in the foreign one.</S>\n    <S sid=\"66\" ssid=\"10\">This step of the process emphasizes recall rather than precision.</S>\n    <S sid=\"67\" ssid=\"11\">For each foreign document, we do not attempt to find the best-matching English document, but rather a set of similar English documents.</S>\n    <S sid=\"68\" ssid=\"12\">The subsequent components of the system are robust enough to filter out the extra noise introduced by the selection of additional (possibly bad) English documents.</S>\n    <S sid=\"69\" ssid=\"13\">We perform document selection using the Lemur IR toolkit3 (Ogilvie and Callan 2001).</S>\n    <S sid=\"70\" ssid=\"14\">We first index all the English documents into a database.</S>\n    <S sid=\"71\" ssid=\"15\">For each foreign document, we take the top five translations of each of its words (according to our probabilistic dictionary) and create an English language query.</S>\n    <S sid=\"72\" ssid=\"16\">The translation probabilities are only used to choose the word translations; they do not appear in the query.</S>\n    <S sid=\"73\" ssid=\"17\">We use the query to run TF-IDF retrieval against the database, take the top 20 English documents returned by Lemur, and pair each of them with the foreign query document.</S>\n    <S sid=\"74\" ssid=\"18\">This document matching procedure is both slow (it looks at all possible document pairs, so it is quadratic in the number of documents) and imprecise (due to noise in the dictionary, the query will contain many wrong words).</S>\n    <S sid=\"75\" ssid=\"19\">We attempt to fix these problems by using the following heuristic: we consider it likely that articles with similar content have publication dates that are close to each other.</S>\n    <S sid=\"76\" ssid=\"20\">Thus, each query is actually run only against English documents published within a window of five days around the publication date of the foreign query document; we retrieve the best 20 of these documents.</S>\n    <S sid=\"77\" ssid=\"21\">Each query is thus run against fewer documents, so it becomes faster and has a better chance of getting the right documents at the top.</S>\n    <S sid=\"78\" ssid=\"22\">Our experiments have shown that the final performance of the system does not depend too much on the size of the window (for example, doubling the size to 10 days made no difference).</S>\n    <S sid=\"79\" ssid=\"23\">However, having no window at all leads to a decrease in the overall performance of the system.</S>\n    <S sid=\"80\" ssid=\"24\">From each foreign document and set of associated English documents, we take all possible sentence pairs and pass them through a word-overlap filter.</S>\n    <S sid=\"81\" ssid=\"25\">The filter verifies that the ratio of the lengths of the two sentences is no greater than two.</S>\n    <S sid=\"82\" ssid=\"26\">It then checks that at least half the words in each sentence have a translation in the other sentence, according to the dictionary.</S>\n    <S sid=\"83\" ssid=\"27\">Pairs that do not fulfill these two conditions are discarded.</S>\n    <S sid=\"84\" ssid=\"28\">The others are passed on to the parallel sentence selection stage.</S>\n    <S sid=\"85\" ssid=\"29\">This step removes most of the noise (i.e., pairs of non-parallel sentences) introduced by our recall-oriented document selection procedure.</S>\n    <S sid=\"86\" ssid=\"30\">It also removes good pairs that fail to pass the filter because the dictionary does not contain the necessary entries; but those pairs could not have been handled reliably anyway, so the overall effect of the filter is to improve the precision and robustness of the system.</S>\n    <S sid=\"87\" ssid=\"31\">However, the filter also accepts many wrong pairs, because the word-overlap condition is weak; for instance, stopwords almost always have a translation on the other side, so if a few of the content For each candidate sentence pair, we need a reliable way of deciding whether the two sentences in the pair are mutual translations.</S>\n    <S sid=\"88\" ssid=\"32\">This is achieved by a Maximum Entropy (ME) classifier (described at length in Section 3), which is the core component of our system.</S>\n    <S sid=\"89\" ssid=\"33\">Those pairs that are classified as being translations of each other constitute the output of the system.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"90\" ssid=\"1\">In the Maximum Entropy (ME) statistical modeling framework, we impose constraints on the model of our data by defining a set of feature functions.</S>\n    <S sid=\"91\" ssid=\"2\">These feature functions emphasize properties of the data that we believe to be useful for the modeling task.</S>\n    <S sid=\"92\" ssid=\"3\">For example, for a sentence pair sp, the word overlap (the percentage of words in either sentence that have a translation in the other) might be a useful indicator of whether the sentences are parallel.</S>\n    <S sid=\"93\" ssid=\"4\">We therefore define a feature function f (sp), whose value is the word overlap of the sentences in sp.</S>\n    <S sid=\"94\" ssid=\"5\">According to the ME principle, the optimal parametric form of the model of our data, taking into account the constraints imposed by the feature functions, is a log linear combination of these functions.</S>\n    <S sid=\"95\" ssid=\"6\">Thus, for our classification problem, we have: where ci is the class (c0=”parallel”, c1=”not parallel”), Z(sp) is a normalization factor, and fij are the feature functions (indexed both by class and by feature).</S>\n    <S sid=\"96\" ssid=\"7\">The resulting model has free parameters λj, the feature weights.</S>\n    <S sid=\"97\" ssid=\"8\">The parameter values that maximize the likelihood of a given training corpus can be computed using various optimization algorithms (see [Malouf 2002] for a comparison of such algorithms).</S>\n    <S sid=\"98\" ssid=\"9\">For our particular classification problem, we need to find feature functions that distinguish between parallel and non-parallel sentence pairs.</S>\n    <S sid=\"99\" ssid=\"10\">For this purpose, we compute and exploit word-level alignments between the sentences in each pair.</S>\n    <S sid=\"100\" ssid=\"11\">A word alignment between two sentences in different languages specifies which words in one sentence are translations of which words in the other.</S>\n    <S sid=\"101\" ssid=\"12\">Word alignments were first introduced in the context of statistical MT, where they are used to estimate the parameters of a translation model (Brown et al. 1990).</S>\n    <S sid=\"102\" ssid=\"13\">Since then, they were found useful in many other NLP applications (e.g., word sense tagging [Diab and Resnik 2002] and question answering [Echihabi and Marcu 2003]).</S>\n    <S sid=\"103\" ssid=\"14\">Figures 3 and 4 give examples of word alignments between two English-Arabic sentence pairs from our comparable corpus.</S>\n    <S sid=\"104\" ssid=\"15\">Each figure contains two alignments.</S>\n    <S sid=\"105\" ssid=\"16\">The one on the left is a correct alignment, produced by a human, while the one on the right Alignments between two parallel sentences. was computed automatically.</S>\n    <S sid=\"106\" ssid=\"17\">As can be seen from the gloss next to the Arabic words, the sentences in Figure 3 are parallel while the sentences in Figure 4 are not.</S>\n    <S sid=\"107\" ssid=\"18\">In a correct alignment between two non-parallel sentences, most words would have no translation equivalents; in contrast, in an alignment between parallel sentences, most words would be aligned.</S>\n    <S sid=\"108\" ssid=\"19\">Automatically computed alignments, however, may have incorrect connections; for example, on the right side of Figure 3, the Arabic word issue is connected to the comma; and in Figure 4, the Arabic word at is connected to the English phrase its case to the.</S>\n    <S sid=\"109\" ssid=\"20\">Such errors are due to noisy dictionary entries and to Alignments between two non-parallel sentences. shortcomings of the model used to generate the alignments.</S>\n    <S sid=\"110\" ssid=\"21\">Thus, merely looking at the number of unconnected words, while helpful, is not discriminative enough.</S>\n    <S sid=\"111\" ssid=\"22\">Still, automatically produced alignments have certain additional characteristics that can be exploited.</S>\n    <S sid=\"112\" ssid=\"23\">We follow Brown et al. (1993) in defining the fertility of a word in an alignment as the number of words it is connected to.</S>\n    <S sid=\"113\" ssid=\"24\">The presence, in an automatically computed alignment between a pair of sentences, of words of high fertility (such as the Arabic word at in Figure 4) is indicative of non-parallelism.</S>\n    <S sid=\"114\" ssid=\"25\">Most likely, these connections were produced because of a lack of better alternatives.</S>\n    <S sid=\"115\" ssid=\"26\">Another aspect of interest is the presence of long contiguous connected spans, which we define as pairs of bilingual substrings in which the words in one substring are connected only to words in the other substring.</S>\n    <S sid=\"116\" ssid=\"27\">Such a span may contain a few words without any connection (a small percentage of the length of the span), but no word with a connection outside the span.</S>\n    <S sid=\"117\" ssid=\"28\">Examples of such spans can be seen in Figure 3: the English strings after saudi mediation failed or to the international court ofjustice together with their Arabic counterparts.</S>\n    <S sid=\"118\" ssid=\"29\">Long contiguous connected spans are indicative of parallelism, since they suggest that the two sentences have long phrases in common.</S>\n    <S sid=\"119\" ssid=\"30\">And, in contrast, long substrings whose words are all unconnected are indicative of non-parallelism.</S>\n    <S sid=\"120\" ssid=\"31\">To summarize, our classifier uses the following features, defined over two sentences and an automatically computed alignment between them.</S>\n    <S sid=\"121\" ssid=\"32\">General features (independent of the word alignment): In order to compute word alignments we need a simple and efficient model.</S>\n    <S sid=\"122\" ssid=\"33\">We want to align a large number of sentences, with many out-of-vocabulary words, in reasonable time.</S>\n    <S sid=\"123\" ssid=\"34\">We also want a model with as few parameters as possible—preferably only wordfor-word translation probabilities.</S>\n    <S sid=\"124\" ssid=\"35\">One such model is the IBM Model 1 (Brown et al. 1993).</S>\n    <S sid=\"125\" ssid=\"36\">According to this model, given foreign sentence (fj1&lt;=j&lt;=m), English sentence (ei1&lt;=i&lt;=l), and translation probabilities t(fj|ei), the best alignment f → e is obtained by linking each foreign word fj to its most likely English translation argmaxeit(fj|ei).</S>\n    <S sid=\"126\" ssid=\"37\">Thus, each foreign word is aligned to exactly one English word (or to a special NULL token).</S>\n    <S sid=\"127\" ssid=\"38\">Due to its simplicity, this model has several shortcomings, some more structural than others (see Moore [2004] for a discussion).</S>\n    <S sid=\"128\" ssid=\"39\">Thus, we use a version that is augmented with two simple heuristics that attempt to alleviate some of these shortcomings.</S>\n    <S sid=\"129\" ssid=\"40\">One possible improvement concerns English words that appear more than once in a sentence.</S>\n    <S sid=\"130\" ssid=\"41\">According to the model, a foreign word that prefers to be aligned with such an English word could be equally well aligned with any instance of that word.</S>\n    <S sid=\"131\" ssid=\"42\">In such situations, instead of arbitrarily choosing the first instance or a random instance, we attempt to make a ”smarter” decision.</S>\n    <S sid=\"132\" ssid=\"43\">First, we create links only for those English words that appear exactly once; next, for words that appear more than once, we choose which instance to link with so that we minimize the number of crossings with already existing links.</S>\n    <S sid=\"133\" ssid=\"44\">The second heuristic attempts to improve the choice of the most likely English translation of a foreign word.</S>\n    <S sid=\"134\" ssid=\"45\">Our translation probabilities are automatically learned from parallel data, and we learn values for both t(fj|ei) and t(ei|fj).</S>\n    <S sid=\"135\" ssid=\"46\">We can therefore decide that the most likely English translation of fj is argmaxei{t(fj|ei),t(ei|fj)}.</S>\n    <S sid=\"136\" ssid=\"47\">Using both sets of probabilities is likely to help us make a better-informed decision.</S>\n    <S sid=\"137\" ssid=\"48\">Using this alignment strategy, we follow (Och and Ney 2003) and compute one alignment for each translation direction (f - 4e and e -4 f), and then combine them.</S>\n    <S sid=\"138\" ssid=\"49\">Och and Ney present three combination methods: intersection, union, and refined (a form of intersection expanded with certain additional neighboring links).</S>\n    <S sid=\"139\" ssid=\"50\">Thus, for each sentence pair, we compute five alignments (two modified-IBMModel-1 plus three combinations) and then extract one set of general features and five sets of alignment features (as described in the previous section).</S>\n    <S sid=\"140\" ssid=\"51\">We create training instances for our classifier from a small parallel corpus.</S>\n    <S sid=\"141\" ssid=\"52\">The simplest way to obtain classifier training data from a parallel corpus is to generate all possible sentence pairs from the corpus (the Cartesian product).</S>\n    <S sid=\"142\" ssid=\"53\">This generates 5,0002 training instances, out of which 5,000 are positive (i.e., belong to class ”parallel”) and the rest are negative.</S>\n    <S sid=\"143\" ssid=\"54\">One drawback of this approach is that the resulting training set is very imbalanced, i.e., it has many more negative examples than positive ones.</S>\n    <S sid=\"144\" ssid=\"55\">Classifiers trained on such data do not achieve good performance; they generally tend to predict the majority class, i.e., classify most sentences as non-parallel (which has indeed been the case in our experiments).</S>\n    <S sid=\"145\" ssid=\"56\">Our solution to this is to downsample, i.e., eliminate a number of (randomly selected) negative instances.</S>\n    <S sid=\"146\" ssid=\"57\">Another problem is that the large majority of sentence pairs in the Cartesian product have low word overlap (i.e., few words that are translations of each other).</S>\n    <S sid=\"147\" ssid=\"58\">As explained in Section 2 (and shown in Figure 2), when extracting data from a comparable corpus, we only apply the classifier on the output of the word-overlap filter.</S>\n    <S sid=\"148\" ssid=\"59\">Thus, low-overlap sentence pairs, which would be discarded by the filter, are unlikely to be useful as training examples.</S>\n    <S sid=\"149\" ssid=\"60\">We therefore use for training only those pairs from the Cartesian product that are accepted by the word-overlap filter.</S>\n    <S sid=\"150\" ssid=\"61\">This has the additional advantage that, since all these pairs have many words in common, the classifier learns to make distinctions that cannot be made based on word overlap alone.</S>\n    <S sid=\"151\" ssid=\"62\">To summarize, we prepare our classifier training set in the following manner: starting from a parallel corpus of about 5,000 sentence pairs, we generate all the sentence pairs in the Cartesian product; we discard the pairs that do not fulfill the conditions of the word-overlap filter; if the resulting set is imbalanced, i.e., the ratio of non-parallel to parallel pairs is greater than five, we balance it by removing randomly chosen nonparallel pairs.</S>\n    <S sid=\"152\" ssid=\"63\">We then compute word alignments and extract feature values.</S>\n    <S sid=\"153\" ssid=\"64\">Using the training set, we compute values for the classifier feature weights using the YASMET4 implementation of the GIS algorithm (Darroch and Ratcliff 1974).</S>\n    <S sid=\"154\" ssid=\"65\">Since we are dealing with few parameters and have sufficiently many training instances, using more advanced training algorithms is unlikely to bring significant improvements.</S>\n    <S sid=\"155\" ssid=\"66\">We test the performance of the classifier by generating test instances from a different parallel corpus (also around 5,000 sentence pairs) and checking how many of these instances are correctly classified.</S>\n    <S sid=\"156\" ssid=\"67\">We prepare the test set by creating the Cartesian product of the sentences in the test parallel corpus and applying the word-overlap filter (we do not perform any balancing).</S>\n    <S sid=\"157\" ssid=\"68\">Although we apply the filter, we still conceptually classify all pairs from the Cartesian product in a two-stage classification process: all pairs discarded by the filter are classified as ”non-parallel,” and for the rest, we obtain predictions from the classifier.</S>\n    <S sid=\"158\" ssid=\"69\">Since this is how we apply the system on truly unseen data, this is the process in whose performance we are interested.</S>\n    <S sid=\"159\" ssid=\"70\">We measure the performance of the classification process by computing precision and recall.</S>\n    <S sid=\"160\" ssid=\"71\">Precision is the ratio of sentence pairs correctly judged as parallel to the total number of pairs judged as parallel by the classifier.</S>\n    <S sid=\"161\" ssid=\"72\">Recall is the ratio of sentence pairs correctly identified as parallel by the classifier to the total number of truly parallel pairs—i.e., the number of pairs in the parallel corpus used to generate the test instances.</S>\n    <S sid=\"162\" ssid=\"73\">Both numbers are expressed as percentages.</S>\n    <S sid=\"163\" ssid=\"74\">More formally: let classified parallel be the total number of sentence pairs from our test set that the classifier judged as parallel, classified well be the number of pairs that the classifier correctly judged as parallel, and true parallel be the total number of parallel pairs in the test set.</S>\n    <S sid=\"164\" ssid=\"75\">Then: classified parallel true parallel There are two factors that influence a classifier’s performance: dictionary coverage and similarity between the domains of the training and test instances.</S>\n    <S sid=\"165\" ssid=\"76\">We performed evaluation experiments to account for both these factors.</S>\n    <S sid=\"166\" ssid=\"77\">All our dictionaries are automatically learned from parallel data; thus, we can create dictionaries of various coverage by learning them from parallel corpora of different sizes.</S>\n    <S sid=\"167\" ssid=\"78\">We use five dictionaries, learned from five initial out-of-domain parallel corpora, whose sizes are 100k, 1M, 10M, 50M, and 95M tokens, as measured on the English side.</S>\n    <S sid=\"168\" ssid=\"79\">Since we want to use the classifier to extract sentence pairs from our in-domain comparable corpus, we test it on instances generated from an in-domain parallel corpus.</S>\n    <S sid=\"169\" ssid=\"80\">In order to measure the effect of the domain difference, we use two training sets: one generated from an in-domain parallel corpus and another one from an out-ofdomain parallel corpus.</S>\n    <S sid=\"170\" ssid=\"81\">In summary, for each language pair, we use the following corpora: Precision and recall of the Arabic-English classifiers.</S>\n    <S sid=\"171\" ssid=\"82\">From each initial, out-of-domain corpus, we learn a dictionary.</S>\n    <S sid=\"172\" ssid=\"83\">We then take the classifier training and test corpora and, using the method described in the previous section, create two sets of training instances and one set of test instances.</S>\n    <S sid=\"173\" ssid=\"84\">We train two classifiers (one on each training set) and evaluate both of them on the test set.</S>\n    <S sid=\"174\" ssid=\"85\">The parallel corpora used for generating training and test instances have around 5k sentence pairs each (approximately 150k English tokens), and generate around 10k training instances (for each training set) and 8k test instances.</S>\n    <S sid=\"175\" ssid=\"86\">Precision and recall of the Chinese-English classifiers.</S>\n    <S sid=\"176\" ssid=\"87\">Figures 5 and 6 show the recall and precision of our classifiers, for both ArabicEnglish and Chinese-English.</S>\n    <S sid=\"177\" ssid=\"88\">The results show that the precision of our classification process is robust with respect to dictionary coverage and training domain.</S>\n    <S sid=\"178\" ssid=\"89\">Even when starting from a very small initial parallel corpus, we can build a high-precision classifier.</S>\n    <S sid=\"179\" ssid=\"90\">Having a good dictionary and training data from the right domain does help though, mainly with respect to recall.</S>\n    <S sid=\"180\" ssid=\"91\">The classifiers achieve high precision because their positive training examples are clean parallel sentence pairs, with high word overlap (since the pairs with low overlap are filtered out); thus, the classification decision frontier is pushed towards “goodlooking” alignments.</S>\n    <S sid=\"181\" ssid=\"92\">The low recall results are partly due to the word-overlap filter (the first stage of the classification process), which discards many parallel pairs.</S>\n    <S sid=\"182\" ssid=\"93\">If we don’t apply the filter before the classifier, the recall results increase by about 20% (with no loss in precision).</S>\n    <S sid=\"183\" ssid=\"94\">However, the filter plays a very important role in keeping the extraction pipeline robust and efficient (as shown in Figure 7, the filter discards 99% of the candidate pairs), so this loss of recall is a price worth paying.</S>\n    <S sid=\"184\" ssid=\"95\">Classifier evaluations using different subsets of features show that most of the classifier performance comes from the general features together with the alignment features concerning the percentage and number of words that have no connection.</S>\n    <S sid=\"185\" ssid=\"96\">However, we expect that in real data, the differences between parallel and non-parallel pairs are less clear than in our test data (see the discussion in Section 7) and can no The amounts of data processed by our system during extraction from the Chinese-English comparable corpus. longer be accounted for only by counting the linked words; thus, the other features should become more important.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"186\" ssid=\"1\">The comparable corpora that we use for parallel sentence extraction are collections of news stories published by the Agence France Presse and Xinhua News agencies.</S>\n    <S sid=\"187\" ssid=\"2\">They are parts of the Arabic, English, and Chinese Gigaword corpora which are available from the Linguistic Data Consortium.</S>\n    <S sid=\"188\" ssid=\"3\">From these collections, for each language pair, we create an in-domain comparable corpus by putting together articles coming from the same agency and the same time period.</S>\n    <S sid=\"189\" ssid=\"4\">Table 1 presents in detail the sources and sizes of the resulting comparable corpora.</S>\n    <S sid=\"190\" ssid=\"5\">The remainder of the section presents the various data sets that we extracted automatically from these corpora, under various experimental conditions.</S>\n    <S sid=\"191\" ssid=\"6\">In the experiments described in Section 3.4, we started out with five out-of-domain initial parallel corpora of various sizes and obtained five dictionaries and five out-ofdomain trained classifiers (per language pair).</S>\n    <S sid=\"192\" ssid=\"7\">We now plug in each of these classifiers (and their associated dictionaries) in our extraction system (Section 2) and apply it to our comparable corpora.</S>\n    <S sid=\"193\" ssid=\"8\">We thus obtain five Arabic-English and five Chinese-English extracted corpora.</S>\n    <S sid=\"194\" ssid=\"9\">Note that in each of these experiments the only resource used by our system is the initial, out-of-domain parallel corpus.</S>\n    <S sid=\"195\" ssid=\"10\">Thus, the experiments fit in the framework of interest described in Section 1, which assumes the availability of (limited amounts of) out-of-domain training data and (large amounts of) in-domain comparable data.</S>\n    <S sid=\"196\" ssid=\"11\">Table 2 shows the sizes of the extracted corpora for each initial corpus size, for both Chinese-English and Arabic-English.</S>\n    <S sid=\"197\" ssid=\"12\">As can be seen, when the initial parallel corpus is very small, the amount of extracted data is also quite small.</S>\n    <S sid=\"198\" ssid=\"13\">This is due to the low coverage of the dictionary learned from that corpus.</S>\n    <S sid=\"199\" ssid=\"14\">Our candidate pair selection step (Section 2.2) discards pairs with too many unknown (or unrelated) words, according to the dictionary; thus, only few sentences fulfill the word-overlap condition of our filter.</S>\n    <S sid=\"200\" ssid=\"15\">As mentioned in Section 1, our goal is to use the extracted data as additional MT training data and obtain better translation performance on a given in-domain MT test set.</S>\n    <S sid=\"201\" ssid=\"16\">A simple way of estimating the usefulness of the data for this purpose is to measure its coverage of the test set, i.e., the percentage of running n-grams from the test corpus that are also in our corpus.</S>\n    <S sid=\"202\" ssid=\"17\">Tables 3 and 4 present the coverage of our extracted corpora.</S>\n    <S sid=\"203\" ssid=\"18\">For each initial corpus size, the first column shows the coverage of that initial corpus, and the second column shows the coverage of the initial corpus plus the extracted corpus.</S>\n    <S sid=\"204\" ssid=\"19\">Each cell contains four numbers that represent the coverage with respect to unigrams, bigrams, trigrams, and 4-grams.</S>\n    <S sid=\"205\" ssid=\"20\">The numbers show that unigram coverage depends only on the size of the corpus (and not on the domain), but for longer n-grams, our in-domain extracted data brings significant improvements in coverage.</S>\n    <S sid=\"206\" ssid=\"21\">The extraction experiments from the previous section are controlled experiments in which we only use limited amounts of parallel data for our extraction system.</S>\n    <S sid=\"207\" ssid=\"22\">In this section, we describe experiments in which the goal is to assess the applicability of our method to data that we mined from the Web.</S>\n    <S sid=\"208\" ssid=\"23\">We obtained comparable corpora from the Web by going to bilingual news websites (such as Al-Jazeera) and downloading news articles in each language independently.</S>\n    <S sid=\"209\" ssid=\"24\">In order to get as many articles as possible, we used the web site’s search engine to get lists of articles and their URLs, and then crawled those lists.</S>\n    <S sid=\"210\" ssid=\"25\">We used the AgentBuilder tool (Ticrea and Minton 2003; Minton, Ticrea, and Beach 2003) for crawling.</S>\n    <S sid=\"211\" ssid=\"26\">The tool can be programmed to automatically initiate searches with different parameters and to identify and extract the desired article URLs (as well as other information such as dates and titles) from the result pages.</S>\n    <S sid=\"212\" ssid=\"27\">Table 5 shows the sources, time periods, and size of the datasets that we downloaded.</S>\n    <S sid=\"213\" ssid=\"28\">For the extraction experiments, we used dictionaries of high coverage, learned from all our available parallel training data.</S>\n    <S sid=\"214\" ssid=\"29\">The sizes of these training corpora, measured in number of English tokens, are as follows: We applied our extraction method on both the LDC-released Gigaword corpora and the Web-downloaded comparable corpora.</S>\n    <S sid=\"215\" ssid=\"30\">For each language pair, we used the highest precision classifier from those presented in Section 3.4.</S>\n    <S sid=\"216\" ssid=\"31\">In order to obtain data of higher quality, we didn’t use all the sentences classified as parallel, but only those for which the probability computed by our classifier was higher than 0.70.</S>\n    <S sid=\"217\" ssid=\"32\">Table 6 shows the amounts of extracted data, measured in number of English tokens.</S>\n    <S sid=\"218\" ssid=\"33\">For ArabicEnglish, we were able to extract from the Gigaword corpora much more data than in our previous experiments (see Table 2), clearly due to the better dictionary.</S>\n    <S sid=\"219\" ssid=\"34\">For ChineseEnglish, there was no increase in the size of extracted data (although the amount from Table 6 is smaller than that from Table 2, it counts only sentence pairs extracted with confidence higher than 0.70).</S>\n    <S sid=\"220\" ssid=\"35\">In the previous section, we measured, for our training corpora, their coverage of the test set (Tables 3 and 4).</S>\n    <S sid=\"221\" ssid=\"36\">We repeated the measurements for the training data from Table 6 and obtained very similar results: using the additional extracted data improves coverage, especially for longer n-grams.</S>\n    <S sid=\"222\" ssid=\"37\">To give the reader an idea of the amount of data that is funneled through our system, we show in Figure 7 the sizes of the data processed by each of the system’s components during extraction from the Gigaword and Web-based Chinese-English comparable corpora.</S>\n    <S sid=\"223\" ssid=\"38\">We use a dictionary learned from a parallel corpus on 190M English tokens and a classifier trained on instances generated from a parallel corpus of 220k English tokens.</S>\n    <S sid=\"224\" ssid=\"39\">We start with a comparable corpus consisting of 500k Chinese articles and 600k English articles.</S>\n    <S sid=\"225\" ssid=\"40\">The article selection step (Section 2.1) outputs 7.5M similar article pairs; from each article pair we generate all possible sentence pairs and obtain 2,400M pairs.</S>\n    <S sid=\"226\" ssid=\"41\">Of these, less than 1% (17M) pass the candidate selection stage (Section 2.2) and are presented to the ME classifier.</S>\n    <S sid=\"227\" ssid=\"42\">The system outputs 430k sentence pairs (9.5M English tokens) that have been classified as parallel (with probability greater than 0.7).</S>\n    <S sid=\"228\" ssid=\"43\">The figure also presents, in the lower part, the parameters that control the filtering at each stage. the particular sentence pair to be parallel; the higher the value, the higher the classifier’s confidence.</S>\n    <S sid=\"229\" ssid=\"44\">Thus, in order to obtain higher precision, we can choose to define as parallel only those pairs for which the classifier probability is above a certain threshold.</S>\n    <S sid=\"230\" ssid=\"45\">In the experiments from Section 4.1, we use the (default) threshold of 0.5, while in Section 4.2 we use 0.7.</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"231\" ssid=\"1\">Our main goal is to extract, from an in-domain comparable corpus, parallel training data that improves the performance of an out-of-domain-trained SMT system.</S>\n    <S sid=\"232\" ssid=\"2\">Thus, we evaluate our extracted corpora by showing that adding them to the out-of-domain training data of a baseline MT system improves its performance.</S>\n    <S sid=\"233\" ssid=\"3\">We first evaluate the extracted corpora presented in Section 4.1.</S>\n    <S sid=\"234\" ssid=\"4\">The extraction system used to obtain each of those corpora made use of a certain initial out-of-domain parallel corpus.</S>\n    <S sid=\"235\" ssid=\"5\">We train a Baseline MT system on that initial corpus.</S>\n    <S sid=\"236\" ssid=\"6\">We then train another MT system (which we call PlusExtracted) on the initial corpus plus the extracted corpus.</S>\n    <S sid=\"237\" ssid=\"7\">In order to compare the quality of our extracted data with that of human-translated data from the same domain, we also train an UpperBound MT system, using the initial corpus plus a corpus of in-domain, human-translated data.</S>\n    <S sid=\"238\" ssid=\"8\">For each initial corpus, we use the same amount of human-translated data as there is extracted data (see Table 2).</S>\n    <S sid=\"239\" ssid=\"9\">Thus, for each language pair and each initial parallel corpus, we compare 3 MT systems: Baseline, PlusExtracted, and UpperBound.</S>\n    <S sid=\"240\" ssid=\"10\">All our MT systems were trained using a variant of the alignment template model described in (Och 2003).</S>\n    <S sid=\"241\" ssid=\"11\">Each system used two language models: a very large one, trained on 800 million English tokens, which is the same for all the systems; and a smaller one, trained only on the English side of the parallel training data for that particular system.</S>\n    <S sid=\"242\" ssid=\"12\">This ensured that any differences in performance are caused only by differences in the training data.</S>\n    <S sid=\"243\" ssid=\"13\">The systems were tested on the news test corpus used for the NIST 2003 MT evaluation.5 Translation performance was measured using the automatic BLEU evaluation metric (Papineni et al. 2002) on four reference translations.</S>\n    <S sid=\"244\" ssid=\"14\">Figures 8 and 9 show the BLEU scores obtained by our MT systems.</S>\n    <S sid=\"245\" ssid=\"15\">The 95% confidence intervals of the scores computed by bootstrap resampling (Koehn 2004) are marked on the graphs; the delta value is around 1.2 for Arabic-English and 1 for Chinese-English.</S>\n    <S sid=\"246\" ssid=\"16\">As the results show, the automatically extracted additional training data yields significant improvements in performance over most initial training corpora for both language pairs.</S>\n    <S sid=\"247\" ssid=\"17\">At least for Chinese-English, the improvements are quite comparable to those produced by the human-translated data.</S>\n    <S sid=\"248\" ssid=\"18\">And, as can be expected, the impact of the extracted data decreases as the size of the initial corpus increases.</S>\n    <S sid=\"249\" ssid=\"19\">In order to check that the classifier really does something important, we performed a few experiments without it.</S>\n    <S sid=\"250\" ssid=\"20\">After the article selection step, we simply paired each foreign document with the best-matching English one, assumed they are parallel, sentence-aligned them with a generic sentence alignment method, and added the resulting data to the training corpus.</S>\n    <S sid=\"251\" ssid=\"21\">The resulting BLEU scores were practically the same as the baseline; thus, our classifier does indeed help to discover higher-quality parallel data.</S>\n    <S sid=\"252\" ssid=\"22\">We also measured the MT performance impact of the extracted corpora described in Section 4.2.</S>\n    <S sid=\"253\" ssid=\"23\">We trained a Baseline MT system on all our available (in-domain and MT performance improvements for Arabic-English. out-of-domain) parallel data, and a PlusExtracted system on the parallel data plus the extracted in-domain data.</S>\n    <S sid=\"254\" ssid=\"24\">Clearly, we have access to no UpperBound system in this case.</S>\n    <S sid=\"255\" ssid=\"25\">The results are presented in the first two rows of Table 7.</S>\n    <S sid=\"256\" ssid=\"26\">Adding the extracted corpus lowers the score for the Arabic-English system and improves the score for the Chinese-English one; however, none of the differences are statistically significant.</S>\n    <S sid=\"257\" ssid=\"27\">Since the baseline systems are trained on such large amounts of data (see Section 4.2), it is not surprising that our extracted corpora have no significant impact.</S>\n    <S sid=\"258\" ssid=\"28\">In an attempt to give a better indication of the value of these corpora, we used them alone as MT training data.</S>\n    <S sid=\"259\" ssid=\"29\">The BLEU scores obtained by the systems we trained on them are presented in the third row of Table 7.</S>\n    <S sid=\"260\" ssid=\"30\">For comparison purposes, the last line of the table shows the scores of systems trained on 10M English tokens of outof-domain data.</S>\n    <S sid=\"261\" ssid=\"31\">As can be seen, our automatically extracted corpora obtain better MT performance than out-of-domain parallel corpora of similar size.</S>\n    <S sid=\"262\" ssid=\"32\">It’s true that this is not a fair comparison, since the extracted corpora were obtained using all our available parallel data.</S>\n    <S sid=\"263\" ssid=\"33\">The numbers do show, however, that the extracted data, although it was obtained automatically, is of good value for machine translation.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"264\" ssid=\"1\">As can be seen from Table 2, the amount of data we can extract from our comparable corpora is adversely affected by poor dictionary coverage.</S>\n    <S sid=\"265\" ssid=\"2\">Thus, if we start with very little parallel data, we do not make good use of the comparable corpora.</S>\n    <S sid=\"266\" ssid=\"3\">One simple way to alleviate this problem is to bootstrap: after we’ve extracted some in-domain data, we can use it to learn a new dictionary and go back and extract again.</S>\n    <S sid=\"267\" ssid=\"4\">Bootstrapping was also successfully applied to this problem by Fung and Cheung (2004).</S>\n    <S sid=\"268\" ssid=\"5\">We performed bootstrapping iterations starting from two very small corpora: 100k English tokens and 1M English tokens, respectively.</S>\n    <S sid=\"269\" ssid=\"6\">After each iteration, we trained MT performance improvements for Chinese-English.</S>\n    <S sid=\"270\" ssid=\"7\">(and evaluated) an MT system on the initial data plus the data extracted in that iteration.</S>\n    <S sid=\"271\" ssid=\"8\">We did not use any of the data extracted in previous iterations since it is mostly a subset of that extracted in the current iteration.</S>\n    <S sid=\"272\" ssid=\"9\">We iterated until there were no further improvements in MT performance on our development data.</S>\n    <S sid=\"273\" ssid=\"10\">Figures 10 and 11 show the sizes of the data extracted at each iteration, for both initial corpus sizes.</S>\n    <S sid=\"274\" ssid=\"11\">Iteration 0 is the one that uses the dictionary learned from the initial corpus.</S>\n    <S sid=\"275\" ssid=\"12\">Starting with 100k words of parallel data, we eventually collect 20M words of in-domain Arabic-English data and 90M words of in-domain Chinese-English data.</S>\n    <S sid=\"276\" ssid=\"13\">Figures 12 and 13 show the BLEU scores of these MT systems.</S>\n    <S sid=\"277\" ssid=\"14\">For comparison purposes, we also plotted on each graph the performance of our best MT system for that language pair, trained on all our available parallel data (Table 7).</S>\n    <S sid=\"278\" ssid=\"15\">As we can see, bootstrapping allows us to extract significantly larger amounts of data, which leads to significantly higher BLEU scores.</S>\n    <S sid=\"279\" ssid=\"16\">Starting with as little as 100k English tokens of parallel data, we obtain MT systems that come within 7–10 BLEU points of systems trained on parallel corpora of more than 100M English tokens.</S>\n    <S sid=\"280\" ssid=\"17\">This shows that using our method, a good-quality MT system can be built from very little parallel data and a large amount of comparable, non-parallel data.</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"281\" ssid=\"1\">We conclude the description of our method by presenting a few sentence pairs extracted by our system.</S>\n    <S sid=\"282\" ssid=\"2\">We chose the examples by looking for cases when a given foreign sentence was judged parallel to several different English sentences.</S>\n    <S sid=\"283\" ssid=\"3\">Figures 14 and 15 show the foreign sentence in Arabic and Chinese, respectively, followed by a human-produced translation in bold italic font, followed by the automatically extracted matching English sentences in normal font.</S>\n    <S sid=\"284\" ssid=\"4\">The sentences are picked from the data sets presented in Section 4.2.</S>\n    <S sid=\"285\" ssid=\"5\">The examples reveal the two main types of errors that our system makes.</S>\n    <S sid=\"286\" ssid=\"6\">The first type concerns cases when the system classifies as parallel sentence pairs that, although they share many content words, express slightly different meanings, as in Figure 15, example 7.</S>\n    <S sid=\"287\" ssid=\"7\">The second concerns pairs in which the two sentences convey different amounts of information.</S>\n    <S sid=\"288\" ssid=\"8\">In such pairs, one of the sentences contains a transSizes of the Chinese-English corpora extracted using bootstrapping, in millions of English tokens.</S>\n    <S sid=\"289\" ssid=\"9\">BLEU scores of the Arabic-English MT systems using bootstrapping. lation of the other, plus additional (often quite long) phrases (Figure 15, examples 1 and 5).</S>\n    <S sid=\"290\" ssid=\"10\">These errors are caused by the noise present in the automatically learned dictionaries and by the use of a weak word alignment model for extracting the classifier BLEU scores of the Chinese-English MT systems using bootstrapping. features.</S>\n    <S sid=\"291\" ssid=\"11\">In an automatically learned dictionary, many words (especially the frequent, non-content ones) will have a lot of spurious translations.</S>\n    <S sid=\"292\" ssid=\"12\">The IBM-1 alignment model takes no account of word order and allows a source word to be connected to arbitrarily many target words.</S>\n    <S sid=\"293\" ssid=\"13\">Alignments computed using this model and a noisy, automatically learned, dictionary will contain many incorrect links.</S>\n    <S sid=\"294\" ssid=\"14\">Thus, if two sentences share several content words, these incorrect links together with the correct links between the common content words will yield an alignment good enough to make the classifier judge the sentence pair as parallel.</S>\n    <S sid=\"295\" ssid=\"15\">The effect of the noise in the dictionary is even more clear for sentence pairs with few words, such as Figure 14, example 6.</S>\n    <S sid=\"296\" ssid=\"16\">The sentences in that example are tables of soccer team statistics.</S>\n    <S sid=\"297\" ssid=\"17\">They are judged parallel because corresponding digits align to each other, and according to our dictionary, the Arabic word for “Mexico” can be translated as any of the country names listed in the example.</S>\n    <S sid=\"298\" ssid=\"18\">These examples also show that the problem of finding only true translation pairs is hard.</S>\n    <S sid=\"299\" ssid=\"19\">Two sentences may share many content words and yet express different meanings (see Figure 14, example 1).</S>\n    <S sid=\"300\" ssid=\"20\">However, our task of getting useful MT training data does not require a perfect solution; as we have seen, even such noisy training pairs can help improve a translation system’s performance.</S>\n  </SECTION>\n  <SECTION title=\"8.\" number=\"9\">\n    <S sid=\"301\" ssid=\"1\">While there is a large body of work on bilingual comparable corpora, most of it is focused on learning word translations (Fung and Yee 1998; Rapp 1999; Diab and Finch 2000; Koehn and Knight 2000; Gaussier et al. 2004).</S>\n    <S sid=\"302\" ssid=\"2\">We are aware of only three previous efforts aimed at discovering parallel sentences.</S>\n    <S sid=\"303\" ssid=\"3\">Zhao and Vogel (2002) describe a generative model for discovering parallel sentences in the Xinhua News ChineseEnglish corpus.</S>\n    <S sid=\"304\" ssid=\"4\">Utiyama et. al (2003) use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus.</S>\n    <S sid=\"305\" ssid=\"5\">Fung and Cheung (2004) present an extraction method similar to ours but focus on “very-non-parallel corpora,” aggregations of Chinese and English news stories from different sources and time periods.</S>\n    <S sid=\"306\" ssid=\"6\">The first two systems extend algorithms designed to perform sentence alignment of parallel texts.</S>\n    <S sid=\"307\" ssid=\"7\">They start by attempting to identify similar article pairs from the two corpora.</S>\n    <S sid=\"308\" ssid=\"8\">Then they treat each of those pairs as parallel texts and align their sentences by defining a sentence pair similarity score and use dynamic programming to find the least-cost alignment over the whole document pair.</S>\n    <S sid=\"309\" ssid=\"9\">In the article pair selection stage, the researchers try to identify, for an article in one language, the best matching article in the other language.</S>\n    <S sid=\"310\" ssid=\"10\">Zhao and Vogel (2002) measure article similarity by defining a generative model in which an English story generates a Chinese story with a given probability.</S>\n    <S sid=\"311\" ssid=\"11\">Utiyama et al. (2003) use the BM25 (Robertson and Walker 1994) similarity measure.</S>\n    <S sid=\"312\" ssid=\"12\">The two works also differ in the way they define the sentence similarity score.</S>\n    <S sid=\"313\" ssid=\"13\">Zhao and Vogel (2002) combine a sentence length model with an IBM Model 1-type translation model.</S>\n    <S sid=\"314\" ssid=\"14\">Utiyama et al. (2003) define a score based on word overlap (i.e., number of word pairs from the two sentences that are translations of each other), which also includes the similarity score of the article pair from which the sentence pair originates.</S>\n    <S sid=\"315\" ssid=\"15\">The performance of these approaches depends heavily on the ability to reliably find similar document pairs.</S>\n    <S sid=\"316\" ssid=\"16\">Moreover, comparable article pairs, even those similar in content, may exhibit great differences at the sentence level (reorderings, additions, etc).</S>\n    <S sid=\"317\" ssid=\"17\">Therefore, they pose hard problems for the dynamic programming alignment approach.</S>\n    <S sid=\"318\" ssid=\"18\">In contrast, our method is more robust.</S>\n    <S sid=\"319\" ssid=\"19\">The document pair selection part plays a minor role; it only acts as a filter.</S>\n    <S sid=\"320\" ssid=\"20\">We do not attempt to find the best-matching English document for each foreign one, but rather a set of similar documents.</S>\n    <S sid=\"321\" ssid=\"21\">And, most importantly, we are able to reliably judge each sentence pair in isolation, without need for context.</S>\n    <S sid=\"322\" ssid=\"22\">On the other hand, the dynamic programming approach enables discovery of many-to-one sentence alignments, whereas our method is limited to finding one-toone alignments.</S>\n    <S sid=\"323\" ssid=\"23\">The approach of Fung and Cheung (2004) is a simpler version of ours.</S>\n    <S sid=\"324\" ssid=\"24\">They match each foreign document with a set of English documents, using a threshold on their cosine similarity.</S>\n    <S sid=\"325\" ssid=\"25\">Then, from each document pair, they generate all possible sentence pairs, compute their cosine similarity, and apply another threshold in order to select the ones that are parallel.</S>\n    <S sid=\"326\" ssid=\"26\">Using the set of extracted sentences, they learn a new dictionary, try to extend their set of matching document pairs (by looking for other documents that contain these sentences), and iterate.</S>\n    <S sid=\"327\" ssid=\"27\">The evaluation methodologies of these previous approaches are less direct than ours.</S>\n    <S sid=\"328\" ssid=\"28\">Utiyama et al. (2003) evaluate their sentence pairs manually; they estimate that about 90% of the sentence pairs in their final corpus are parallel.</S>\n    <S sid=\"329\" ssid=\"29\">Fung and Cheung (2004) also perform a manual evaluation of the extracted sentences and estimate their precision to be 65.7% after bootstrapping.</S>\n    <S sid=\"330\" ssid=\"30\">In addition, they also estimate the quality of a lexicon automatically learned from those sentences.</S>\n    <S sid=\"331\" ssid=\"31\">Zhao and Vogel (2002) go one step further and show that the sentences extracted with their method improve the accuracy of automatically computed word alignments, to an F-score of 52.56% over a baseline of 46.46%.</S>\n    <S sid=\"332\" ssid=\"32\">In a subsequent publication, Vogel (2003) evaluates these sentences in the context of an MT system and shows that they bring improvement under special circumstances (i.e., a language model constructed from reference translations) designed to reduce the noise introduced by the automatically extracted corpus.</S>\n    <S sid=\"333\" ssid=\"33\">We go even further and demonstrate that our method can extract data that improves end-to-end MT performance without any special processing.</S>\n    <S sid=\"334\" ssid=\"34\">Moreover, we show that our approach works even when only a limited amount of initial parallel data (i.e., a low-coverage dictionary) is available.</S>\n    <S sid=\"335\" ssid=\"35\">The problem of aligning sentences in comparable corpora was also addressed for monolingual texts.</S>\n    <S sid=\"336\" ssid=\"36\">Barzilay and Elhadad (2003) present a method of aligning sentences in two comparable English corpora for the purpose of building a training set of text-totext rewriting examples.</S>\n    <S sid=\"337\" ssid=\"37\">Monolingual parallel sentence detection presents a particular challenge: there are many sentence pairs that have low lexical overlap but are nevertheless parallel.</S>\n    <S sid=\"338\" ssid=\"38\">Therefore pairs cannot be judged in isolation, and context becomes an important factor.</S>\n    <S sid=\"339\" ssid=\"39\">Barzilay and Elhadad (2003) make use of contextual information by detecting the topical structure of the articles in the two corpora and aligning them at paragraph level based on the topic assigned to each paragraph.</S>\n    <S sid=\"340\" ssid=\"40\">Afterwards, they proceed and align sentences within paragraph pairs using dynamic programming.</S>\n    <S sid=\"341\" ssid=\"41\">Their results show that both the induced topical structure and the paragraph alignment improve the precision of their extraction method.</S>\n    <S sid=\"342\" ssid=\"42\">A line of research that is both complementary and related to ours is that of Resnik and Smith (2003).</S>\n    <S sid=\"343\" ssid=\"43\">Their STRAND Web-mining system has a purpose that is similar to ours: to identify translational pairs.</S>\n    <S sid=\"344\" ssid=\"44\">However, STRAND focuses on extracting pairs of parallel Web pages rather than sentences.</S>\n    <S sid=\"345\" ssid=\"45\">Resnik and Smith (2003) show that their approach is able to find large numbers of similar document pairs.</S>\n    <S sid=\"346\" ssid=\"46\">Their system is potentially a good way of acquiring comparable corpora from the Web that could then be mined for parallel sentences using our method.</S>\n  </SECTION>\n  <SECTION title=\"9.\" number=\"10\">\n    <S sid=\"347\" ssid=\"1\">The most important feature of our parallel sentence selection approach is its robustness.</S>\n    <S sid=\"348\" ssid=\"2\">Comparable corpora are inherently noisy environments, where even similar content may be expressed in very different ways.</S>\n    <S sid=\"349\" ssid=\"3\">Moreover, out-of-domain corpora introduce additional difficulties related to limited dictionary coverage.</S>\n    <S sid=\"350\" ssid=\"4\">Therefore, the ability to reliably judge sentence pairs in isolation is crucial.</S>\n    <S sid=\"351\" ssid=\"5\">Comparable corpora of interest are usually of large size; thus, processing them requires efficient algorithms.</S>\n    <S sid=\"352\" ssid=\"6\">The computational processes involved in our system are quite modest.</S>\n    <S sid=\"353\" ssid=\"7\">All the operations necessary for the classification of a sentence pair (filter, word alignment computation, and feature extraction) can be implemented efficiently and scaled up to very large amounts of data.</S>\n    <S sid=\"354\" ssid=\"8\">The task can be easily parallelized for increased speed.</S>\n    <S sid=\"355\" ssid=\"9\">For example, extracting data from 600k English documents and 500k Chinese documents (Section 4.2) required only about 7 days of processing time on 10 processors.</S>\n    <S sid=\"356\" ssid=\"10\">The data that we extract is useful.</S>\n    <S sid=\"357\" ssid=\"11\">Its impact on MT performance is comparable to that of human-translated data of similar size and domain.</S>\n    <S sid=\"358\" ssid=\"12\">Thus, although we have focused our experiments on the particular scenario where there is little in-domain training data available, we believe that our method can be useful for increasing the amount of training data, regardless of the domain of interest.</S>\n    <S sid=\"359\" ssid=\"13\">As we have shown, this could be particularly effective for language pairs for which only very small amounts of parallel data are available.</S>\n    <S sid=\"360\" ssid=\"14\">By acquiring a large comparable corpus and performing a few bootstrapping iterations, we can obtain a training corpus that yields a competitive MT system.</S>\n    <S sid=\"361\" ssid=\"15\">We suspect our approach can be used on comparable corpora coming from any domain.</S>\n    <S sid=\"362\" ssid=\"16\">The only domain-dependent element of the system is the date window parameter of the article selection stage (Figure 7); for other domains, this can be replaced with a more appropriate indication of where the parallel sentences are likely to be found.</S>\n    <S sid=\"363\" ssid=\"17\">For example, if the domain were that of technical manuals, one would cluster printer manuals and aircraft manuals separately.</S>\n    <S sid=\"364\" ssid=\"18\">It is important to note that our work assumes that the comparable corpus does contain parallel sentences (which is the case for our data).</S>\n    <S sid=\"365\" ssid=\"19\">Whether this is true for comparable corpora from other domains is an empirical question outside the scope of this article; however, both our results and those of Resnik and Smith (2003) strongly indicate that good data is available on the Web.</S>\n    <S sid=\"366\" ssid=\"20\">Lack of parallel corpora is a major bottleneck in the development of SMT systems for most language pairs.</S>\n    <S sid=\"367\" ssid=\"21\">The method presented in this paper is a step towards the important goal of automatic acquisition of such corpora.</S>\n    <S sid=\"368\" ssid=\"22\">Comparable texts are available on the Web in large quantities for many language pairs and domains.</S>\n    <S sid=\"369\" ssid=\"23\">In this article, we have shown how they can be efficiently mined for parallel sentences.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"11\">\n    <S sid=\"370\" ssid=\"1\">This work was supported by DARPA-ITO grant NN66001-00-1-9814 and NSF grant IIS-0326276.</S>\n    <S sid=\"371\" ssid=\"2\">The experiments were run on University of Southern California’s high-performance computer cluster HPC (http://www.usc.edu/hpcc).</S>\n    <S sid=\"372\" ssid=\"3\">We would like to thank Hal Daum´e III, Alexander Fraser, Radu Soricut, as well as the anonymous reviewers, for their helpful comments.</S>\n    <S sid=\"373\" ssid=\"4\">Any remaining errors are of course our own.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "bd5d2ad5547cf25c8a707f177e6597cc",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Improving Machine Translation Performance By Exploiting Non-Parallel Corpora"
                }
            }
        },
        {
            "index": 15,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper presents a high precision pronoun resolution system that is capable of greater than 90% precision with 60% and better recall for some pronouns. It is suggested that the system is resolving a sub-set of anaphors that do not require general world knowledge or sophisticated linguistic processing for successful resolution. The system does this by being very sensitive to ambiguity, and only resolving pronouns when very high confidence rules have been satisfied. The system is capable of 'noticing' ambiguity because it requires that there be a unique antecedent within a salience ranking, and the salience rankings are not total orders, i.e. two or more antecedents can be equally salient. Given the nature of the systems rules, it is very likely that they are largely domain independent and that they reflect processing strategies used by humans for general language comprehension. The system has been evaluated in two distinct experiments which support the overall validity of the approach.",
                    "filename": "CogNIAC_High_Precision_Coreference_With_Limited_Knowledge_And_Linguistic_Resources",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W97-1306/Documents_xml/W97-1306.xml",
                    "title": "CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources"
                },
                "llm": {
                    "descriptions": [
                        "Abstract:\n\nThis paper introduces CogNIAC, a high-precision coreference resolution engine that operates with limited knowledge and linguistic resources. The engine is designed on the premise that a subset of anaphora can be resolved without general-purpose reasoning, requiring only sentence detection, part-of-speech tagging, simple noun phrase recognition, and basic semantic category information. CogNIAC differentiates itself from similar algorithms by avoiding resolution in ambiguous contexts, thereby achieving high precision. The paper discusses the challenges of pronoun resolution, the limitations due to knowledge sources, and the strategies employed to mimic human resolution practices. CogNIAC's performance is evaluated against Hobbs' Naive Algorithm and in the context of Wall Street Journal data, demonstrating respectable precision and recall in certain conditions. Possible applications include information retrieval, language learning, and information extraction. The paper concludes that while CogNIAC minimizes the role of world knowledge, it demonstrates the potential to reliably resolve pronouns with non-knowledge-intensive techniques, promising future improvements and integration into more knowledge-rich applications.",
                        "This paper introduces CogNIAC, a pronoun resolution engine designed to determine anaphora with high precision while operating with limited knowledge sources. Unlike traditional algorithms that guess in ambiguous situations, CogNIAC refrains from resolving pronouns when faced with uncertainty. It utilizes basic linguistic resources, such as sentence detection, part-of-speech tagging, and simple noun phrase recognition, and relies on sentence structure and English language regularities to resolve pronouns. The paper presents two experiments comparing CogNIAC's performance against the Naive Algorithm and reports findings from its application in narrative texts and Wall Street Journal data. The results indicate that CogNIAC can achieve respectable recall and over 90% precision without the need for extensive world knowledge. The paper discusses the potential applications of high precision coreference, the role of world knowledge in anaphora resolution, and future developments planned for CogNIAC, emphasizing its utility within varied domains and its integration into the University of Pennsylvania's coreference resolution software.",
                        "The academic paper presents CogNIAC, a high precision coreference resolution engine that operates with limited knowledge and linguistic resources. CogNIAC is designed to resolve pronouns by avoiding guesswork in ambiguous contexts, leveraging regularities of English usage and syntactic constraints rather than world knowledge. The engine employs a set of high precision rules implemented in Perl, utilizing part-of-speech tagging, noun phrase recognition, and semantic category information. The paper compares CogNIAC's performance with traditional algorithms, demonstrating that it achieves high precision while refraining from resolving pronouns in cases of ambiguity. The system is evaluated on narrative texts and Wall Street Journal data, and it is posited as a valuable asset for applications requiring high precision over recall, such as information retrieval, language learning, and information extraction. The paper concludes that CogNIAC successfully delimits coreference resolution tasks that require extensive world knowledge and suggests future enhancements to accommodate more complex anaphoric expressions and verb semantics.",
                        "This paper introduces CogNIAC, a high-precision coreference resolution engine that operates with limited linguistic resources and knowledge sources, avoiding the resolution of pronouns in ambiguous contexts. CogNIAC requires basic linguistic processing like sentence detection, part-of-speech tagging, and simple noun phrase recognition, along with semantic information such as gender and number. Unlike algorithms that resolve pronouns amidst ambiguity, CogNIAC abstains from making guesses when the context does not provide clear antecedents, leading to much higher precision albeit potentially lower recall. The paper evaluates CogNIAC's performance through experiments with narratives and news articles, comparing it to other algorithms like Hobbs' Naive Algorithm. Results show that CogNIAC achieves high precision with respectable recall for certain types of pronominal anaphora, without extensive knowledge sources. Possible applications include information retrieval and text coherence checking, with future work aiming to address more complex coreference scenarios and incorporate verb semantics to constrain possible antecedents."
                    ],
                    "filename": "cogniac_high_precision_coreferen-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 157 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources</S>\n  <SECTION title=\"1 Introduction:\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">Pronoun resolution is one of the 'classic' computational linguistics problems.</S>\n    <S sid=\"8\" ssid=\"2\">It is also widely considered to be inherently an A.I. complete' task-meaning that resolution of pronouns requires full world knowledge and inference.</S>\n    <S sid=\"9\" ssid=\"3\">CogNIAC is a pronoun resolution engine designed around the assumption that there is a sub-class of anaphora that does not require general purpose reasoning.</S>\n    <S sid=\"10\" ssid=\"4\">The kinds of information CogNIAC does require includes: sentence detection, part-of-speech tagging, simple noun phrase recognition, basic semantic category information like, gender, number, and in one configuration, partial parse trees.</S>\n    <S sid=\"11\" ssid=\"5\">What distinguishes CogNIAC from algorithms that use similar sorts of information is that it will not resolve a pronoun in circumstances of ambiguity.</S>\n    <S sid=\"12\" ssid=\"6\">Crucially, ambiguity is a function of how much knowledge an understander has.</S>\n    <S sid=\"13\" ssid=\"7\">Since CogNIAC does not have as rich a representation of world knowledge as humans, it finds much more ambiguity in texts than humans do.</S>\n    <S sid=\"14\" ssid=\"8\">2 A path to high precision pronominal resolution-- avoid guesswork in ambiguous contexts: It is probably safe to say that few referring pronouns are conveyed without the speaker/writer having an antecedent in mind.</S>\n    <S sid=\"15\" ssid=\"9\">Ambiguity occurs when the perceiver cannot recover from the context what conveyer has in mind.</S>\n    <S sid=\"16\" ssid=\"10\">I have found myself uttering pronouns which the hearer has no chance of recovering the antecedent to because they are not attending to the same part of the external environment, &amp;quot;He sure looks familiar&amp;quot;, or in text I am so focused on the context of what I am writing that use a pronoun to refer to a highly salient concept for me, but the antecedent may completely evade a reader without my familiarity with the topic.</S>\n    <S sid=\"17\" ssid=\"11\">Of course it is possible to explicitly leave the reader hanging as in, &amp;quot;Earl and Dave were working together when suddenly he fell into the threshing machine.&amp;quot; Humans, unlike most coreference algorithms, notice such cases of ambiguity and can then ask for clarification or at least grumble about how we cannot climb into the writers head to figure out what they meant.</S>\n    <S sid=\"18\" ssid=\"12\">But in that grumble we have articulated the essence of the problem--we don't have sufficient knowledge to satisfy ourselves that an antecedent has been found.</S>\n    <S sid=\"19\" ssid=\"13\">Pronoun resolution systems have extremely limited knowledge sources, they cannot access a fraction of human common sense knowledge.</S>\n    <S sid=\"20\" ssid=\"14\">To appreciate this consider the following text with grammatical tags replacing words with pronouns and names left in place: The city council VERBGROUP the women NP CC they VB NN Mariana VBD PP Sarah TO VB herself PP DT MD NN Without lexical knowledge a human attempting to resolve the pronouns is in much the knowledge impoverished position of the typical coreference algorithm.</S>\n    <S sid=\"21\" ssid=\"15\">It is no surprise that texts with so little information provided in them tend to be more ambiguous than the texts in fleshed out form.</S>\n    <S sid=\"22\" ssid=\"16\">The conclusion to draw from this example is that the limiting factor in CogNIAC is knowledge sources, not an artificial restriction on domains or kinds of coreference.</S>\n    <S sid=\"23\" ssid=\"17\">This point will be resumed in the discussion section when what the consequences of fuller knowledge sources would be on CogNIAC.</S>\n    <S sid=\"24\" ssid=\"18\">For noun phrase anaphora, gathering semantically possible antecedents amounts to running all the noun phrases in a text through various databases for number and gender, and perhaps then a classifier that determines whether a noun phrase is a company, person or place'.</S>\n    <S sid=\"25\" ssid=\"19\">This set of candidate antecedents rarely has more than 5 members when some reasonable locality constraints are adhered to, and this set almost always contains the actual antecedent.</S>\n    <S sid=\"26\" ssid=\"20\">The remainder of the coreference resolution process amounts to picking the right entity from this set.</S>\n    <S sid=\"27\" ssid=\"21\">For the kinds of data considered here (narratives and newspaper articles) there is a rarely a need for general world knowledge in assembling the initial set of possible antecedents for pronouns.</S>\n    <S sid=\"28\" ssid=\"22\">This does not address the issue of inferred antecedents, event reference, discourse deixis and many other sorts of referring phenomenon which clearly require the use of world knowledge but are beyond the scope of this work.</S>\n    <S sid=\"29\" ssid=\"23\">As it happens, recognizing the possible antecedents of these pronouns is within the capabilities of current knowledge sources.</S>\n    <S sid=\"30\" ssid=\"24\">Better knowledge sources could be used to reduce the space of possible antecedents.</S>\n    <S sid=\"31\" ssid=\"25\">For example the well known [Winograd 19721 alternation: The city council refused to give the women a permit because they {feared/advocated} violence.</S>\n    <S sid=\"32\" ssid=\"26\">There are two semantically possible antecedents to they: The city council, and the women.</S>\n    <S sid=\"33\" ssid=\"27\">The problem is picking the correct one.</S>\n    <S sid=\"34\" ssid=\"28\">Dependent on verb choice, they strongly prefers one antecedent to the other.</S>\n    <S sid=\"35\" ssid=\"29\">Capturing this generalization requires a sophisticated theory of verb meaning as relates to pronoun resolution.</S>\n    <S sid=\"36\" ssid=\"30\">Speaking anecdotally, these kinds of resolutions happen quite often in text.</S>\n    <S sid=\"37\" ssid=\"31\">CogNIAC recognizes knowledge intensive coreference and does not attempt to resolve such instances.</S>\n    <S sid=\"38\" ssid=\"32\">I The named entity task at MUC-6 used a similar classification task and the best system performance was 96% precision/97% recall.</S>\n    <S sid=\"39\" ssid=\"33\">Fortunately not all instances of pronominal anaphora require world knowledge for successful resolution.</S>\n    <S sid=\"40\" ssid=\"34\">In lieu of full world knowledge, CogNIAC uses regularities of English usage in an attempt to mimic strategies used by humans when resolving pronouns.</S>\n    <S sid=\"41\" ssid=\"35\">For example, the syntax of a sentence highly constrains a reflexive pronoun's antecedent.</S>\n    <S sid=\"42\" ssid=\"36\">Also if there is just one possible antecedent in entire the prior discourse, then that entity is nearly always the correct antecedent.</S>\n    <S sid=\"43\" ssid=\"37\">CogNIAC consists of a set of such observations implemented in Perl.</S>\n    <S sid=\"44\" ssid=\"38\">CogNIAC has been used with a range of linguistic resources, ranging from scenarios where almost no linguistic processing of the text is done at all to partial parse trees being provided.</S>\n    <S sid=\"45\" ssid=\"39\">At the very least, there must be sufficient linguistic resources to recognize pronouns in the text and the space of candidate antecedents must be identified.</S>\n    <S sid=\"46\" ssid=\"40\">For the first experiment the text has been part of speech tagged and basal noun phrases have been identified with 11' (i.e. noun phrases that have no nested noun phrases) as shown below: [ Mariana/NNP ] motioned/VBD for/IN [ Sarah/NNP] to/TO seatNB [herself/PRP ] on/IN [ a/DT twoseater/NN lounge/NN ] In addition, finite clauses were identified (by hand for experiment 1) and various regular expressions are used to identify subjects, objects and what verbs take as arguments for the purposes of coreference restrictions.</S>\n    <S sid=\"47\" ssid=\"41\">With this level of linguistic annotation, nearly all the parts of CogNIAC can be used to resolve pronouns.</S>\n    <S sid=\"48\" ssid=\"42\">The core rules of CogNIAC are given below, with their performance on training data provided (200 pronouns of narrative text).</S>\n    <S sid=\"49\" ssid=\"43\">In addition, examples where the rules successfully apply have been provided for most of the rules with the relevant anaphors and antecedents in boldface.</S>\n    <S sid=\"50\" ssid=\"44\">The term 'possible antecedents' refers to the set of entities from the discourse that are compatible with an anaphor's gender, number and coreference restrictions (i.e. non-reflexive pronouns cannot corefer with the other arguments of its verb/preposition etc.)</S>\n    <S sid=\"51\" ssid=\"45\">.</S>\n    <S sid=\"52\" ssid=\"46\">Mariana motioned for Sarah to seat herself on a two-seater lounge. sentence, then pick i as the antecedent: 114 correct, and 2 incorrect.</S>\n    <S sid=\"53\" ssid=\"47\">Rupert Murdock's News Corp. confirmed his interest in buying back the ailing New York Post.</S>\n    <S sid=\"54\" ssid=\"48\">But analysts said that if he winds up bidding for the paper,.... possessive pronoun and there is a single exact string match i of the possessive in the prior sentence, then pick i as the antecedent: 4 correct, and 1 incorrect.</S>\n    <S sid=\"55\" ssid=\"49\">After he was dry, Joe carefully laid out the damp towel in front of his locker.</S>\n    <S sid=\"56\" ssid=\"50\">Travis went over to his locker, took out a towel and started to dry off.</S>\n    <S sid=\"57\" ssid=\"51\">5) Unique Current Sentence: If there is a single possible antecedent in the read-in portion of the current sentence, then pick i as the antecedent: 21 correct, and 1 incorrect.</S>\n    <S sid=\"58\" ssid=\"52\">Like a large bear, he sat motionlessly in the lounge in one of the faded armchairs, watching Constantin.</S>\n    <S sid=\"59\" ssid=\"53\">After a week Constantin tired of reading the old novels in the bottom shelf of the bookcase-somewhere among the gray well thumbed pages he had hoped to find a message from one of his predecessors</S>\n  </SECTION>\n  <SECTION title=\"6) Unique Subject/ Subject Pronoun:\" number=\"2\">\n    <S sid=\"60\" ssid=\"1\">If the subject of the prior sentence contains a single possible antecedent i, and the anaphor is the subject of the current sentence, then pick i as the antecedent: 11 correct, and 0 incorrect.</S>\n    <S sid=\"61\" ssid=\"2\">Besides, if he provoked Malek, uncertainties were introduced, of which there were already far too many.</S>\n    <S sid=\"62\" ssid=\"3\">He noticed the supervisor enter the lounge ...</S>\n    <S sid=\"63\" ssid=\"4\">The method of resolving pronouns within CogNIAC works as follows: Pronouns are resolved left-to-right in the text.</S>\n    <S sid=\"64\" ssid=\"5\">For each pronoun, the rules are applied in the presented order.</S>\n    <S sid=\"65\" ssid=\"6\">For a given rule, if an antecedent is found, then the appropriate annotations are made to the text and no more rules are tried for that pronoun, otherwise the next rule is tried.</S>\n    <S sid=\"66\" ssid=\"7\">If no rules resolve the pronoun, then it is left unresolved.</S>\n    <S sid=\"67\" ssid=\"8\">These rules are individually are high precision rules, and collectively they add up to reasonable recall.</S>\n    <S sid=\"68\" ssid=\"9\">The precision is 97% (121/125) and the recall is 60% (121/201) for 198 pronouns of training data.</S>\n  </SECTION>\n  <SECTION title=\"3 Evaluation:\" number=\"3\">\n    <S sid=\"69\" ssid=\"1\">The Naive Algorithm [Hobbs 1976] works by specifying a total order on noun phrases in the prior discourse and comparing each noun phrase against the selectional restrictions (i.e. gender, number) of the anaphor, and taking the antecedent to be the first one to satisfy them.</S>\n    <S sid=\"70\" ssid=\"2\">The specification of the ordering constitutes a traversal order of the syntax tree of the anaphors clause and from there to embedding clauses and prior clauses.</S>\n    <S sid=\"71\" ssid=\"3\">The Winograd sentences, with either verb, would yield the following ordering of possible antecedents: The city council &gt; the women The algorithm would resolve they to The city council.</S>\n    <S sid=\"72\" ssid=\"4\">This is incorrect on one choice of verb, but the algorithm does not integrate the verb information into the salience ranking.</S>\n    <S sid=\"73\" ssid=\"5\">In comparison, none of the six rules of CogNIAC would resolve the pronoun.</S>\n    <S sid=\"74\" ssid=\"6\">Rules have been tried that resolved a subject pronoun of a nested clause with the subject of the dominating clause, but no configuration has been found that yielded sufficient precision2.</S>\n    <S sid=\"75\" ssid=\"7\">Consequently, they is not resolved'.</S>\n    <S sid=\"76\" ssid=\"8\">The naive algorithm has some interesting properties.</S>\n    <S sid=\"77\" ssid=\"9\">First it models relative salience as relative depth in a search space.</S>\n    <S sid=\"78\" ssid=\"10\">For two candidate antecedents a and b, if a is encountered before b in the search space, then a is more salient than b.</S>\n    <S sid=\"79\" ssid=\"11\">Second, the relative saliency of all candidate antecedents is totally ordered, that is, for any two candidate antecedents a and b , a is more salient than b xor b is more salient than a.</S>\n    <S sid=\"80\" ssid=\"12\">2 In experiment 2, discussed below, the rule 'subject same clause' would resolve they to the city council, but it was added to the MUC-6 system without testing, and has shown itself to not be a high precision rule.</S>\n    <S sid=\"81\" ssid=\"13\">CogNIAC shares several features of the Naive Algorithm: circumstances of many possible antecedents, and will not resolve pronouns in such cases.</S>\n    <S sid=\"82\" ssid=\"14\">The Naive Algorithm has no means of noting ambiguity and will resolve a pronoun as long as there is at least one possible antecedent.</S>\n    <S sid=\"83\" ssid=\"15\">Perhaps the most convincing reason to endorse partially ordered salience rankings is that salience distinctions fade as the discourse moves on.</S>\n    <S sid=\"84\" ssid=\"16\">Earl was working with Ted the other day.</S>\n    <S sid=\"85\" ssid=\"17\">He fell into the threshing machine.</S>\n    <S sid=\"86\" ssid=\"18\">Earl was working with Ted the other day.</S>\n    <S sid=\"87\" ssid=\"19\">All of the sudden, the cows started making a ruckus.</S>\n    <S sid=\"88\" ssid=\"20\">The noise was unbelievable.</S>\n    <S sid=\"89\" ssid=\"21\">He fell into the threshing machine.</S>\n    <S sid=\"90\" ssid=\"22\">In the first example 'He' takes `Earl' as antecedent, which is what rule 6, Unique Subject/Subject Pronoun, would resolve the pronoun to.</S>\n    <S sid=\"91\" ssid=\"23\">However in the second example, the use of `He' is ambiguous--a distinction that existed before is now gone.</S>\n    <S sid=\"92\" ssid=\"24\">The Naive Algorithm would still maintain a salience distinction between 'Earl' and `Ted', where CogNIAC has no rule that makes a salience distinction between subject and object of a sentence which has two intervening sentences.</S>\n    <S sid=\"93\" ssid=\"25\">The closest rule would be Unique in Discourse, rule 1, which does not yield a unique antecedent.</S>\n    <S sid=\"94\" ssid=\"26\">CogNIAC has been evaluated in two different contexts.</S>\n    <S sid=\"95\" ssid=\"27\">The goal of the first experiment was to establish relative performance of CogNIAC to Hobbs' Naive Algorithm--a convenient benchmark that allows indirect comparison to other algorithms.</S>\n    <S sid=\"96\" ssid=\"28\">The second experiment reports results on Wall Street Journal data.</S>\n    <S sid=\"97\" ssid=\"29\">The chosen domain for comparison with Hobbs' Naive Algorithm was narrative texts about two persons of the same gender told from a third person perspective.</S>\n    <S sid=\"98\" ssid=\"30\">The motivation for this data was that we wanted to maximize the ambiguity of resolving pronouns.</S>\n    <S sid=\"99\" ssid=\"31\">Only singular third person pronouns were considered.</S>\n    <S sid=\"100\" ssid=\"32\">The text was pre-processed with a part-of-speech tagger over which basal noun phrases were delimited and finite clauses and their relative nesting were identified by machine.</S>\n    <S sid=\"101\" ssid=\"33\">This pre-processing was subjected to hand correction in order to make comparison with Hobbs as fair as possible since that was an entirely hand executed algorithm, but CogNIAC was otherwise machine run and scored.</S>\n    <S sid=\"102\" ssid=\"34\">Errors were not chained, i.e. in left-to-right processing of the text, earlier mistakes were corrected before processing the next noun phrase.</S>\n    <S sid=\"103\" ssid=\"35\">Since the Naive Algorithm resolves all pronouns, two lower precision rules were added to rules 1-6) for comparisons sake.</S>\n    <S sid=\"104\" ssid=\"36\">The rules are: The last two rules are lower precision than the first six, but perform well enough to merit their inclusion in a 'resolve all pronouns' configuration.</S>\n    <S sid=\"105\" ssid=\"37\">Rule 7 performed reasonably well with 77% precision in training (10/13 correct for 201 pronouns), and rule 8 performed with 65% precision in training (44/63 correct).</S>\n    <S sid=\"106\" ssid=\"38\">The first six rules each had a precision of greater than 90% for the training data with the exception of rule 4 which had a precision of 80% for 5 resolutions.</S>\n    <S sid=\"107\" ssid=\"39\">The summary performance of the Naive Algorithm and CogNIAC (including all 8 rules) for the first 100 or so pronouns in three narrative texts are: Results for 298 third person pronouns in text about two same gender people.</S>\n    <S sid=\"108\" ssid=\"40\">Since both the Naive Algorithm and the resolve all pronouns configuration of CogNIAC are required to resolve all pronouns, precision and recall figures are not appropriate.</S>\n    <S sid=\"109\" ssid=\"41\">Instead % correct figures are given.</S>\n    <S sid=\"110\" ssid=\"42\">The high precision version of CogNIAC is reported with recall (number correct/number of instances of coreference) and precision (number correct/number of guesses) measures.</S>\n    <S sid=\"111\" ssid=\"43\">The conclusion to draw from these results is: if forced to commit to all anaphors, CogNIAC performs comparably to the Naive Algorithm.</S>\n    <S sid=\"112\" ssid=\"44\">Lappin and Leass 3 Rule 7 is based on the primitives of Centering Theory (Grosz, Joshi and Weinstein '86).</S>\n    <S sid=\"113\" ssid=\"45\">The Cb of an utterance is the highest ranked NP (Ranking being: Subject &gt; All other NPs) from the prior finite clause realized anaphorically in the current finite clause.</S>\n    <S sid=\"114\" ssid=\"46\">Please see Baldwin '95 for a full discussion of the details of the rule.</S>\n    <S sid=\"115\" ssid=\"47\">1994 correctly resolved 86% of 360 pronouns in computer manuals.</S>\n    <S sid=\"116\" ssid=\"48\">Lapin and Leass run Hobbs' algorithm on the their data and the Naive Algorithm is correct 82% of the time--4% worse.</S>\n    <S sid=\"117\" ssid=\"49\">This allows indirect comparison with CogNIAC, with the suggestive conclusion that the resolve all pronouns configuration of CogNIAC, like the Naive Algorithm, is at least in the ballpark of more modern approaches&amp; The breakdown of the individual rules is as follows: Performance of individual rules in Experiment 1.</S>\n    <S sid=\"118\" ssid=\"50\">Note the high precision of rules 1 - 6).</S>\n    <S sid=\"119\" ssid=\"51\">Recall = #correct/#actual, Precision = #correct/#guessed Far more interesting to consider is the performance of the high precision rules 1 through 6.</S>\n    <S sid=\"120\" ssid=\"52\">The first four rules perform quite well at 96% precision (148/154) and 50% recall (148/298).</S>\n    <S sid=\"121\" ssid=\"53\">Adding in rules 5 and 6 resolves a total of 190 pronouns correctly, with only 16 mistakes, a precision of 92% and recall of 64%.</S>\n    <S sid=\"122\" ssid=\"54\">This contrasts strongly with the resolve-all-pronouns results of 78%.</S>\n    <S sid=\"123\" ssid=\"55\">The last two rules, 7 and 8 performed quite badly on the test data.</S>\n    <S sid=\"124\" ssid=\"56\">Despite their poor performance, CogNIAC still remained comparable to the Naive Algorithm.</S>\n    <S sid=\"125\" ssid=\"57\">3.2.2 Experiment 2-- All pronouns in MUC-6 evaluation: CogNIAC was used as the pronoun component in the University Pennsylvania's coreference entry5 in the MUC-6 evaluation.</S>\n    <S sid=\"126\" ssid=\"58\">Pronominal anaphora constitutes 17% of coreference annotations in the evaluation data used.</S>\n    <S sid=\"127\" ssid=\"59\">The remaining instances of anaphora included common noun anaphora and coreferent instances of proper nouns.</S>\n    <S sid=\"128\" ssid=\"60\">As a result being part of a larger system, changes were made to CogNIAC to make it fit in better with the other components of the overall system in addition to adding rules that were specialized for the new kinds of pronominal anaphora.</S>\n    <S sid=\"129\" ssid=\"61\">These changes include: 4 This is not to say that RAP was not an advancement of the state of the art.</S>\n    <S sid=\"130\" ssid=\"62\">A significant aspect of that research is that both RAP and the Naive Algorithm were machine executed--the Naive Algorithm was not machine executed in either the Hobbs 76 paper or in the evaluation in this work.</S>\n    <S sid=\"131\" ssid=\"63\">A total of thirty articles were used in the formal evaluation, of which I chose the first fifteen for closer analysis.</S>\n    <S sid=\"132\" ssid=\"64\">The remaining fifteen were retained for future evaluations.</S>\n    <S sid=\"133\" ssid=\"65\">The performance of CogNIAC was as follows: The precision (73%) is quite a bit worse than that encountered in the narrative.</S>\n    <S sid=\"134\" ssid=\"66\">The performance of the individual rules was quite different from the narrative texts, as shown in the table below: The results for CogNIAC for all pronouns in the first 15 articles of the MUC-6 evaluation.</S>\n    <S sid=\"135\" ssid=\"67\">Upon closer examination approximately 75% of the errors were due to factors outside the scope of the CogNIAC pronominal resolution component.</S>\n    <S sid=\"136\" ssid=\"68\">Software problems accounted for 20% of the incorrect cases, another 30% were due to semantic errors like misclassification of a noun phrase into person or company, singular/plural etc.</S>\n    <S sid=\"137\" ssid=\"69\">The remaining errors were due to incorrect noun phrase identification, failure to recognize pleonastic-it or other cases where there is no instance of an antecedent.</S>\n    <S sid=\"138\" ssid=\"70\">However, 25% of the errors were due directly to the rules of CogNIAC being plain wrong.</S>\n  </SECTION>\n  <SECTION title=\"4 Discussion:\" number=\"4\">\n    <S sid=\"139\" ssid=\"1\">CogNIAC is both an engineering effort and a different approach to information processing in variable knowledge contexts.</S>\n    <S sid=\"140\" ssid=\"2\">Each point is addressed in turn.</S>\n    <S sid=\"141\" ssid=\"3\">A question raised by a reviewer asked whether there was any use for high precision coreference given that it is not resolving as much coreference as other methods.</S>\n    <S sid=\"142\" ssid=\"4\">In the first experiment, the high precision version of CogNIAC correctly resolved 62% of the pronouns as compared to the resolve all pronouns version which resolved 79% of them--a 27% loss of overall recall.</S>\n    <S sid=\"143\" ssid=\"5\">The answer to this question quite naturally depends on the application coreference is being used in.</S>\n    <S sid=\"144\" ssid=\"6\">Some examples follow.</S>\n    <S sid=\"145\" ssid=\"7\">Information retrieval is characterized as a process by which a query is used to retrieve relevant documents from a text database.</S>\n    <S sid=\"146\" ssid=\"8\">Queries are typically natural language based or Boolean expressions.</S>\n    <S sid=\"147\" ssid=\"9\">Documents are retrieved and ranked for relevance using various string matching techniques with query terms in a document and the highest scoring documents are presented to the user first.</S>\n    <S sid=\"148\" ssid=\"10\">The role that coreference resolution might play in information retrieval is that retrieval algorithms that a) count the number of matches to a query term in a document, or b) count the proximity of matches to query terms, would benefit by noticing alternative realizations of the terms like 'he' in place 'George Bush'.</S>\n    <S sid=\"149\" ssid=\"11\">In such an application, high precision coreference would be more useful than high recall coreference if the information retrieval engine was returning too many irrelevant documents but getting a reasonable number of relevant documents.</S>\n    <S sid=\"150\" ssid=\"12\">The coreference would only help the scores of presumably relevant documents, but at the expense of missing some relevant documents.</S>\n    <S sid=\"151\" ssid=\"13\">A higher recall, lower precision algorithm would potentially add more irrelevant documents.</S>\n    <S sid=\"152\" ssid=\"14\">A direct application of the &amp;quot;ambiguity noticing&amp;quot; ability of CogNIAC is in checking the coherence of pronoun use in text for children and English as a second language learners.</S>\n    <S sid=\"153\" ssid=\"15\">Ambiguous pronoun use is a substantial problem for beginning writers and language learners.</S>\n    <S sid=\"154\" ssid=\"16\">CogNIAC could scan texts as they are being written and evaluate whether there was sufficient syntactic support from the context to resolve the pronoun--if not, then the user could be notified of a potentially ambiguous use.</S>\n    <S sid=\"155\" ssid=\"17\">It is not clear that CogNIAC's current levels of performance could support such an application, but it is a promising application.</S>\n    <S sid=\"156\" ssid=\"18\">Information extraction amounts to filling in template like data structures from free text.</S>\n    <S sid=\"157\" ssid=\"19\">Typically the patterns which are used to fill the templates are hand built.</S>\n    <S sid=\"158\" ssid=\"20\">The latest MUC-6 evaluation involved management changes at companies.</S>\n    <S sid=\"159\" ssid=\"21\">A major problem in information extraction is the fact that the desired information can be spread over many sentences in the text and coreference resolution is essential to relate relevant sentences to the correct individuals, companies etc.</S>\n    <S sid=\"160\" ssid=\"22\">The MUC-6 coreference task was developed with the idea that it would aid information extraction technologies.</S>\n    <S sid=\"161\" ssid=\"23\">The consequences for an incorrectly resolved pronoun can be devastating to the final template filling task--one runs the risk of conflating information about one individual with another.</S>\n    <S sid=\"162\" ssid=\"24\">High precision coreference appears to be a natural candidate for such applications.</S>\n    <S sid=\"163\" ssid=\"25\">CogNIAC effectively circumscribes those cases where coreference can be done with high confidence and those cases that require greater world knowledge, but how might CogNIAC be a part of a more knowledge rich coreference application?</S>\n    <S sid=\"164\" ssid=\"26\">CogNIAC as a set of seven or so high precision rules would act as an effective filter on what a more knowledge rich application would have to resolve.</S>\n    <S sid=\"165\" ssid=\"27\">But the essential component behind CogNIAC is not the rules themselves, but the control structure of behind its coreference resolution algorithm.</S>\n    <S sid=\"166\" ssid=\"28\">This control structure could control general inference techniques as well.</S>\n    <S sid=\"167\" ssid=\"29\">An interesting way to look at CogNIAC is as a search procedure.</S>\n    <S sid=\"168\" ssid=\"30\">The Naive Algorithm can be over simplified as depth first search over parse trees.</S>\n    <S sid=\"169\" ssid=\"31\">Depth first search is also a perfectly reasonable control structure for an inference engine-- as it is with PROLOG.</S>\n    <S sid=\"170\" ssid=\"32\">The search structure of CogNIAC could be characterized as parallel iterative deepening with solutions being accepted only if a unique solution is found to the depth of the parallel search.</S>\n    <S sid=\"171\" ssid=\"33\">But there is not enough room in this paper to explore the general properties of CogNIAC's search and evaluation strategy.</S>\n    <S sid=\"172\" ssid=\"34\">Another angle on CogNIAC's role with more robust knowledge sources is to note that the recall limitations of CogNIAC for the class of pronouns/data considered are due to insufficient filtering mechanisms on candidate antecedents.</S>\n    <S sid=\"173\" ssid=\"35\">There is not a need to expand the space of candidate antecedents with additional knowledge, but rather eliminate semantically plausible antecedents with constraints from verb knowledge and other sources of constraints currently not available to the system.</S>\n    <S sid=\"174\" ssid=\"36\">However, there are classes of coreference that require strong knowledge representation to assemble the initial set of candidate antecedents.</S>\n    <S sid=\"175\" ssid=\"37\">This includes the realm of inferred definites &amp;quot;I went to the house and opened the door&amp;quot; and synonymy between definite common nouns as in &amp;quot;the tax' and 'the levy.</S>\n    <S sid=\"176\" ssid=\"38\">Hobbs 1976 ultimately rejects the Naive Algorithm as a stand-alone solution to the pronoun resolution problem.</S>\n    <S sid=\"177\" ssid=\"39\">In that rejection he states: The naive algorithm does not work.</S>\n    <S sid=\"178\" ssid=\"40\">Anyone can think of examples where it fails.</S>\n    <S sid=\"179\" ssid=\"41\">In these cases it not only fails; it gives no indication that it has failed and offers no help in finding the real antecedent.</S>\n    <S sid=\"180\" ssid=\"42\">Hobbs then articulates a vision of what the appropriate technology is, which entails inference over an encoding of world knowledge.</S>\n    <S sid=\"181\" ssid=\"43\">But is world knowledge inherent in resolving all pronouns as Hobbs skepticism seems to convey?</S>\n    <S sid=\"182\" ssid=\"44\">It has not been clear up to this point whether any anaphora can be resolved with high confidence given that there are clear examples which can only be resolved with sophisticated world knowledge, e.g. the Winograd city council sentences.</S>\n    <S sid=\"183\" ssid=\"45\">But the results from the first and second experiments demonstrate that it is possible to have respectable recall with very high precision (greater than 90%) for some kinds of pronominal resolution.</S>\n    <S sid=\"184\" ssid=\"46\">However, good performance does not necessarily falsify Hobbs' skepticism.</S>\n    <S sid=\"185\" ssid=\"47\">The high precision component of CogNIAC still makes mistakes, 8-9% error for the first experiment--it is harder to evaluate the second experiment.</S>\n    <S sid=\"186\" ssid=\"48\">If it were the case that integration of world knowledge would have prevented those errors, then Hobbs' skepticism still holds since CogNIAC has only minimized the role of world knowledge, not eliminated it.</S>\n    <S sid=\"187\" ssid=\"49\">In looking at the mistakes made in the second experiment, there were no examples that appeared to be beyond the scope of further improving the syntactic rules or expanding the basic categorization of noun phrases into person, company or place.</S>\n    <S sid=\"188\" ssid=\"50\">For the data considered so far, there does appear to be a class of anaphors that can be reliably recognized and resolved with non-knowledge intensive techniques.</S>\n    <S sid=\"189\" ssid=\"51\">Whether this holds in general remains an open question, but it is a central design assumption behind the system.</S>\n    <S sid=\"190\" ssid=\"52\">A more satisfying answer to Hobbs' skepticism is contained in the earlier suggestive conjecture that world knowledge facilitates anaphora by eliminating ambiguity.</S>\n    <S sid=\"191\" ssid=\"53\">This claim can be advanced to say that world knowledge comes into play in those cases of anaphora that do not fall under the purview of rules 1 through 7 and their refinements.</S>\n    <S sid=\"192\" ssid=\"54\">If this is correct, then the introduction of better world knowledge sources will help in the recall of the system rather than the precision.</S>\n    <S sid=\"193\" ssid=\"55\">Ultimately, the utility of CogNIAC is a function of how it performs.</S>\n    <S sid=\"194\" ssid=\"56\">The high precision rules of CogNIAC performed very well, greater than 90% precision with good recall for the first experiment.</S>\n    <S sid=\"195\" ssid=\"57\">In the second experiment, components other than the rules of CogNIAC began to degrade the performance of the system unduly.</S>\n    <S sid=\"196\" ssid=\"58\">But there is promise in the high precision core of CogNIAC across varied domains.</S>\n  </SECTION>\n  <SECTION title=\"5 The future of CogNIAC:\" number=\"5\">\n    <S sid=\"197\" ssid=\"1\">CogNIAC is currently the common noun and pronoun resolution component of the University of Pennsylvania's coreference resolution software and general NLP software (Camp).</S>\n    <S sid=\"198\" ssid=\"2\">This paper does not address the common noun coreference aspects of the system but there are some interesting parallels with pronominal coreference.</S>\n    <S sid=\"199\" ssid=\"3\">Some changes planned include the following sorts of coreference: The processing of split antecedents, John called Mary.</S>\n    <S sid=\"200\" ssid=\"4\">They went to a movie.</S>\n    <S sid=\"201\" ssid=\"5\">This class of coreference is quite challenging because the plural anaphor 'they' must be able to collect a set of antecedents from the prior discourse--but how far should it look back, and once it has found two antecedents, should it continue to look for more?</S>\n    <S sid=\"202\" ssid=\"6\">Event reference is a class of coreference that will also prove to be quite challenging.</S>\n    <S sid=\"203\" ssid=\"7\">For example: The computer won the match.</S>\n    <S sid=\"204\" ssid=\"8\">It was a great triumph.</S>\n    <S sid=\"205\" ssid=\"9\">The antecedent to 'It' could be any of 'The computer', 'the match' or the event of winning.</S>\n    <S sid=\"206\" ssid=\"10\">The space of ambiguity will certainly grow substantially when events are considered as candidate antecedents.</S>\n    <S sid=\"207\" ssid=\"11\">Currently the system uses no verb semantics to try and constrain possible coreference.</S>\n    <S sid=\"208\" ssid=\"12\">While the Winograd sentences are too difficult for current robust lexical semantic systems, simpler generalizations about what can fill an argument are possible, consider: The price of aluminum rose today due to large purchases by ALCOA Inc.</S>\n    <S sid=\"209\" ssid=\"13\">It claimed that it was not trying to corner the market.</S>\n    <S sid=\"210\" ssid=\"14\">Since 'It' is an argument to 'claimed' , a verb that requires that its subject be animate, we can eliminate 'The price of aluminum' and 'today' from consideration, leaving `ALCOA Inc.' as the sole singular antecedent from the prior sentence.</S>\n    <S sid=\"211\" ssid=\"15\">Work has been done along these lines by Dagan '90.</S>\n  </SECTION>\n  <SECTION title=\"6 Acknowledgments:\" number=\"6\">\n    <S sid=\"212\" ssid=\"1\">I would like to thank my advisors Ellen Prince and Aravind Joshi for their support.</S>\n    <S sid=\"213\" ssid=\"2\">Also the comments of two anonymous reviewers proved quite helpful.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "cef819662caa94e6318f6fa09e0e594f",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "CogNIAC: High Precision Coreference With Limited Knowledge And Linguistic Resources"
                }
            }
        },
        {
            "index": 16,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers. Each parser performs abstract computations using the operations of a semiring. The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings. We also show how to use the same representation, interpreted differently, to compute outside values. The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.",
                    "filename": "Semiring_Parsing",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J99-4004/Documents_xml/J99-4004.xml",
                    "title": "Semiring Parsing"
                },
                "llm": {
                    "descriptions": [
                        "**Abstract:**\n\nThis paper presents a unified framework for describing parsers using semiring parsing, integrating parsing algorithms, deductive parsing, and algebraic theory applied to formal languages. The framework utilizes semirings to perform abstract computations, enabling a single representation to describe various parsers that compute different values such as recognition, Viterbi scores, and inside and outside probabilities. The system is applicable to a wide range of parsers, including Earley's algorithm and tree adjoining grammar parsing. Additionally, it simplifies the development of new parsers, the specification of parsers across tasks by changing semirings, and the computation of complex values like outside probabilities. The paper also demonstrates the use of item-based descriptions for parsing and grammar transformations in any ω-continuous semiring, offering practical advantages such as mechanical computation of outside values, verification for arbitrary semirings, and separation of infinite sum computations from the parsing process.",
                        "This paper presents a generalized system for describing parsers that compute a variety of values including recognition, derivation forests, Viterbi, n-best, inside values, and outside values through the use of semiring operations. By synthesizing parsing algorithms, deductive parsing, and algebraic theory, the authors propose that different parsers can be described using a single representation by substituting the operations of different semirings. The paper introduces formalisms for describing parsers and demonstrates how to compute both forward and reverse values in w-continuous semirings. It also addresses the efficient computation of infinite sums which occur in some parsing scenarios. The methodology simplifies the computation of outside values and ensures that parsers can be constructed to work with any w-continuous semiring, thus facilitating the parsing process and providing a framework that can be adapted to various parsing requirements.",
                        "This paper presents a comprehensive synthesis of parsing algorithms, deductive parsing, and algebra theory to develop a general system for describing parsers through semirings. By substituting operations of various semirings, the system allows the use of a single representation for parsers that compute different types of values such as recognition, derivation forests, Viterbi scores, and other measures. The authors demonstrate how this representation can be used to describe a wide variety of parsers, including Earley's algorithm and prefix value computation. They also provide a formalism for describing parsers that can generate parsers for outside values and for general grammars. The paper discusses the computation of both forward and reverse values in semiring parsers, and introduces methods to handle infinite sums for loop-containing parsers. Additionally, the paper explores the application of semiring parsing to grammar transformations and other parsing-related problems, highlighting the practical value of simplifying the computation of outside values and adapting parsers across different semirings.",
                        "This paper presents a unified framework for parsing in computational linguistics, integrating concepts from algebra, formal languages, and parsing algorithms into a comprehensive system that describes parsers using semirings. This approach enables a single representation to be used for various parsers by substituting different semirings, allowing for the computation of recognition, derivation forests, Viterbi scores, n-best lists, inside and outside values, among others. The authors demonstrate how their system can be applied to a wide range of parsers, including Earley's algorithm and tree adjoining grammar parsing. The paper also introduces reverse values for commutative semirings and explores the use of item-based descriptions in grammar transformations. The work aims to simplify the computation of outside values, ensure parsers work across different semirings, and effectively handle infinite sums in semiring computations."
                    ],
                    "filename": "semiring_parsing-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 110 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Semiring Parsing</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">We synthesize work on parsing algorithms, deductive parsing, and the theory of algebra applied to formal languages into a general system for describing parsers.</S>\n    <S sid=\"7\" ssid=\"2\">Each parser performs abstract computations using the operations of a semiring.</S>\n    <S sid=\"8\" ssid=\"3\">The system allows a single, simple representation to be used for describing parsers that compute recognition, derivation forests, Viterbi, n-best, inside values, and other values, simply by substituting the operations of different semirings.</S>\n    <S sid=\"9\" ssid=\"4\">We also show how to use the same representation, interpreted differently, to compute outside values.</S>\n    <S sid=\"10\" ssid=\"5\">The system can be used to describe a wide variety of parsers, including Earley's algorithm, tree adjoining grammar parsing, Graham Harrison Ruzzo parsing, and prefix value computation.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"11\" ssid=\"1\">For a given grammar and string, there are many interesting quantities we can compute.</S>\n    <S sid=\"12\" ssid=\"2\">We can determine whether the string is generated by the grammar; we can enumerate all of the derivations of the string; if the grammar is probabilistic, we can compute the inside and outside probabilities of components of the string.</S>\n    <S sid=\"13\" ssid=\"3\">Traditionally, a different parser description has been needed to compute each of these values.</S>\n    <S sid=\"14\" ssid=\"4\">For some parsers, such as CKY parsers, all of these algorithms (except for the outside parser) strongly resemble each other.</S>\n    <S sid=\"15\" ssid=\"5\">For other parsers, such as Earley parsers, the algorithms for computing each value are somewhat different, and a fair amount of work can be required to construct each one.</S>\n    <S sid=\"16\" ssid=\"6\">We present a formalism for describing parsers such that a single simple description can be used to generate parsers that compute all of these quantities and others.</S>\n    <S sid=\"17\" ssid=\"7\">This will be especially useful for finding parsers for outside values, and for parsers that can handle general grammars, like Earley-style parsers.</S>\n    <S sid=\"18\" ssid=\"8\">Although our description format is not limited to context-free grammars (CFGs), we will begin by considering parsers for this common formalism.</S>\n    <S sid=\"19\" ssid=\"9\">The input string will be denoted w1 w2.. wn.</S>\n    <S sid=\"20\" ssid=\"10\">We will refer to the complete string as the sentence.</S>\n    <S sid=\"21\" ssid=\"11\">A CFG G is a 4-tuple (N, E, R, S) where N is the set of nonterminals including the start symbol S. E is the set of terminal symbols, and R is the set of rules, each of the form A —&gt; a for A c N and a E (N u E)*.</S>\n    <S sid=\"22\" ssid=\"12\">We will use the symbol = for immediate derivation and for its reflexive, transitive closure.</S>\n    <S sid=\"23\" ssid=\"13\">We will illustrate the similarity of parsers for computing different values using the CKY algorithm as an example.</S>\n    <S sid=\"24\" ssid=\"14\">We can write this algorithm in its iterative form as shown in Figure 1.</S>\n    <S sid=\"25\" ssid=\"15\">Here, we explicitly construct a Boolean chart, chart[1..n,L.INI, 1..n + 1].</S>\n    <S sid=\"26\" ssid=\"16\">Element char*, A, j] contains TRUE if and only if A we .</S>\n    <S sid=\"27\" ssid=\"17\">.</S>\n    <S sid=\"28\" ssid=\"18\">. w1_1.</S>\n    <S sid=\"29\" ssid=\"19\">The algorithm consists of a first set of loops to handle the singleton productions, a second set of loops to handle the binary productions, and a return of the start symbol's chart entry.</S>\n    <S sid=\"30\" ssid=\"20\">Next, we consider probabilistic grammars, in which we associate a probability with every rule, P(A —&gt; a).</S>\n    <S sid=\"31\" ssid=\"21\">These probabilities can be used to associate a probability for 1 := 2 to n /* length, shortest to longest */ for s 1 to n— / +1 /* start position */ CKY inside algorithm. with a particular derivation, equal to the product of the rule probabilities used in the derivation, or to associate a probability with a set of derivations, A w,. equal to the sum of the probabilities of the individual derivations.</S>\n    <S sid=\"32\" ssid=\"22\">We call this latter probability the inside probability of i,A,j.</S>\n    <S sid=\"33\" ssid=\"23\">We can rewrite the CKY algorithm to compute the inside probabilities, as shown in Figure 2 (Baker 1979; Lan i and Young 1990).</S>\n    <S sid=\"34\" ssid=\"24\">Notice how similar the inside algorithm is to the recognition algorithm: essentially, all that has been done is to substitute + for V, x for A, and P(A w5) and P(A —&gt; BC) for TRUE.</S>\n    <S sid=\"35\" ssid=\"25\">For many parsing algorithms, this, or a similarly simple modification, is all that is needed to create a probabilistic version of the algorithm.</S>\n    <S sid=\"36\" ssid=\"26\">On the other hand, a simple substitution is not always sufficient.</S>\n    <S sid=\"37\" ssid=\"27\">To give a trivial example, if in the CKY recognition algorithm we had written chart[s, A, s+1] := chart[s, A, s+1] v chart[s, B, s+t] A chart[s+t, C, s+1]; instead of the less natural chart[s, A, s +1] := chart[s, A, s +1] V chart[s,B, s+t] A chart[s+t,C, s +1] A TRUE; larger changes would be necessary to create the inside algorithm.</S>\n    <S sid=\"38\" ssid=\"28\">Besides recognition, four other quantities are commonly computed by parsing algorithms: derivation forests, Viterbi scores, number of parses, and outside probabilities.</S>\n    <S sid=\"39\" ssid=\"29\">The first quantity, a derivation forest, is a data structure that allows one to efficiently compute the set of legal derivations of the input string.</S>\n    <S sid=\"40\" ssid=\"30\">The derivation forest is typically found by modifying the recognition algorithm to keep track of &amp;quot;back pointers&amp;quot; for each cell of how it was produced.</S>\n    <S sid=\"41\" ssid=\"31\">The second quantity often computed is the Viterbi score, the probability of the most probable derivation of the sentence.</S>\n    <S sid=\"42\" ssid=\"32\">This can typically be computed by substituting x for A and max for V. Less commonly computed is the total number of parses of the sentence, which, like the inside values, can be computed using multiplication and addition; unlike for the inside values, the probabilities of the rules are not multiplied into the scores.</S>\n    <S sid=\"43\" ssid=\"33\">There is one last commonly computed quantity, the outside probabilities, which we will describe later, in Section 4.</S>\n    <S sid=\"44\" ssid=\"34\">One of the key points of this paper is that all five of these commonly computed quantities can be described as elements of complete semirings (Kuich 1997).</S>\n    <S sid=\"45\" ssid=\"35\">The relationship between grammars and semirings was discovered by Chomsky and Schtitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teitelbaum (1973).</S>\n    <S sid=\"46\" ssid=\"36\">A complete semiring is a set of values over which a multiplicative operator and a commutative additive operator have been defined, and for which infinite summations are defined.</S>\n    <S sid=\"47\" ssid=\"37\">For parsing algorithms satisfying certain conditions, the multiplicative and additive operations of any complete semiring can be used in place of A and V. and correct values will be returned.</S>\n    <S sid=\"48\" ssid=\"38\">We will give a simple normal form for describing parsers, then precisely define complete semirings, and the conditions for correctness.</S>\n    <S sid=\"49\" ssid=\"39\">We now describe our normal form for parsers, which is very similar to that used by Shieber, Schabes, and Pereira (1995) and by Sikkel (1993).</S>\n    <S sid=\"50\" ssid=\"40\">This work can be thought of as a generalization from their work in the Boolean semiring to semirings in general.</S>\n    <S sid=\"51\" ssid=\"41\">In most parsers, there is at least one chart of some form.</S>\n    <S sid=\"52\" ssid=\"42\">In our normal form, we will use a corresponding, equivalent concept, items.</S>\n    <S sid=\"53\" ssid=\"43\">Rather than, for instance, a chart element chart [i, A, j], we will use an item [i, A,]].</S>\n    <S sid=\"54\" ssid=\"44\">Furthermore, rather than use explicit, procedural descriptions, such as chart[s, A, s +1] := chart[s, A, s +1] v chart[s,B, s+ t] A chart[s+t, C, 9+1] A TRUE we will use inference rules such as The meaning of an inference rule is that if the top line is all true, then we can conclude the bottom line.</S>\n    <S sid=\"55\" ssid=\"45\">For instance, this example inference rule can be read as saying that if A BC and B w, wk_i and C wk w/_i, then A .</S>\n    <S sid=\"56\" ssid=\"46\">• The general form for an inference rule will be where if the conditions A1 Ak are all true, then we infer that B is also true.</S>\n    <S sid=\"57\" ssid=\"47\">The A, can be either items, or (in an extension of the usual convention for inference rules) rules, such as R(A BC).</S>\n    <S sid=\"58\" ssid=\"48\">We write R(A BC) rather than A BC to indicate that we could be interested in a value associated with the rule, such as the probability of the rule if we were computing inside probabilities.</S>\n    <S sid=\"59\" ssid=\"49\">If an A, is in the form R(.</S>\n    <S sid=\"60\" ssid=\"50\">.</S>\n    <S sid=\"61\" ssid=\"51\">.</S>\n    <S sid=\"62\" ssid=\"52\">), we call it a rule.</S>\n    <S sid=\"63\" ssid=\"53\">All of the A, must be rules or items; when we wish to refer to both rules and items, we use the word terms.</S>\n    <S sid=\"64\" ssid=\"54\">We now give an example of an item-based description, and its semantics.</S>\n    <S sid=\"65\" ssid=\"55\">Figure 3 gives a description of a CKY-style parser.</S>\n    <S sid=\"66\" ssid=\"56\">For this example, we will use the inside semiring, whose additive operator is addition and whose multiplicative operator is multiplication.</S>\n    <S sid=\"67\" ssid=\"57\">We use the input string xxx to the following grammar: The effect of the unary rule will exactly parallel the first set of loops in the CKY inside algorithm.</S>\n    <S sid=\"68\" ssid=\"58\">We will instantiate the free variables of the unary rule in every possible way.</S>\n    <S sid=\"69\" ssid=\"59\">For instance, we instantiate the free variable i with the value 1, and the free variable A with the nontermirtal X.</S>\n    <S sid=\"70\" ssid=\"60\">Since w1 = x, the instantiated rule is then Because the value of the top line of the instantiated unary rule, R(X x), has value 0.8, we deduce that the bottom line, [1, X, 2], has value 0.8.</S>\n    <S sid=\"71\" ssid=\"61\">We instantiate the rule in two other ways, and compute the following chart values: The effect of the binary rule will parallel the second set of loops for the CKY inside algorithm.</S>\n    <S sid=\"72\" ssid=\"62\">Consider the instantiation i -= 1, k = 2, j = 3, A = X, B = X, C = X, We use the multiplicative operator of the semiring of interest to multiply together the values of the top line, deducing that [I, X, 3] = 0.2 x 0.8 x 0.8 = 0.128.</S>\n    <S sid=\"73\" ssid=\"63\">Similarly, There are two more ways to instantiate the conditions of the binary rule: The first has the value 1 x 0.8 x 0.128 = 0.1024, and the second also has the value 0.1024.</S>\n    <S sid=\"74\" ssid=\"64\">When there is more than one way to derive a value for an item, we use the additive operator of the semiring to sum them up.</S>\n    <S sid=\"75\" ssid=\"65\">Thus, [1, S. 4] = 0.2048.</S>\n    <S sid=\"76\" ssid=\"66\">Since [1, S, 4] is the goal item for the CKY parser, we know that the inside value for xxx is 0.2048.</S>\n    <S sid=\"77\" ssid=\"67\">The goal item exactly parallels the return statement of the CKY inside algorithm.</S>\n    <S sid=\"78\" ssid=\"68\">Many parsers are much more complicated than the CKY parser, and we will need to expand our notation a bit to describe them.</S>\n    <S sid=\"79\" ssid=\"69\">Earley's algorithm (Earley 1970) exhibits most of the complexities we wish to discuss.</S>\n    <S sid=\"80\" ssid=\"70\">Earley's algorithm is often described as a bottom-up parser with top-down filtering.</S>\n    <S sid=\"81\" ssid=\"71\">In a probabilistic framework, the bottomup sections compute probabilities, while the top-down filtering nonprobabilistically removes items that cannot be derived.</S>\n    <S sid=\"82\" ssid=\"72\">To capture these differences, we expand our notation for deduction rules, to the following: Ci • • CI are side conditions, interpreted nonprobabilistically, while A1 • Ak are main conditions with values in whichever semiring we are using.'</S>\n    <S sid=\"83\" ssid=\"73\">While the values of all main conditions are multiplied together to yield the value for the item under the line, the side conditions are interpreted in a Boolean manner: if all of them are nonzero, the rule can be used, but if any of them are zero, it cannot be.</S>\n    <S sid=\"84\" ssid=\"74\">Other than for checking whether they are zero or nonzero, their values are ignored.</S>\n    <S sid=\"85\" ssid=\"75\">Figure 4 gives an item-based description of Earley's parser.</S>\n    <S sid=\"86\" ssid=\"76\">We assume the addition of a distinguished nonterminal S' with a single rule S' S. An item of the form [i, A —&gt; a /3,j] asserts that A =&gt; a/3 4 w, .</S>\n    <S sid=\"87\" ssid=\"77\">.</S>\n    <S sid=\"88\" ssid=\"78\">.</S>\n    <S sid=\"89\" ssid=\"79\">The prediction rule includes a side condition, making it a good example.</S>\n    <S sid=\"90\" ssid=\"80\">The rule is: Through the prediction rule, Earley's algorithm guarantees that an item of the form [j,B • -y, can only be produced if S w1 wi_iBS for some b; this top-down filtering leads to significantly more efficient parsing for some grammars than the CKY algorithm.</S>\n    <S sid=\"91\" ssid=\"81\">The prediction rule combines side and main conditions.</S>\n    <S sid=\"92\" ssid=\"82\">The side condition, [i, A —&gt; a • BO, j], provides the top-down filtering, ensuring that only items that might be used later by the completion rule can be predicted, while the main condition, R(B —&gt; 7), provides the probability of the relevant rule.</S>\n    <S sid=\"93\" ssid=\"83\">The side condition is interpreted in a Boolean fashion, while the main condition's actual probability is used.</S>\n    <S sid=\"94\" ssid=\"84\">Unlike the CKY algorithm, Barley's algorithm can handle grammars with epsilon (e), unary, and n-ary branching rules.</S>\n    <S sid=\"95\" ssid=\"85\">In some cases, this can significantly complicate parsing.</S>\n    <S sid=\"96\" ssid=\"86\">For instance, given unary rules A —+ B and B A, a cycle exists.</S>\n    <S sid=\"97\" ssid=\"87\">This kind of cycle may allow an infinite number of different derivations, requiring an infinite summation to compute the inside probabilities.</S>\n    <S sid=\"98\" ssid=\"88\">The ability of item-based parsers to handle these infinite loops with relative ease is a major attraction.</S>\n    <S sid=\"99\" ssid=\"89\">This paper will simplify the development of new parsers in three important ways.</S>\n    <S sid=\"100\" ssid=\"90\">First, it will simplify specification of parsers: the item-based description is simpler than a procedural description.</S>\n    <S sid=\"101\" ssid=\"91\">Second, it will make it easier to generalize parsers across tasks: a single item-based description can be used to compute values for a variety of applications, simply by changing semirings.</S>\n    <S sid=\"102\" ssid=\"92\">This will be especially advantageous for parsers that can handle loops resulting from rules like A A and computations resulting from € productions, both of which typically lead to infinite sums.</S>\n    <S sid=\"103\" ssid=\"93\">In these cases, the procedure for computing an infinite sum differs from semiring to semiring, and the fact that we can specify that a parser computes an infinite sum separately from its method of computing that sum will be very helpful.</S>\n    <S sid=\"104\" ssid=\"94\">The third use of these techniques is for computing outside probabilities, values related to the inside probabilities that we will define later.</S>\n    <S sid=\"105\" ssid=\"95\">Unlike the other quantities we wish to compute, outside probabilities cannot be computed by simply substituting a different semiring into either an iterative or item-based description.</S>\n    <S sid=\"106\" ssid=\"96\">Instead, we will show how to compute the outside probabilities using a modified interpreter of the same item-based description used for computing the other values.</S>\n    <S sid=\"107\" ssid=\"97\">In the next section, we describe the basics of semiring parsing.</S>\n    <S sid=\"108\" ssid=\"98\">In Section 3, we derive formulas for computing most of the values in semiring parsers, except outside values, and then in Section 4, show how to compute outside values as well. hi Section 5, we give an algorithm for interpreting an item-based description, followed in Section 6 by examples of using semiring parsers to solve a variety of problems.</S>\n    <S sid=\"109\" ssid=\"99\">Section 7 discusses previous work, and Section 8 concludes the paper.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"110\" ssid=\"1\">In this section we first describe the inputs to a semiring parser: a semiring, an itembased description, and a grammar.</S>\n    <S sid=\"111\" ssid=\"2\">Next, we give the conditions under which a semiring parser gives correct results.</S>\n    <S sid=\"112\" ssid=\"3\">At the end of this section we discuss three especially complicated and interesting semirings.</S>\n    <S sid=\"113\" ssid=\"4\">In this subsection, we define and discuss semirings (see Kuich [19971 for an introduction).</S>\n    <S sid=\"114\" ssid=\"5\">A semiring has two operations, ED and 0, that intuitively have most (but not necessarily all) of the properties of the conventional + and x operations on the positive integers.</S>\n    <S sid=\"115\" ssid=\"6\">In particular, we require the following properties: 0 is associative and commutative; 0 is associative and distributes over ED.</S>\n    <S sid=\"116\" ssid=\"7\">If 0 is commutative, we will say that the semiring is commutative.</S>\n    <S sid=\"117\" ssid=\"8\">We assume an additive identity element, which we write as 0, and a multiplicative identity element, which we write as 1.</S>\n    <S sid=\"118\" ssid=\"9\">Both addition and multiplication can be defined over finite sets of elements; if the set is empty, then the value is the respective identity element, 0 or 1.</S>\n    <S sid=\"119\" ssid=\"10\">We also assume that x = x = 0 for all x.</S>\n    <S sid=\"120\" ssid=\"11\">In other words, a semiring is just like a ring, except that the additive operator need not have an inverse.</S>\n    <S sid=\"121\" ssid=\"12\">We will write (A, ED, 0, 0, 1) to indicate a semiring over the set A with additive operator 0, multiplicative operator 0, additive identity 0, and multiplicative identity 1.</S>\n    <S sid=\"122\" ssid=\"13\">For parsers with loops, i.e., those in which an item can be used to derive itself, we will also require that sums of an infinite number of elements be well defined.</S>\n    <S sid=\"123\" ssid=\"14\">In particular, we will require that the semirings be complete (Kuich 1997, 611).</S>\n    <S sid=\"124\" ssid=\"15\">This means that sums of an infinite number of elements should be associative and commutative, just like finite sums, and that multiplication should distribute over infinite sums, just as it does over finite ones.</S>\n    <S sid=\"125\" ssid=\"16\">All of the semirings we will deal with in this paper are complete.2 All of the semirings we discuss here are also co-continuous.</S>\n    <S sid=\"126\" ssid=\"17\">Intuitively, this means that if any partial sum of an infinite sequence is less than or equal to some value, recognition string probability prob. of best derivation number of derivations set of derivations best derivation best n derivations then the infinite sum is also less than or equal to that value.3 This important property makes it easy to compute, or at least approximate, infinite sums.</S>\n    <S sid=\"127\" ssid=\"18\">There will be several especially useful semirings in this paper, which are defined in Figure 5.</S>\n    <S sid=\"128\" ssid=\"19\">We will write rb, to indicate the set of real numbers from a to b inclusive, with similar notation for the natural numbers, N. We will write E to indicate the set of all derivations in some canonical form, and 2E to indicate the set of all sets of derivations in canonical form.</S>\n    <S sid=\"129\" ssid=\"20\">There are three derivation semirings: the derivation forest semiring, the Viterbi-derivation semiring, and the Viterbi-n-best semiring.</S>\n    <S sid=\"130\" ssid=\"21\">The operators used in the derivation semirings (., max, x ,max, and x) will be described later, in Section 2.5.</S>\n    <S sid=\"131\" ssid=\"22\">The inside semiring includes all nonnegative real numbers, to be closed under addition, and includes infinity to be closed under infinite sums, while the Viterbi semiring contains only numbers up to 1, since under max this still leads to closure.</S>\n    <S sid=\"132\" ssid=\"23\">The three derivation forest semirings can be used to find especially important values: the derivation forest semiring computes all derivations of a sentence; the Viterbiderivation semiring computes the most probable derivation; and the Viterbi-n-best semiring computes the n most probable derivations.</S>\n    <S sid=\"133\" ssid=\"24\">A derivation is simply a list of rules from the grammar.</S>\n    <S sid=\"134\" ssid=\"25\">From a derivation, a parse tree can be derived, so the derivation forest semiring is analogous to conventional parse forests.</S>\n    <S sid=\"135\" ssid=\"26\">Unlike the other semirings, all three of these semirings are noncommutative.</S>\n    <S sid=\"136\" ssid=\"27\">The additive operation of these semirings is essentially union or maximum, while the multiplicative operation is essentially concatenation.</S>\n    <S sid=\"137\" ssid=\"28\">These semirings are described in more detail in Section 2.5.</S>\n    <S sid=\"138\" ssid=\"29\">A semiring parser requires an item-based description of the parsing algorithm, in the form given earlier.</S>\n    <S sid=\"139\" ssid=\"30\">So far, we have skipped one important detail of semiring parsing.</S>\n    <S sid=\"140\" ssid=\"31\">In a simple recognition system, as used in deduction systems, all that matters is whether an item can be deduced or not.</S>\n    <S sid=\"141\" ssid=\"32\">Thus, in these simple systems, the order of processing items is relatively unimportant, as long as some simple constraints are met.</S>\n    <S sid=\"142\" ssid=\"33\">On the other hand, for a semiring such as the inside semiring, there are important ordering constraints: we cannot compute the inside value of an item until the inside values of all of its children have been computed.</S>\n    <S sid=\"143\" ssid=\"34\">Thus, we need to impose an ordering on the items, in such a way that no item precedes any item on which it depends.</S>\n    <S sid=\"144\" ssid=\"35\">We will assign each item x to a &amp;quot;bucket&amp;quot; B, writing bucket(x) = B and saying that item x is associated with B.</S>\n    <S sid=\"145\" ssid=\"36\">We order the buckets in such a way that if item y depends on item x, then bucket(x) &lt; bucket(y).</S>\n    <S sid=\"146\" ssid=\"37\">For some pairs of items, it may be that both depend, directly or indirectly, on each other; we associate these items with special &amp;quot;looping&amp;quot; buckets, whose values may require infinite sums to compute.</S>\n    <S sid=\"147\" ssid=\"38\">We will also call a bucket looping if an item associated with it depends on itself.</S>\n    <S sid=\"148\" ssid=\"39\">One way to achieve a bucketing with the required ordering constraints (suggested by Fernando Pereira) is to create a graph of the dependencies, with a node for each item, and an edge from each item x to each item b that depends on it.</S>\n    <S sid=\"149\" ssid=\"40\">We then separate the graph into its strongly connected components (maximal sets of nodes all reachable from each other), and perform a topological sort.</S>\n    <S sid=\"150\" ssid=\"41\">Items forming singleton strongly connected components are associated with their own buckets; items forming nonsingleton strongly connected components are associated with the same looping bucket.</S>\n    <S sid=\"151\" ssid=\"42\">See also Section 5.</S>\n    <S sid=\"152\" ssid=\"43\">Later, when we discuss algorithms for interpreting an item-based description, we will need another concept.</S>\n    <S sid=\"153\" ssid=\"44\">Of all the items associated with a bucket B, we will be able to find derivations for only a subset.</S>\n    <S sid=\"154\" ssid=\"45\">If we can derive an item x associated with bucket B, we write x E B, and say that item x is in bucket B.</S>\n    <S sid=\"155\" ssid=\"46\">For example, the goal item of a parser will almost always be associated with the last bucket; if the sentence is grammatical, the goal item will be in the last bucket, and if it is not grammatical, it will not be.</S>\n    <S sid=\"156\" ssid=\"47\">It will be useful to assume that there is a single, variable-free goal item, and that this goal item does not occur as a condition for any rules.</S>\n    <S sid=\"157\" ssid=\"48\">We could always add a [old-goal] new goal item [goal] and a rule where [old-goal] is[goal] the goal in the original description.</S>\n    <S sid=\"158\" ssid=\"49\">A semiring parser also requires a grammar as input.</S>\n    <S sid=\"159\" ssid=\"50\">We will need a list of rules in the grammar, and a function, R(rule), that gives the value for each rule in the grammar.</S>\n    <S sid=\"160\" ssid=\"51\">This latter function will be semiring-specific.</S>\n    <S sid=\"161\" ssid=\"52\">For instance, for computing the inside and Viterbi probabilities, the value of a grammar rule is just the conditional probability of that rule, or 0 if it is not in the grammar.</S>\n    <S sid=\"162\" ssid=\"53\">For the Boolean semiring, the value is TRUE if the rule is in the grammar, FALSE otherwise.</S>\n    <S sid=\"163\" ssid=\"54\">R(rule) replaces the set of rules R of a conventional grammar description; a rule is in the grammar if R(rule) 0.</S>\n    <S sid=\"164\" ssid=\"55\">We will say that a semiring parser works correctly if, for any grammar, input, and semiring, the value of the input according to the grammar equals the value of the input using the parser.</S>\n    <S sid=\"165\" ssid=\"56\">In this subsection, we will define the value of an input according to the grammar, define the value of an input using the parser, and give a sufficient condition for a semiring parser to work correctly.</S>\n    <S sid=\"166\" ssid=\"57\">From this point onwards, unless we specifically mention otherwise, we will assume that some fixed semiring, item-based description, and grammar have been given, without specifically mentioning which ones.</S>\n    <S sid=\"167\" ssid=\"58\">2.4.1 Value According to Grammar.</S>\n    <S sid=\"168\" ssid=\"59\">Consider a derivation E, consisting of grammar rules el, e2, .</S>\n    <S sid=\"169\" ssid=\"60\">.</S>\n    <S sid=\"170\" ssid=\"61\">,em.</S>\n    <S sid=\"171\" ssid=\"62\">We define the value of the derivation according to the grammar to be simply the product (in the semiring) of the values of the rules used in E: Then we can define the value of a sentence that can be derived using grammar derivations El, E2, , Ek to be: where k is potentially infinite.</S>\n    <S sid=\"172\" ssid=\"63\">In other words, the value of the sentence according to the grammar is the sum of the values of all derivations.</S>\n    <S sid=\"173\" ssid=\"64\">We will assume that in each grammar formalism there is some way to define derivations uniquely; for instance, in CFGs, one way would be using left-most derivations.</S>\n    <S sid=\"174\" ssid=\"65\">For simplicity, we will simply refer to derivations, rather than, for example, left-most derivations, since we are never interested in nonunique derivations.</S>\n    <S sid=\"175\" ssid=\"66\">A short example will help clarify.</S>\n    <S sid=\"176\" ssid=\"67\">We consider the following grammar: tion is S • aAA AGIA aaa, which has value R(S AA) 0 R(A a) 0 R(A AA) 0 R(A a) 0 R(A ---+ a).</S>\n    <S sid=\"177\" ssid=\"68\">The value of the sentence is the sum of the values of the two derivations, 2.4.2 Item Derivations.</S>\n    <S sid=\"178\" ssid=\"69\">Next, we define item derivations, i.e., derivations using the item-based description of the parser.</S>\n    <S sid=\"179\" ssid=\"70\">We define item derivation in such a way that for a correct parser description, there is exactly one item derivation for each grammar derivation.</S>\n    <S sid=\"180\" ssid=\"71\">The value of a sentence using the parser is the sum of the value of all item derivations of the goal item.</S>\n    <S sid=\"181\" ssid=\"72\">Just as with grammar derivations, individual item derivations are finite, but there may be infinitely many item or grammar derivations of a sentence.</S>\n    <S sid=\"182\" ssid=\"73\">We say that al ' b' • ak c1 ci is an instantiation of deduction rule A1 ' • ' —1 .</S>\n    <S sid=\"183\" ssid=\"74\">. whenever the first expression is a variable-free instance of the second; that is, the first expression is the result of consistently substituting constant terms for each variable in the second.</S>\n    <S sid=\"184\" ssid=\"75\">Now, we can define an item derivation tree.</S>\n    <S sid=\"185\" ssid=\"76\">Intuitively, an item derivation Grammar derivation, grammar derivation tree, item derivation tree, and derivation value. tree for x just gives a way of deducing x from the grammar rules.</S>\n    <S sid=\"186\" ssid=\"77\">We define an item derivation tree recursively.</S>\n    <S sid=\"187\" ssid=\"78\">The base case is rules of the grammar: (r) is an item derivation tree, where r is a rule of the grammar.</S>\n    <S sid=\"188\" ssid=\"79\">Also, if D a„ .</S>\n    <S sid=\"189\" ssid=\"80\">.</S>\n    <S sid=\"190\" ssid=\"81\">.</S>\n    <S sid=\"191\" ssid=\"82\">, Da„ , Da, are derivation trees headed by al ak, ci respectively, and if al ' b&amp;quot; ak c1 cj is the instantiation of a deduction rule, then (b: Da„ .</S>\n    <S sid=\"192\" ssid=\"83\">, Dak) is also a derivation tree.</S>\n    <S sid=\"193\" ssid=\"84\">Notice that the D„ .</S>\n    <S sid=\"194\" ssid=\"85\">.</S>\n    <S sid=\"195\" ssid=\"86\">.</S>\n    <S sid=\"196\" ssid=\"87\">Dc, do not occur in this tree: they are side conditions, and although their existence is required to prove that c1 ci could be derived, they do not contribute to the value of the tree.</S>\n    <S sid=\"197\" ssid=\"88\">We will write al • • • ak to indicate that there is an item derivation tree of the form (b: Da„ .</S>\n    <S sid=\"198\" ssid=\"89\">.</S>\n    <S sid=\"199\" ssid=\"90\">.</S>\n    <S sid=\"200\" ssid=\"91\">,Dak).</S>\n    <S sid=\"201\" ssid=\"92\">As mentioned in Section 2.2, we will write x E B if bucket(x) --= B and there is an item derivation tree for x.</S>\n    <S sid=\"202\" ssid=\"93\">We can continue the example of parsing aaa, now using the item-based CKY parser of Figure 3.</S>\n    <S sid=\"203\" ssid=\"94\">There are two item derivation trees for the goal item; in Figure 6, we give the first as an example, displaying it as a tree, rather than with angle bracket notation, for simplicity.</S>\n    <S sid=\"204\" ssid=\"95\">Notice that an item derivation is a tree, not a directed graph.</S>\n    <S sid=\"205\" ssid=\"96\">Thus, an item subderivation could occur multiple times in a given item derivation.</S>\n    <S sid=\"206\" ssid=\"97\">This means that we can have a one-to-one correspondence between item derivations and grammar derivations; loops in the grammar lead to an infinite number of grammar derivations, and an infinite number of corresponding item derivations. would allow derivations such as S AAA = BAA AA = BA A = B €.</S>\n    <S sid=\"207\" ssid=\"98\">We would include the exact same item derivation showing A z B E three times.</S>\n    <S sid=\"208\" ssid=\"99\">Similarly, for a derivation such asA-B-i4-13-A-a, we would have a corresponding item derivation tree that included multiple uses of the A —&gt; B and B —&gt; A rules.</S>\n    <S sid=\"209\" ssid=\"100\">2.4.3 Value of Item Derivation.</S>\n    <S sid=\"210\" ssid=\"101\">The value of an item derivation D, V (D), is the product of the value of its rules, R(r), in the same order that they appear in the item derivation tree.</S>\n    <S sid=\"211\" ssid=\"102\">Since rules occur only in the leaves of item derivation trees, the order is precisely determined.</S>\n    <S sid=\"212\" ssid=\"103\">For an item derivation tree D with rule values d1, d2,.</S>\n    <S sid=\"213\" ssid=\"104\">, d1 as its leaves, Alternatively, we can write this equation recursively as R(D) if D is a rule V(D)= {oki 1 V(D) if v _ (b: D1, Dk) Continuing our example, the value of the item derivation tree of Figure 6 is the same as the value of the first grammar derivation.</S>\n    <S sid=\"214\" ssid=\"105\">Let inner(x) represent the set of all item derivation trees headed by an item x.</S>\n    <S sid=\"215\" ssid=\"106\">Then the value of x is the sum of all the values of all item derivation trees headed by x.</S>\n    <S sid=\"216\" ssid=\"107\">Formally, The value of a sentence is just the value of the goal item, V(goal).</S>\n    <S sid=\"217\" ssid=\"108\">2.4.4 Iso-valued Derivations.</S>\n    <S sid=\"218\" ssid=\"109\">In certain cases, a particular grammar derivation and a particular item derivation will have the same value for any semiring and any rule value function R. In this case, we say that the two derivations are iso-valued.</S>\n    <S sid=\"219\" ssid=\"110\">In particular, if and only if the same rules occur in the same order in both derivations, then their values will always be the same, and they are iso-valued.</S>\n    <S sid=\"220\" ssid=\"111\">In Figure 6, the grammar derivation and item derivation meet this condition.</S>\n    <S sid=\"221\" ssid=\"112\">In some cases, a grammar derivation and an item derivation will have the same value for any commutative semiring and any rule value function.</S>\n    <S sid=\"222\" ssid=\"113\">In this case, we say that the derivations are commutatively iso-valued.</S>\n    <S sid=\"223\" ssid=\"114\">Finishing our example, the value of the goal item given our example sentence is just the sum of the values of the two item-based derivations, 2.4.5 Conditions for Correctness.</S>\n    <S sid=\"224\" ssid=\"115\">We can now specify the conditions for an item-based description to be correct.</S>\n    <S sid=\"225\" ssid=\"116\">Given an item-based description I, if for every grammar G, there exists a one-to-one correspondence between the item derivations using I and the grammar derivations, and the corresponding derivations are iso-valued, then for every complete semiring, the value of a given input w1 wn is the same according to the grammar as the value of the goal item.</S>\n    <S sid=\"226\" ssid=\"117\">(If the semiring is commutative, then the corresponding derivations need only be commutatively iso-valued.)</S>\n    <S sid=\"227\" ssid=\"118\">The proof is very simple; essentially, each term in each sum occurs in the other.</S>\n    <S sid=\"228\" ssid=\"119\">By hypothesis, for a given input, there are grammar derivations E1 Ek (for 0 &lt; k &lt; oo) and corresponding item derivation trees D1 .</S>\n    <S sid=\"229\" ssid=\"120\">.</S>\n    <S sid=\"230\" ssid=\"121\">.</S>\n    <S sid=\"231\" ssid=\"122\">Dk of the goal item.</S>\n    <S sid=\"232\" ssid=\"123\">Since corresponding items are iso-valued, for all i, V(E) = V(DO.</S>\n    <S sid=\"233\" ssid=\"124\">(If the semiring is commutative, then since the items are commutatively iso-valued, it is still the case that for all i, V(E1) = V (D,).)</S>\n    <S sid=\"234\" ssid=\"125\">Now, since the value of the string according to the grammar is just @, V(E) = ED, V(D), and the value of the goal item is el, V (D,), the value of the string according to the grammar equals the value of the goal item.</S>\n    <S sid=\"235\" ssid=\"126\">0 There is one additional condition for an item-based description to be usable in practice, which is that there be only a finite number of derivable items for a given input sentence; there may, however, be an infinite number of derivations of any item.</S>\n    <S sid=\"236\" ssid=\"127\">All of the semirings we use should be familiar, except for the derivation semirings, which we now describe.</S>\n    <S sid=\"237\" ssid=\"128\">These semirings, unlike the other semirings described in Figure 5, are not commutative under their multiplicative operator, concatenation.</S>\n    <S sid=\"238\" ssid=\"129\">In many parsers, it is conventional to compute parse forests: compact representations of the set of trees consistent with the input.</S>\n    <S sid=\"239\" ssid=\"130\">We will use a related concept, derivation forests, a compact representation of the set of derivations consistent with the input, which corresponds to the parse forest for CFGs, but is easily extended to other formalisms.</S>\n    <S sid=\"240\" ssid=\"131\">Often, we will not be interested in the set of all derivations, but only in the most probable derivation.</S>\n    <S sid=\"241\" ssid=\"132\">The Viterbi-derivation semiring computes this value.</S>\n    <S sid=\"242\" ssid=\"133\">Alternatively, we might want the n best derivations, which would be useful if the output of the parser were passed to another stage, such as semantic disambiguation; this value is computed by the Viterbi-n-best derivation semiring.</S>\n    <S sid=\"243\" ssid=\"134\">Notice that each of the derivation semirings can also be used to create transducers.</S>\n    <S sid=\"244\" ssid=\"135\">That is, we simply associate strings rather than grammar rules with each rule value.</S>\n    <S sid=\"245\" ssid=\"136\">Instead of grammar rule concatenation, we perform string concatenation.</S>\n    <S sid=\"246\" ssid=\"137\">The derivation semiring then corresponds to nondeterministic transductions; the Viterbi semiring corresponds to a weighted or probabilistic transducer; and the Viterbi-n-best semiring could be used to get n-best lists from probabilistic transducers. where a derivation is a list of rules of the grammar.'</S>\n    <S sid=\"247\" ssid=\"138\">Sets containing one rule, such as { (X —&gt; YZ)} for a CFG, constitute the primitive elements of the semiring.</S>\n    <S sid=\"248\" ssid=\"139\">The additive operator U produces a union of derivations, and the multiplicative operator produces the concatenation, one derivation concatenated with the next.</S>\n    <S sid=\"249\" ssid=\"140\">The concatenation operation 0 is defined on both derivations and sets of derivations; when applied to a set of derivations, it produces the set of pairwise concatenations.</S>\n    <S sid=\"250\" ssid=\"141\">The additive identity is simply the empty set, 0: union with the empty set is an identity operation.</S>\n    <S sid=\"251\" ssid=\"142\">The multiplicative identity is the set containing the empty derivation, { }: concatenation with the empty derivation is an identity operation.</S>\n    <S sid=\"252\" ssid=\"143\">Derivations need not be complete.</S>\n    <S sid=\"253\" ssid=\"144\">For instance, for CFGs, {(X —&gt; YZ, Y —&gt; y)} is a valid element, as is {(Y —&gt; y, X —&gt; x)}.</S>\n    <S sid=\"254\" ssid=\"145\">In fact, { (X —&gt; A, B —&gt; b)} is a valid element, although it could not occur in a valid grammar derivation, or in a correctly functioning parser.</S>\n    <S sid=\"255\" ssid=\"146\">An example of concatenation Potentially, derivation forests are sets of infinitely many items.</S>\n    <S sid=\"256\" ssid=\"147\">However, it is still possible to store them using finite-sized representations.</S>\n    <S sid=\"257\" ssid=\"148\">Elsewhere (Goodman 1998), we show how to implement derivation forests efficiently, using pointers, in a manner analogous to the typical implementation of parse forests, and also similar to the work of Billot and Lang (1989).</S>\n    <S sid=\"258\" ssid=\"149\">Using these techniques, both union and concatenation can be implemented in constant time, and even infinite unions will be reasonably efficient. probable derivation of the sentence, given a probabilistic grammar.</S>\n    <S sid=\"259\" ssid=\"150\">Elements of this semiring are a pair, a real number v and a derivation forest E, i.e., the set of derivations with score v. We define max, the additive operator, as In typical practical Viterbi parsers, when two derivations have the same value, one of the derivations is arbitrarily chosen.</S>\n    <S sid=\"260\" ssid=\"151\">In practice, this is usually a fine solution, and one that could be used in a real-world implementation of the ideas in this paper, but from a theoretical viewpoint, the arbitrary choice destroys the associative property of the additive operator, max.</S>\n    <S sid=\"261\" ssid=\"152\">To preserve associativity, we keep derivation forests of all elements that tie for best.</S>\n    <S sid=\"262\" ssid=\"153\">The definition for max is only defined for two elements.</S>\n    <S sid=\"263\" ssid=\"154\">Since the operator is associative, it is clear how to define max for any finite number of elements, but we also v need infinite summations to be definited.</S>\n    <S sid=\"264\" ssid=\"155\">We use the supremum, sup: the supremum of a set is the smallest value at least as large as all elements of the set; that is, it is a maximum that is defined in the infinite case.</S>\n    <S sid=\"265\" ssid=\"156\">We can now define max for the case of infinite sums.</S>\n    <S sid=\"266\" ssid=\"157\">Let Vit where E • D represents the concatenation of the two derivation forests. best semiring, which is used for constructing n-best lists.</S>\n    <S sid=\"267\" ssid=\"158\">Intuitively, the value of a string using this semiring will be the n most likely derivations of that string (unless there are fewer than n total derivations.)</S>\n    <S sid=\"268\" ssid=\"159\">In practice, this is actually how a Viterbi-n-best semiring would typically be implemented.</S>\n    <S sid=\"269\" ssid=\"160\">From a theoretical viewpoint, however, this implementation is inadequate, since we must also define infinite sums and be sure that the distributive property holds.</S>\n    <S sid=\"270\" ssid=\"161\">Elsewhere (Goodman 1998), we give a mathematically precise definition of the semiring that handles these cases.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"271\" ssid=\"1\">Recall that the value of an item x is just V(x) = @DEinner(x)11(D), the sum of the values of all derivation trees headed by x.</S>\n    <S sid=\"272\" ssid=\"2\">This definition may require summing over exponentially many or even infinitely many terms.</S>\n    <S sid=\"273\" ssid=\"3\">In this section, we give relatively efficient formulas for computing the values of items.</S>\n    <S sid=\"274\" ssid=\"4\">There are three cases that must be handled.</S>\n    <S sid=\"275\" ssid=\"5\">First is the base case, when x is a rule.</S>\n    <S sid=\"276\" ssid=\"6\">In this case, inner(x) is trivially {(x)}, the set containing the single derivation tree x.</S>\n    <S sid=\"277\" ssid=\"7\">Thus, V(x) GDEmner(x) 17(D) = The second and third cases occur when x is an item.</S>\n    <S sid=\"278\" ssid=\"8\">Recall that each item is associated with a bucket, and that the buckets are ordered.</S>\n    <S sid=\"279\" ssid=\"9\">Each item x is either associated with a nonlooping bucket, in which case its value depends only on the values of items in earlier buckets; or with a looping bucket, in which case its value depends potentially on the values of other items in the same bucket.</S>\n    <S sid=\"280\" ssid=\"10\">In the case when the item is associated with a nonlooping bucket, if we compute items in the same order as their buckets, we can assume that the values of items al ... ak contributing to the value of item b are known.</S>\n    <S sid=\"281\" ssid=\"11\">We give a formula for computing the value of item b that depends only on the values of items in earlier buckets.</S>\n    <S sid=\"282\" ssid=\"12\">For the final case, in which x is associated with a looping bucket, infinite loops may occur, when the value of two items in the same bucket are mutually dependent, or an item depends on its own value.</S>\n    <S sid=\"283\" ssid=\"13\">These infinite loops may require computation of infinite sums.</S>\n    <S sid=\"284\" ssid=\"14\">Still, we can express these infinite sums in a relatively simple form, allowing them to be efficiently computed or approximated.</S>\n    <S sid=\"285\" ssid=\"15\">If an item x is not in a looping bucket, then Let us expand our notion of inner to include deduction rules: inner(a1' al) is the set of all derivation trees of the form (b: (ai .)</S>\n    <S sid=\"286\" ssid=\"16\">(a2 .</S>\n    <S sid=\"287\" ssid=\"17\">.)</S>\n    <S sid=\"288\" ssid=\"18\">.</S>\n    <S sid=\"289\" ssid=\"19\">.</S>\n    <S sid=\"290\" ssid=\"20\">.</S>\n    <S sid=\"291\" ssid=\"21\">(ak .</S>\n    <S sid=\"292\" ssid=\"22\">.)).</S>\n    <S sid=\"293\" ssid=\"23\">For any item derivation tree that is not a simple rule, there is some al ...ak,b such that D E inner(al b&amp;quot; ak).</S>\n    <S sid=\"294\" ssid=\"24\">Thus, for any item x, Substituting this back into Equation 6, we get completing the proof.</S>\n    <S sid=\"295\" ssid=\"25\">0 Now, we address the case in which x is an item in a looping bucket.</S>\n    <S sid=\"296\" ssid=\"26\">This case requires computation of an infinite sum.</S>\n    <S sid=\"297\" ssid=\"27\">We will write out this infinite sum, and discuss how to compute it exactly in all cases, except for one, where we approximate it.</S>\n    <S sid=\"298\" ssid=\"28\">Consider the derivable items x1 ... xn, in some looping bucket B.</S>\n    <S sid=\"299\" ssid=\"29\">If we build up derivation trees incrementally, when we begin processing bucket B, only those trees with no items from bucket B will be available, what we will call zeroth generation derivation trees.</S>\n    <S sid=\"300\" ssid=\"30\">We can put these zeroth generation trees together to form first generation trees, headed by elements in B.</S>\n    <S sid=\"301\" ssid=\"31\">We can combine these first generation trees with each other and with zeroth generation trees to form second generation trees, and so on.</S>\n    <S sid=\"302\" ssid=\"32\">Formally, we define the generation of a derivation tree headed by x in bucket B to be the largest number of items in B we can encounter on a path from the root to a leaf.</S>\n    <S sid=\"303\" ssid=\"33\">Consider the set of all trees of generation at most g headed by x.</S>\n    <S sid=\"304\" ssid=\"34\">Call this set inner&lt;g(x,B).</S>\n    <S sid=\"305\" ssid=\"35\">We can define the &lt;g generation value of an item x in bucket B, V&lt;g(x,B): Intuitively, as g increases, for x E B, inner&lt;g(x,B) becomes closer and closer to inner(x).</S>\n    <S sid=\"306\" ssid=\"36\">That is, the finite sum of values in the former approaches the infinite sum of values in the latter.</S>\n    <S sid=\"307\" ssid=\"37\">For w-continuous semirings (which includes all of the semirings considered in this paper), an infinite sum is equal to the supremum of the partial sums (Kuich 1997, 613).</S>\n    <S sid=\"308\" ssid=\"38\">Thus, It will be easier to compute the supremum if we find a simple formula for V&lt;g(x,B).</S>\n    <S sid=\"309\" ssid=\"39\">Notice that for items x e B, there will be no generation 0 derivations, so V&lt;0(x, B) = 0.</S>\n    <S sid=\"310\" ssid=\"40\">Thus, generation 0 makes a trivial base for a recursive formula.</S>\n    <S sid=\"311\" ssid=\"41\">Now, we can consider the general case: Theorem 3 For x an item in a looping bucket B, and for g &gt; 1, The proof parallels that of Theorem 2 (Goodman 1998).</S>\n    <S sid=\"312\" ssid=\"42\">A formula for V&lt;g(x,B) is useful, but what we really need is specific techniques for computing the supremum, V(x) = supg V&lt;g(x,B).</S>\n    <S sid=\"313\" ssid=\"43\">For all w-continuous semirings, the supremum of iteratively approximating the value of a set of polynomial equations, as we are essentially doing in Equation 7, is equal to the smallest solution to the equations (Kuich 1997, 622).</S>\n    <S sid=\"314\" ssid=\"44\">In particular, consider the equations: where V &lt;0„(x, B) can be thought of as indicating IB I different variables, one for each item x in the looping bucket B.</S>\n    <S sid=\"315\" ssid=\"45\">Equation 7 represents the iterative approximation of Equation 8, and therefore the smallest solution to Equation 8 represents the supremum of Equation 7.</S>\n    <S sid=\"316\" ssid=\"46\">One fact will be useful for several semirings: whenever the values of all items x E B at generation g +1 are the same as the values of all items in the preceding generation, g, they will be the same at all succeeding generations, as well.</S>\n    <S sid=\"317\" ssid=\"47\">Thus, the value at generation g will be the value of the supremum.</S>\n    <S sid=\"318\" ssid=\"48\">Elsewhere (Goodman 1998), we give a trivial proof of this fact.</S>\n    <S sid=\"319\" ssid=\"49\">Now, we can consider various semiring-specific algorithms for computing the supremum.</S>\n    <S sid=\"320\" ssid=\"50\">Most of these algorithms are well known, and we have simply extended them from specific parsers (described in Section 7) to the general case, or from one semiring to another.</S>\n    <S sid=\"321\" ssid=\"51\">Notice in this section the wide variety of different algorithms, one for each semiring, and some of them fairly complicated.</S>\n    <S sid=\"322\" ssid=\"52\">In a conventional system, these algorithms are interweaved with the parsing algorithm, conflating computation of infinite sums with parsing.</S>\n    <S sid=\"323\" ssid=\"53\">The result is algorithms that are both harder to understand, and less portable to other semirings.</S>\n    <S sid=\"324\" ssid=\"54\">We first examine the simplest case, the Boolean semiring.</S>\n    <S sid=\"325\" ssid=\"55\">Notice that whenever a particular item has value TRUE at generation g, it must also have value TRUE at generation g+1, since if the item can be derived in at most g generations then it can certainly be derived in at most g +1 generations.</S>\n    <S sid=\"326\" ssid=\"56\">Thus, since the number of TRUE valued items is nondecreasing, and is at most IBI, eventually the values of all items must not change from one generation to the next.</S>\n    <S sid=\"327\" ssid=\"57\">Therefore, for the Boolean semiring, a simple algorithm suffices: keep computing successive generations, until no change is detected in some generation; the result is the supremum.</S>\n    <S sid=\"328\" ssid=\"58\">We can perform this computation efficiently if we keep track of items that change value in generation g and only examine items that depend on them in generation g+1.</S>\n    <S sid=\"329\" ssid=\"59\">This algorithm is then similar to the algorithm of Shieber, Schabes, and Pereira (1993).</S>\n    <S sid=\"330\" ssid=\"60\">For the counting semiring, the Viterbi semiring, and the derivation forest semiring, we need the concept of a derivation subgraph.</S>\n    <S sid=\"331\" ssid=\"61\">In Section 2.2 we considered the strongly connected components of the dependency graph, consisting of items that for some sentence could possibly depend on each other, and we put these possibly interdependent items together in looping buckets.</S>\n    <S sid=\"332\" ssid=\"62\">For a given sentence and grammar, not all items will have derivations.</S>\n    <S sid=\"333\" ssid=\"63\">We will find the subgraph of the dependency graph of items with derivations, and compute the strongly connected components of this subgraph.</S>\n    <S sid=\"334\" ssid=\"64\">The strongly connected components of this subgraph correspond to loops that actually occur given the sentence and the grammar, as opposed to loops that might occur for some sentence and grammar, given the parser alone.</S>\n    <S sid=\"335\" ssid=\"65\">We call this subgraph the derivation subgraph, and we will say that items in a strongly connected component of the derivation subgraph are part of a loop.</S>\n    <S sid=\"336\" ssid=\"66\">Now, we can discuss the counting semiring (integers under + and x).</S>\n    <S sid=\"337\" ssid=\"67\">In the counting semiring, for each item, there are three cases: the item can be in a loop; the item can depend (directly or indirectly) on an item in a loop; or the item does not depend on loops.</S>\n    <S sid=\"338\" ssid=\"68\">If the item is in a loop or depends on a loop, its value is infinite.</S>\n    <S sid=\"339\" ssid=\"69\">If the item does not depend on a loop in the current bucket, then its value becomes fixed after some generation.</S>\n    <S sid=\"340\" ssid=\"70\">We can now give the algorithm: first, compute successive generations until the set of items in B does not change from one generation to the next.</S>\n    <S sid=\"341\" ssid=\"71\">Next, compute the derivation subgraph, and its strongly connected components.</S>\n    <S sid=\"342\" ssid=\"72\">Items in a strongly connected component (a loop) have an infinite number of derivations, and thus an infinite value.</S>\n    <S sid=\"343\" ssid=\"73\">Compute items that depend directly or indirectly on items in loops: these items also have infinite value.</S>\n    <S sid=\"344\" ssid=\"74\">Any other items can only be derived in finitely many ways using items in the current bucket, so compute successive generations until the values of these items do not change.</S>\n    <S sid=\"345\" ssid=\"75\">The method for solving the infinite summation for the derivation forest semiring depends on the implementation of derivation forests.</S>\n    <S sid=\"346\" ssid=\"76\">Essentially, that representation will use pointers to efficiently represent derivation forests.</S>\n    <S sid=\"347\" ssid=\"77\">Pointers, in various forms, allow one to efficiently represent infinite circular references, either directly (Goodman 1999), or indirectly (Goodman 1998).</S>\n    <S sid=\"348\" ssid=\"78\">Roughly, the algorithm we will use is to compute the derivation subgraph, and then create pointers analogous to the directed edges in the derivation subgraph, including pointers in loops whenever there is a loop in the derivation subgraph (corresponding to an infinite number of derivations).</S>\n    <S sid=\"349\" ssid=\"79\">Details are given elsewhere (Goodman 1998).</S>\n    <S sid=\"350\" ssid=\"80\">As in the finite case, this representation is equivalent to that of Billot and Lang (1989).</S>\n    <S sid=\"351\" ssid=\"81\">For the Viterbi semiring, the algorithm is analogous to the Boolean case.</S>\n    <S sid=\"352\" ssid=\"82\">Derivations using loops in these semirings will always have values no greater than derivations not using loops, since the value with the loop will be the same as some value without the loop, multiplied by some set of rule probabilities that are at most 1.</S>\n    <S sid=\"353\" ssid=\"83\">Since the additive operation is max, these lower (or at most equal) looping derivations do not change the value of an item.</S>\n    <S sid=\"354\" ssid=\"84\">Therefore, we can simply compute successive generations until values fail to change from one iteration to the next.</S>\n    <S sid=\"355\" ssid=\"85\">Now, consider implementations of the Viterbi-derivation semiring in practice, in which we keep only a representative derivation, rather than the whole derivation forest.</S>\n    <S sid=\"356\" ssid=\"86\">In this case, loops do not change values, and we use the same algorithm as for the Viterbi semiring.</S>\n    <S sid=\"357\" ssid=\"87\">In an implementation of the Viterbi-n-best semiring, in practice, loops can change values, but at most n times, so the same algorithm used for the Viterbi semiring still works.</S>\n    <S sid=\"358\" ssid=\"88\">Elsewhere (Goodman 1998), we describe theoretically correct implementations for both the Viterbi-derivation and Viterbin-best semirings that keep all values in the event of ties, preserving addition's associativity.</S>\n    <S sid=\"359\" ssid=\"89\">The last semiring we consider is the inside semiring.</S>\n    <S sid=\"360\" ssid=\"90\">This semiring is the most difficult.</S>\n    <S sid=\"361\" ssid=\"91\">There are two cases of interest, one of which we can solve exactly, and the other of which requires approximations.</S>\n    <S sid=\"362\" ssid=\"92\">In many cases involving looping buckets, all deduction rules will be of the form cf7x, where al and b are items in the looping bucket, and x is either a rule, or an item in a previously computed bucket.</S>\n    <S sid=\"363\" ssid=\"93\">This case corresponds to the items used for deducing singleton productions, such as those Earley's algorithm uses for rules of the form A —&gt; B and B A.</S>\n    <S sid=\"364\" ssid=\"94\">In this case, Equation 8 forms a set of linear equations that can be solved by matrix inversion.</S>\n    <S sid=\"365\" ssid=\"95\">In the more general case, as is likely to happen with epsilon rules, we get a set of nonlinear equations, and must solve them by approximation techniques, such as simply computing successive generations for many iterations.'</S>\n    <S sid=\"366\" ssid=\"96\">Stolcke (1993) provides an excellent discussion of these cases, including a discussion of sparse matrix inversion, useful for speeding up some computations.</S>\n    <S sid=\"367\" ssid=\"97\">5 Note that even in the case where we can only use approximation techniques, this algorithm is relatively efficient.</S>\n    <S sid=\"368\" ssid=\"98\">By assumption, in this case, there is at least one deduction rule with two items in the current generation; thus, the number of deduction trees over which we are summing grows exponentially with the number of generations: a linear amount of computation yields the sum of the values of exponentially many trees.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"369\" ssid=\"1\">The previous section showed how to compute several of the most commonly used values for parsers, including Boolean, inside, Viterbi, counting, and derivation forest values, among others.</S>\n    <S sid=\"370\" ssid=\"2\">Noticeably absent from the list are the outside probabilities, which we define below.</S>\n    <S sid=\"371\" ssid=\"3\">In general, computing outside probabilities is significantly more complicated than computing inside probabilities.</S>\n    <S sid=\"372\" ssid=\"4\">In this section, we show how to compute outside probabilities from the same item-based descriptions used for computing inside values.</S>\n    <S sid=\"373\" ssid=\"5\">Outside probabilities have many uses, including for reestimating grammar probabilities (Baker 1979), for improving parser performance on some criteria (Goodman 1996b), for speeding parsing in some formalisms, such as data-oriented parsing (Goodman 1996a), and for good thresholding algorithms (Goodman 1997).</S>\n    <S sid=\"374\" ssid=\"6\">We will show that by substituting other semirings, we can get values analogous to the outside probabilities for any commutative semiring; elsewhere (Goodman 1998) we have shown that we can get similar values for many noncommutative semirings as well.</S>\n    <S sid=\"375\" ssid=\"7\">We will refer to these analogous quantities as reverse values.</S>\n    <S sid=\"376\" ssid=\"8\">For instance, the quantity analogous to the outside value for the Viterbi semiring will be called the reverse Viterbi value.</S>\n    <S sid=\"377\" ssid=\"9\">Notice that the inside semiring values of a hidden Markov model (1-IMM) correspond to the forward values of HMMs, and the reverse inside values of an HMM correspond to the backwards values.</S>\n    <S sid=\"378\" ssid=\"10\">Compare the outside algorithm (Baker 1979; Lan i and Young 1990), given in Figure 7, to the inside algorithm of Figure 2.</S>\n    <S sid=\"379\" ssid=\"11\">Notice that while the inside and recognition algorithms are very similar, the outside algorithm is quite a bit different.</S>\n    <S sid=\"380\" ssid=\"12\">In particular, while the inside and recognition algorithms looped over items from shortest to longest, the outside algorithm loops over items in the reverse order, from longest to shortest.</S>\n    <S sid=\"381\" ssid=\"13\">Also, compare the inside algorithm's main loop formula to the outside algorithm's main loop formula.</S>\n    <S sid=\"382\" ssid=\"14\">While there is clearly a relationship between the two equations, the exact pattern of the relationship is not obvious.</S>\n    <S sid=\"383\" ssid=\"15\">Notice that the outside formula is about twice as complicated as the inside formula.</S>\n    <S sid=\"384\" ssid=\"16\">This doubled complexity is typical of outside formulas, and partially explains why the item-based description format is so useful: descriptions for the simpler inside values can be developed with relative ease, and then automatically used to compute the twice-as-complicated outside values.'</S>\n    <S sid=\"385\" ssid=\"17\">Item derivation tree of [goal] and outer tree of [b].</S>\n    <S sid=\"386\" ssid=\"18\">For a context-free grammar, using the CKY parser of Figure 3, recall that the inside probability for an item [i, A, j] is P(A --÷ w,. wi.A).</S>\n    <S sid=\"387\" ssid=\"19\">The outside probability for the same item is P(S Wt_lAwf wn).</S>\n    <S sid=\"388\" ssid=\"20\">Thus, the outside probability has the property that when multiplied by the inside probability, it gives the probability that the start symbol generates the sentence using the given item, P(S wlAw wn w1 • • • wn)• This probability equals the sum of the probabilities of all derivations using the given item.</S>\n    <S sid=\"389\" ssid=\"21\">Formally, letting P(D) represent the probability of a particular derivation, and C(D, [i, X, j]) represent the number of occurrences of item [i, X, j] in derivation D (which for some parsers could be more than one if X were part of a loop), The reverse values in general have an analogous meaning.</S>\n    <S sid=\"390\" ssid=\"22\">Let C(D, x) represent the number of occurrences (the count) of item x in item derivation tree D. Then, for an item x, the reverse value Z(x) should have the property Notice that we have multiplied an element of the semiring, V(D), by an integer, C(D, x).</S>\n    <S sid=\"391\" ssid=\"23\">This multiplication is meant to indicate repeated addition, using the additive operator of the semiring.</S>\n    <S sid=\"392\" ssid=\"24\">Thus, for instance, in the Viterbi semiring, multiplying by a count other than 0 has no effect, since x x = max(x, x) = x, while in the inside semiring, it corresponds to actual multiplication.</S>\n    <S sid=\"393\" ssid=\"25\">This value represents the sum of the values of all derivation trees that the item x occurs in; if an item x occurs more than once in a derivation tree D, then the value of D is counted more than once.</S>\n    <S sid=\"394\" ssid=\"26\">To formally define the reverse value of an item x, we must first define the outer trees outer(x).</S>\n    <S sid=\"395\" ssid=\"27\">Consider an item derivation tree of the goal item, containing one or more instances of item x.</S>\n    <S sid=\"396\" ssid=\"28\">Remove one of these instances of x, and its children too, leaving a gap in its place.</S>\n    <S sid=\"397\" ssid=\"29\">This tree is an outer tree of x.</S>\n    <S sid=\"398\" ssid=\"30\">Figure 8 shows an item derivation tree of the goal item, including a subderivation of an item b, derived from terms a1,.</S>\n    <S sid=\"399\" ssid=\"31\">, ak.</S>\n    <S sid=\"400\" ssid=\"32\">It also shows an outer tree of b, with b and its children removed; the spot b was removed from is labeled (b). parse regular grammars, and tend to be less useful.</S>\n    <S sid=\"401\" ssid=\"33\">Thus, in most parsers of interest, k &gt; 1, and the complexity of (at least some) outside equations, when the sum is written out, is at least doubled.</S>\n    <S sid=\"402\" ssid=\"34\">For an outer tree D E outer(x), we define its value, Z(D), to be the product of the values of all rules in D, OrED R(r)• Then, the reverse value of an item can be formally defined as Next, we argue that this last expression equals the expression on the right-hand side of Equation 9, EDD v(D)cp,x).</S>\n    <S sid=\"403\" ssid=\"35\">For an item x, any outer part of an item derivation tree for x can be combined with any inner part to form a complete item derivation tree.</S>\n    <S sid=\"404\" ssid=\"36\">That is, any 0 E outer(x) and any I E inner(x) can be combined to form an item derivation tree D containing x, and any item derivation tree D containing x can be decomposed into such outer and inner trees.</S>\n    <S sid=\"405\" ssid=\"37\">Thus, the list of all combinations of outer and inner trees corresponds exactly to the list of all item derivation trees containing x.</S>\n    <S sid=\"406\" ssid=\"38\">In fact, for an item derivation tree D containing C(D, x) instances of x, there are C(D, x) ways to form D from combinations of outer and inner trees.</S>\n    <S sid=\"407\" ssid=\"39\">Also, notice that for D combined from 0 and I completing the proof.</S>\n    <S sid=\"408\" ssid=\"40\">0 There is a simple, recursive formula for efficiently computing reverse values.</S>\n    <S sid=\"409\" ssid=\"41\">Recall that the basic equation for computing forward values not involved in loops was At this point, for conciseness, we introduce a nonstandard notation.</S>\n    <S sid=\"410\" ssid=\"42\">We will soon be using many sequences of the form 1, 2, .</S>\n    <S sid=\"411\" ssid=\"43\">..,j-2,j-1,j+1,j+ 2, .. k— 1,k.</S>\n    <S sid=\"412\" ssid=\"44\">We denote such sequences by 1, k. By extension, we will also write f(1), (k) to indicate a sequence of the form f (1),f (2), ,f(j — 2),f(j — 1) , f(j + 1), f(j + 2),.</S>\n    <S sid=\"413\" ssid=\"45\">,f (k — 1) , f (k).</S>\n    <S sid=\"414\" ssid=\"46\">Now, we can give a simple formula for computing reverse values Z(x) not involved in loops: Theorem 5 For items x E B where B is nonlooping, The simple case is when x is the goal item.</S>\n    <S sid=\"415\" ssid=\"47\">Since an outer tree of the goal item is a derivation of the goal item, with the goal item and its children removed, and since we assumed in Section 2.2 that the goal item can only appear in the root of a derivation tree, the outer trees of the goal item are all empty.</S>\n    <S sid=\"416\" ssid=\"48\">Thus, As mentioned in Section 2.1, the value of the empty product is the multiplicative identity.</S>\n    <S sid=\"417\" ssid=\"49\">Now, we consider the general case.</S>\n    <S sid=\"418\" ssid=\"50\">We need to expand our concept of outer to include deduction rules, where outer(j, al ak) is an item derivation tree of the goal item with one subtree removed, a subtree headed by al whose parent is b and whose siblings are headed by al, ak.</S>\n    <S sid=\"419\" ssid=\"51\">Notice that for every outer tree D E outer(x), there is exactly one], al,.</S>\n    <S sid=\"420\" ssid=\"52\">, ak, and b such that x = aj and D E outer(j,a1 b* • ak): this corresponds to the deduction rule used at the spot in the tree where the subtree headed by x was deleted.</S>\n    <S sid=\"421\" ssid=\"53\">Figure 9 illustrates the idea of putting together an outer tree of b with inner trees for al, .7)., ak to form an outer tree of x = al. Using this observation, ak,b s.t. al ak A x=aj DEouter ak.)</S>\n    <S sid=\"422\" ssid=\"54\">Combining an outer tree with inner trees to form an outer tree.</S>\n    <S sid=\"423\" ssid=\"55\">Now, consider all of the outer trees outer(j,a1 ak).</S>\n    <S sid=\"424\" ssid=\"56\">For each item derivation tree Dai E inner(ak) and for each outer tree Db E outer(b), there will be one outer tree in the set outer(j,a1 .b.. ak.. ) Similarly, each tree in outer(j,a1 ak) can be decomposed into an outer tree in outer(b) and derivation trees for (21, TL, ak.</S>\n    <S sid=\"425\" ssid=\"57\">Then, j,ai... ak,b s.t. i; al A x—a, completing the general case.</S>\n    <S sid=\"426\" ssid=\"58\">Computing the reverse values for loops is somewhat more complicated, and as in the forward case, requires an infinite sum, and the use of the concept of generation.</S>\n    <S sid=\"427\" ssid=\"59\">We define the generation g of an outer tree D of item x in bucket B to be the number of items in bucket B on the path between the root and the removal point, inclusive.</S>\n    <S sid=\"428\" ssid=\"60\">We can then let Z&lt;g(x,B) represent the sum of the values of all trees headed by x of generation at most g. In the base case, Z&lt;0(x,B) = 0.</S>\n    <S sid=\"429\" ssid=\"61\">For w-continuous semirings, Z&lt;g(x,B) approaches Z(x) as g approaches oo.</S>\n    <S sid=\"430\" ssid=\"62\">We can give a recursive equation for Z&lt;g(x,B) as follows, using a proof similar to that of Theorem 5 (Goodman 1998): For items x E B and g &gt; 1,</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"431\" ssid=\"1\">Executing a semiring parser is fairly simple.</S>\n    <S sid=\"432\" ssid=\"2\">There is, however, one issue that must be dealt with before we can actually begin parsing.</S>\n    <S sid=\"433\" ssid=\"3\">A semiring parser computes the values of items in the order of the buckets they fall into.</S>\n    <S sid=\"434\" ssid=\"4\">Thus, before we can begin parsing, we need to know which items fall into which buckets, and the ordering of those buckets.</S>\n    <S sid=\"435\" ssid=\"5\">There are three approaches to determining the buckets and ordering that we will discuss in this section.</S>\n    <S sid=\"436\" ssid=\"6\">The first approach is a simple, brute-force enumeration of all items, derivable or not, followed by a topological sort.</S>\n    <S sid=\"437\" ssid=\"7\">This approach will have suboptimal time and space complexity for some item-based descriptions.</S>\n    <S sid=\"438\" ssid=\"8\">The second approach is to use an agenda parser in the Boolean semiring to determine the derivable items and their dependencies, and to then perform a topological sort.</S>\n    <S sid=\"439\" ssid=\"9\">This approach has optimal time complexity, but typically suboptimal space complexity.</S>\n    <S sid=\"440\" ssid=\"10\">The final approach is to use bucketing code specific to the item-based interpreter.</S>\n    <S sid=\"441\" ssid=\"11\">This achieves optimal performance for additional programming effort.</S>\n    <S sid=\"442\" ssid=\"12\">The simplest way to determine the bucketing is to simply enumerate all possible items for the given item-based description, grammar, and input sentence.</S>\n    <S sid=\"443\" ssid=\"13\">Then, we compute the strongly connected components and a partial ordering; both steps can be done in time proportional to the number of items plus the number of dependencies (Cormen, Leiserson, and Rivest 1990, Chap.</S>\n    <S sid=\"444\" ssid=\"14\">23).</S>\n    <S sid=\"445\" ssid=\"15\">For some parsers, this technique has optimal time complexity, although poor space complexity In particular, for the CKY algorithm, the time complexity is optimal, but since it requires computing and storing all possible 0(n3) dependencies between the items, it takes significantly more space than the 0(n2) space required in the best implementation.</S>\n    <S sid=\"446\" ssid=\"16\">In general, the brute-force technique raises the space complexity to be the same as the time complexity.</S>\n    <S sid=\"447\" ssid=\"17\">Furthermore, for some algorithms, such as Earley's algorithm, there could be a significant time complexity added as well.</S>\n    <S sid=\"448\" ssid=\"18\">In particular, Earley's algorithm may not need to examine all possible items.</S>\n    <S sid=\"449\" ssid=\"19\">For certain grammars, Earley's algorithm examines only a linear number of items and a linear number of dependencies, even though there are 0(n2) possible items, and 0(n3) possible dependencies.</S>\n    <S sid=\"450\" ssid=\"20\">Thus the brute-force approach would require 0(n3) time and space instead of 0(n) time and space, for these grammars.</S>\n    <S sid=\"451\" ssid=\"21\">The next approach to finding the bucketing solves the time complexity problem.</S>\n    <S sid=\"452\" ssid=\"22\">In this approach, we first parse in the Boolean semiring, using the agenda parser described by Shieber, Schabes, and Pereira (1995), and then we perform a topological sort.</S>\n    <S sid=\"453\" ssid=\"23\">The techniques that Shieber, Schabes, and Pereira use work well for the Boolean semiring, where items only have value TRUE or FALSE, but cannot be used directly for for current := first bucket to last bucket if current is a looping bucket other semirings.</S>\n    <S sid=\"454\" ssid=\"24\">For other semirings, we need to make sure that the values of items are not computed until after the values of all items they depend on are computed.</S>\n    <S sid=\"455\" ssid=\"25\">However, we can use the algorithm of Shieber, Schabes, and Pereira to compute all of the items that are derivable, and to store all of the dependencies between the items.</S>\n    <S sid=\"456\" ssid=\"26\">Then we perform a topological sort on the items.</S>\n    <S sid=\"457\" ssid=\"27\">The time complexity of both the agenda parser and the topological sort will be proportional to the number of dependencies, which will be proportional to the optimal time complexity.</S>\n    <S sid=\"458\" ssid=\"28\">Unfortunately, we still have the space complexity problem, since again, the space used will be proportional to the number of dependencies, rather than to the number of items.</S>\n    <S sid=\"459\" ssid=\"29\">The third approach to bucketing is to create algorithm-specific bucketing code; this results in parsers with both optimal time and optimal space complexity.</S>\n    <S sid=\"460\" ssid=\"30\">For instance, in a CKY-style parser, we can simply create one bucket for each length, and place each item into the bucket for its length.</S>\n    <S sid=\"461\" ssid=\"31\">For some algorithms, such as Earley's algorithm, special-purpose code for bucketing might have to be combined with code to make sure all and only derivable items are considered (using triggering techniques described by Shieber, Schabes, and Pereira) in order to achieve optimal performance.</S>\n    <S sid=\"462\" ssid=\"32\">Once we have the bucketing, the parsing step is fairly simple.</S>\n    <S sid=\"463\" ssid=\"33\">The basic algorithm appears in Figure 10.</S>\n    <S sid=\"464\" ssid=\"34\">We simply loop over each item in each bucket.</S>\n    <S sid=\"465\" ssid=\"35\">There are two types of buckets: looping buckets, and nonlooping buckets.</S>\n    <S sid=\"466\" ssid=\"36\">If the current bucket is a looping bucket, we compute the infinite sum needed to determine the bucket's values; in a working system, we substitute semiring-specific code for this section, as described in Section 3.2.</S>\n    <S sid=\"467\" ssid=\"37\">If the bucket is not a looping bucket, we simply compute all of the possible instantiations that could contribute to the values of items in that bucket.</S>\n    <S sid=\"468\" ssid=\"38\">Finally, we return the value of the goal item.</S>\n    <S sid=\"469\" ssid=\"39\">The reverse semiring parser interpreter is very similar to the forward semiring parser interpreter.</S>\n    <S sid=\"470\" ssid=\"40\">The differences are that in the reverse semiring parser interpreter, we traverse the buckets in reverse order, and we use the formulas for the reverse values, rather than the forward values.</S>\n    <S sid=\"471\" ssid=\"41\">Elsewhere (Goodman 1998), we give a simple inductive proof to show that both interpreters compute the correct values.</S>\n    <S sid=\"472\" ssid=\"42\">There are two other implementation issues.</S>\n    <S sid=\"473\" ssid=\"43\">First, for some parsers, it will be possible to discard some items.</S>\n    <S sid=\"474\" ssid=\"44\">That is, some items serve the role of temporary variables, and can be discarded after they are no longer needed, especially if only the forward values are going to be computed.</S>\n    <S sid=\"475\" ssid=\"45\">Also, some items do not depend on the input string, but only on the rule value function of the grammar.</S>\n    <S sid=\"476\" ssid=\"46\">The values of these items can be precomputed.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"477\" ssid=\"1\">In this section, we survey other results that are described in more detail elsewhere (Goodman 1998), including examples of formalisms that can be parsed using itembased descriptions, and other uses for the technique of semiring parsing.</S>\n    <S sid=\"478\" ssid=\"2\">Nondeterministic finite-state automata (NFAs) and HMMs turn out to be examples of the same underlying formalism, whose values are simply computed in different semirings.</S>\n    <S sid=\"479\" ssid=\"3\">Other semirings lead to other interesting values.</S>\n    <S sid=\"480\" ssid=\"4\">For HMMs, notice that the forward values are simply the forward inside values; the backward values are the reverse values of the inside semiring; and Viterbi values are the forward values of the Viterbi semiring.</S>\n    <S sid=\"481\" ssid=\"5\">For NFAs, we can use the Boolean semiring to determine whether a string is in the language of an NFA; we can use the counting semiring to determine how many state sequences there are in the NFA for a given string; and we can use the derivation forest semiring to get a compact representation of all state sequences in an NFA for an input string.</S>\n    <S sid=\"482\" ssid=\"6\">A single item-based description can be used to find all of these values.</S>\n    <S sid=\"483\" ssid=\"7\">For language modeling, it may be useful to compute the prefix probability of a string.</S>\n    <S sid=\"484\" ssid=\"8\">That is, given a string wn, we may wish to know the total probability of all sentences beginning with that string, where 01 vk represent words that could possibly follow w1 wn.</S>\n    <S sid=\"485\" ssid=\"9\">Jelinek and Lafferty (1991) and Stolcke (1993) both give algorithms for computing these prefix probabilities.</S>\n    <S sid=\"486\" ssid=\"10\">Elsewhere (Goodman 1998), we show how to produce an item-based description of a prefix parser.</S>\n    <S sid=\"487\" ssid=\"11\">There are two main advantages to using an item-based description: ease of derivation, and reusability.</S>\n    <S sid=\"488\" ssid=\"12\">First, the conventional derivations are somewhat complex, requiring a fair amount of inside-semiring-specific mathematics.</S>\n    <S sid=\"489\" ssid=\"13\">In contrast, using item-based descriptions, we only need to derive a parser that has the property that there is one item derivation for each (complete) grammar derivation that would produce the prefix.</S>\n    <S sid=\"490\" ssid=\"14\">The value of any prefix given the parser will then automatically be the sum of all grammar derivations that include that prefix.</S>\n    <S sid=\"491\" ssid=\"15\">The other advantage is that the same description can be used to compute many values, not just the prefix probability.</S>\n    <S sid=\"492\" ssid=\"16\">For instance, we can use this description with the Viterbi-derivation semiring to find the most likely derivation that includes this prefix.</S>\n    <S sid=\"493\" ssid=\"17\">With this most likely derivation, we could begin interpretation of a sentence even before the sentence was finished being spoken to a speech recognition system.</S>\n    <S sid=\"494\" ssid=\"18\">We could even use the Viterbi-n-best semiring to find the n most likely derivations that include this prefix, if we wanted to take into account ambiguities present in parses of the prefix.</S>\n    <S sid=\"495\" ssid=\"19\">There has been quite a bit of previous work on the intersection of formal language theory and algebra, as described by Kuich (1997), among others.</S>\n    <S sid=\"496\" ssid=\"20\">This previous work has made heavy use of the fact that there is a strong correspondence between algebraic equations in certain noncommutative semirings, and CFGs.</S>\n    <S sid=\"497\" ssid=\"21\">This correspondence has made it possible to manipulate algebraic systems, rather than grammar systems, simplifying many operations.</S>\n    <S sid=\"498\" ssid=\"22\">On the other hand, there is an inherent limit to such an approach, namely a limit to context-free systems.</S>\n    <S sid=\"499\" ssid=\"23\">It is then perhaps slightly surprising that we can avoid these limitations, and create item-based descriptions of parsers for weakly context-sensitive grammars, such as tree adjoining grammars (TAGs).</S>\n    <S sid=\"500\" ssid=\"24\">We avoid the limitations of previous approaches using two techniques.</S>\n    <S sid=\"501\" ssid=\"25\">One technique is to compute derivation trees, rather than parse trees, for TAGs.</S>\n    <S sid=\"502\" ssid=\"26\">Computing derivation trees for TAGs is significantly easier than computing parse trees, since the derivation trees are context-free.</S>\n    <S sid=\"503\" ssid=\"27\">The other trick we use is to create a set of equations for each grammar and string length rather than creating a set of equations for each grammar, as earlier formulations did.</S>\n    <S sid=\"504\" ssid=\"28\">Because the number of equations grows with the string length with our technique, we can recognize strings in weakly context-sensitive languages.</S>\n    <S sid=\"505\" ssid=\"29\">Goodman (1998) gives a further explication of this subject, including an item-based description for a simple TAG parser.</S>\n    <S sid=\"506\" ssid=\"30\">Our goal in this section has been to show that item-based descriptions can be used to simply describe almost all parsers of interest.</S>\n    <S sid=\"507\" ssid=\"31\">One parsing algorithm that would seem particularly difficult to describe is Tomita's graph-structured-stack LR parsing algorithm.</S>\n    <S sid=\"508\" ssid=\"32\">This algorithm at first glance bears little resemblance to other parsing algorithms.</S>\n    <S sid=\"509\" ssid=\"33\">Despite this lack of similarity, Sikkel (1993) gives an item-based description for a Tomita-style parser for the Boolean semiring, which is also more efficient than Tomita's algorithm.</S>\n    <S sid=\"510\" ssid=\"34\">Sikkel's parser can be easily converted to our format, where it can be used for w-continuous semirings in general.</S>\n    <S sid=\"511\" ssid=\"35\">Graham, Harrison, and Ruzzo (1980) describe a parser similar to Earley's, but with several speedups that lead to significant improvements.</S>\n    <S sid=\"512\" ssid=\"36\">Essentially, there are three improvements in the GHR parser.</S>\n    <S sid=\"513\" ssid=\"37\">First, epsilon productions are precomputed; second, unary productions are precomputed; and, finally, completion is separated into two steps, allowing better dynamic programming.</S>\n    <S sid=\"514\" ssid=\"38\">Goodman (1998) gives a full item-based description of a GHR parser.</S>\n    <S sid=\"515\" ssid=\"39\">The forward values of many of the items in our parser related to unary and epsilon productions can be computed off-line, once per grammar, which is an idea due to Stolcke (1993).</S>\n    <S sid=\"516\" ssid=\"40\">Since reverse values require entire strings, the reverse values of these items cannot be computed until the input string is known.</S>\n    <S sid=\"517\" ssid=\"41\">Because we use a single item-based description for precomputed items and nonprecomputed items, and for forward and reverse values, this combination of off-line and on-line computation is easily and compactly specified.</S>\n    <S sid=\"518\" ssid=\"42\">We can apply the same techniques to grammar transformations that we have so far applied to parsing.</S>\n    <S sid=\"519\" ssid=\"43\">Consider a grammar transformation, such as the Chomsky normal form (CNF) grammar transformation, which takes a grammar with epsilon, unary, and n-ary branching productions, and converts it into one in which all productions are of the form A —&gt; BC or A —&gt; a.</S>\n    <S sid=\"520\" ssid=\"44\">For any sentence w1 ... wn its value under the original grammar in the Boolean semiring (TRUE if the sentence can be generated by the grammar, FALSE otherwise) is the same as its value under a transformed grammar.</S>\n    <S sid=\"521\" ssid=\"45\">Therefore, we say that this grammar transformation is value preserving under the Boolean semiring.</S>\n    <S sid=\"522\" ssid=\"46\">We can generalize this concept of value preserving to other semirings.</S>\n    <S sid=\"523\" ssid=\"47\">Elsewhere (Goodman 1998), we show that using essentially the same item-based descriptions we have used for parsing, we can specify grammar transformations.</S>\n    <S sid=\"524\" ssid=\"48\">The concept of value preserving grammar transformation is already known in the intersection of formal language theory and algebra (Kuich 1997; Kuich and Salomaa 1986; Teitelbaum 1973).</S>\n    <S sid=\"525\" ssid=\"49\">Our contribution is to show that these value preserving transformations can be written as simple item-based descriptions, allowing the same computational machinery to be used for grammar transformations as is used for parsing, and to some extent showing the relationship between certain grammar transformations and certain parsers, such as that of Graham, Harrison, and Ruzzo (1980).</S>\n    <S sid=\"526\" ssid=\"50\">This uniform method of specifying grammar transformations is similar to, but clearer than, similar techniques used with covering grammars (Nijholt 1980; Leermakers 1989).</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"527\" ssid=\"1\">The previous work in this area is extensive, including work in deductive parsing, work in statistical parsing, and work in the combination of formal language theory and algebra.</S>\n    <S sid=\"528\" ssid=\"2\">This paper can be thought of as synthetic, combining the work in all three areas, although in the course of synthesis, several general formulas have been found, most notably the general formula for reverse values.</S>\n    <S sid=\"529\" ssid=\"3\">A comprehensive examination of all three areas is beyond the scope of this paper, but we can touch on a few significant areas of each.</S>\n    <S sid=\"530\" ssid=\"4\">First, there is the work in deductive parsing.</S>\n    <S sid=\"531\" ssid=\"5\">This work in some sense dates back to Earley (1970), in which the use of items in parsers is introduced.</S>\n    <S sid=\"532\" ssid=\"6\">More recent work (Pereira and Warren 1983; Pereira and Shieber 1987) demonstrates how to use deduction engines for parsing.</S>\n    <S sid=\"533\" ssid=\"7\">Finally, both Shieber, Schabes, and Pereira (1995) and Sikkel (1993) have shown how to specify parsers in a simple, interpretable, item-based format.</S>\n    <S sid=\"534\" ssid=\"8\">This format is roughly the format we have used here, although there are differences due to the fact that their work was strictly in the Boolean semiring.</S>\n    <S sid=\"535\" ssid=\"9\">Work in statistical parsing has also greatly influenced this work.</S>\n    <S sid=\"536\" ssid=\"10\">We can trace this work back to research in HMMs by Baum and his colleagues (Baum and Eagon 1967; Baum 1972).</S>\n    <S sid=\"537\" ssid=\"11\">In particular, the work of Baum developed the concept of backward probabilities (in the inside semiring), as well as many of the techniques for computing in the inside semiring.</S>\n    <S sid=\"538\" ssid=\"12\">Viterbi (1967) developed corresponding algorithms for computing in the Viterbi semiring.</S>\n    <S sid=\"539\" ssid=\"13\">Baker (1979) extended the work of Baum and his colleagues to PCFGs, including to computation of the outside values (or reverse inside values in our terminology).</S>\n    <S sid=\"540\" ssid=\"14\">Baker's work is described by Lan i and Young (1990).</S>\n    <S sid=\"541\" ssid=\"15\">Baker's work was only for PCFGs in CNF, avoiding the need to compute infinite summations.</S>\n    <S sid=\"542\" ssid=\"16\">Jelinek and Lafferty (1991) showed how to compute some of the infinite summations in the inside semiring, those needed to compute the prefix probabilities of PCFGs in CNF.</S>\n    <S sid=\"543\" ssid=\"17\">Stolcke (1993) showed how to use the same techniques to compute inside probabilities for Earley parsing, dealing with the difficult problems of unary transitions, and the more difficult problems of epsilon transitions.</S>\n    <S sid=\"544\" ssid=\"18\">He thus solved all of the important problems encountered in using an item-based parser to compute the inside and outside values (forward and reverse inside values); he also showed how to compute the forward Viterbi values.</S>\n    <S sid=\"545\" ssid=\"19\">The final area of work is in formal language theory and algebra.</S>\n    <S sid=\"546\" ssid=\"20\">Although it is not widely known, there has been quite a bit of work showing how to use formal power series to elegantly derive results in formal language theory, dating back to Chomsky and Schiitzenberger (1963).</S>\n    <S sid=\"547\" ssid=\"21\">The major classic results can be derived in this framework, but with the added benefit that they apply to all commutative w-continuous semirings.</S>\n    <S sid=\"548\" ssid=\"22\">The most accessible introduction to this literature we have found is by Kuich (1997).</S>\n    <S sid=\"549\" ssid=\"23\">There are also books by Salomaa and Soittola (1978) and Kuich and Salomaa (1986).</S>\n    <S sid=\"550\" ssid=\"24\">One piece of work deserves special mention.</S>\n    <S sid=\"551\" ssid=\"25\">Teitelbaum (1973) showed that any semiring could be used in the CKY algorithm, laying the foundation for much of the work that followed.</S>\n    <S sid=\"552\" ssid=\"26\">In summary, this paper synthesizes work from several different related fields, including deductive parsing, statistical parsing, and formal language theory; we emulate and expand on the earlier synthesis of Teitelbaum.</S>\n    <S sid=\"553\" ssid=\"27\">The synthesis here is powerful: by generalizing and integrating many results, we make the computation of a much wider variety of values possible.</S>\n    <S sid=\"554\" ssid=\"28\">There has also been recent similar work by Tendeau (1997b, 1997a).</S>\n    <S sid=\"555\" ssid=\"29\">Tendeau (1997b) gives an Earley-like algorithm that can be adapted to work with complete semirings satisfying certain conditions.</S>\n    <S sid=\"556\" ssid=\"30\">Unlike our version of Earley's algorithm, Tendeau's version requires time O(n1) where L is the length of the longest right-hand side, as opposed to 0(n3) for the classic version, and for our description.</S>\n    <S sid=\"557\" ssid=\"31\">While one could split right-hand sides of rules to make them binary branching, speeding Tendeau's version up, this would then change values in the derivation semirings.</S>\n    <S sid=\"558\" ssid=\"32\">Tendeau (1997b, 1997a) introduces a parse forest semiring, similar to our derivation forest semiring, in that it encodes a parse forest succinctly.</S>\n    <S sid=\"559\" ssid=\"33\">To implement this semiring, Tendeau's version of rule value functions take as their input not only a nonterminal, but also the span that it covers; this is somewhat less elegant than our version.</S>\n    <S sid=\"560\" ssid=\"34\">Tendeau (1997a) gives a generic description for dynamic programming algorithms.</S>\n    <S sid=\"561\" ssid=\"35\">His description is very similar to our item-based descriptions, except that it does not include side conditions.</S>\n    <S sid=\"562\" ssid=\"36\">Thus, algorithms such as Earley's algorithm cannot be described in Tendeau's formalism in a way that captures their efficiency.</S>\n    <S sid=\"563\" ssid=\"37\">There are some similarities between our work and the work of Koller, McAllester, and Pfeffer (1997), who create a general formalism for handling stochastic programs that makes it easy to compute inside and outside probabilities.</S>\n    <S sid=\"564\" ssid=\"38\">While their formalism is more general than item-based descriptions, in that it is a good way to express any stochastic program, it is also less compact than ours for expressing most dynamic programming algorithms.</S>\n    <S sid=\"565\" ssid=\"39\">Our formalism also has advantages for approximating infinite sums, which we can do efficiently, and in some cases exactly.</S>\n    <S sid=\"566\" ssid=\"40\">It would be interesting to try to extend item-based descriptions to capture some of the formalisms covered by Koller, McAllester, and Pfeffer, including Bayes' nets.</S>\n  </SECTION>\n  <SECTION title=\"8.\" number=\"9\">\n    <S sid=\"567\" ssid=\"1\">In this paper, we have given a simple item-based description format that can be used to describe a very wide variety of parsers.</S>\n    <S sid=\"568\" ssid=\"2\">These parsers include the CKY algorithm, Earley's algorithm, prefix probability computation, a TAG parsing algorithm, Graham, Harrison, Ruzzo (GHR) parsing, and HMM computations.</S>\n    <S sid=\"569\" ssid=\"3\">We have shown that this description format makes it easy to find parsers that compute values in any w-continuous semiring.</S>\n    <S sid=\"570\" ssid=\"4\">The same description can be used to find reverse values in commutative wcontinuous semirings, and in many noncommutative ones as well.</S>\n    <S sid=\"571\" ssid=\"5\">This description format can also be used to describe grammar transformations, including transformations to CNF and GNF, which preserve values in any commutative w-continuous semiring.</S>\n    <S sid=\"572\" ssid=\"6\">While theoretical in nature, this paper is of some practical value.</S>\n    <S sid=\"573\" ssid=\"7\">There are three reasons the results of this paper would be used in practice: first, these techniques make computation of the outside values simple and mechanical; second, these techniques make it easy to show that a parser will work in any w-continuous semiring; and third, these techniques isolate computation of infinite sums in a given semiring from the parser specification process.</S>\n    <S sid=\"574\" ssid=\"8\">Perhaps the most useful application of these results is in finding formulas for outside values.</S>\n    <S sid=\"575\" ssid=\"9\">For parsers such as CKY parsers, finding outside formulas is not particularly burdensome, but for complicated parsers such as TAG parsers, GHR parsers, and others, it can require a fair amount of thought to find these equations through conventional reasoning.</S>\n    <S sid=\"576\" ssid=\"10\">With these techniques, the formulas can be found in a simple mechanical way.</S>\n    <S sid=\"577\" ssid=\"11\">The second advantage comes from clarifying the conditions under which a parser can be converted from computing values in the Boolean semiring (a recognizer) to computing values in any w-continuous semiring.</S>\n    <S sid=\"578\" ssid=\"12\">We should note that because in the Boolean semiring, infinite summations can be computed trivially and because repeatedly adding a term does not change results, it is not uncommon for parsers that work in the Boolean semiring to require significant modification for other semirings.</S>\n    <S sid=\"579\" ssid=\"13\">For parsers like CKY parsers, verifying that the parser will work in any semiring is trivial, but for other parsers the conditions are more complex.</S>\n    <S sid=\"580\" ssid=\"14\">With the techniques in this paper, all that is necessary is to show that there is a one-to-one correspondence between item derivations and grammar derivations.</S>\n    <S sid=\"581\" ssid=\"15\">Once that has been shown, any w-continuous semiring can be used.</S>\n    <S sid=\"582\" ssid=\"16\">The third use of this paper is to separate the computation of infinite sums from the main parsing process.</S>\n    <S sid=\"583\" ssid=\"17\">Infinite sums can come from several different phenomena, such as loops from productions of the form A A; productions involving c; and left recursion.</S>\n    <S sid=\"584\" ssid=\"18\">In traditional procedural specifications, the solution to these difficult problems is intermixed with the parser specification, and makes the parser specific to semirings using the same techniques for solving the summations.</S>\n    <S sid=\"585\" ssid=\"19\">It is important to notice that the algorithms for solving these infinite summations vary fairly widely, depending on the semiring.</S>\n    <S sid=\"586\" ssid=\"20\">On the one hand, Boolean infinite summations are nearly trivial to compute.</S>\n    <S sid=\"587\" ssid=\"21\">For other semirings, such as the counting semiring, or derivation forest semiring, more complicated computations are required, including the detection of loops.</S>\n    <S sid=\"588\" ssid=\"22\">Finally, for the inside semiring, in most cases only approximate techniques can be used, although in some cases, matrix inversion can be used.</S>\n    <S sid=\"589\" ssid=\"23\">Thus, the actual parsing algorithm, if specified procedurally, can vary quite a bit depending on the semiring.</S>\n    <S sid=\"590\" ssid=\"24\">On the other hand, using our techniques makes infinite sums easier to deal with in two ways.</S>\n    <S sid=\"591\" ssid=\"25\">First, these difficult problems are separated out, relegated conceptually to the parser interpreter, where they can be ignored by the constructor of the parsing algorithm.</S>\n    <S sid=\"592\" ssid=\"26\">Second, because they are separated out, they can be solved once, rather than again and again.</S>\n    <S sid=\"593\" ssid=\"27\">Both of these advantages make it significantly easier to construct parsers.</S>\n    <S sid=\"594\" ssid=\"28\">Even in the case where, for efficiency, loops are precomputed offline, as in GHR parsing, the same item-based representation and interpreter can be used.</S>\n    <S sid=\"595\" ssid=\"29\">In summary, the techniques of this paper will make it easier to compute outside values, easier to construct parsers that work for any w-continuous semiring, and easier to compute infinite sums in those semirings.</S>\n    <S sid=\"596\" ssid=\"30\">In 1973, Teitelbaum wrote: We have pointed out the relevance of the theory of algebraic power series in noncommuting variables in order to minimize further piecemeal rediscovery (page 199).</S>\n    <S sid=\"597\" ssid=\"31\">Many of the techniques needed to parse in specific semirings continue to be rediscovered, and outside formulas are derived without observation of the basic formulas given here.</S>\n    <S sid=\"598\" ssid=\"32\">We hope this paper will bring about Teitelbaum's wish.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "a20a94e917bf3f197e4e1254b01c53d0",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Semiring Parsing"
                }
            }
        },
        {
            "index": 17,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "For centuries, the deep connection between languages has brought about major discoveries about human communication. In this paper we investigate how this powerful source of information can be exploited for unsupervised language learning. In particular, we study the task of morphological segmentation of multiple languages. We present a nonparametric Bayesian model that jointly induces morpheme segmentations of each language under consideration and at the same time identifies cross-lingual morpheme pator We apply our model to three Semitic languages: Arabic, Hebrew, Aramaic, as well as to English. Our results demonstrate that learning morphological models in tandem reduces error by up to 24% relative to monolingual models. Furthermore, we provide evidence that our joint model achieves better performance when applied to languages from the same family.",
                    "filename": "Unsupervised_Multilingual_Learning_for_Morphological_Segmentation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P08-1084/Documents_xml/P08-1084.xml",
                    "title": "Unsupervised Multilingual Learning for Morphological Segmentation"
                },
                "llm": {
                    "descriptions": [
                        "This paper investigates whether cross-linguistic correspondences can enhance unsupervised language learning, particularly in morphological segmentation, and whether related languages offer more benefits than distant ones for this task. The authors introduce a hierarchical Bayesian model for unsupervised multilingual learning that automatically induces segmentation and morpheme alignment from a multilingual corpus. The model favors segmentations leading to high-frequency morphemes and treats consistently co-occurring bilingual morpheme pairs as abstract morphemes generated by a language-independent process. Tested on a corpus of parallel phrases from the Hebrew Bible and its Arabic, Aramaic, and English translations, the model demonstrates improved accuracy in morphological segmentation. The results indicate that both related and unrelated language pairs benefit from multilingual learning, with related languages showing more significant gains when their common structures are explicitly modeled.",
                        "This paper investigates the potential of using cross-lingual patterns to improve unsupervised morphological segmentation, which is the process of dividing words into morphemes, the smallest meaning-bearing units of language. The authors propose a hierarchical Bayesian model that leverages both similarities and differences across languages to identify morphemes and their alignments in a multilingual context. The model is tested on a corpus of short parallel phrases from the Hebrew Bible and its translations into Arabic, Aramaic, and English. Results indicate that related language pairs, especially when modeled with explicit phonetic correspondences, benefit more from multilingual learning than unrelated pairs, with up to a 24% reduction in relative error over monolingual models. The study suggests that multilingual models can effectively utilize cross-lingual information for unsupervised language learning tasks.",
                        "This paper presents a study on unsupervised multilingual learning for morphological segmentation, exploring the use of cross-lingual correspondences to enhance language analysis. The authors propose a hierarchical Bayesian model capable of inducing morpheme segmentation and alignment across languages by leveraging similarities and differences in linguistic structures. The model performs joint bilingual analysis, aligning morphemes that share common semantic or syntactic functions, while also accounting for language-specific morphemes. Tested on a multilingual corpus of short parallel phrases from the Hebrew Bible and its Arabic, Aramaic, and English translations, the results show that the model reduces segmentation errors by up to 24%, demonstrating the benefits of multilingual learning for both related and unrelated language pairs. The research confirms that exploiting cross-lingual patterns can improve unsupervised morphological segmentation, especially when related languages and their shared structures are modeled explicitly.",
                        "This paper presents a study on unsupervised multilingual learning for morphological segmentation, exploring the automatic division of words into morphemes across different languages. The authors propose a hierarchical Bayesian model that identifies optimal morphemes for two languages while finding compact multilingual representations. The model is designed to leverage both similarities and differences across languages, treating consistent bilingual morpheme pairs as abstract morphemes generated by a language-independent process. Tested on a multilingual corpus of short parallel phrases from the Hebrew Bible and its translations, the model demonstrates that cross-lingual patterns can be exploited to improve segmentation accuracy, with relative error reductions of up to 24%. The findings suggest that related language pairs benefit more from multilingual learning when common structures, such as phonetic correspondences, are explicitly modeled. The paper concludes with the potential application of this approach to other unsupervised linguistic analysis tasks and further exploration of the role of language relatedness."
                    ],
                    "filename": "unsupervised_multilingual_learni-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 124 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Unsupervised Multilingual Learning for Morphological Segmentation</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"7\" ssid=\"1\">For centuries, the deep connection between human languages has fascinated linguists, anthropologists and historians (Eco, 1995).</S>\n    <S sid=\"8\" ssid=\"2\">The study of this connection has made possible major discoveries about human communication: it has revealed the evolution of languages, facilitated the reconstruction of proto-languages, and led to understanding language universals.</S>\n    <S sid=\"9\" ssid=\"3\">The connection between languages should be a powerful source of information for automatic linguistic analysis as well.</S>\n    <S sid=\"10\" ssid=\"4\">In this paper we investigate two questions: (i) Can we exploit cross-lingual correspondences to improve unsupervised language learning?</S>\n    <S sid=\"11\" ssid=\"5\">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>\n    <S sid=\"12\" ssid=\"6\">We study these two questions in the context of unsupervised morphological segmentation, the automatic division of a word into morphemes (the basic units of meaning).</S>\n    <S sid=\"13\" ssid=\"7\">For example, the English word misunderstanding would be segmented into mis understand - ing.</S>\n    <S sid=\"14\" ssid=\"8\">This task is an informative testbed for our exploration, as strong correspondences at the morphological level across various languages have been well-documented (Campbell, 2004).</S>\n    <S sid=\"15\" ssid=\"9\">The model presented in this paper automatically induces a segmentation and morpheme alignment from a multilingual corpus of short parallel phrases.1 For example, given parallel phrases meaning in my land in English, Arabic, Hebrew, and Aramaic, we wish to segment and align morphemes as follows:</S>\n  </SECTION>\n  <SECTION title=\"Arabic: Hebrew: Aramaic:\" number=\"2\">\n    <S sid=\"16\" ssid=\"1\">This example illustrates the potential benefits of unsupervised multilingual learning.</S>\n    <S sid=\"17\" ssid=\"2\">The three Semitic languages use cognates (words derived from a common ancestor) to represent the word land.</S>\n    <S sid=\"18\" ssid=\"3\">They also use an identical suffix (-y) to represent the first person possessive pronoun (my).</S>\n    <S sid=\"19\" ssid=\"4\">These similarities in form should guide the model by constraining the space of joint segmentations.</S>\n    <S sid=\"20\" ssid=\"5\">The corresponding English phrase lacks this resemblance to its Semitic counterparts.</S>\n    <S sid=\"21\" ssid=\"6\">However, in this as in many cases, no segmentation is required for English as all the morphemes are expressed as individual words.</S>\n    <S sid=\"22\" ssid=\"7\">For this reason, English should provide a strong source of disambiguation for highly inflected languages, such as Arabic and Hebrew.</S>\n    <S sid=\"23\" ssid=\"8\">In general, we pose the following question.</S>\n    <S sid=\"24\" ssid=\"9\">In which scenario will multilingual learning be most effective?</S>\n    <S sid=\"25\" ssid=\"10\">Will it be for related languages, which share a common core of linguistic features, or for distant languages, whose linguistic divergence can provide strong sources of disambiguation?</S>\n    <S sid=\"26\" ssid=\"11\">As a first step towards answering this question, we propose a model which can take advantage of both similarities and differences across languages.</S>\n    <S sid=\"27\" ssid=\"12\">This joint bilingual model identifies optimal morphemes for two languages and at the same time finds compact multilingual representations.</S>\n    <S sid=\"28\" ssid=\"13\">For each language in the pair, the model favors segmentations which yield high frequency morphemes.</S>\n    <S sid=\"29\" ssid=\"14\">Moreover, bilingual morpheme pairs which consistently share a common semantic or syntactic function are treated as abstract morphemes, generated by a single language-independent process.</S>\n    <S sid=\"30\" ssid=\"15\">These abstract morphemes are induced automatically by the model from recurring bilingual patterns.</S>\n    <S sid=\"31\" ssid=\"16\">For example, in the case above, the tuple (in, fy, b-, b-) would constitute one of three abstract morphemes in the phrase.</S>\n    <S sid=\"32\" ssid=\"17\">When a morpheme occurs in one language without a direct counterpart in the other language, our model can explain away the stray morpheme as arising through a language-specific process.</S>\n    <S sid=\"33\" ssid=\"18\">To achieve this effect in a probabilistic framework, we formulate a hierarchical Bayesian model with Dirichlet Process priors.</S>\n    <S sid=\"34\" ssid=\"19\">This framework allows us to define priors over the infinite set of possible morphemes in each language.</S>\n    <S sid=\"35\" ssid=\"20\">In addition, we define a prior over abstract morphemes.</S>\n    <S sid=\"36\" ssid=\"21\">This prior can incorporate knowledge of the phonetic relationship between the two alphabets, giving potential cognates greater prior likelihood.</S>\n    <S sid=\"37\" ssid=\"22\">The resulting posterior distributions concentrate their probability mass on a small group of recurring and stable patterns within and between languages.</S>\n    <S sid=\"38\" ssid=\"23\">We test our model on a multilingual corpus of short parallel phrases drawn from the Hebrew Bible and Arabic, Aramaic, and English translations.</S>\n    <S sid=\"39\" ssid=\"24\">The Semitic language family, of which Hebrew, Arabic, and Aramaic are members, is known for a highly productive morphology (Bravmann, 1977).</S>\n    <S sid=\"40\" ssid=\"25\">Our results indicate that cross-lingual patterns can indeed be exploited successfully for the task of unsupervised morphological segmentation.</S>\n    <S sid=\"41\" ssid=\"26\">When modeled in tandem, gains are observed for all language pairs, reducing relative error by as much as 24%.</S>\n    <S sid=\"42\" ssid=\"27\">Furthermore, our experiments show that both related and unrelated language pairs benefit from multilingual learning.</S>\n    <S sid=\"43\" ssid=\"28\">However, when common structures such as phonetic correspondences are explicitly modeled, related languages provide the most benefit.</S>\n  </SECTION>\n  <SECTION title=\"2 Related Work\" number=\"3\">\n    <S sid=\"44\" ssid=\"1\">Multilingual Language Learning Recently, the availability of parallel corpora has spurred research on multilingual analysis for a variety of tasks ranging from morphology to semantic role labeling (Yarowsky et al., 2000; Diab and Resnik, 2002; Xi and Hwa, 2005; Pad´o and Lapata, 2006).</S>\n    <S sid=\"45\" ssid=\"2\">Most of this research assumes that one language has annotations for the task of interest.</S>\n    <S sid=\"46\" ssid=\"3\">Given a parallel corpus, the annotations are projected from this source language to its counterpart, and the resulting annotations are used for supervised training in the target language.</S>\n    <S sid=\"47\" ssid=\"4\">In fact, Rogati et al., (2003) employ this method to learn arabic morphology assuming annotations provided by an English stemmer.</S>\n    <S sid=\"48\" ssid=\"5\">An alternative approach has been proposed by Feldman, Hana and Brew (2004; 2006).</S>\n    <S sid=\"49\" ssid=\"6\">While their approach does not require a parallel corpus it does assume the availability of annotations in one language.</S>\n    <S sid=\"50\" ssid=\"7\">Rather than being fully projected, the source annotations provide co-occurrence statistics used by a model in the resource-poor target language.</S>\n    <S sid=\"51\" ssid=\"8\">The key assumption here is that certain distributional properties are invariant across languages from the same language families.</S>\n    <S sid=\"52\" ssid=\"9\">An example of such a property is the distribution of part-of-speech bigrams.</S>\n    <S sid=\"53\" ssid=\"10\">Hana et al., (2004) demonstrate that adding such statistics from an annotated Czech corpus improves the performance of a Russian part-of-speech tagger over a fully unsupervised version.</S>\n    <S sid=\"54\" ssid=\"11\">The approach presented here differs from previous work in two significant ways.</S>\n    <S sid=\"55\" ssid=\"12\">First, we do not assume supervised data in any of the languages.</S>\n    <S sid=\"56\" ssid=\"13\">Second, we learn a single multilingual model, rather than asymmetrically handling one language at a time.</S>\n    <S sid=\"57\" ssid=\"14\">This design allows us to capitalize on structural regularities across languages for the mutual benefit of each language.</S>\n    <S sid=\"58\" ssid=\"15\">Unsupervised morphology is an active area of research (Schone and Jurafsky, 2000; Goldsmith, 2001; Adler and Elhadad, 2006; Creutz and Lagus, 2007; Dasgupta and Ng, 2007).</S>\n    <S sid=\"59\" ssid=\"16\">Most existing algorithms derive morpheme lexicons by identifying recurring patterns in string distribution.</S>\n    <S sid=\"60\" ssid=\"17\">The goal is to optimize the compactness of the data representation by finding a small lexicon of highly frequent strings.</S>\n    <S sid=\"61\" ssid=\"18\">Our work builds on probabilistic segmentation approaches such as Morfessor (Creutz and Lagus, 2007).</S>\n    <S sid=\"62\" ssid=\"19\">In these approaches, models with short description length are preferred.</S>\n    <S sid=\"63\" ssid=\"20\">Probabilities are computed for both the morpheme lexicon and the representation of the corpus conditioned on the lexicon.</S>\n    <S sid=\"64\" ssid=\"21\">A locally optimal segmentation is identified using a task-specific greedy search.</S>\n    <S sid=\"65\" ssid=\"22\">In contrast to previous approaches, our model induces morphological segmentation for multiple related languages simultaneously.</S>\n    <S sid=\"66\" ssid=\"23\">By representing morphemes abstractly through the simultaneous alignment and segmentation of data in two languages, our algorithm capitalizes on deep connections between morpheme usage across different languages.</S>\n  </SECTION>\n  <SECTION title=\"3 Multilingual Morphological Segmentation\" number=\"4\">\n    <S sid=\"67\" ssid=\"1\">The underlying assumption of our work is that structural commonality across different languages is a powerful source of information for morphological analysis.</S>\n    <S sid=\"68\" ssid=\"2\">In this section, we provide several examples that motivate this assumption.</S>\n    <S sid=\"69\" ssid=\"3\">The main benefit of joint multilingual analysis is that morphological structure ambiguous in one language is sometimes explicitly marked in another language.</S>\n    <S sid=\"70\" ssid=\"4\">For example, in Hebrew, the preposition meaning “in”, b-, is always prefixed to its nominal argument.</S>\n    <S sid=\"71\" ssid=\"5\">On the other hand, in Arabic, the most common corresponding particle is fy, which appears as a separate word.</S>\n    <S sid=\"72\" ssid=\"6\">By modeling crosslingual morpheme alignments while simultaneously segmenting, the model effectively propagates information between languages and in this case would be encouraged to segment the Hebrew prefix b-.</S>\n    <S sid=\"73\" ssid=\"7\">Cognates are another important means of disambiguation in the multilingual setting.</S>\n    <S sid=\"74\" ssid=\"8\">Consider translations of the phrase “...and they wrote it...”: In both languages, the triliteral root ktb is used to express the act of writing.</S>\n    <S sid=\"75\" ssid=\"9\">By considering the two phrases simultaneously, the model can be encouraged to split off the respective Hebrew and Arabic prefixes w- and f- in order to properly align the cognate root ktb.</S>\n    <S sid=\"76\" ssid=\"10\">In the following section, we describe a model that can model both generic cross-lingual patterns (fy and b-), as well as cognates between related languages (ktb for Hebrew and Arabic).</S>\n  </SECTION>\n  <SECTION title=\"4 Model\" number=\"5\">\n    <S sid=\"77\" ssid=\"1\">Overview In order to simultaneously model probabilistic dependencies across languages as well as morpheme distributions within each language, we employ a hierarchical Bayesian model.2 Our segmentation model is based on the notion that stable recurring string patterns within words are indicative of morphemes.</S>\n    <S sid=\"78\" ssid=\"2\">In addition to learning independent morpheme patterns for each language, the model will prefer, when possible, to join together frequently occurring bilingual morpheme pairs into single abstract morphemes.</S>\n    <S sid=\"79\" ssid=\"3\">The model is fully unsupervised and is driven by a preference for stable and high frequency cross-lingual morpheme patterns.</S>\n    <S sid=\"80\" ssid=\"4\">In addition the model can incorporate character-to-character phonetic correspondences between alphabets as prior information, thus allowing the implicit modeling of cognates.</S>\n    <S sid=\"81\" ssid=\"5\">Our aim is to induce a model which concentrates probability on highly frequent patterns while still allowing for the possibility of those previously unseen.</S>\n    <S sid=\"82\" ssid=\"6\">Dirichlet processes are particularly suitable for such conditions.</S>\n    <S sid=\"83\" ssid=\"7\">In this framework, we can encode prior knowledge over the infinite sets of possible morpheme strings as well as abstract morphemes.</S>\n    <S sid=\"84\" ssid=\"8\">Distributions drawn from a Dirichlet process nevertheless produce sparse representations with most probability mass concentrated on a small number of observed and predicted patterns.</S>\n    <S sid=\"85\" ssid=\"9\">Our model utilizes a Dirichlet process prior for each language, as well as for the cross-lingual links (abstract morphemes).</S>\n    <S sid=\"86\" ssid=\"10\">Thus, a distribution over morphemes and morpheme alignments is first drawn from the set of Dirichlet processes and then produces the observed data.</S>\n    <S sid=\"87\" ssid=\"11\">In practice, we never deal with such distributions directly, but rather integrate over them during Gibbs sampling.</S>\n    <S sid=\"88\" ssid=\"12\">In the next section we describe our model’s “generative story” for producing the data we observe.</S>\n    <S sid=\"89\" ssid=\"13\">We formalize our model in the context of two languages £ and F. However, the formulation can be extended to accommodate evidence from multiple languages as well.</S>\n    <S sid=\"90\" ssid=\"14\">We provide an example of parallel phrase generation in Figure 1.</S>\n    <S sid=\"91\" ssid=\"15\">High-level Generative Story We have a parallel corpus of several thousand short phrases in the two languages £ and F. Our model provides a generative story explaining how these parallel phrases were probabilistically created.</S>\n    <S sid=\"92\" ssid=\"16\">The core of the model consists of three components: a distribution A over bilingual morpheme pairs (abstract morphemes), a distribution E over stray morphemes in language £ occurring without a counterpart in language F, and a similar distribution F for stray morphemes in language F. As usual for hierarchical Bayesian models, the generative story begins by drawing the model parameters themselves – in our case the three distributions A, E, and F. These three distributions are drawn from three separate Dirichlet processes, each with appropriately defined base distributions.</S>\n    <S sid=\"93\" ssid=\"17\">The Dirichlet processes ensure that the resulting distributions concentrate their probability mass on a small number of morphemes while holding out reasonable probability for unseen possibilities.</S>\n    <S sid=\"94\" ssid=\"18\">Once A, E, and F have been drawn, we model our parallel corpus of short phrases as a series of independent draws from a phrase-pair generation model.</S>\n    <S sid=\"95\" ssid=\"19\">For each new phrase-pair, the model first chooses the number and type of morphemes to be generated.</S>\n    <S sid=\"96\" ssid=\"20\">In particular, it must choose how many unaligned stray morphemes from language £, unaligned stray morphemes from language F, and abstract morphemes are to compose the parallel phrases.</S>\n    <S sid=\"97\" ssid=\"21\">These three numbers, respectively denoted as m, n, and k, are drawn from a Poisson distribution.</S>\n    <S sid=\"98\" ssid=\"22\">This step is illustrated in Figure 1 part (a).</S>\n    <S sid=\"99\" ssid=\"23\">The model then proceeds to independently draw m language £ morphemes from distribution E, n language-F morphemes from distribution F, and k abstract morphemes from distribution A.</S>\n    <S sid=\"100\" ssid=\"24\">This step is illustrated in part (b) of Figure 1.</S>\n    <S sid=\"101\" ssid=\"25\">The m + k resulting language-£ morphemes are then ordered and fused to form a phrase in language £, and likewise for the n + k resulting languageF morphemes.</S>\n    <S sid=\"102\" ssid=\"26\">The ordering and fusing decisions are modeled as draws from a uniform distribution over the set of all possible orderings and fusings for sizes m, n, and k. These final steps are illustrated in parts (c)-(d) of Figure 1.</S>\n    <S sid=\"103\" ssid=\"27\">Now we describe the model more formally.</S>\n    <S sid=\"104\" ssid=\"28\">Stray Morpheme Distributions Sometimes a morpheme occurs in a phrase in one language without a corresponding foreign language morpheme in the parallel phrase.</S>\n    <S sid=\"105\" ssid=\"29\">We call these “stray morphemes,” and we employ language-specific morpheme distributions to model their generation.</S>\n    <S sid=\"106\" ssid=\"30\">For each language, we draw a distribution over all possible morphemes (finite-length strings composed of characters in the appropriate alphabet) from a Dirichlet process with concentration parameter α and base distribution Pe or Pf respectively: The base distributions Pe and Pf can encode prior knowledge about the properties of morphemes in each of the two languages, such as length and character n-grams.</S>\n    <S sid=\"107\" ssid=\"31\">For simplicity, we use a geometric distribution over the length of the string with a final end-morpheme character.</S>\n    <S sid=\"108\" ssid=\"32\">The distributions E and F which result from the respective Dirichlet processes place most of their probability mass on a small number of morphemes with the degree of concentration As before, the resulting distribution A will give non-zero probability to all abstract morphemes (e, f).</S>\n    <S sid=\"109\" ssid=\"33\">The base distribution acts as a prior on such pairs.</S>\n    <S sid=\"110\" ssid=\"34\">To define we can simply use a mixture of geometric distributions in the lengths of the component morphemes.</S>\n    <S sid=\"111\" ssid=\"35\">However, if the languages £ and are related and the regular phonetic correspondences between the letter in the two alphabets are known, then we can use to assign higher likelihood to potential cognates.</S>\n    <S sid=\"112\" ssid=\"36\">In particular we define the prior f) to be the probabilistic string-edit distance (Ristad and Yianilos, 1998) between and the known phonetic correspondences to parameterize the string-edit model.</S>\n    <S sid=\"113\" ssid=\"37\">In particular, insertion and deletion probabilities are held constant for all characters, and substitution probabilities are determined based on the known sound correspondences.</S>\n    <S sid=\"114\" ssid=\"38\">We report results for both the simple geometric prior as well as the string-edit prior.</S>\n    <S sid=\"115\" ssid=\"39\">Phrase Generation To generate a bilingual parallel phrase, we first draw m, n, and k independently from a Poisson distribution.</S>\n    <S sid=\"116\" ssid=\"40\">These three integers represent the number and type of the morphemes d the number of coupled bilingual morpheme pairs, respectively. controlled by the prior Nevertheless, some nonzero probability is reserved for every possible string.</S>\n    <S sid=\"117\" ssid=\"41\">We note that these single-language morpheme distributions also serve as monolingual segmentation models, and similar models have been successfully applied to the task of word boundary detection (Goldwater et al., 2006).</S>\n    <S sid=\"118\" ssid=\"42\">Abstract Morpheme Distribution To model the connections between morphemes across languages, we further define a model for bilingual morpheme pairs, or abstract morphemes.</S>\n    <S sid=\"119\" ssid=\"43\">This model assigns probabilities to all pairs of morphemes is, all pairs of finite strings from the respective alphabets (e, f).</S>\n    <S sid=\"120\" ssid=\"44\">Intuitively, we wish to assign high probability to pairs of morphemes that play similar syntactic or semantic roles (e.g.</S>\n    <S sid=\"121\" ssid=\"45\">(fy, b-) for in Arabic and Hebrew).</S>\n    <S sid=\"122\" ssid=\"46\">These morpheme pairs can thus be viewed as representing abstract morphemes.</S>\n    <S sid=\"123\" ssid=\"47\">As with the stray morpheme models, we wish to define a distribution which concentrates probability mass on a small number of highly co-occurring morpheme pairs while still holding out some probability for all other pairs.</S>\n    <S sid=\"124\" ssid=\"48\">We define this abstract morpheme model A as a draw from an that compose the parallel phrase, giving the number of stray morphemes in each language £ and an m, n, k ti Given these values, we now draw the appropriate number of stray an d abstract morphemes from the corresponding distributions: e1, ..., em ∼ E f1, ..., fn ∼ F (ei, fi), ..., (ek, f�k) ∼ A The sets of morphemes drawn for each language are then ordered: ˜e1, ..., ˜em+k ∼ ORDER|e1, ..., em, ei, ..., ek ˜f1, ..., ˜fn+k ∼ ORDER|f1, ..., fn, f1, ..., fk Finally the ordered morphemes are fused into the words that form the parallel phrases: To keep the model as simple as possible, we employ uniform distributions over the sets of orderings and fusings.</S>\n    <S sid=\"125\" ssid=\"49\">In other words, given a set of r morphemes (for each language), we define the distribution over permutations of the morphemes to simply be ORDER(·|r) = 1��.</S>\n    <S sid=\"126\" ssid=\"50\">Then, given a fixed morpheme order, we consider fusing each adjacent morpheme into a single word.</S>\n    <S sid=\"127\" ssid=\"51\">Again, we simply model the distribution over the r − 1 fusing decisions uniformly as FUSE(·|r) = 1 2''−1 .</S>\n    <S sid=\"128\" ssid=\"52\">Implicit Alignments Note that nowhere do we explicitly assign probabilities to morpheme alignments between parallel phrases.</S>\n    <S sid=\"129\" ssid=\"53\">However, our model allows morphemes to be generated in precisely one of two ways: as a lone stray morpheme or as part of a bilingual abstract morpheme pair.</S>\n    <S sid=\"130\" ssid=\"54\">Thus, our model implicitly assumes that each morpheme is either unaligned, or aligned to exactly one morpheme in the opposing language.</S>\n    <S sid=\"131\" ssid=\"55\">If we are given a parallel phrase with already segmented morphemes we can easily induce the distribution over alignments implied by our model.</S>\n    <S sid=\"132\" ssid=\"56\">As we will describe in the next section, drawing from these induced alignment distributions plays a crucial role in our inference procedure.</S>\n    <S sid=\"133\" ssid=\"57\">Inference Given our corpus of short parallel bilingual phrases, we wish to make segmentation decisions which yield a set of morphemes with high joint probability.</S>\n    <S sid=\"134\" ssid=\"58\">To assess the probability of a potential morpheme set, we need to marginalize over all possible alignments (i.e. possible abstract morpheme pairings and stray morpheme assignments).</S>\n    <S sid=\"135\" ssid=\"59\">We also need to marginalize over all possible draws of the distributions A, E, and F from their respective Dirichlet process priors.</S>\n    <S sid=\"136\" ssid=\"60\">We achieve these aims by performing Gibbs sampling.</S>\n    <S sid=\"137\" ssid=\"61\">Sampling We follow (Neal, 1998) in the derivation of our blocked and collapsed Gibbs sampler.</S>\n    <S sid=\"138\" ssid=\"62\">Gibbs sampling starts by initializing all random variables to arbitrary starting values.</S>\n    <S sid=\"139\" ssid=\"63\">At each iteration, the sampler selects a random variable Xi, and draws a new value for Xi from the conditional distribution of Xi given the current value of the other variables: P(Xi|X_i).</S>\n    <S sid=\"140\" ssid=\"64\">The stationary distribution of variables derived through this procedure is guaranteed to converge to the true joint distribution of the random variables.</S>\n    <S sid=\"141\" ssid=\"65\">However, if some variables can be jointly sampled, then it may be beneficial to perform block sampling of these variables to speed convergence.</S>\n    <S sid=\"142\" ssid=\"66\">In addition, if a random variable is not of direct interest, we can avoid sampling it directly by marginalizing it out, yielding a collapsed sampler.</S>\n    <S sid=\"143\" ssid=\"67\">We utilize variable blocking by jointly sampling multiple segmentation and alignment decisions.</S>\n    <S sid=\"144\" ssid=\"68\">We also collapse our Gibbs sampler in the standard way, by using predictive posteriors marginalized over all possible draws from the Dirichlet processes (resulting in Chinese Restaurant Processes).</S>\n    <S sid=\"145\" ssid=\"69\">Resampling For each bilingual phrase, we resample each word in the phrase in turn.</S>\n    <S sid=\"146\" ssid=\"70\">For word w in language E, we consider at once all possible segmentations, and for each segmentation all possible alignments.</S>\n    <S sid=\"147\" ssid=\"71\">We keep fixed the previously sampled segmentation decisions for all other words in the phrase as well as sampled alignments involving morphemes in other words.</S>\n    <S sid=\"148\" ssid=\"72\">We are thus considering at once: all possible segmentations of w along with all possible alignments involving morphemes in w with some subset of previously sampled languageF morphemes.3 The sampling formulas are easily derived as products of the relevant Chinese Restaurant Processes (with a minor adjustment to take into account the number of stray and abstract morphemes resulting from each decision).</S>\n    <S sid=\"149\" ssid=\"73\">See (Neal, 1998) for general formulas for Gibbs sampling from distributions with Dirichlet process priors.</S>\n    <S sid=\"150\" ssid=\"74\">All results reported are averaged over five runs using simulated annealing.</S>\n  </SECTION>\n  <SECTION title=\"5 Experimental Set-Up\" number=\"6\">\n    <S sid=\"151\" ssid=\"1\">Morpheme Definition For the purpose of these experiments, we define morphemes to include conjunctions, prepositional and pronominal affixes, plural and dual suffixes, particles, definite articles, and roots.</S>\n    <S sid=\"152\" ssid=\"2\">We do not model cases of infixed morpheme transformations, as those cannot be modeled by linear segmentation.</S>\n    <S sid=\"153\" ssid=\"3\">Dataset As a source of parallel data, we use the Hebrew Bible and translations.</S>\n    <S sid=\"154\" ssid=\"4\">For the Hebrew version, we use an edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006).</S>\n    <S sid=\"155\" ssid=\"5\">This Bible edition is augmented by gold standard morphological analysis (including segmentation) performed by biblical scholars.</S>\n    <S sid=\"156\" ssid=\"6\">For the Arabic, Aramaic, and English versions, fied by augmenting the model with a pair of “morphemeidentity” variables deterministically drawn from each abstract morpheme.</S>\n    <S sid=\"157\" ssid=\"7\">Thus the identity of the drawn morphemes can be retained even while resampling their generation mechanism. we use the Van Dyke Arabic translation,4 Targum Onkelos,5 and the Revised Standard Version (Nelson, 1952), respectively.</S>\n    <S sid=\"158\" ssid=\"8\">We obtained gold standard segmentations of the Arabic translation with a hand-crafted Arabic morphological analyzer which utilizes manually constructed word lists and compatibility rules and is further trained on a large corpus of hand-annotated Arabic data (Habash and Rambow, 2005).</S>\n    <S sid=\"159\" ssid=\"9\">The accuracy of this analyzer is reported to be 94% for full morphological analyses, and 98%-99% when part-of-speech tag accuracy is not included.</S>\n    <S sid=\"160\" ssid=\"10\">We don’t have gold standard segmentations for the English and Aramaic portions of the data, and thus restrict our evaluation to Hebrew and Arabic.</S>\n    <S sid=\"161\" ssid=\"11\">To obtain our corpus of short parallel phrases, we preprocessed each language pair using the Giza++ alignment toolkit.6 Given word alignments for each language pair, we extract a list of phrase pairs that form independent sets in the bipartite alignment graph.</S>\n    <S sid=\"162\" ssid=\"12\">This process allows us to group together phrases like fy s.bah. in Arabic and bbqr in Hebrew while being reasonably certain that all the relevant morphemes are contained in the short extracted phrases.</S>\n    <S sid=\"163\" ssid=\"13\">The number of words in such phrases ranges from one to four words in the Semitic languages and up to six words in English.</S>\n    <S sid=\"164\" ssid=\"14\">Before performing any experiments, a manual inspection of the generated parallel phrases revealed that many infrequent phrase pairs occurred merely as a result of noisy translation and alignment.</S>\n    <S sid=\"165\" ssid=\"15\">Therefore, we eliminated all parallel phrases that occur fewer than five times.</S>\n    <S sid=\"166\" ssid=\"16\">As a result of this process, we obtain 6,139 parallel short phrases in Arabic, Hebrew, Aramaic, and English.</S>\n    <S sid=\"167\" ssid=\"17\">The average number of morphemes per word in the Hebrew data is 1.8 and is 1.7 in Arabic.</S>\n    <S sid=\"168\" ssid=\"18\">For the bilingual models which employs probabilistic string-edit distance as a prior on abstract morphemes, we parameterize the string-edit model with the chart of Semitic consonant relationships listed on page xxiv of (Thackston, 1999).</S>\n    <S sid=\"169\" ssid=\"19\">All pairs of corresponding letters are given equal substitution probability, while all other letter pairs are given substitution probability of zero.</S>\n    <S sid=\"170\" ssid=\"20\">Evaluation Methods Following previous work, we evaluate the performance of our automatic segmentation algorithm using F-score.</S>\n    <S sid=\"171\" ssid=\"21\">This measure is the harmonic mean of recall and precision, which are calculated on the basis of all possible segmentation points.</S>\n    <S sid=\"172\" ssid=\"22\">The evaluation is performed on a random set of 1/5 of the parallel phrases which is unseen during the training phase.</S>\n    <S sid=\"173\" ssid=\"23\">During testing, we do not allow the models to consider any multilingual evidence.</S>\n    <S sid=\"174\" ssid=\"24\">This restriction allows us to simulate future performance on purely monolingual data.</S>\n    <S sid=\"175\" ssid=\"25\">Baselines Our primary purpose is to compare the performance of our bilingual model with its fully monolingual counterpart.</S>\n    <S sid=\"176\" ssid=\"26\">However, to demonstrate the competitiveness of this baseline model, we also provide results using MORFESSOR (Creutz and Lagus, 2007), a state-of-the-art unsupervised system for morphological segmentation.</S>\n    <S sid=\"177\" ssid=\"27\">While developed originally for Finnish, this system has been successfully applied to a range of languages including German, Turkish and English.</S>\n    <S sid=\"178\" ssid=\"28\">The probabilistic formulation of this model is close to our monolingual segmentation model, but it uses a greedy search specifically designed for the segmentation task.</S>\n    <S sid=\"179\" ssid=\"29\">We use the publicly available implementation of this system.</S>\n    <S sid=\"180\" ssid=\"30\">To provide some idea of the inherent difficulty of this segmentation task, we also provide results from a random baseline which makes segmentation decisions based on a coin weighted with the true segmentation frequency.</S>\n  </SECTION>\n  <SECTION title=\"6 Results\" number=\"7\">\n    <S sid=\"181\" ssid=\"1\">Table 1 shows the performance of the various automatic segmentation methods.</S>\n    <S sid=\"182\" ssid=\"2\">The first three rows provide baselines, as mentioned in the previous section.</S>\n    <S sid=\"183\" ssid=\"3\">Our primary baseline is MONOLINGUAL, which is the monolingual counterpart to our model and only uses the language-specific distributions E or F. The next three rows shows the performance of various bilingual models that don’t use character-tocharacter phonetic correspondences to capture cognate information.</S>\n    <S sid=\"184\" ssid=\"4\">We find that with the exception of the HEBREW(+ARAMAIC) pair, the bilingual models show marked improvement over MONOLINGUAL.</S>\n    <S sid=\"185\" ssid=\"5\">We notice that in general, adding English – which has comparatively little morphological ambiguity – is about as useful as adding a more closely related Semitic language.</S>\n    <S sid=\"186\" ssid=\"6\">However, once characterto-character phonetic correspondences are added as an abstract morpheme prior (final two rows), we find the performance of related language pairs outstrips English, reducing relative error over MONOLINGUAL by 10% and 24% for the Hebrew/Arabic pair.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusions and Future Work\" number=\"8\">\n    <S sid=\"187\" ssid=\"1\">We started out by posing two questions: (i) Can we exploit cross-lingual patterns to improve unsupervised analysis?</S>\n    <S sid=\"188\" ssid=\"2\">(ii) Will this joint analysis provide more or less benefit when the languages belong to the same family?</S>\n    <S sid=\"189\" ssid=\"3\">The model and results presented in this paper answer the first question in the affirmative, at least for the task of morphological segmentation.</S>\n    <S sid=\"190\" ssid=\"4\">We also provided some evidence that considering closely related languages may be more beneficial than distant pairs if the model is able to explicitly represent shared language structure (the characterto-character phonetic correspondences in our case).</S>\n    <S sid=\"191\" ssid=\"5\">In the future, we hope to apply similar multilingual models to other core unsupervised analysis tasks, including part-of-speech tagging and grammar induction, and to further investigate the role that language relatedness plays in such models.</S>\n    <S sid=\"192\" ssid=\"6\">7</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "7903b8f167b56c622a27fa8561ca1d4c",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Unsupervised Multilingual Learning for Morphological Segmentation"
                }
            }
        },
        {
            "index": 18,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the of the IHMM are estimated a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the",
                    "filename": "Indirect-HMM-based_Hypothesis_Alignment_for_Combining_Outputs_from_Machine_Translation_Systems",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D08-1011/Documents_xml/D08-1011.xml",
                    "title": "Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems"
                },
                "llm": {
                    "descriptions": [
                        "**Abstract:**\n\nThis paper presents an Indirect Hidden Markov Model (IHMM)-based method for hypothesis alignment in machine translation (MT) system combination. The method addresses the challenges of synonym matching and word ordering during the alignment process by utilizing a similarity model and a distortion model. The similarity model incorporates both semantic and surface word similarity, while the distortion model employs a smoothed distance-based approach for modeling the first-order dependency of word ordering. The IHMM-based alignment method was compared to the Translation Edit Rate (TER)-based method using NIST MT08 Chinese-to-English test data. The results show that the IHMM-based method outperforms the TER-based method, yielding a case sensitive BLEU score that is 1.0 BLEU point higher, and scales effectively when combining a larger number of systems. The paper concludes that the proposed IHMM-based method provides superior performance in MT system combination, achieving the best reported result on the NIST MT08 constrained training track with a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission.",
                        "**Abstract:**\n\nThis paper presents an Indirect Hidden Markov Model (IHMM)-based method for hypothesis alignment in confusion-network-based machine translation (MT) system combination. The IHMM approach aims to address two key challenges in MT hypothesis alignment: the matching of synonymous words and the proper ordering of words with different translations. Unlike traditional HMMs which rely on maximum likelihood estimation and extensive training data, the IHMM parameters are estimated using word semantic similarity, word surface similarity, and a distance-based distortion penalty, leveraging both semantic and surface word similarity for synonym matching and a smoothed distance-based distortion model for word ordering.\n\nThe authors demonstrate the efficacy of their approach by comparing the IHMM-based combination method with a TER-based method on the Chinese-to-English test set of the constrained training track of the 2008 NIST Open MT Evaluation. The IHMM-based method outperformed both the best single system and the TER-based combination method, achieving a superior translation quality as measured by BLEU scores. Furthermore, the paper shows that the IHMM-based system can scale well, producing better results when combining outputs from a larger number of systems. The proposed IHMM-based hypothesis alignment method is thus a robust and effective approach for improving the performance of MT system combinations.",
                        "Abstract:\n\nThis paper presents an Indirect-Hidden Markov Model (IHMM)-based method for hypothesis alignment in confusion-network-based machine translation (MT) system combination. The proposed method addresses the challenges of synonym matching and word ordering, which are critical for aligning multiple MT hypotheses to construct a consensus output. The IHMM uses a similarity model to align synonymous words and a distortion model to handle word reordering. The similarity model incorporates both semantic and surface similarities without relying on large amounts of training data, while the distortion model uses a smoothed distance-based approach to capture the first-order dependency of word ordering.\n\nThe IHMM-based method was evaluated on the Chinese-to-English test set from the constrained training track of the 2008 NIST Open MT Evaluation, where it outperformed the best individual MT system and a TER-based combination method. The IHMM-based system combination achieved a case sensitive BLEU score of 34.82%, which is 3.9 points higher than the best official submission of MT08.\n\nThe results demonstrate that the IHMM-based hypothesis alignment method effectively combines the outputs of multiple MT systems, leading to significant improvements in translation quality. The method can be scaled up to combine more systems, further enhancing the performance of the combined MT output.",
                        "**Abstract:**\n\nThis paper presents an Indirect Hidden Markov Model (IHMM)-based method for hypothesis alignment in machine translation (MT) system combination. The focus is on addressing two key challenges in MT hypothesis alignment: synonym matching and word ordering. The IHMM approach models both synonym matching and word ordering by using a similarity model and a distortion model. The similarity model incorporates both semantic and surface word similarity, while the distortion model uses a smoothed distance-based approach. Unlike traditional HMMs that require large training data, the IHMM parameters are estimated indirectly from various sources, including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The proposed IHMM-based method outperformed the TER-based method on the NIST MT08 Chinese-to-English test set. Furthermore, the method is shown to scale well with the combination of more systems, achieving a case sensitive BLEU score of 34.82, which surpasses the best official submission of MT08 by 3.9 BLEU points. The results demonstrate the effectiveness of integrating both semantic and surface similarity in hypothesis alignment for MT system combination and highlight the potential of the IHMM-based approach for improving MT output quality."
                    ],
                    "filename": "indirecthmmbased_hypothesis_alig-write_xml_paper_abstract",
                    "generation_prompt_nickname": "write_xml_paper_abstract",
                    "generation_prompt_text": "Read the following academic paper (in XML format) and create an abstract for it.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">System combination has been applied successfully to various machine translation tasks.</S>\n    <S sid=\"7\" ssid=\"2\">Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007).</S>\n    <S sid=\"8\" ssid=\"3\">A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores.</S>\n    <S sid=\"9\" ssid=\"4\">The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by using posterior probability estimates, or by using a combination of these measures and other features.</S>\n    <S sid=\"10\" ssid=\"5\">Constructing a confusion network requires choosing one of the hypotheses as the backbone (also called “skeleton” in the literature), and other hypotheses are aligned to it at the word level.</S>\n    <S sid=\"11\" ssid=\"6\">High quality hypothesis alignment is crucial to the performance of the resulting system combination.</S>\n    <S sid=\"12\" ssid=\"7\">However, there are two challenging issues that make MT hypothesis alignment difficult.</S>\n    <S sid=\"13\" ssid=\"8\">First, different hypotheses may use different synonymous words to express the same meaning, and these synonyms need to be aligned to each other.</S>\n    <S sid=\"14\" ssid=\"9\">Second, correct translations may have different word orderings in different hypotheses and these words need to be properly reordered in hypothesis alignment.</S>\n    <S sid=\"15\" ssid=\"10\">In this paper, we propose an indirect hidden Markov model (IHMM) for MT hypothesis alignment.</S>\n    <S sid=\"16\" ssid=\"11\">The HMM provides a way to model both synonym matching and word ordering.</S>\n    <S sid=\"17\" ssid=\"12\">Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distancebased distortion penalty, without using large amount of training data.</S>\n    <S sid=\"18\" ssid=\"13\">Our combined SMT system using the proposed method gave the best result on the Chinese-to-English test in the constrained training track of the 2008 NIST Open MT Evaluation (MT08).</S>\n  </SECTION>\n  <SECTION title=\"2 Confusion-network-based MT system combination\" number=\"2\">\n    <S sid=\"19\" ssid=\"1\">The current state-of-the-art is confusion-networkbased MT system combination as described by Rosti and colleagues (Rosti et al., 2007a, Rosti et al., 2007b).</S>\n    <S sid=\"20\" ssid=\"2\">The major steps are illustrated in Figure 1.</S>\n    <S sid=\"21\" ssid=\"3\">In Fig.</S>\n    <S sid=\"22\" ssid=\"4\">1 (a), hypotheses from different MT systems are first collected.</S>\n    <S sid=\"23\" ssid=\"5\">Then in Fig.</S>\n    <S sid=\"24\" ssid=\"6\">1 (b), one of the hypotheses is selected as the backbone for hypothesis alignment.</S>\n    <S sid=\"25\" ssid=\"7\">This is usually done by a sentence-level minimum Bayes risk (MBR) method which selects a hypothesis that has the minimum average distance compared to all hypotheses.</S>\n    <S sid=\"26\" ssid=\"8\">The backbone determines the word order of the combined output.</S>\n    <S sid=\"27\" ssid=\"9\">Then as illustrated in Fig.</S>\n    <S sid=\"28\" ssid=\"10\">1 (c), all other hypotheses are aligned to the backbone.</S>\n    <S sid=\"29\" ssid=\"11\">Note that in Fig.</S>\n    <S sid=\"30\" ssid=\"12\">1 (c) the symbol ε denotes a null word, which is inserted by the alignment normalization algorithm described in section 3.4.</S>\n    <S sid=\"31\" ssid=\"13\">Fig.</S>\n    <S sid=\"32\" ssid=\"14\">1 (c) also illustrates the handling of synonym alignment (e.g., aligning “car” to “sedan”), and word re-ordering of the hypothesis.</S>\n    <S sid=\"33\" ssid=\"15\">Then in Fig.</S>\n    <S sid=\"34\" ssid=\"16\">1 (d), a confusion network is constructed based on the aligned hypotheses, which consists of a sequence of sets in which each word is aligned to a list of alternative words (including null) in the same set.</S>\n    <S sid=\"35\" ssid=\"17\">Then, a set of global and local features are used to decode the confusion network.</S>\n  </SECTION>\n  <SECTION title=\"3 Indirect-HMM-based Hypothesis Alignment\" number=\"3\">\n    <S sid=\"36\" ssid=\"1\">In confusion-network-based system combination for SMT, a major difficulty is aligning hypotheses to the backbone.</S>\n    <S sid=\"37\" ssid=\"2\">One possible statistical model for word alignment is the HMM, which has been widely used for bilingual word alignment (Vogel et al., 1996, Och and Ney, 2003).</S>\n    <S sid=\"38\" ssid=\"3\">In this paper, we propose an indirect-HMM method for monolingual hypothesis alignment.</S>\n    <S sid=\"39\" ssid=\"4\">Let denote the backbone, �' _ ( ,...,dj) a hypothesis to be aligned to e; , and the alignment that specifies the position of the backbone word aligned to each hypothesis word.</S>\n    <S sid=\"40\" ssid=\"5\">We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence.</S>\n    <S sid=\"41\" ssid=\"6\">We use a first-order HMM, assuming that the emission probability depends only on the backbone word, and the transition probability p(aj I aj_,,I) depends only on the position of the last state and the length of the backbone.</S>\n    <S sid=\"42\" ssid=\"7\">Treating the alignment as hidden variable, the conditional probability that the hypothesis is generated by the backbone is given by As in HMM-based bilingual word alignment (Och and Ney, 2003), we also associate a null with each backbone word to allow generating hypothesis words that do not align to any backbone word.</S>\n    <S sid=\"43\" ssid=\"8\">In HMM-based hypothesis alignment, emission probabilities model the similarity between a backbone word and a hypothesis word, and will be referred to as the similarity model.</S>\n    <S sid=\"44\" ssid=\"9\">The transition probabilities model word reordering, and will be called the distortion model.</S>\n    <S sid=\"45\" ssid=\"10\">The similarity model, which specifies the emission probabilities of the HMM, models the similarity between a backbone word and a hypothesis word.</S>\n    <S sid=\"46\" ssid=\"11\">Since both words are in the same language, the similarity model can be derived based on both semantic similarity and surface similarity, and the overall similarity model is a linear interpolation of the two: where and reflect the semantic and surface similarity between and e; , respectively, and α is the interpolation factor.</S>\n    <S sid=\"47\" ssid=\"12\">Since the semantic similarity between two target words is source-dependent, the semantic similarity model is derived by using the source word sequence as a hidden layer: where is the source sentence.</S>\n    <S sid=\"48\" ssid=\"13\">Moreover, in order to handle the case that two target words are synonyms but neither of them has counter-part in the source sentence, a null is introduced on the source side, which is represented by f0.</S>\n    <S sid=\"49\" ssid=\"14\">The last step in (3) assumes that first ei generates all source words including null.</S>\n    <S sid=\"50\" ssid=\"15\">Then ej’ is generated by all source words including null.</S>\n    <S sid=\"51\" ssid=\"16\">In the common SMT scenario where a large amount of bilingual parallel data is available, we can estimate the translation probabilities from a source word to a target word and vice versa via conventional bilingual word alignment.</S>\n    <S sid=\"52\" ssid=\"17\">Then both p(fk I e;) and in (3) can be derived: where is the translation model from the source-to-target word alignment model, and p(fk I e;) , which enforces the sum-to-1 constraint over all words in the source sentence, takes the following form, where A2s (fk I e;) is the translation model from the target-to-source word alignment model.</S>\n    <S sid=\"53\" ssid=\"18\">In our method, A2s (null I e;) for all target words is simply a constant pnull, whose value is optimized on held-out data 1.</S>\n    <S sid=\"54\" ssid=\"19\">The surface similarity model can be estimated in several ways.</S>\n    <S sid=\"55\" ssid=\"20\">A very simple model could be based on exact match: the surface similarity model, per,,.</S>\n    <S sid=\"56\" ssid=\"21\">(elj I , would take the value 1.0 if e’= e, and 0 otherwise 2 .</S>\n    <S sid=\"57\" ssid=\"22\">However, a smoothed surface similarity model is used in our method.</S>\n    <S sid=\"58\" ssid=\"23\">If the target language uses alphabetic orthography, as English does, we treat words as letter sequences and the similarity measure can be the length of the longest matched prefix (LMP) or the length of the longest common subsequence (LCS) between them.</S>\n    <S sid=\"59\" ssid=\"24\">Then, this raw similarity measure is transformed to a surface similarity score between 0 and 1 through an exponential mapping, where is computed as and is the raw similarity measure of ej’ ei, which is the length of the LMP or LCS of ej’ and ei. and p is a smoothing factor that characterizes the mapping, Thus as p approaches infinity, backs off to the exact match model.</S>\n    <S sid=\"60\" ssid=\"25\">We found the smoothed similarity model of (4) yields slightly better results than the exact match model.</S>\n    <S sid=\"61\" ssid=\"26\">Both LMP- and LCS- based methods achieve similar performance but the computation of LMP is faster.</S>\n    <S sid=\"62\" ssid=\"27\">Therefore, we only report results of the LMP-based smoothed similarity model.</S>\n    <S sid=\"63\" ssid=\"28\">The distortion model, which specifies the transition probabilities of the HMM, models the first-order dependencies of word ordering.</S>\n    <S sid=\"64\" ssid=\"29\">In bilingual HMM-based word alignment, it is commonly assumed that transition probabilities Following Och and Ney (2003), we use a fixed value p0 for the probability of jumping to a null state, which can be optimized on held-out data, and the overall distortion model becomes As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets.</S>\n    <S sid=\"65\" ssid=\"30\">In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6).</S>\n    <S sid=\"66\" ssid=\"31\">The probability mass for transitions with jump distance larger than 6 and less than -4 is uniformly divided.</S>\n    <S sid=\"67\" ssid=\"32\">By doing this, only a handful of c(d) parameters need to be estimated.</S>\n    <S sid=\"68\" ssid=\"33\">Although it is possible to estimate them using the EM algorithm on a small development set, we found that a particularly simple model, described below, works surprisingly well in our experiments.</S>\n    <S sid=\"69\" ssid=\"34\">Since both the backbone and the hypothesis are in the same language, It seems intuitive that the distortion model should favor monotonic alignment and only allow non-monotonic alignment with a certain penalty.</S>\n    <S sid=\"70\" ssid=\"35\">This leads us to use a distortion model of the following form, where K is a tuning factor optimized on held-out data.</S>\n    <S sid=\"71\" ssid=\"36\">As shown in Fig.</S>\n    <S sid=\"72\" ssid=\"37\">2, the value of distortion score peaks at d=1, i.e., the monotonic alignment, and decays for non-monotonic alignments depending on how far it diverges from the monotonic alignment.</S>\n    <S sid=\"73\" ssid=\"38\">Given an HMM, the Viterbi alignment algorithm can be applied to find the best alignment between the backbone and the hypothesis, However, the alignment produced by the algorithm cannot be used directly to build a confusion network.</S>\n    <S sid=\"74\" ssid=\"39\">There are two reasons for this.</S>\n    <S sid=\"75\" ssid=\"40\">First, the alignment produced may contain 1-N mappings between the backbone and the hypothesis whereas 1-1 mappings are required in order to build a confusion network.</S>\n    <S sid=\"76\" ssid=\"41\">Second, if hypothesis words are aligned to a null in the backbone or vice versa, we need to insert actual nulls into the right places in the hypothesis and the backbone, respectively.</S>\n    <S sid=\"77\" ssid=\"42\">Therefore, we need to normalize the alignment produced by Viterbi search.</S>\n    <S sid=\"78\" ssid=\"43\">First, whenever more than one hypothesis words are aligned to one backbone word, we keep the link which gives the highest occupation probability computed via the forward-backward algorithm.</S>\n    <S sid=\"79\" ssid=\"44\">The other hypothesis words originally aligned to the backbone word will be aligned to the null associated with that backbone word.</S>\n    <S sid=\"80\" ssid=\"45\">Second, for the hypothesis words that are aligned to a particular null on the backbone side, a set of nulls are inserted around that backbone word associated with the null such that no links cross each other.</S>\n    <S sid=\"81\" ssid=\"46\">As illustrated in Fig.</S>\n    <S sid=\"82\" ssid=\"47\">3 (a), if a hypothesis word e2’ is aligned to the backbone word e2, a null is inserted in front of the backbone word e2 linked to the hypothesis word e1’ that comes before e2’.</S>\n    <S sid=\"83\" ssid=\"48\">Nulls are also inserted for other hypothesis words such as e3’ and e4’ after the backbone word e2.</S>\n    <S sid=\"84\" ssid=\"49\">If there is no hypothesis word aligned to that backbone word, all nulls are inserted after that backbone word .3 For a backbone word that is aligned to no hypothesis word, a null is inserted on the hypothesis side, right after the hypothesis word which is aligned to the immediately preceding backbone word.</S>\n    <S sid=\"85\" ssid=\"50\">An example is shown in Fig.</S>\n    <S sid=\"86\" ssid=\"51\">3 (b).</S>\n  </SECTION>\n  <SECTION title=\"4 Related work\" number=\"4\">\n    <S sid=\"87\" ssid=\"1\">The two main hypothesis alignment methods for system combination in the previous literature are GIZA++ and TER-based methods.</S>\n    <S sid=\"88\" ssid=\"2\">Matusov et al. (2006) proposed using GIZA++ to align words between different MT hypotheses, where all hypotheses of the test corpus are collected to create hypothesis pairs for GIZA++ training.</S>\n    <S sid=\"89\" ssid=\"3\">This approach uses the conventional HMM model bootstrapped from IBM Model-1 as implemented in GIZA++, and heuristically combines results from aligning in both directions.</S>\n    <S sid=\"90\" ssid=\"4\">System combination based on this approach gives an improvement over the best single system.</S>\n    <S sid=\"91\" ssid=\"5\">However, the number of hypothesis pairs for training is limited by the size of the test corpus.</S>\n    <S sid=\"92\" ssid=\"6\">Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples.</S>\n    <S sid=\"93\" ssid=\"7\">Therefore, GIZA++ training on such a data set may be unreliable.</S>\n    <S sid=\"94\" ssid=\"8\">Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al.</S>\n    <S sid=\"95\" ssid=\"9\">(2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment.</S>\n    <S sid=\"96\" ssid=\"10\">TER (Snover et al., 2006) measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis.</S>\n    <S sid=\"97\" ssid=\"11\">The best alignment is the one that gives the minimum number of translation edits.</S>\n    <S sid=\"98\" ssid=\"12\">TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007).</S>\n    <S sid=\"99\" ssid=\"13\">However, when searching for the optimal alignment, the TER-based method uses a strict surface hard match for counting edits.</S>\n    <S sid=\"100\" ssid=\"14\">Therefore, it is not able to handle synonym matching well.</S>\n    <S sid=\"101\" ssid=\"15\">Moreover, although TER-based alignment allows phrase shifts to accommodate the non-monotonic word ordering, all non-monotonic shifts are penalized equally no matter how short or how long the move is, and this penalty is set to be the same as that for substitution, deletion, and insertion edits.</S>\n    <S sid=\"102\" ssid=\"16\">Therefore, its modeling of non-monotonic word ordering is very coarse-grained.</S>\n    <S sid=\"103\" ssid=\"17\">In contrast to the GIZA++-based method, our IHMM-based method has a similarity model estimated using bilingual word alignment HMMs that are trained on a large amount of bi-text data.</S>\n    <S sid=\"104\" ssid=\"18\">Moreover, the surface similarity information is explicitly incorporated in our model, while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006).</S>\n    <S sid=\"105\" ssid=\"19\">On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model.</S>\n    <S sid=\"106\" ssid=\"20\">There have been other hypothesis alignment methods.</S>\n    <S sid=\"107\" ssid=\"21\">Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al.</S>\n    <S sid=\"108\" ssid=\"22\">(2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005).</S>\n  </SECTION>\n  <SECTION title=\"5 Evaluation\" number=\"5\">\n    <S sid=\"109\" ssid=\"1\">In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008).</S>\n    <S sid=\"110\" ssid=\"2\">We compare to the TER-based method used by Rosti et al. (2007).</S>\n    <S sid=\"111\" ssid=\"3\">In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections.</S>\n    <S sid=\"112\" ssid=\"4\">In our implementation, the backbone is selected with MBR.</S>\n    <S sid=\"113\" ssid=\"5\">Only the top hypothesis from each single system is considered as a backbone.</S>\n    <S sid=\"114\" ssid=\"6\">A uniform posteriori probability is assigned to all hypotheses.</S>\n    <S sid=\"115\" ssid=\"7\">TER is used as loss function in the MBR computation.</S>\n    <S sid=\"116\" ssid=\"8\">Similar to (Rosti et al., 2007), each word in the confusion network is associated with a word posterior probability.</S>\n    <S sid=\"117\" ssid=\"9\">Given a system S, each of its hypotheses is assigned with a rank-based score of 1/(1+r)η, where r is the rank of the hypothesis, and η is a rank smoothing parameter.</S>\n    <S sid=\"118\" ssid=\"10\">The system specific rank-based score of a word w for a given system S is the sum of all the rank-based scores of the hypotheses in system S that contain the word w at the given position (after hypothesis alignment).</S>\n    <S sid=\"119\" ssid=\"11\">This score is then normalized by the sum of the scores of all the alternative words at the same position and from the same system S to generate the system specific word posterior.</S>\n    <S sid=\"120\" ssid=\"12\">Then, the total word posterior of w over all systems is a sum of these system specific posteriors weighted by system weights.</S>\n    <S sid=\"121\" ssid=\"13\">Beside the word posteriors, we use language model scores and a word count as features for confusion network decoding.</S>\n    <S sid=\"122\" ssid=\"14\">Therefore, for an M-way system combination that uses N LMs, a total of M+N+1 decoding parameters, including M-1 system weights, one rank smoothing factor, N language model weights, and one weight for the word count feature, are optimized using Powell’s method (Brent, 1973) to maximize BLEU score on a development set4 .</S>\n    <S sid=\"123\" ssid=\"15\">Two language models are used in our experiments.</S>\n    <S sid=\"124\" ssid=\"16\">One is a trigram model estimated from the English side of the parallel training data, and the other is a 5-gram model trained on the English GigaWord corpus from LDC using the MSRLM toolkit (Nguyen et al, 2007).</S>\n    <S sid=\"125\" ssid=\"17\">4 The parameters of IHMM are not tuned by maximum-BLEU training.</S>\n    <S sid=\"126\" ssid=\"18\">In order to reduce the fluctuation of BLEU scores caused by the inconsistent translation output length, an unsupervised length adaptation method has been devised.</S>\n    <S sid=\"127\" ssid=\"19\">We compute an expected length ratio between the MT output and the source sentences on the development set after maximumBLEU training.</S>\n    <S sid=\"128\" ssid=\"20\">Then during test, we adapt the length of the translation output by adjusting the weight of the word count feature such that the expected output/source length ratio is met.</S>\n    <S sid=\"129\" ssid=\"21\">In our experiments, we apply length adaptation to the system combination output at the level of the whole test corpus.</S>\n    <S sid=\"130\" ssid=\"22\">The development (dev) set used for system combination parameter training contains 1002 sentences sampled from the previous NIST MT Chinese-to-English test sets: 35% from MT04, 55% from MT05, and 10% from MT06-newswire.</S>\n    <S sid=\"131\" ssid=\"23\">The test set is the MT08 Chinese-to-English “current” test set, which includes 1357 sentences from both newswire and web-data genres.</S>\n    <S sid=\"132\" ssid=\"24\">Both dev and test sets have four references per sentence.</S>\n    <S sid=\"133\" ssid=\"25\">As inputs to the system combination, 10-best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems.</S>\n    <S sid=\"134\" ssid=\"26\">All outputs on the MT08 test set were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008).</S>\n    <S sid=\"135\" ssid=\"27\">However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead.</S>\n    <S sid=\"136\" ssid=\"28\">In our main experiments, outputs from a total of eight single MT systems were combined.</S>\n    <S sid=\"137\" ssid=\"29\">As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al.</S>\n    <S sid=\"138\" ssid=\"30\">(2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b).</S>\n    <S sid=\"139\" ssid=\"31\">All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.</S>\n    <S sid=\"140\" ssid=\"32\">These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data.</S>\n    <S sid=\"141\" ssid=\"33\">The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition of MT08.</S>\n    <S sid=\"142\" ssid=\"34\">In the IHMM-based method, the smoothing factor for surface similarity model is set to ρ = 3, the interpolation factor of the overall similarity model is set to α = 0.3, and the controlling factor of the distance-based distortion parameters is set to K=2.</S>\n    <S sid=\"143\" ssid=\"35\">These settings are optimized on the dev set.</S>\n    <S sid=\"144\" ssid=\"36\">Individual system results and system combination results using both IHMM and TER alignment, on both the dev and test sets, are presented in Table 1.</S>\n    <S sid=\"145\" ssid=\"37\">The TER-based hypothesis alignment tool used in our experiments is the publicly available TER Java program, TERCOM (Snover et al., 2006).</S>\n    <S sid=\"146\" ssid=\"38\">Default settings of TERCOM are used in the following experiments.</S>\n    <S sid=\"147\" ssid=\"39\">On the dev set, the case insensitive BLEU score of the IHMM-based 8-way system combination output is about 5.8 points higher than that of the best single system.</S>\n    <S sid=\"148\" ssid=\"40\">Compared to the TER-based method, the IHMM-based method is about 1.5 BLEU points better.</S>\n    <S sid=\"149\" ssid=\"41\">On the MT08 test set, the IHMM-based system combination gave a case sensitive BLEU score of 30.89%.</S>\n    <S sid=\"150\" ssid=\"42\">It outperformed the best single system by 4.7 BLEU points and the TER-based system combination by 1.0 BLEU points.</S>\n    <S sid=\"151\" ssid=\"43\">Note that the best single system on the dev set and the test set are different.</S>\n    <S sid=\"152\" ssid=\"44\">The different single systems are optimized on different tuning sets, so this discrepancy between dev set and test set results is presumably due to differing degrees of mismatch between the dev and test sets and the various tuning sets.</S>\n    <S sid=\"153\" ssid=\"45\">In order to evaluate how well our method performs when we combine more systems, we collected MT outputs on MT08 from seven additional single systems as summarized in Table 2.</S>\n    <S sid=\"154\" ssid=\"46\">These systems belong to two groups.</S>\n    <S sid=\"155\" ssid=\"47\">Sys-9 to Sys-12 are in the first group.</S>\n    <S sid=\"156\" ssid=\"48\">They are syntaxaugmented hierarchical systems similar to those described by Shen et al. (2008) using different Chinese word segmentation and language models.</S>\n    <S sid=\"157\" ssid=\"49\">The second group has Sys-13 to Sys-15.</S>\n    <S sid=\"158\" ssid=\"50\">Sys-13 is a phrasal system proposed by Koehn et al. (2003), Sys-14 is a hierarchical system proposed by Chiang (2007), and Sys-15 is a syntax-based system proposed by Galley et al.</S>\n    <S sid=\"159\" ssid=\"51\">(2006).</S>\n    <S sid=\"160\" ssid=\"52\">All seven systems were trained within the confines of the constrained training condition of NIST MT08 evaluation.</S>\n    <S sid=\"161\" ssid=\"53\">We collected 10-best MT outputs only on the MT08 test set from these seven extra systems.</S>\n    <S sid=\"162\" ssid=\"54\">No MT outputs on our dev set are available from them at present.</S>\n    <S sid=\"163\" ssid=\"55\">Therefore, we directly adopt system combination parameters trained for the previous 8way system combination, except the system weights, which are re-set by the following heuristics: First, the total system weight mass 1.0 is evenly divided among the three groups of single systems: {Sys-1~8}, {Sys-9~12}, and {Sys13~15}.</S>\n    <S sid=\"164\" ssid=\"56\">Each group receives a total system weight mass of 1/3.</S>\n    <S sid=\"165\" ssid=\"57\">Then the weight mass is further divided in each group: in the first group, the original weights of systems 1~8 are multiplied by 1/3; in the second and third groups, the weight mass is evenly distributed within the group, i.e., 1/12 for each system in group 2, and 1/9 for each system in group 35.</S>\n    <S sid=\"166\" ssid=\"58\">Length adaptation is applied to control the final output length, where the same expected length ratio of the previous 8-way system combination is adopted.</S>\n    <S sid=\"167\" ssid=\"59\">The results of the 15-way system combination are presented in Table 3.</S>\n    <S sid=\"168\" ssid=\"60\">It shows that the IHMMbased method is still about 1 BLEU point better than the TER-based method.</S>\n    <S sid=\"169\" ssid=\"61\">Moreover, combining 15 single systems gives an output that has a NIST BLEU score of 34.82%, which is 3.9 points better than the best submission to the NIST MT08 constrained training track (NIST, 2008).</S>\n    <S sid=\"170\" ssid=\"62\">To our knowledge, this is the best result reported on this task.</S>\n    <S sid=\"171\" ssid=\"63\">In this section, we evaluate the effect of the semantic similarity model and the surface similarity model by varying the interpolation weight α of (2).</S>\n    <S sid=\"172\" ssid=\"64\">The results on both the dev and test sets are reported in Table 4.</S>\n    <S sid=\"173\" ssid=\"65\">In one extreme case, α = 1, the overall similarity model is based only on semantic similarity.</S>\n    <S sid=\"174\" ssid=\"66\">This gives a case insensitive BLEU score of 41.70% and a case sensitive BLEU score of 28.92% on the dev and test set, respectively.</S>\n    <S sid=\"175\" ssid=\"67\">The accuracy is significantly improved to 43.62% on the dev set and 30.89% on test set when α = 0.3.</S>\n    <S sid=\"176\" ssid=\"68\">In another extreme case, α = 0, in which only the surface similarity model is used for the overall similarity model, the performance degrades by about 0.2 point.</S>\n    <S sid=\"177\" ssid=\"69\">Therefore, the surface similarity information seems more important for monolingual hypothesis alignment, but both sub-models are useful.</S>\n    <S sid=\"178\" ssid=\"70\">We investigate the effect of the distance-based distortion model by varying the controlling factor K in (6).</S>\n    <S sid=\"179\" ssid=\"71\">For example, setting K=1.0 gives a lineardecay distortion model, and setting K=2.0 gives a quadratic smoothed distance-based distortion model.</S>\n    <S sid=\"180\" ssid=\"72\">As shown in Table 5, the optimal result can be achieved using a properly smoothed distancebased distortion model.</S>\n  </SECTION>\n  <SECTION title=\"6 Conclusion\" number=\"6\">\n    <S sid=\"181\" ssid=\"1\">Synonym matching and word ordering are two central issues for hypothesis alignment in confusion-network-based MT system combination.</S>\n    <S sid=\"182\" ssid=\"2\">In this paper, an IHMM-based method is proposed for hypothesis alignment.</S>\n    <S sid=\"183\" ssid=\"3\">It uses a similarity model for synonym matching and a distortion model for word ordering.</S>\n    <S sid=\"184\" ssid=\"4\">In contrast to previous methods, the similarity model explicitly incorporates both semantic and surface word similarity, which is critical to monolingual word alignment, and a smoothed distance-based distortion model is used to model the first-order dependency of word ordering, which is shown to be better than simpler approaches.</S>\n    <S sid=\"185\" ssid=\"5\">Our experimental results show that the IHMMbased hypothesis alignment method gave superior results on the NIST MT08 C2E test set compared to the TER-based method.</S>\n    <S sid=\"186\" ssid=\"6\">Moreover, we show that our system combination method can scale up to combining more systems and produce a better output that has a case sensitive BLEU score of 34.82, which is 3.9 BLEU points better than the best official submission of MT08.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgement\" number=\"7\">\n    <S sid=\"187\" ssid=\"1\">The authors are grateful to Chris Quirk, Arul Menezes, Kristina Toutanova, William Dolan, Mu Li, Chi-Ho Li, Dongdong Zhang, Long Jiang, Ming Zhou, George Foster, Roland Kuhn, Jing Zheng, Wen Wang, Necip Fazil Ayan, Dimitra Vergyri, Nicolas Scheffer, Andreas Stolcke, Kevin Knight, Jens-Soenke Voeckler, Spyros Matsoukas, and Antti-Veikko Rosti for assistance with the MT systems and/or for the valuable suggestions and discussions.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "dc4ccf11777a3493a25bcfbcaade81c3",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Indirect-HMM-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems"
                }
            }
        },
        {
            "index": 19,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We how to solve this dilemma with sama simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",
                    "filename": "Incorporating_Non-Local_Information_Into_Information_Extraction_Systems_By_Gibbs_Sampling",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/P05-1045/Documents_xml/P05-1045.xml",
                    "title": "Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a novel approach to enhancing information extraction systems by introducing non-local dependencies into statistical models using Gibbs sampling, an approximate inference algorithm. Traditional models, like Hidden Markov Models (HMMs), Conditional Markov Models (CMMs), and Conditional Random Fields (CRFs), are limited by their reliance on local information, which impedes their ability to capture the non-local structure inherent in natural language. By incorporating Gibbs sampling, the authors demonstrate how constraints, such as label consistency, can be integrated into a CRF model, significantly improving performance on established information extraction tasks. The paper also compares the effectiveness of Gibbs sampling to the Viterbi algorithm and discusses the computational costs associated with the proposed technique. The results show that the inclusion of non-local information through Gibbs sampling yields a stable and effective method for enhancing the accuracy of information extraction systems.",
                        "This academic paper presents a method for improving information extraction systems by incorporating non-local structures into statistical models using Gibbs sampling. Traditional models like Hidden Markov Models and Conditional Random Fields are constrained to local information, limiting their effectiveness. By using Gibbs sampling, an approximate inference algorithm, the paper demonstrates the ability to capture non-local dependencies, such as label consistency, without the need for exact inference. This approach leads to significant improvements in information extraction tasks, as evidenced by performance gains on established datasets like the CoNLL 2003 English named entity recognition dataset and the CMU Seminar Announcements dataset. However, the method increases computational complexity, as demonstrated by longer testing times. The study suggests a broader potential for applying this technique to other natural language processing tasks that could benefit from modeling non-local structures.",
                        "This paper presents a method for enhancing information extraction systems by incorporating non-local dependencies using Gibbs sampling, an approximate inference algorithm. Traditional sequence models like Hidden Markov Models (HMMs), Conditional Markov Models (CMMs), and Conditional Random Fields (CRFs) are limited by their reliance on local information, which can lead to inaccuracies in tasks that benefit from non-local structure, such as named entity recognition (NER). The authors demonstrate that by applying Gibbs sampling to CRFs, constraints such as label consistency can be introduced, leading to significant improvements in performance on established information extraction tasks. The approach is validated on two datasets, showing an increase in F1 accuracy, and is suggested to have potential applications in statistical parsing and other NLP tasks. However, the computational cost of the method is noted as a significant drawback.",
                        "This paper presents a method for enhancing information extraction systems by incorporating non-local dependencies using Gibbs sampling, an approximate inference algorithm. Traditional natural language processing models, like Hidden Markov Models and Conditional Random Fields, are limited by their focus on local structure, overlooking the significant non-local structure present in language. The authors address this limitation by introducing Gibbs sampling to add non-local constraints to sequence models. They empirically demonstrate that their approach yields significant improvements in named entity recognition and information extraction tasks. The paper compares the performance of Gibbs sampling with the Viterbi algorithm and shows that with sufficient runtime, Gibbs sampling achieves comparable accuracy. The method proves to be computationally expensive but offers a probabilistically well-founded and adaptable solution for incorporating long-distance dependencies in statistical models."
                    ],
                    "filename": "incorporating_nonlocal_informati-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 132 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Most statistical models currently used in natural language processing represent only local structure.</S>\n    <S sid=\"7\" ssid=\"2\">Although this constraint is critical in enabling tractable model inference, it is a key limitation in many tasks, since natural language contains a great deal of nonlocal structure.</S>\n    <S sid=\"8\" ssid=\"3\">A general method for solving this problem is to relax the requirement of exact inference, substituting approximate inference algorithms instead, thereby permitting tractable inference in models with non-local structure.</S>\n    <S sid=\"9\" ssid=\"4\">One such algorithm is Gibbs sampling, a simple Monte Carlo algorithm that is appropriate for inference in any factored probabilistic model, including sequence models and probabilistic context free grammars (Geman and Geman, 1984).</S>\n    <S sid=\"10\" ssid=\"5\">Although Gibbs sampling is widely used elsewhere, there has been extremely little use of it in natural language processing.1 Here, we use it to add non-local dependencies to sequence models for information extraction.</S>\n    <S sid=\"11\" ssid=\"6\">Statistical hidden state sequence models, such as Hidden Markov Models (HMMs) (Leek, 1997; Freitag and McCallum, 1999), Conditional Markov Models (CMMs) (Borthwick, 1999), and Conditional Random Fields (CRFs) (Lafferty et al., 2001) are a prominent recent approach to information extraction tasks.</S>\n    <S sid=\"12\" ssid=\"7\">These models all encode the Markov property: decisions about the state at a particular position in the sequence can depend only on a small local window.</S>\n    <S sid=\"13\" ssid=\"8\">It is this property which allows tractable computation: the Viterbi, Forward Backward, and Clique Calibration algorithms all become intractable without it.</S>\n    <S sid=\"14\" ssid=\"9\">However, information extraction tasks can benefit from modeling non-local structure.</S>\n    <S sid=\"15\" ssid=\"10\">As an example, several authors (see Section 8) mention the value of enforcing label consistency in named entity recognition (NER) tasks.</S>\n    <S sid=\"16\" ssid=\"11\">In the example given in Figure 1, the second occurrence of the token Tanjug is mislabeled by our CRF-based statistical NER system, because by looking only at local evidence it is unclear whether it is a person or organization.</S>\n    <S sid=\"17\" ssid=\"12\">The first occurrence of Tanjug provides ample evidence that it is an organization, however, and by enforcing label consistency the system should be able to get it right.</S>\n    <S sid=\"18\" ssid=\"13\">We show how to incorporate constraints of this form into a CRF model by using Gibbs sampling instead of the Viterbi algorithm as our inference procedure, and demonstrate that this technique yields significant improvements on two established IE tasks.</S>\n    <S sid=\"19\" ssid=\"14\">In hidden state sequence models such as HMMs, CMMs, and CRFs, it is standard to use the Viterbi algorithm, a dynamic programming algorithm, to infer the most likely hidden state sequence given the input and the model (see, e.g., Rabiner (1989)).</S>\n    <S sid=\"20\" ssid=\"15\">Although this is the only tractable method for exact computation, there are other methods for computing an approximate solution.</S>\n    <S sid=\"21\" ssid=\"16\">Monte Carlo methods are a simple and effective class of methods for approximate inference based on sampling.</S>\n    <S sid=\"22\" ssid=\"17\">Imagine we have a hidden state sequence model which defines a probability distribution over state sequences conditioned on any given input.</S>\n    <S sid=\"23\" ssid=\"18\">With such a model M we should be able to compute the conditional probability PM(s|o) of any state sequence s = {s0, ... , sN} given some observed input sequence o = {o0, ... , oN}.</S>\n    <S sid=\"24\" ssid=\"19\">One can then sample sequences from the conditional distribution defined by the model.</S>\n    <S sid=\"25\" ssid=\"20\">These samples are likely to be in high probability areas, increasing our chances of finding the maximum.</S>\n    <S sid=\"26\" ssid=\"21\">The challenge is how to sample sequences efficiently from the conditional distribution defined by the model.</S>\n    <S sid=\"27\" ssid=\"22\">Gibbs sampling provides a clever solution (Geman and Geman, 1984).</S>\n    <S sid=\"28\" ssid=\"23\">Gibbs sampling defines a Markov chain in the space of possible variable assignments (in this case, hidden state sequences) such that the stationary distribution of the Markov chain is the joint distribution over the variables.</S>\n    <S sid=\"29\" ssid=\"24\">Thus it is called a Markov Chain Monte Carlo (MCMC) method; see Andrieu et al. (2003) for a good MCMC tutorial.</S>\n    <S sid=\"30\" ssid=\"25\">In practical terms, this means that we can walk the Markov chain, occasionally outputting samples, and that these samples are guaranteed to be drawn from the target distribution.</S>\n    <S sid=\"31\" ssid=\"26\">Furthermore, the chain is defined in very simple terms: from each state sequence we can only transition to a state sequence obtained by changing the state at any one position i, and the distribution over these possible transitions is just where s−i is all states except si.</S>\n    <S sid=\"32\" ssid=\"27\">In other words, the transition probability of the Markov chain is the conditional distribution of the label at the position given the rest of the sequence.</S>\n    <S sid=\"33\" ssid=\"28\">This quantity is easy to compute in any Markov sequence model, including HMMs, CMMs, and CRFs.</S>\n    <S sid=\"34\" ssid=\"29\">One easy way to walk the Markov chain is to loop through the positions i from 1 to N, and for each one, to resample the hidden state at that position from the distribution given in Equation 1.</S>\n    <S sid=\"35\" ssid=\"30\">By outputting complete sequences at regular intervals (such as after resampling all N positions), we can sample sequences from the conditional distribution defined by the model.</S>\n    <S sid=\"36\" ssid=\"31\">This is still a gravely inefficient process, however.</S>\n    <S sid=\"37\" ssid=\"32\">Random sampling may be a good way to estimate the shape of a probability distribution, but it is not an efficient way to do what we want: find the maximum.</S>\n    <S sid=\"38\" ssid=\"33\">However, we cannot just transition greedily to higher probability sequences at each step, because the space is extremely non-convex.</S>\n    <S sid=\"39\" ssid=\"34\">We can, however, borrow a technique from the study of non-convex optimization and use simulated annealing (Kirkpatrick et al., 1983).</S>\n    <S sid=\"40\" ssid=\"35\">Geman and Geman (1984) show that it is easy to modify a Gibbs Markov chain to do annealing; at time t we replace the distribution in (1) with where c = {c0, ... , cT} defines a cooling schedule.</S>\n    <S sid=\"41\" ssid=\"36\">At each step, we raise each value in the conditional distribution to an exponent and renormalize before sampling from it.</S>\n    <S sid=\"42\" ssid=\"37\">Note that when c = 1 the distribution is unchanged, and as c → 0 the distribution becomes sharper, and when c = 0 the distribution places all of its mass on the maximal outcome, having the effect that the Markov chain always climbs uphill.</S>\n    <S sid=\"43\" ssid=\"38\">Thus if we gradually decrease c from 1 to 0, the Markov chain increasingly tends to go uphill.</S>\n    <S sid=\"44\" ssid=\"39\">This annealing technique has been shown to be an effective technique for stochastic optimization (Laarhoven and Arts, 1987).</S>\n    <S sid=\"45\" ssid=\"40\">To verify the effectiveness of Gibbs sampling and simulated annealing as an inference technique for hidden state sequence models, we compare Gibbs and Viterbi inference methods for a basic CRF, without the addition of any non-local model.</S>\n    <S sid=\"46\" ssid=\"41\">The results, given in Table 1, show that if the Gibbs sampler is run long enough, its accuracy is the same as a Viterbi decoder.</S>\n  </SECTION>\n  <SECTION title=\"3 A Conditional Random Field Model\" number=\"2\">\n    <S sid=\"47\" ssid=\"1\">Our basic CRF model follows that of Lafferty et al. (2001).</S>\n    <S sid=\"48\" ssid=\"2\">We choose a CRF because it represents the state of the art in sequence modeling, allowing both discriminative training and the bi-directional flow of probabilistic information across the sequence.</S>\n    <S sid=\"49\" ssid=\"3\">A CRF is a conditional sequence model which represents the probability of a hidden state sequence given some observations.</S>\n    <S sid=\"50\" ssid=\"4\">In order to facilitate obtaining the conditional probabilities we need for Gibbs sampling, we generalize the CRF model in a way that is consistent with the Markov Network literature (see Cowell et al. (1999)): we create a linear chain of cliques, where each clique, c, represents the probabilistic relationship between an adjacent pair of states2 using a clique potential φc, which is just a table containing a value for each possible state assignment.</S>\n    <S sid=\"51\" ssid=\"5\">The table is not a true probability distribution, as it only accounts for local interactions within the clique.</S>\n    <S sid=\"52\" ssid=\"6\">The clique potentials themselves are defined in terms of exponential models conditioned on features of the observation sequence, and must be instantiated for each new observation sequence.</S>\n    <S sid=\"53\" ssid=\"7\">The sequence of potentials in the clique chain then defines the probability of a state sequence (given the observation sequence) as where φi(si−1, si) is the element of the clique potential at position i corresponding to states si−1 and si.3 Although a full treatment of CRF training is beyond the scope of this paper (our technique assumes the model is already trained), we list the features used by our CRF for the two tasks we address in Table 2.</S>\n    <S sid=\"54\" ssid=\"8\">During training, we regularized our exponential models with a quadratic prior and used the quasi-Newton method for parameter optimization.</S>\n    <S sid=\"55\" ssid=\"9\">As is customary, we used the Viterbi algorithm to infer the most likely state sequence in a CRF.</S>\n    <S sid=\"56\" ssid=\"10\">The clique potentials of the CRF, instantiated for some observation sequence, can be used to easily compute the conditional distribution over states at a position given in Equation 1.</S>\n    <S sid=\"57\" ssid=\"11\">Recall that at position i we want to condition on the states in the rest of the sequence.</S>\n    <S sid=\"58\" ssid=\"12\">The state at this position can be influenced by any other state that it shares a clique with; in particular, when the clique size is 2, there are 2 such cliques.</S>\n    <S sid=\"59\" ssid=\"13\">In this case the Markov blanket of the state (the minimal set of states that renders a state conditionally independent of all other states) consists of the two neighboring states and the observation sequence, all of which are observed.</S>\n    <S sid=\"60\" ssid=\"14\">The conditional distribution at position i can then be computed simply as where the factor tables F in the clique chain are already conditioned on the observation sequence.</S>\n  </SECTION>\n  <SECTION title=\"4 Datasets and Evaluation\" number=\"3\">\n    <S sid=\"61\" ssid=\"1\">We test the effectiveness of our technique on two established datasets: the CoNLL 2003 English named entity recognition dataset, and the CMU Seminar Announcements information extraction dataset.</S>\n    <S sid=\"62\" ssid=\"2\">This dataset was created for the shared task of the Seventh Conference on Computational Natural Language Learning (CoNLL),4 which concerned named entity recognition.</S>\n    <S sid=\"63\" ssid=\"3\">The English data is a collection of Reuters newswire articles annotated with four entity types: person (PER), location (LOC), organization (ORG), and miscellaneous (MISC).</S>\n    <S sid=\"64\" ssid=\"4\">The data is separated into a training set, a development set (testa), and a test set (testb).</S>\n    <S sid=\"65\" ssid=\"5\">The training set contains 945 documents, and approximately 203,000 tokens.</S>\n    <S sid=\"66\" ssid=\"6\">The development set has 216 documents and approximately 51,000 tokens, and the test set has 231 documents and approximately 46,000 tokens.</S>\n    <S sid=\"67\" ssid=\"7\">We evaluate performance on this task in the manner dictated by the competition so that results can be properly compared.</S>\n    <S sid=\"68\" ssid=\"8\">Precision and recall are evaluated on a per-entity basis (and combined into an F1 score).</S>\n    <S sid=\"69\" ssid=\"9\">There is no partial credit; an incorrect entity boundary is penalized as both a false positive and as a false negative.</S>\n    <S sid=\"70\" ssid=\"10\">This dataset was developed as part of Dayne Freitag’s dissertation research Freitag (1998).5 It consists of 485 emails containing seminar announcements at Carnegie Mellon University.</S>\n    <S sid=\"71\" ssid=\"11\">It is annotated for four fields: speaker, location, start time, and end time.</S>\n    <S sid=\"72\" ssid=\"12\">Sutton and McCallum (2004) used 5-fold cross validation when evaluating on this dataset, so we obtained and used their data splits, so that results can be properly compared.</S>\n    <S sid=\"73\" ssid=\"13\">Because the entire dataset is used for testing, there is no development set.</S>\n    <S sid=\"74\" ssid=\"14\">We also used their evaluation metric, which is slightly different from the method for CoNLL data.</S>\n    <S sid=\"75\" ssid=\"15\">Instead of evaluating precision and recall on a per-entity basis, they are evaluated on a per-token basis.</S>\n    <S sid=\"76\" ssid=\"16\">Then, to calculate the overall F1 score, the F1 scores for each class are averaged.</S>\n  </SECTION>\n  <SECTION title=\"5 Models of Non-local Structure\" number=\"4\">\n    <S sid=\"77\" ssid=\"1\">Our models of non-local structure are themselves just sequence models, defining a probability distribution over all possible state sequences.</S>\n    <S sid=\"78\" ssid=\"2\">It is possible to flexibly model various forms of constraints in a way that is sensitive to the linguistic structure of the data (e.g., one can go beyond imposing just exact identity conditions).</S>\n    <S sid=\"79\" ssid=\"3\">One could imagine many ways of defining such models; for simplicity we use the form where the product is over a set of violation types A, and for each violation type A we specify a penalty parameter θλ.</S>\n    <S sid=\"80\" ssid=\"4\">The exponent #(A, s, o) is the count of the number of times that the violation A occurs in the state sequence s with respect to the observation sequence o.</S>\n    <S sid=\"81\" ssid=\"5\">This has the effect of assigning sequences with more violations a lower probability.</S>\n    <S sid=\"82\" ssid=\"6\">The particular violation types are defined specifically for each task, and are described in the following two sections.</S>\n    <S sid=\"83\" ssid=\"7\">This model, as defined above, is not normalized, and clearly it would be expensive to do so.</S>\n    <S sid=\"84\" ssid=\"8\">This doesn’t matter, however, because we only use the model for Gibbs sampling, and so only need to compute the conditional distribution at a single position i (as defined in Equation 1).</S>\n    <S sid=\"85\" ssid=\"9\">One (inefficient) way to compute this quantity is to enumerate all possible sequences differing only at position i, compute the score assigned to each by the model, and renormalize.</S>\n    <S sid=\"86\" ssid=\"10\">Although it seems expensive, this computation can be made very efficient with a straightforward memoization technique: at all times we maintain data structures representing the relationship between entity labels and token sequences, from which we can quickly compute counts of different types of violations.</S>\n    <S sid=\"87\" ssid=\"11\">Label consistency structure derives from the fact that within a particular document, different occurrences of a particular token sequence are unlikely to be labeled as different entity types.</S>\n    <S sid=\"88\" ssid=\"12\">Although any one occurrence may be ambiguous, it is unlikely that all instances are unclear when taken together.</S>\n    <S sid=\"89\" ssid=\"13\">The CoNLL training data empirically supports the strength of the label consistency constraint.</S>\n    <S sid=\"90\" ssid=\"14\">Table 3 shows the counts of entity labels for each pair of identical token sequences within a document, where both are labeled as an entity.</S>\n    <S sid=\"91\" ssid=\"15\">Note that inconsistent labelings are very rare.6 In addition, we also want to model subsequence constraints: having seen Geoff Woods earlier in a document as a person is a good indicator that a subsequent occurrence of Woods should also be labeled as a person.</S>\n    <S sid=\"92\" ssid=\"16\">However, if we examine all cases of the labelings of other occurrences of subsequences of a labeled entity, we find that the consistency constraint does not hold nearly so strictly in this case.</S>\n    <S sid=\"93\" ssid=\"17\">As an example, one document contains references to both The China Daily, a newspaper, and China, the country.</S>\n    <S sid=\"94\" ssid=\"18\">Counts of subsequence labelings within a document are listed in Table 4.</S>\n    <S sid=\"95\" ssid=\"19\">Note that there are many offdiagonal entries: the China Daily case is the most common, occurring 328 times in the dataset.</S>\n    <S sid=\"96\" ssid=\"20\">The penalties used in the long distance constraint model for CoNLL are the Empirical Bayes estimates taken directly from the data (Tables 3 and 4), except that we change counts of 0 to be 1, so that the distribution remains positive.</S>\n    <S sid=\"97\" ssid=\"21\">So the estimate of a PER also being an ORG is 5 3151; there were 5 instance of an entity being labeled as both, PER appeared 3150 times in the data, and we add 1 to this for smoothing, because PER-MISC never occured.</S>\n    <S sid=\"98\" ssid=\"22\">However, when we have a phrase labeled differently in two different places, continuing with the PER-ORG example, it is unclear if we should penalize it as PER that is also an ORG or an ORG that is also a PER.</S>\n    <S sid=\"99\" ssid=\"23\">To deal with this, we multiply the square roots of each estimate together to form the penalty term.</S>\n    <S sid=\"100\" ssid=\"24\">The penalty term is then multiplied in a number of times equal to the length of the offending entity; this is meant to “encourage” the entity to shrink.7 For example, say we have a document with three entities, Rotor Volgograd twice, once labeled as PER and once as ORG, and Rotor, labeled as an ORG.</S>\n    <S sid=\"101\" ssid=\"25\">The likelihood of a PER also being an ORG is 5 3151, and of an ORG also Due to the lack of a development set, our consistency model for the CMU Seminar Announcements is much simpler than the CoNLL model, the numbers where selected due to our intuitions, and we did not spend much time hand optimizing the model.</S>\n    <S sid=\"102\" ssid=\"26\">Specifically, we had three constraints.</S>\n    <S sid=\"103\" ssid=\"27\">The first is that all entities labeled as start time are normalized, and are penalized if they are inconsistent.</S>\n    <S sid=\"104\" ssid=\"28\">The second is a corresponding constraint for end times.</S>\n    <S sid=\"105\" ssid=\"29\">The last constraint attempts to consistently label the speakers.</S>\n    <S sid=\"106\" ssid=\"30\">If a phrase is labeled as a speaker, we assume that the last word is the speaker’s last name, and we penalize for each occurrance of that word which is not also labeled speaker.</S>\n    <S sid=\"107\" ssid=\"31\">For the start and end times the penalty is multiplied in based on how many words are in the entity.</S>\n    <S sid=\"108\" ssid=\"32\">For the speaker, the penalty is only multiplied in once.</S>\n    <S sid=\"109\" ssid=\"33\">We used a hand selected penalty of exp −4.0.</S>\n  </SECTION>\n  <SECTION title=\"6 Combining Sequence Models\" number=\"5\">\n    <S sid=\"110\" ssid=\"1\">In the previous section we defined two models of non-local structure.</S>\n    <S sid=\"111\" ssid=\"2\">Now we would like to incorporate them into the local model (in our case, the trained CRF), and use Gibbs sampling to find the most likely state sequence.</S>\n    <S sid=\"112\" ssid=\"3\">Because both the trained CRF and the non-local models are themselves sequence models, we simply combine the two models into a factored sequence model of the following form where M is the local CRF model, L is the new nonlocal model, and F is the factored model.8 In this form, the probability again looks difficult to compute (because of the normalizing factor, a sum over all hidden state sequences of length N).</S>\n    <S sid=\"113\" ssid=\"4\">However, since we are only using the model for Gibbs sampling, we never need to compute the distribution explicitly.</S>\n    <S sid=\"114\" ssid=\"5\">Instead, we need only the conditional probability of each position in the sequence, which can be computed as</S>\n  </SECTION>\n  <SECTION title=\"7 Results and Discussion\" number=\"6\">\n    <S sid=\"115\" ssid=\"1\">In our experiments we compare the impact of adding the non-local models with Gibbs sampling to our baseline CRF implementation.</S>\n    <S sid=\"116\" ssid=\"2\">In the CoNLL named entity recognition task, the non-local models increase the F1 accuracy by about 1.3%.</S>\n    <S sid=\"117\" ssid=\"3\">Although such gains may appear modest, note that they are achieved relative to a near state-of-the-art NER system: the winner of the CoNLL English task reported an F1 score of 88.76.</S>\n    <S sid=\"118\" ssid=\"4\">In contrast, the increases published by Bunescu and Mooney (2004) are relative to a baseline system which scores only 80.9% on the same task.</S>\n    <S sid=\"119\" ssid=\"5\">Our performance is similar on the CMU Seminar Announcements dataset.</S>\n    <S sid=\"120\" ssid=\"6\">We show the per-field F1 results that were reported by Sutton and McCallum (2004) for comparison, and note that we are again achieving gains against a more competitive baseline system.</S>\n    <S sid=\"121\" ssid=\"7\">For all experiments involving Gibbs sampling, we used a linear cooling schedule.</S>\n    <S sid=\"122\" ssid=\"8\">For the CoNLL dataset we collected 200 samples per trial, and for the CMU Seminar Announcements we collected 100 samples.</S>\n    <S sid=\"123\" ssid=\"9\">We report the average of all trials, and in all cases we outperform the baseline with greater than 95% confidence, using the standard t-test.</S>\n    <S sid=\"124\" ssid=\"10\">The trials had low standard deviations - 0.083% and 0.007% and high minimun F-scores - 86.72%, and 92.28% - for the CoNLL and CMU Seminar Announcements respectively, demonstrating the stability of our method.</S>\n    <S sid=\"125\" ssid=\"11\">The biggest drawback to our model is the computational cost.</S>\n    <S sid=\"126\" ssid=\"12\">Taking 100 samples dramatically increases test time.</S>\n    <S sid=\"127\" ssid=\"13\">Averaged over 3 runs on both Viterbi and Gibbs, CoNLL testing time increased from 55 to 1738 seconds, and CMU Seminar Announcements testing time increases from 189 to 6436 seconds.</S>\n  </SECTION>\n  <SECTION title=\"8 Related Work\" number=\"7\">\n    <S sid=\"128\" ssid=\"1\">Several authors have successfully incorporated a label consistency constraint into probabilistic sequence model named entity recognition systems.</S>\n    <S sid=\"129\" ssid=\"2\">Mikheev et al. (1999) and Finkel et al.</S>\n    <S sid=\"130\" ssid=\"3\">(2004) incorporate label consistency information by using adhoc multi-stage labeling procedures that are effective but special-purpose.</S>\n    <S sid=\"131\" ssid=\"4\">Malouf (2002) and Curran and Clark (2003) condition the label of a token at a particular position on the label of the most recent previous instance of that same token in a prior sentence of the same document.</S>\n    <S sid=\"132\" ssid=\"5\">Note that this violates the Markov property, but is achieved by slightly relaxing the requirement of exact inference.</S>\n    <S sid=\"133\" ssid=\"6\">Instead of finding the maximum likelihood sequence over the entire document, they classify one sentence at a time, allowing them to condition on the maximum likelihood sequence of previous sentences.</S>\n    <S sid=\"134\" ssid=\"7\">This approach is quite effective for enforcing label consistency in many NLP tasks, however, it permits a forward flow of information only, which is not sufficient for all cases of interest.</S>\n    <S sid=\"135\" ssid=\"8\">Chieu and Ng (2002) propose a solution to this problem: for each token, they define additional features taken from other occurrences of the same token in the document.</S>\n    <S sid=\"136\" ssid=\"9\">This approach has the added advantage of allowing the training procedure to automatically learn good weightings for these “global” features relative to the local ones.</S>\n    <S sid=\"137\" ssid=\"10\">However, this approach cannot easily be extended to incorporate other types of non-local structure.</S>\n    <S sid=\"138\" ssid=\"11\">The most relevant prior works are Bunescu and Mooney (2004), who use a Relational Markov Network (RMN) (Taskar et al., 2002) to explicitly models long-distance dependencies, and Sutton and McCallum (2004), who introduce skip-chain CRFs, which maintain the underlying CRF sequence model (which (Bunescu and Mooney, 2004) lack) while adding skip edges between distant nodes.</S>\n    <S sid=\"139\" ssid=\"12\">Unfortunately, in the RMN model, the dependencies must be defined in the model structure before doing any inference, and so the authors use crude heuristic part-of-speech patterns, and then add dependencies between these text spans using clique templates.</S>\n    <S sid=\"140\" ssid=\"13\">This generates a extremely large number of overlapping candidate entities, which then necessitates additional templates to enforce the constraint that text subsequences cannot both be different entities, something that is more naturally modeled by a CRF.</S>\n    <S sid=\"141\" ssid=\"14\">Another disadvantage of this approach is that it uses loopy beliefpropagation and a voted perceptron for approximate learning and inference – ill-founded and inherently unstable algorithms which are noted by the authors to have caused convergence problems.</S>\n    <S sid=\"142\" ssid=\"15\">In the skip-chain CRFs model, the decision of which nodes to connect is also made heuristically, and because the authors focus on named entity recognition, they chose to connect all pairs of identical capitalized words.</S>\n    <S sid=\"143\" ssid=\"16\">They also utilize loopy belief propagation for approximate learning and inference.</S>\n    <S sid=\"144\" ssid=\"17\">While the technique we propose is similar mathematically and in spirit to the above approaches, it differs in some important ways.</S>\n    <S sid=\"145\" ssid=\"18\">Our model is implemented by adding additional constraints into the model at inference time, and does not require the preprocessing step necessary in the two previously mentioned works.</S>\n    <S sid=\"146\" ssid=\"19\">This allows for a broader class of long-distance dependencies, because we do not need to make any initial assumptions about which nodes should be connected, and is helpful when you wish to model relationships between nodes which are the same class, but may not be similar in any other way.</S>\n    <S sid=\"147\" ssid=\"20\">For instance, in the CMU Seminar Announcements dataset, we can normalize all entities labeled as a start time and penalize the model if multiple, nonconsistent times are labeled.</S>\n    <S sid=\"148\" ssid=\"21\">This type of constraint cannot be modeled in an RMN or a skip-CRF, because it requires the knowledge that both entities are given the same class label.</S>\n    <S sid=\"149\" ssid=\"22\">We also allow dependencies between multi-word phrases, and not just single words.</S>\n    <S sid=\"150\" ssid=\"23\">Additionally, our model can be applied on top of a pre-existing trained sequence model.</S>\n    <S sid=\"151\" ssid=\"24\">As such, our method does not require complex training procedures, and can instead leverage all of the established methods for training high accuracy sequence models.</S>\n    <S sid=\"152\" ssid=\"25\">It can indeed be used in conjunction with any statistical hidden state sequence model: HMMs, CMMs, CRFs, or even heuristic models.</S>\n    <S sid=\"153\" ssid=\"26\">Third, our technique employs Gibbs sampling for approximate inference, a simple and probabilistically well-founded algorithm.</S>\n    <S sid=\"154\" ssid=\"27\">As a consequence of these differences, our approach is easier to understand, implement, and adapt to new applications.</S>\n  </SECTION>\n  <SECTION title=\"9 Conclusions\" number=\"8\">\n    <S sid=\"155\" ssid=\"1\">We have shown that a constraint model can be effectively combined with an existing sequence model in a factored architecture to successfully impose various sorts of long distance constraints.</S>\n    <S sid=\"156\" ssid=\"2\">Our model generalizes naturally to other statistical models and other tasks.</S>\n    <S sid=\"157\" ssid=\"3\">In particular, it could in the future be applied to statistical parsing.</S>\n    <S sid=\"158\" ssid=\"4\">Statistical context free grammars provide another example of statistical models which are restricted to limiting local structure, and which could benefit from modeling nonlocal structure.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgements\" number=\"9\">\n    <S sid=\"159\" ssid=\"1\">This work was supported in part by the Advanced Researchand Development Activity (ARDA)’s Advanced Question Answeringfor Intelligence (AQUAINT) Program.</S>\n    <S sid=\"160\" ssid=\"2\">Additionally, we would like to that our reviewers for their helpful comments.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "71c62a1343e1bcfbdfcd8b11493e84f2",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Incorporating Non-Local Information Into Information Extraction Systems By Gibbs Sampling"
                }
            }
        },
        {
            "index": 20,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Syntactic reordering approaches are an effective method for handling word-order differences between source and target lan guages in statistical machine translation(SMT) systems. This paper introduces a reordering approach for translation from Chinese to English. We describe a set of syntac tic reordering rules that exploit systematic differences between Chinese and English word order. The resulting system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English in terms of their word order. We evaluated the reordering approach within the MOSES phrase-based SMT system (Koehn et al, 2007). The reordering approach improved the BLEU score for the MOSES system from 28.52 to 30.86 on the NIST 2006 evaluation data. Wealso conducted a series of experiments to an alyze the accuracy and impact of different types of reordering rules.",
                    "filename": "Chinese_Syntactic_Reordering_for_Statistical_Machine_Translation",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/D07-1077/Documents_xml/D07-1077.xml",
                    "title": "Chinese Syntactic Reordering for Statistical Machine Translation"
                },
                "llm": {
                    "descriptions": [
                        "This paper explores the application of syntactic reordering rules to improve the accuracy of Statistical Machine Translation (SMT) from Chinese to English. The authors describe a process that involves parsing Chinese sentences and applying a series of transformations to the parse trees to reorder them into a structure that more closely resembles English syntax. This reordering is used as a preprocessing step for both training and test data in an SMT system. The paper demonstrates that, despite the lower accuracy of Chinese parsers, the use of syntactic reordering significantly enhances translation performance, resulting in an increase in BLEU score from 28.52 to 30.86 on the NIST 2006 evaluation data using the MOSES phrase-based SMT system. The research highlights the benefits of incorporating syntactic information from the source language in SMT and provides insights into the precision and impact of different reordering rules.",
                        "This paper introduces a syntactic reordering approach to improve statistical machine translation (SMT) from Chinese to English. Recognizing the significant word order differences between the two languages, the authors propose reordering rules that transform the structure of Chinese sentences to more closely resemble English syntax. This preprocessing step is applied to both training and test data within an existing phrase-based SMT system. The paper discusses the development of these rules based on the Penn Chinese Treebank guidelines and their application to verb phrases, noun phrases, and localizer phrases. The effectiveness of the reordering process is demonstrated by an increase in translation accuracy, with the system's BLEU score improving from 28.52 to 30.86. The paper also discusses the challenges posed by the relatively low accuracy of Chinese parsers and the necessity of word-segmentation in Chinese. Results confirm that, despite these issues, Chinese parses are indeed beneficial for SMT.",
                        "Abstract:\nThis paper presents a syntactic reordering approach for improving statistical machine translation (SMT) from Chinese to English by addressing the fundamental word order discrepancies between the two languages. The authors propose a set of syntactic reordering rules that are applied to Chinese sentences during preprocessing to align them more closely with English syntax. These rules are based on the analysis of phrase types from the Penn Chinese Treebank and focus on verb phrases, noun phrases, and localizer phrases. The reordering process is integrated into the preprocessing of both training and test data for an existing SMT system. Empirical results on the NIST 2006 evaluation data using the MOSES phrase-based SMT system demonstrate that the syntactic reordering significantly improves translation accuracy, yielding an increase from 28.52 to 30.86 BLEU score. The study validates the effectiveness of these reordering rules despite the lower accuracy of Chinese parsers and the complexity of Chinese word segmentation.",
                        "This paper discusses the implementation of syntactic reordering rules for Chinese to English statistical machine translation (SMT). Syntactic reordering is essential due to the significant word order differences between Chinese and English, which can impede effective translation. By transforming Chinese sentence structures to more closely resemble English structures, the authors aim to enhance the performance of SMT systems. This approach involves using a set of syntactic reordering rules that exploit the systematic differences in word order between the two languages. The reordering is applied during preprocessing of both training and test data. The paper presents the development of these rules and evaluates their impact on translation accuracy using the MOSES phrase-based SMT system. The results show an improvement in BLEU score from 28.52 to 30.86, indicating that despite the lower accuracy of Chinese parsers, Chinese syntactic information is beneficial for SMT. The paper also examines the precision of the reordering rules and the contribution of each rule type to overall translation accuracy."
                    ],
                    "filename": "chinese_syntactic_reordering_for-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 137 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n\t<S sid=\"0\">Chinese Syntactic Reordering for Statistical Machine Translation</S><SECTION title=\"Introduction\" number=\"1\">\n\t\t\t<S sid=\"8\" ssid=\"8\">Syntactic reordering approaches are an effective method for handling systematic differences in word order between source and target languages within the context of statistical machine translation (SMT) systems (Xia and McCord, 2004; Collins et al, 2005).</S>\n\t\t\t<S sid=\"9\" ssid=\"9\">In reordering approaches, sentences in the source language are first parsed, for example using a Treebank-trained parser.</S>\n\t\t\t<S sid=\"10\" ssid=\"10\">A series of transformations is then applied to the resulting parse tree, with the goal of transforming the source language sentence into a word order that is closer to that of the targetlanguage.</S>\n\t\t\t<S sid=\"11\" ssid=\"11\">The reordering process is used to prepro cess both the training and test data used within an existing SMT system.</S>\n\t\t\t<S sid=\"12\" ssid=\"12\">Reordering approaches have given significant improvements in performance fortranslation from French to English (Xia and Mc Cord, 2004) and from German to English (Collins et al, 2005).This paper describes a syntactic reordering approach for translation from Chinese to English.</S>\n\t\t\t<S sid=\"13\" ssid=\"13\">Figure 1 gives an example illustrating some of the dif ferences in word order between the two languages.</S>\n\t\t\t<S sid=\"14\" ssid=\"14\">The example shows a Chinese sentence whose literal translation in English is: this is French delegation at WinterOlympics on achieve DEC best accom plishment and where a natural translation would be this is the best accomplishment that the French delegation achieved at the Winter Olympics As exemplified by this sentence, Chinese differsfrom English in several important respects: for ex ample, relative clauses appear before the noun being modified; prepositional phrases often appear before the head they modify; and so on.</S>\n\t\t\t<S sid=\"15\" ssid=\"15\">It can be seen that some significant reordering of the input is requiredto produce a good English translation.</S>\n\t\t\t<S sid=\"16\" ssid=\"16\">For this ex ample, application of reordering rules leads to a newChinese string whose word-by-word English para phrase is: 737 Before syntactic reordering After syntactic reordering IP NP PN ?(this) VP VC(is) NP CP IP NP NR {I(French) NN ?L?(delegation) VP PP P 3(at) LCP NP NN ?G (Winter) NR $?</S>\n\t\t\t<S sid=\"17\" ssid=\"17\">(Olympics) LC ?(on) VP-A VV (achieve) DEC (DEC) ADJP JJ ?(best) NPB NN ?1(accomplishment) IP NP PN ?(this) VP VC(is) NP ADJP JJ ?(best) NPB NN ?1(accomplishment) CP DEC (DEC) IP NP NR {I(French) NN ?L?(delegation) VP VP-A VV (achieve) PP P 3(at) LCP LC ?(on) NP NN ?G (Winter) NR $?</S>\n\t\t\t<S sid=\"18\" ssid=\"18\">(Olympics) Figure 1: Original (left) and reordered (right) parse trees for the Chinese sentence ???{I?L?3 ?G $????1,?</S>\n\t\t\t<S sid=\"19\" ssid=\"19\">which translates into ?This is the best accomplishment that the French delegation achieved at the Winter Olympics?</S>\n\t\t\t<S sid=\"20\" ssid=\"20\">in English.</S>\n\t\t\t<S sid=\"21\" ssid=\"21\">this is best accomplishment DEC French delegation achieve at on Winter Olympics This reordering is relatively easy to express using syntactic transformations?for example, it is simpleto move the entire relative clause ?French delegation at Winter Olympics on achieve DEC? to a posi tion that is after the noun phrase it modifies, namely ?best accomplishment.?</S>\n\t\t\t<S sid=\"22\" ssid=\"22\">Phrase-based systems arequite limited in their ability to perform transformations of this type.</S>\n\t\t\t<S sid=\"23\" ssid=\"23\">More recently developed hier archical systems (e.g., (Yamada and Knight, 2001; Chiang, 2005; Marcu et al, 2006)) may be betterequipped to deal with reordering of this type; how ever, in this example they would effectively have to first identify the span of the relative clause, and then move it into the correct position, without any explicit representation of the source language syntax.In this paper, we describe a set of syntactic reordering rules that exploit systematic differences between Chinese and English word order.</S>\n\t\t\t<S sid=\"24\" ssid=\"24\">The result ing system is used as a preprocessor for both training and test sentences, transforming Chinese sentences to be much closer to English.</S>\n\t\t\t<S sid=\"25\" ssid=\"25\">We report results forthe method on the NIST 2006 evaluation data, us ing the MOSES phrase-based SMT system (Koehnet al, 2007).</S>\n\t\t\t<S sid=\"26\" ssid=\"26\">The reordering rules give an improve ment in accuracy from 28.52 to 30.86 BLEU score.</S>\n\t\t\t<S sid=\"27\" ssid=\"27\">A concern for methods that make use of Chinese parsers is that these parsers are typically of relativelylow accuracy, particularly given that Chinese re quires a word-segmentation step that is not required in languages such as English.</S>\n\t\t\t<S sid=\"28\" ssid=\"28\">Our results show that Chinese parses are useful in SMT in spite of this problem.</S>\n\t\t\t<S sid=\"29\" ssid=\"29\">We report results showing the precisionof the reordering rules?essentially testing how of ten the Chinese sentences are correctly reordered?</S>\n\t\t\t<S sid=\"30\" ssid=\"30\">to give more insight into this issue.</S>\n\t\t\t<S sid=\"31\" ssid=\"31\">We also report experiments which assess the impact of each type of reordering rule on translation accuracy.</S>\n\t</SECTION>\n\t<SECTION title=\"Related Work. \" number=\"2\">\n\t\t\t<S sid=\"32\" ssid=\"1\">A number of researchers (Brown et al, 1992; Bergeret al, 1996; Niessen and Ney, 2004; Xia and McCord, 2004; Collins et al, 2005) have described ap proaches that preprocess the source language input in SMT systems.</S>\n\t\t\t<S sid=\"33\" ssid=\"2\">We are not, however, aware of work on this topic for translation from Chinese to English.</S>\n\t\t\t<S sid=\"34\" ssid=\"3\">Brown et al (1992) describe an analysis component for French which moves phrases around (in addition to other transformations) so the source and target sentences are closer to each other in word order.</S>\n\t\t\t<S sid=\"35\" ssid=\"4\">Berger et al (1996) describe an approach for French that reorders phrases of the form NOUN1 deNOUN2.</S>\n\t\t\t<S sid=\"36\" ssid=\"5\">Xia and McCord (2004) describe an approach for French, where reordering rules that operate on context-free rule productions are acquired au 738tomatically.</S>\n\t\t\t<S sid=\"37\" ssid=\"6\">Niessen and Ney (2004) describe an ap proach for translation from German to English that combines verbs with associated particles, and alsoreorders questions.</S>\n\t\t\t<S sid=\"38\" ssid=\"7\">Collins et al (2005) also describe an approach for German, concentrating on re ordering German clauses, which have quite different word order from clauses in English.</S>\n\t\t\t<S sid=\"39\" ssid=\"8\">Our approach is most similar to that of Collins et al (2005).</S>\n\t\t\t<S sid=\"40\" ssid=\"9\">Most SMT systems employ some mechanism that allows reordering of the source language during translation (i.e., non-monotonic decoding).</S>\n\t\t\t<S sid=\"41\" ssid=\"10\">TheMOSES phrase-based system that we use has a rel atively simple reordering model which has a fixed penalty for reordering moves in the decoder.</S>\n\t\t\t<S sid=\"42\" ssid=\"11\">Moresophisticated models include reordering parameters that are sensitive to lexical information (Till mann, 2004; Kumar and Byrne, 2005; Koehn et al., 2005).</S>\n\t\t\t<S sid=\"43\" ssid=\"12\">The model of Chiang (2005) employsa synchronous context-free grammar to allow hierarchical approaches to reordering.</S>\n\t\t\t<S sid=\"44\" ssid=\"13\">The syntax based models of Yamada and Knight (2001) andMarcu et al (2006) build a full parse tree in the target language, again effectively allowing hierarchi cal reordering based on synchronous grammars.</S>\n\t\t\t<S sid=\"45\" ssid=\"14\">Itis worth noting that none of these approaches to re ordering make use of explicit syntactic information in the source language?for example, none of the methods make use of an existing source-language parser (the systems of Yamada and Knight (2001) and Marcu et al (2006) make use of a parser in the target language, i.e., English).</S>\n\t\t\t<S sid=\"46\" ssid=\"15\">Finally, note that a number of statistical MT systems make use of source language syntax in transducer-style approaches; see (Lin, 2004; Ding and Palmer, 2005; Quirk et al, 2005; Liu et al,2006; Huang et al, 2006).</S>\n\t\t\t<S sid=\"47\" ssid=\"16\">In contrast to the prepro cessing approach, they attempt to incorporate syntax directly into the decoding stage.</S>\n\t</SECTION>\n\t<SECTION title=\"Chinese Syntactic Reordering Rules. \" number=\"3\">\n\t\t\t<S sid=\"48\" ssid=\"1\">We used the Penn Chinese Treebank guidelines (Xueet al, 2005) in searching for a suitable set of reordering rules.</S>\n\t\t\t<S sid=\"49\" ssid=\"2\">We examined all phrase types in the Treebank; potentially phrases of any type could be can didates for reordering rules.</S>\n\t\t\t<S sid=\"50\" ssid=\"3\">Table 1 provides a list of Treebank phrase tags for easy reference.</S>\n\t\t\t<S sid=\"51\" ssid=\"4\">We ruled out several phrase types as not requiring reordering ADJP adjective phrase ADVP adverbial phrase headed by AD (adverb) CLP classifier phrase CP clause headed by C (complementizer) DNP phrase formed by ?XP+DEG?</S>\n\t\t\t<S sid=\"52\" ssid=\"5\">DP determiner phrase DVP phrase formed by ?XP+DEV?</S>\n\t\t\t<S sid=\"53\" ssid=\"6\">FRAG fragment IP simple clause headed by I (INFL) LCP phrase formed by ?XP+LC?</S>\n\t\t\t<S sid=\"54\" ssid=\"7\">LST list marker NP noun phrase PP preposition phrase PRN parenthetical QP quantifier phrase UCP unidentical coordination phrase VP verb phrase Table 1: Penn Chinese Treebank phrase tags.</S>\n\t\t\t<S sid=\"55\" ssid=\"8\">rules.</S>\n\t\t\t<S sid=\"56\" ssid=\"9\">For example, Chinese ADJPs, ADVPs, DPs,QPs, and PPs all have similar internal word order ing to their English counterparts.</S>\n\t\t\t<S sid=\"57\" ssid=\"10\">Also similar are a group of special structures such as LST, FRAG, and PRN.</S>\n\t\t\t<S sid=\"58\" ssid=\"11\">We identified three categories that we consideredto be the most prominent candidates for reorder ing.</S>\n\t\t\t<S sid=\"59\" ssid=\"12\">These phrases include VPs (verb phrases), NPs (noun phrases), and LCPs (localizer phrases, which frequently map to prepositional phrases in English).</S>\n\t\t\t<S sid=\"60\" ssid=\"13\">In the following, we discuss each of the three main categories in more detail.</S>\n\t\t\t<S sid=\"61\" ssid=\"14\">3.1 Verb Phrases.</S>\n\t\t\t<S sid=\"62\" ssid=\"15\">In Chinese, verb phrase modifiers typically occur in pre-verbal position.</S>\n\t\t\t<S sid=\"63\" ssid=\"16\">VP modifiers can be ADVPs, temporal and spatial NPs, QP, PPs, CPs, IPs, DVPs, and LCPs.</S>\n\t\t\t<S sid=\"64\" ssid=\"17\">The ADVPs are simple adverbs, which can occur both preverbal and postverbal in an English verb phrase, so we do not attempt to move them.</S>\n\t\t\t<S sid=\"65\" ssid=\"18\">Similarly, the CP, IP, and DVP modifiers are typically adverbial phrases, which do not have afixed position in English verb phrases.</S>\n\t\t\t<S sid=\"66\" ssid=\"19\">In the follow ing, we only consider cases involving PPs, LCPs, temporal and spatial NPs, and QPs.</S>\n\t\t\t<S sid=\"67\" ssid=\"20\">PPs and LCPs Figure 2 shows an example verb phrase with a PP modifier, which translates literally 739 VP PP P 3(at) NP-A NPB NN ??(Eastern) NN ??(Division) VP-A VV ?(rank) QP OD 1?(10th) Figure 2: Example VP with PP modifier.</S>\n\t\t\t<S sid=\"68\" ssid=\"21\">The phrase translates into ?ranks 10th in the Eastern Division.?</S>\n\t\t\t<S sid=\"69\" ssid=\"22\">VP NP NPB NT U(same day) NT ??(morning) VP-A VV uL(issue) NP-A NPB NN (?(statement) Figure 3: Example VP with temporal NP modifier.</S>\n\t\t\t<S sid=\"70\" ssid=\"23\">The phrase translates into ?issued a statement that morning.?</S>\n\t\t\t<S sid=\"71\" ssid=\"24\">into ?at Eastern Division rank 10th.?</S>\n\t\t\t<S sid=\"72\" ssid=\"25\">Recognizingthat PPs in English verb phrases almost always occur after the verb, we use a simple VP(PP:VP) re ordering rule which states that a PP in a parent VP needs to be repositioned after the sibling VP.</S>\n\t\t\t<S sid=\"73\" ssid=\"26\">LCPs are similar to PPs and typically map to prepositional phrases in English.</S>\n\t\t\t<S sid=\"74\" ssid=\"27\">Thus they are handled similarly to PPs, i.e., LCPs in a parent VP are repositioned after the sibling VP.</S>\n\t\t\t<S sid=\"75\" ssid=\"28\">NPs Figure 3 gives an example of a verb phrasewith a temporal NP modifier, which literally trans lates into ?same day morning issue statement.?</S>\n\t\t\t<S sid=\"76\" ssid=\"29\">InEnglish, temporal phrases such as these almost al ways occur after the head verb.</S>\n\t\t\t<S sid=\"77\" ssid=\"30\">Conveniently, the Chinese Treebank uses the part of speech (POS) tag NT for temporal nouns.</S>\n\t\t\t<S sid=\"78\" ssid=\"31\">Thus, we use a rule whichstates that a preverbal NP will be repositioned af ter the sibling VP if there is at least one NT in the NP subtree.</S>\n\t\t\t<S sid=\"79\" ssid=\"32\">A similar rule might apply to locative NPS; however, there is no special POS tag in the Treebank marking locations,1 so we do not have a syntax-based reordering rule to handle locative NPs.QPs QP modifiers in verb phrases often corre spond to time-related concepts such as duration and frequency.</S>\n\t\t\t<S sid=\"80\" ssid=\"33\">Figure 4 shows an example verb phrase with a QP modifier, literally translating into ?many time injured.?</S>\n\t\t\t<S sid=\"81\" ssid=\"34\">Since temporal phrases almost alwaysoccur after the verb in English verb phrases, we han 1One can argue that NR (proper nouns) in that context are likely to be places.</S>\n\t\t\t<S sid=\"82\" ssid=\"35\">However, there also exist many exceptions, and so we decided not to exploit the NR tag.</S>\n\t\t\t<S sid=\"83\" ssid=\"36\">VP QP CD ?(many) CLP M g(time) VP-A VV ??(injured) Figure 4: Example VP with QP modifier.</S>\n\t\t\t<S sid=\"84\" ssid=\"37\">The phrase translates into ?injured many times.?</S>\n\t\t\t<S sid=\"85\" ssid=\"38\">NP-A DNP PP P ?(to) NP-A NPB NR 9n??(Zimbabwe) DEG (DEG) NPB NN ?L(financial) NN ?(aid)Figure 5: An example Chinese NP with a DNP mod ifier headed by a PP.</S>\n\t\t\t<S sid=\"86\" ssid=\"39\">The phrase translates into ?the financial aid to Zimbabwe?</S>\n\t\t\t<S sid=\"87\" ssid=\"40\">in English.</S>\n\t\t\t<S sid=\"88\" ssid=\"41\">dle such cases by a simple rule which states that theQP in a parent VP will be repositioned after the sib ling VP.</S>\n\t\t\t<S sid=\"89\" ssid=\"42\">3.2 Noun Phrases.</S>\n\t\t\t<S sid=\"90\" ssid=\"43\">Noun phrases in Chinese can take several types of modifiers: for example, phrases of type QP, DP, ADJP, NP, DNP, and CP.</S>\n\t\t\t<S sid=\"91\" ssid=\"44\">The placement of QP, DP, and ADJP modifiers is somewhat similar to English in that these phrases typically occur before the noun they modify.</S>\n\t\t\t<S sid=\"92\" ssid=\"45\">The case of NP modifiers in NPs is very limited in the Chinese Treebank, since most noun-noun sequences form compounds in a single NP.</S>\n\t\t\t<S sid=\"93\" ssid=\"46\">Hence we only developed reordering rules to handle DNP and clausal (CP) modifiers.</S>\n\t\t\t<S sid=\"94\" ssid=\"47\">DNPs DNPs are formed by ?XP+DEG,?</S>\n\t\t\t<S sid=\"95\" ssid=\"48\">where XP can be a phrase of the type ADJP, QP, PP, LCP, or NP.</S>\n\t\t\t<S sid=\"96\" ssid=\"49\">When the XP is an ADJP or a QP, no reordering is needed because the word order is the same as that of English.When the XP is a PP or an LCP, the DNP essentially corresponds to a prepositional phrase in En glish, which almost always appears after the noun being modified.</S>\n\t\t\t<S sid=\"97\" ssid=\"50\">Figure 5 shows an example where the XP in the DNP is a PP.</S>\n\t\t\t<S sid=\"98\" ssid=\"51\">The reordering rule to handle these two cases states that, if a parent NP has a child DNP which in turn has a child PP or LCP, then the DNP is repositioned after the last sibling NP.</S>\n\t\t\t<S sid=\"99\" ssid=\"52\">Figure 6 shows an example noun phrase for which the XP in the DNP is NP.</S>\n\t\t\t<S sid=\"100\" ssid=\"53\">On the surface, the Chinese?NP1 DEG NP2?</S>\n\t\t\t<S sid=\"101\" ssid=\"54\">sequence is analogous to the En glish possessive structure of ?NP1?s NP2?</S>\n\t\t\t<S sid=\"102\" ssid=\"55\">and does 740 NP-A DNP NP DP DT T(this) CLP M ?(measure word) NPB NN E?(technique) DEG (DEG) NPB NN ??(mastery) Figure 6: An example Chinese NP phrase with a DNP modifier headed by a NP.</S>\n\t\t\t<S sid=\"103\" ssid=\"56\">The phrase translates into ?the mastery of this technique?</S>\n\t\t\t<S sid=\"104\" ssid=\"57\">in English.</S>\n\t\t\t<S sid=\"105\" ssid=\"58\">not require reordering, for example, ??(Sue) (?s)*l(friend)?</S>\n\t\t\t<S sid=\"106\" ssid=\"59\">in Chinese and ?Sue?s friend?</S>\n\t\t\t<S sid=\"107\" ssid=\"60\">in En glish.</S>\n\t\t\t<S sid=\"108\" ssid=\"61\">However, the Chinese possessive structure?NP1 DEG NP2?</S>\n\t\t\t<S sid=\"109\" ssid=\"62\">can express more sophisticated re lationships which are inappropriate for the ?NP1?sNP2?</S>\n\t\t\t<S sid=\"110\" ssid=\"63\">expression.</S>\n\t\t\t<S sid=\"111\" ssid=\"64\">For example, the phrase in Fig ure 6 can only be translated into ?the mastery of this technique,?</S>\n\t\t\t<S sid=\"112\" ssid=\"65\">but not ?this technique?s mastery.?We decide to reorder DNPs of the ?NP+DEG?</S>\n\t\t\t<S sid=\"113\" ssid=\"66\">for mat, because they often can only map to the ?NP2 of NP1?</S>\n\t\t\t<S sid=\"114\" ssid=\"67\">expression in English.</S>\n\t\t\t<S sid=\"115\" ssid=\"68\">Additionally, the ?NP2 of NP1?</S>\n\t\t\t<S sid=\"116\" ssid=\"69\">expression is more general and can replace ?NP1?s NP2?</S>\n\t\t\t<S sid=\"117\" ssid=\"70\">in many cases.</S>\n\t\t\t<S sid=\"118\" ssid=\"71\">One exception is when the NP is a pronoun (PN), e.g., ?(he) (?s) ? i(name),?</S>\n\t\t\t<S sid=\"119\" ssid=\"72\">in which case the DNP acts simply like a possessive pronoun.</S>\n\t\t\t<S sid=\"120\" ssid=\"73\">Our reordering rule thus states that, if a parent NP has a child DNPwhich in turn hasa child NP that is not a PN, then the DNP is reposi tioned after the last sibling NP.CPs Relative clauses correspond to the CP cate gory in the Treebank.</S>\n\t\t\t<S sid=\"121\" ssid=\"74\">Figure 7 shows an examplenoun phrase with two nested CP modifiers.</S>\n\t\t\t<S sid=\"122\" ssid=\"75\">As illus trated in the figure, relative clauses in Chinese also occur before the noun they modify, which makes the word order of this sentence quite different from that of the English translation.</S>\n\t\t\t<S sid=\"123\" ssid=\"76\">Such distortions in the word reordering will be quite difficult for the word or phrase-based alignment model to capture.</S>\n\t\t\t<S sid=\"124\" ssid=\"77\">However, with the application of a reordering ruleto reposition the child CP after its sibling NP un der a parent NP, and the PP VP reordering rule for VP introduced previously, the sentence can be easily transformed into ?French delegation participate 8th handicap people Winter Olympics hold at US Salt Lake City,?</S>\n\t\t\t<S sid=\"125\" ssid=\"78\">a sentence whose word order is much closer to that of English.</S>\n\t\t\t<S sid=\"126\" ssid=\"79\">CP is typically formed by ?IP+DEC?, in which DEC?s only function is to mark the IP as a relative NP CP IP VP VV ?\\ (participate) NP CP IP VP PP P 3 (at) NP NR {I(US) NR ??</S>\n\t\t\t<S sid=\"127\" ssid=\"80\">(Salt Lake City) VP VV ?1 (hold) DEC  (DEC) QP OD 1l (8th) CLP M 3 (measure word) NPB NN ?; (handicap people) NR ? ?</S>\n\t\t\t<S sid=\"128\" ssid=\"81\">(Winter Olympics) DEC  (DEC) NPB NR {I (French) NPB NN ?L?</S>\n\t\t\t<S sid=\"129\" ssid=\"82\">(delegation)Figure 7: An example with two nested CP modifiers.</S>\n\t\t\t<S sid=\"130\" ssid=\"83\">The phrase translates into ?the French delega tion participating in the 8th Special Winter Olympics held in Salt Lake City US.?</S>\n\t\t\t<S sid=\"131\" ssid=\"84\">LCP IP NP-A NPB NN ?(accident) VP VV u)(happen) LC </S></SECTION></PAPER>",
                    "generation_prompt_uid": "486953b40577b0a31c57d57ed10b6968",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Chinese Syntactic Reordering for Statistical Machine Translation"
                }
            }
        },
        {
            "index": 21,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies. The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank. It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery. In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary. We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design offuture treebanks.",
                    "filename": "CCGbank_A_Corpus_of_CCG_Derivations_and_Dependency_Structures_Extracted_from_the_Penn_Treebank",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/J07-3004/Documents_xml/J07-3004.xml",
                    "title": "CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank"
                },
                "llm": {
                    "descriptions": [
                        "This paper introduces CCGbank, a corpus derived from the Penn Treebank, which provides Combinatory Categorial Grammar (CCG) derivations and word-word dependency structures for 99.4% of the sentences in the original Treebank. The authors developed an algorithm to translate the phrase-structure annotations of the Treebank into CCG derivations and dependencies, addressing various linguistic challenges and inconsistencies in the process. CCGbank is available from the Linguistic Data Consortium and has been employed to train wide-coverage statistical parsers that achieve state-of-the-art performance in dependency recovery. The corpus is expected to facilitate the development of robust parsers capable of generating detailed syntactic and semantic analyses, and may inform the construction of future treebanks and the extraction of other linguistically expressive grammars.",
                        "This paper introduces CCGbank, a corpus derived from the Penn Treebank, providing a resource that contains Combinatory Categorial Grammar (CCG) derivations and word-word dependencies. The paper describes an algorithm for transforming phrase-structure annotations into CCG derivations and discusses preprocessing steps necessary to align with the linguistic adequacy of CCG. The authors explore the implications of their work for extracting other expressive grammars from the Treebank and for future treebank design. CCGbank has been used to train statistical parsers, yielding state-of-the-art dependency recovery rates. The paper also addresses the challenges of distinguishing between complements and adjuncts, the treatment of multi-word expressions, and the representation of various syntactic constructions within the corpus. Despite some limitations and inconsistencies in the original Treebank annotations, CCGbank serves as a rich resource for wide-coverage parsing and potential semantic interpretation tasks.",
                        "This paper introduces the creation of CCGbank, a corpus derived from the Penn Treebank that contains Combinatory Categorial Grammar (CCG) derivations and word-word dependencies. An algorithm was developed to translate phrase-structure trees from the Penn Treebank into CCG derivations while addressing noise and inconsistencies in the original annotation. The resulting CCGbank covers 99.4% of the Penn Treebank sentences, providing a resource for training statistical parsers that can recover both local and long-range dependencies. The paper discusses preprocessing steps, the distinction between complements and adjuncts, and other challenges faced during translation. CCGbank facilitates the construction of wide-coverage parsers, the development of semantic interpretations, and may support tasks like summarization and question answering. The experiences and techniques reported are also valuable for future treebank designs and other formalisms' corpus translations.",
                        "This paper introduces CCGbank, a corpus of Combinatory Categorial Grammar (CCG) derivations and dependency structures derived from the Penn Treebank. The authors present an algorithm that translates the Penn Treebank into CCG derivations, capturing both local and long-range word-word dependencies. CCGbank covers 99.4% of the Penn Treebank sentences and facilitates the training of statistical parsers capable of state-of-the-art dependency recovery. To enhance linguistic adequacy, the authors performed extensive analysis and preprocessing to correct noise and inconsistencies in the original Penn Treebank annotations. The corpus is useful for training parsers that can handle complex linguistic phenomena, and it is suggested that the experiences from this project could benefit the design of future treebanks and the extraction of grammars in other formalisms. CCGbank is available from the Linguistic Data Consortium and has already aided in developing wide-coverage CCG parsers and potential semantic interpretation tools."
                    ],
                    "filename": "ccgbank_a_corpus_of_ccg_derivati-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 130 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank</S>\n  <SECTION title=\"\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">This article presents an algorithm for translating the Penn Treebank into a corpus of Combinatory Categorial Grammar (CCG) derivations augmented with local and long-range word–word dependencies.</S>\n    <S sid=\"7\" ssid=\"2\">The resulting corpus, CCGbank, includes 99.4% of the sentences in the Penn Treebank.</S>\n    <S sid=\"8\" ssid=\"3\">It is available from the Linguistic Data Consortium, and has been used to train widecoverage statistical parsers that obtain state-of-the-art rates of dependency recovery.</S>\n    <S sid=\"9\" ssid=\"4\">In order to obtain linguistically adequate CCG analyses, and to eliminate noise and inconsistencies in the original annotation, an extensive analysis of the constructions and annotations in the Penn Treebank was called for, and a substantial number of changes to the Treebank were necessary.</S>\n    <S sid=\"10\" ssid=\"5\">We discuss the implications of our findings for the extraction of other linguistically expressive grammars from the Treebank, and for the design offuture treebanks.</S>\n  </SECTION>\n  <SECTION title=\"1.\" number=\"2\">\n    <S sid=\"11\" ssid=\"1\">In order to understand a newspaper article, or any other piece of text, it is necessary to construct a representation of its meaning that is amenable to some form of inference.</S>\n    <S sid=\"12\" ssid=\"2\">This requires a syntactic representation which is transparent to the underlying semantics, making the local and long-range dependencies between heads, arguments, and modifiers explicit.</S>\n    <S sid=\"13\" ssid=\"3\">It also requires a grammar that has sufficient coverage to deal with the vocabulary and the full range of constructions that arise in free text, together with a parsing model that can identify the correct analysis among the many alternatives that such a wide-coverage grammar will generate even for the simplest sentences.</S>\n    <S sid=\"14\" ssid=\"4\">Given our current machine learning techniques, such parsing models typically need to be trained on relatively large treebanks—that is, text corpora hand-labeled with detailed syntactic structures.</S>\n    <S sid=\"15\" ssid=\"5\">Because such annotation requires linguistic expertise, and is therefore difficult to produce, we are currently limited to at most a few treebanks per language.</S>\n    <S sid=\"16\" ssid=\"6\">One of the largest and earliest such efforts is the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993; Marcus et al. 1994), which contains a one-million word subcorpus of Wall Street Journal text that has become the de facto standard training and test data for statistical parsers.</S>\n    <S sid=\"17\" ssid=\"7\">Its annotation, which is based on generic phrasestructure grammar (with coindexed traces and other null elements indicating non-local dependencies) and function tags on nonterminal categories providing (a limited degree of) syntactic role information, is designed to facilitate the extraction of the underlying predicate–argument structure.</S>\n    <S sid=\"18\" ssid=\"8\">Statistical parsing on the Penn Treebank has made great progress by focusing on the machine-learning or algorithmic aspects (Magerman 1994; Ratnaparkhi 1998; Collins 1999; Charniak 2000; Henderson 2004; McDonald, Crammer, and Pereira 2005).</S>\n    <S sid=\"19\" ssid=\"9\">However, this has often resulted in parsing models and evaluation measures that are both based on reduced representations which simplify or ignore the linguistic information represented by function tags and null elements in the original Treebank.</S>\n    <S sid=\"20\" ssid=\"10\">(One exception is Collins 1999, whose Model 2 includes a distinction between arguments and adjuncts, and whose Model 3 additionally captures wh-movement in relative clauses with a GPSG-like “slash-feature-passing” mechanism.)</S>\n    <S sid=\"21\" ssid=\"11\">The reasons for this shift away from linguistic adequacy are easy to trace.</S>\n    <S sid=\"22\" ssid=\"12\">The very healthy turn towards quantitative evaluation interacts with the fact that just about every dimension of linguistic variation exhibits a Zipfian distribution, where a very small proportion of the available alternatives accounts for most of the data.</S>\n    <S sid=\"23\" ssid=\"13\">This creates a temptation to concentrate on capturing the few high-frequency cases at the top end of the distribution, and to ignore the “long tail” of rare events such as non-local dependencies.</S>\n    <S sid=\"24\" ssid=\"14\">Despite the fact that these occur in a large number of sentences, they affect only a small number of words, and have thus a small impact on overall dependency recovery.</S>\n    <S sid=\"25\" ssid=\"15\">Although there is now a sizable literature on trace and function-tag insertion algorithms (Blaheta and Charniak 2000; Johnson 2002; Campbell 2004), and integrated parsing with function tags or null elements (Dienes and Dubey 2003a, 2003b; Merlo and Musillo 2005; Gabbard, Kulick, and Marcus 2006), such approaches typically require additional pre- or postprocessing steps that are likely to add further noise and errors to the parser output.</S>\n    <S sid=\"26\" ssid=\"16\">A completely integrated approach that is based on a syntactic representation which allows direct recovery of the underlying predicate–argument structure might therefore be preferable.</S>\n    <S sid=\"27\" ssid=\"17\">Such representations are provided by grammar formalisms that are more expressive than simple phrase-structure grammar, like Lexical-Functional Grammar (LFG) (Kaplan and Bresnan 1982), Head-driven Phrase-Structure Grammar (HPSG) (Pollard and Sag 1994), Tree-Adjoining Grammar (TAG) (Joshi and Schabes 1992), Minimalist Program–related Grammars (Stabler 2004), or Combinatory Categorial Grammar (CCG) (Steedman 1996, 2000).</S>\n    <S sid=\"28\" ssid=\"18\">However, until very recently, only handwritten grammars, which lack the wide coverage and robustness of Treebank parsers, were available for these formalisms (Butt et al. 1999; XTAG-group 1999; Copestake and Flickinger 2000; OpenCCG1 [White and Baldridge 2003; White 2006]).</S>\n    <S sid=\"29\" ssid=\"19\">Because treebank annotation for individual formalisms is prohibitively expensive, there have been a number of efforts to extract TAGs, LFGs, and, more recently, HPSGs, from the Penn Treebank (Xia 1999; Chen and Vijay-Shanker 2000; Xia, Palmer, and Joshi 2000; Xia 2001; Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2004; O’Donovan et al.</S>\n    <S sid=\"30\" ssid=\"20\">2005; Shen and Joshi 2005; Chen, Bangalore, and Vijay-Shanker 2006).</S>\n    <S sid=\"31\" ssid=\"21\">Statistical parsers that are trained on these TAG and HPSG corpora have been presented by Chiang (2000) and Miyao and Tsujii (2005), whereas the LFG parsing system of Cahill et al. (2004) uses a postprocessing step on the output of a Treebank parser to recover predicate–argument dependencies.</S>\n    <S sid=\"32\" ssid=\"22\">In this article we present an algorithmic method for obtaining a corpus of CCG derivations and dependency structures from the Penn Treebank, together with some observations that we believe carry wider implications for similar attempts with other grammar formalisms and corpora.</S>\n    <S sid=\"33\" ssid=\"23\">Earlier versions of the resulting corpus, CCGbank, have already been used to build a number of wide-coverage statistical parsers (Clark, Hockenmaier, and Steedman 2002; Hockenmaier and Steedman 2002; Hockenmaier 2003b, 2003a; Clark and Curran 2004, 2007), which recover both local and long-range dependencies directly and in a single pass.</S>\n    <S sid=\"34\" ssid=\"24\">CCG is a linguistically expressive, but efficiently parseable, lexicalized grammar formalism that was specifically designed to provide a base-generative account of coordinate and relativized constructions like the following: CCG directly captures the non-local dependencies involved in these and other constructions, including control and raising, via an enriched notion of syntactic types, without the need for syntactic movement, null elements, or traces.</S>\n    <S sid=\"35\" ssid=\"25\">It also provides a “surface-compositional” syntax–semantics interface, in which monotonic rules of semantic composition are paired one-to-one with rules of syntactic composition.</S>\n    <S sid=\"36\" ssid=\"26\">The corresponding predicate–argument structure or logical form can therefore be directly obtained from any derivation if the semantic interpretation of each lexical entry is known.</S>\n    <S sid=\"37\" ssid=\"27\">In this article and in CCGbank, we approximate such semantic interpretations with dependency graphs that include most semantically relevant non-anaphoric local and long-range dependencies.</S>\n    <S sid=\"38\" ssid=\"28\">Although certain decisions taken by the builders of the original Penn Treebank mean that the syntactic derivations that can be obtained from the Penn Treebank are not always semantically correct (as we will discuss), subsequent work by Bos et al. (2004) and Bos (2005) has demonstrated that the output of parsers trained on CCGbank can also be directly translated into logical forms such as Discourse Representation Theory structures (Kamp and Reyle 1993), which can then be used as input to a theorem prover in applications like question answering and textual entailment recognition.</S>\n    <S sid=\"39\" ssid=\"29\">Translating the Treebank into this more demanding formalism has revealed certain sources of noise and inconsistency in the original annotation that have had to be corrected in order to permit induction of a linguistically correct grammar.</S>\n    <S sid=\"40\" ssid=\"30\">Because of this preprocessing, the dependency structures in CCGbank are likely to be more consistent than those extracted directly from the Treebank via heuristics such as those given by Magerman (1994) and Collins (1999), and therefore may also be of immediate use for dependency-based approaches.</S>\n    <S sid=\"41\" ssid=\"31\">However, the structure of certain constructions, such as compound nouns or fragments, is deliberately underspecified in the Penn Treebank.</S>\n    <S sid=\"42\" ssid=\"32\">Although we have attempted to semi-automatically restore the missing structure wherever possible, in many cases this would have required additional manual annotation, going beyond the scope of our project.</S>\n    <S sid=\"43\" ssid=\"33\">We suspect that these properties of the original Treebank will affect any similar attempt to extract dependency structures or grammars for other expressive formalisms.</S>\n    <S sid=\"44\" ssid=\"34\">The Penn Treebank is the earliest (and still the largest) corpus of its kind; we hope that our experiences will extend its useful life, and help in the design of future treebanks.</S>\n  </SECTION>\n  <SECTION title=\"2.\" number=\"3\">\n    <S sid=\"45\" ssid=\"1\">Combinatory Categorial Grammar (CCG) was originally developed as a “near-contextfree” theory of natural language grammar, with a very free definition of derivational structure adapted to the analysis of coordination and unbounded dependency without movement or deletion transformations.</S>\n    <S sid=\"46\" ssid=\"2\">It has been successfully applied to the analysis of coordination, relative clauses and related constructions, intonation structure, binding and control, and quantifier scope alternation, in a number of languages—see Steedman and Baldridge (2006) for a recent review.</S>\n    <S sid=\"47\" ssid=\"3\">Extensions of CCG to other languages and word-orders are discussed by Hoffman (1995), Kang (1995), Bozsahin (1998), Komagata (1999), Steedman (2000), Trechsel (2000), Baldridge (2002), and C¸ akıcı (2005).</S>\n    <S sid=\"48\" ssid=\"4\">The derivations in CCGbank follow the analyses of Steedman (1996, 2000), except where noted.</S>\n    <S sid=\"49\" ssid=\"5\">Categorial Grammars are strongly lexicalized, in the sense that the grammar is entirely defined by a lexicon in which words (and other lexical items) are associated with one or more specific categories which completely define their syntactic behavior.</S>\n    <S sid=\"50\" ssid=\"6\">The set of categories consists of basic categories (e.g., S, NP, PP) and complex categories of the form X/Y or X\\Y, representing functors with (basic or complex) argument category Y and result category X. Functor categories of the form X/Y expect their argument Y to its right, whereas those of the form X\\Y expect Y to their left.2 These functor categories encode subcategorization information, that is, the number and directionality of expected arguments.</S>\n    <S sid=\"51\" ssid=\"7\">English intransitive verbs and verb phrases have the category S\\NP: they take a (subject) NP to their left as argument and yield a sentence.</S>\n    <S sid=\"52\" ssid=\"8\">English transitive verbs have the category (S\\NP)/NP: they take an (object) NP to their right to yield a verb phrase (S\\NP), which in turn takes a (subject) NP to its left to form a sentence S. Each syntactic category also has a corresponding semantic interpretation (here given as a A-expression).</S>\n    <S sid=\"53\" ssid=\"9\">Hence, the lexical entry for ditransitive give can be written as follows:3 In our translation algorithm, we use simple word–word dependency structures to approximate the underlying semantic interpretation.</S>\n    <S sid=\"54\" ssid=\"10\">A universal set of syntactic combinatory rules defines how constituents can be combined.</S>\n    <S sid=\"55\" ssid=\"11\">All variants of categorial grammar since Ajdukiewicz (1935) and Bar-Hillel (1953) include function application, where a functor X/Y or X\\Y is applied to an argument Y: These rules give rise to derivations like the following:4 This derivation is isomorphic to a traditional context-free derivation tree like the following (the semantics is omitted): CCG additionally introduces a set of rule schemata based on the combinators of combinatory logic (Curry and Feys 1958), which enable succinct analyses of extraction and coordination constructions.</S>\n    <S sid=\"56\" ssid=\"12\">It is a distinctive property of CCG that all syntactic rules are purely type-driven, unlike traditional structure-dependent transformations.</S>\n    <S sid=\"57\" ssid=\"13\">Composition and substitution allow two functors to combine into another functor, whereas type-raising is a unary rule that exchanges the roles of functor and argument: For example, the following is the derivation of a relative clause related to (4): We will see further examples of their use later.</S>\n    <S sid=\"58\" ssid=\"14\">Such rules induce additional derivational ambiguity, even in canonical sentences like (4).</S>\n    <S sid=\"59\" ssid=\"15\">However, our translation algorithm yields normal form derivations (Hepple and Morrill 1989; Wittenburg and Wall 1991; K¨onig 1994; Eisner 1996), which use composition and type-raising only when syntactically necessary.</S>\n    <S sid=\"60\" ssid=\"16\">For coordination, we will use a binarized version of the following ternary rule schema:5 For further explanation and linguistics and computational motivation for this theory of grammar, the reader is directed to Steedman (1996, 2000).</S>\n    <S sid=\"61\" ssid=\"17\">The syntactic derivations in CCGbank are accompanied with bilexical head-dependency structures, which are defined in terms of the lexical heads of functor categories and their arguments.</S>\n    <S sid=\"62\" ssid=\"18\">The derivation in (6) corresponds to the following dependency structure, which includes the long-range dependency between give and money: The dependency structures in CCGbank are intended to include all non-anaphoric local and long-range dependencies relevant to determining semantic predicate–argument relations, and hence approximate more fine-grained semantic representations.</S>\n    <S sid=\"63\" ssid=\"19\">In this, they differ crucially from the bilexical surface dependencies used by the parsing models of Collins (1999) and Charniak (2000) and returned by the dependency parser of McDonald, Crammer, and Pereira (2005).</S>\n    <S sid=\"64\" ssid=\"20\">In order to obtain such non-local dependencies, certain types of lexical category such as relative pronouns or raising and control verbs require additional coindexation information (described subsequently).</S>\n    <S sid=\"65\" ssid=\"21\">We believe that CCGbank’s extensive annotation of non-local predicate–argument dependencies is one of its most useful features for researchers using other expressive grammar formalisms, including LFG, HPSG, and TAG, facilitating comparisons in terms of error analyses of particular constructions or types of dependency, such as non-subject extracted relative clauses.</S>\n    <S sid=\"66\" ssid=\"22\">Because these dependency structures provide a suitable approximation of the underlying semantics, and because each interpretation unambiguously corresponds to one dependency structure (but may be obtained from multiple, equivalent, derivations), we furthermore follow Lin (1998) and Carroll, Minnen, and Briscoe (1999) in regarding them as a fairer, and ultimately more useful, standard against which to evaluate the output of parsers trained on CCGbank than the syntactic derivations themselves.</S>\n  </SECTION>\n  <SECTION title=\"3.\" number=\"4\">\n    <S sid=\"67\" ssid=\"1\">The Wall Street Journal subcorpus of the Penn Treebank contains about 50,000 sentences, or 1 million words, annotated with part-of-speech tags and phrase-structure trees: These trees are relatively flat: modals and auxiliaries introduce a new VP level, whereas verb modifiers and arguments typically appear all at the same level, as sisters of the main verb.</S>\n    <S sid=\"68\" ssid=\"2\">A similarly flat annotation style is adopted at the sentence level.</S>\n    <S sid=\"69\" ssid=\"3\">NPs are flat as well, with all complex modifiers appearing at the same NP level, and compound nouns typically lacking any internal structure.</S>\n    <S sid=\"70\" ssid=\"4\">The translation algorithm needs to identify syntactic heads, and has to distinguish between complements and modifiers.</S>\n    <S sid=\"71\" ssid=\"5\">In the Treebank, this information is not explicit.</S>\n    <S sid=\"72\" ssid=\"6\">Although some non-terminal nodes carry additional function tags, such as -SBJ (subject) or -TMP (temporal modifier), truly problematic cases such as prepositional phrases are often marked with tags such as -CLR (“closely related”) or -DIR (“direction”), which are not always reliable or consistent indicators that a constituent is a modifier or an argument.</S>\n    <S sid=\"73\" ssid=\"7\">The Treebank uses various types of null elements and traces to encode non-local dependencies.</S>\n    <S sid=\"74\" ssid=\"8\">These are essential for our algorithm since they make it possible to obtain correct CCG derivations for relative clauses, wh-questions, and coordinate constructions such as right node raising.</S>\n    <S sid=\"75\" ssid=\"9\">Their treatment is discussed in Sections 6.2 and 6.3.</S>\n  </SECTION>\n  <SECTION title=\"4.\" number=\"5\">\n    <S sid=\"76\" ssid=\"1\">In order to obtain CCG derivations from the Penn Treebank, we need to define a mapping from phrase structure trees to CCG derivations, including a treatment of the null elements in the Treebank.</S>\n    <S sid=\"77\" ssid=\"2\">We also need to modify the Treebank where its syntactic analyses differ from CCG, and clean up certain sources of noise that would otherwise result in incorrect CCG derivations.</S>\n    <S sid=\"78\" ssid=\"3\">We will begin by ignoring null elements, and assume that Penn Treebank trees are entirely consistent with CCG analyses.</S>\n    <S sid=\"79\" ssid=\"4\">The basic algorithm then consists of four steps: Similar algorithms for phrase-structure trees without traces or other null elements have been suggested by Buszkowski and Penn (1990) and Osborne and Briscoe (1998).</S>\n    <S sid=\"80\" ssid=\"5\">We illustrate this basic algorithm using the previous example (9).</S>\n    <S sid=\"81\" ssid=\"6\">Then we will extend this algorithm to deal with coordination, and introduce a modification to cope with the fact that certain word classes, such as participials, can act as modifiers of a large number of constituent types.</S>\n    <S sid=\"82\" ssid=\"7\">Section 5 summarizes the most important preprocessing steps that were necessary to obtain the desired CCG analyses from the Treebank trees.</S>\n    <S sid=\"83\" ssid=\"8\">Section 6 extends this basic algorithm to deal with the null elements in the Treebank.</S>\n    <S sid=\"84\" ssid=\"9\">First, the constituent type of each node (head (h), complement (c), or adjunct (a)) is determined, using heuristics adapted from Magerman (1994) and Collins (1999), which take the label of a node and its parent into account.6 We assume that NP daughters of VPs are complements, unless they carry a function tag such as -LOC, -DIR, -TMP, and so on, but treat all PPs as adjuncts unless they carry the -CLR function tag.</S>\n    <S sid=\"85\" ssid=\"10\">In our example, we therefore treat passing as transitive, even though it should subcategorize for the PP: This binarization process inserts dummy nodes into the tree such that all children to the left of the head branch off in a right-branching tree, and then all children to the right of the head branch off in a left-branching tree.7 We assign CCG categories to the nodes in this binary tree in the following manner: 4.3.1 The Root Node.</S>\n    <S sid=\"86\" ssid=\"11\">The category of the root node is determined by the label of the root of the Treebank tree (e.g., {VP} → S\\NP, {S, SINV, SQ} → S).8 If the root node has the category S, it typically carries a feature that distinguishes different types of sentences, such as declaratives (S[dcl]), wh-questions (S[wq]), yes–no questions (S[q]), or fragments (S[frg]).</S>\n    <S sid=\"87\" ssid=\"12\">In our running example, the root is S[dcl], because its Treebank label is S, and its head word, the auxiliary, has the POS tag VBZ.</S>\n    <S sid=\"88\" ssid=\"13\">4.3.2 Head and Complement.</S>\n    <S sid=\"89\" ssid=\"14\">The category of a complement child is defined by a similar mapping from Treebank labels to categories, for example, {NP} → NP, {PP} → PP.9 The CCG category of the head is a function which takes the category of the complement as argument and returns the category of the parent node.</S>\n    <S sid=\"90\" ssid=\"15\">The direction of the slash is given by the position of the complement relative to the head: The VP that is headed by the main verb passing is a complement of the auxiliary.</S>\n    <S sid=\"91\" ssid=\"16\">Because the POS tag of passing is VBG, the CCG category of the complement VP is S[ng]\\NP (present participle) and the lexical category of is is therefore (S[dcl]\\NP)/(S[ng]\\NP): is just passing the buck to young people Other VP features include [to] (to infinitival), [b] (bare infinitival), S[pt] (past participle), [pss] (passive), or [ng] (present participle).</S>\n    <S sid=\"92\" ssid=\"17\">4.3.3 Head and Adjunct.</S>\n    <S sid=\"93\" ssid=\"18\">According to the Treebank annotation and the assumptions of the algorithm, our example has two VP adjuncts: the adverb just, and, because of its -DIR function tag, the PP to young people.</S>\n    <S sid=\"94\" ssid=\"19\">In both cases, the adjunct category depends on the category of the parent, and the category of the head child is copied from the parent: Given a parent category C, the category of an adjunct child is a unary functor C'/C' if the adjunct child is to the left of the head child (a premodifier), or C'\\C' if it is to the right Function composition reduces the number of lexical categories of adjuncts. of the head (a postmodifier).</S>\n    <S sid=\"95\" ssid=\"20\">In most cases, the category C' is equal to the parent category C without any features such as [dcl], [ng], and so forth, and the modifier combines with the head via simple function application.</S>\n    <S sid=\"96\" ssid=\"21\">As shown in Figure 1, in many cases, a more elegant (and general) analysis can be obtained if we allow modifiers to compose with the head.</S>\n    <S sid=\"97\" ssid=\"22\">For example, regularly has the category (S\\NP)\\(S\\NP) in sentences such as I visit certain places regularly, because it modifies the verb phrase visit certain places, which has the category S[dcl]\\NP.</S>\n    <S sid=\"98\" ssid=\"23\">But in the corresponding relative clause places that I visit regularly or with heavy NP shift (I visit regularly certain places in Europe), regularly modifies visit, that is, a constituent with category (S[dcl]\\NP)/NP.</S>\n    <S sid=\"99\" ssid=\"24\">Without function composition, the category of regularly would have to be ((S\\NP)/NP)\\((S\\NP)/NP), but (crossed) composition allows the ordinary category (S\\NP)\\(S\\NP) to also work in this case.</S>\n    <S sid=\"100\" ssid=\"25\">Therefore, if the parent (and head) category C is of the form X/$, the algorithm strips off all outermost forward arguments /$ (and syntactic features) from C to obtain C'.</S>\n    <S sid=\"101\" ssid=\"26\">Similarly, if C is of the form X\\$, all outermost backward arguments \\$ (and syntactic features) are stripped off from C to obtain C'.</S>\n    <S sid=\"102\" ssid=\"27\">4.3.4 Head and Punctuation Mark.</S>\n    <S sid=\"103\" ssid=\"28\">With the exception of some dashes and parentheses (see Section 4), the category of a punctuation mark is identical to its POS tag, and the head has the same category as its parent.</S>\n    <S sid=\"104\" ssid=\"29\">4.3.5 The Final Derivation.</S>\n    <S sid=\"105\" ssid=\"30\">Figure 2 shows the complete CCG derivation of our example.</S>\n    <S sid=\"106\" ssid=\"31\">The category assignment procedure corresponds to a top-down normal-form derivation, which almost always uses function application.</S>\n    <S sid=\"107\" ssid=\"32\">In the basic case presented here, composition is only used to provide a uniform analysis of adjuncts.</S>\n    <S sid=\"108\" ssid=\"33\">Long-range dependencies represented in the Penn Treebank by traces such as *T* and *RNR* require extensions to the basic algorithm, which result in derivations that make use of typeraising, composition, and (occasionally) substitution rules like those in (5) wherever syntactically necessary.</S>\n    <S sid=\"109\" ssid=\"34\">We defer explanation of these rules until Section 6, which presents the constructions that motivate them.</S>\n    <S sid=\"110\" ssid=\"35\">Finally, we need to obtain the word–word dependencies which approximate the underlying predicate–argument structure.</S>\n    <S sid=\"111\" ssid=\"36\">This is done by a bottom-up procedure, which simply retraces the steps in the CCG derivation that we have now obtained.</S>\n    <S sid=\"112\" ssid=\"37\">The CCG derivation with corresponding dependencies and dependency graph for example (9).</S>\n    <S sid=\"113\" ssid=\"38\">All categories in CCGbank, including results and arguments of complex categories, are associated with a corresponding list of lexical heads.</S>\n    <S sid=\"114\" ssid=\"39\">This list can be empty (in the case of yet uninstantiated arguments of functor categories), or it can consist of one or more tokens.</S>\n    <S sid=\"115\" ssid=\"40\">Lexical categories have one lexical head, the word itself—for example, He for the first NP, and is for the (S[dcl]\\NP)/(S[b]\\NP).</S>\n    <S sid=\"116\" ssid=\"41\">All dependencies are defined in terms of the heads of lexical functor categories and of their arguments.</S>\n    <S sid=\"117\" ssid=\"42\">In order to distinguish the slots filled by different arguments, we number the arguments of complex lexical categories from left to right in the category notation (that is, from innermost to outermost argument in a purely applicative derivation), for example, (S[ng]\\NP1)/NP2, or ((S[b]\\NP1)/(S[to]\\NP)2)/NP3.</S>\n    <S sid=\"118\" ssid=\"43\">In lexical functor categories such as that of the auxiliary, (S[dcl]\\NP)/(S[b]\\NP), the lexical head of all result categories (S[dcl]\\NP and S[dcl]) is identical to the lexical head of the entire category (i.e., is).</S>\n    <S sid=\"119\" ssid=\"44\">But in functor categories that represent modifiers, such as the adverb (S\\NP)/(S\\NP), the head of the result (the modified verb phrase) comes from the argument (the unmodified verb phrase).</S>\n    <S sid=\"120\" ssid=\"45\">We use indices on the categories to represent this information: (S\\NP)i/(S\\NP)i.</S>\n    <S sid=\"121\" ssid=\"46\">In CCGbank, modifier categories are easily identified by the fact that they are of the form X|X or (X|X) |... (with  |either / or \\), where X does not have any of the features described previously, such as [dcl], [b].</S>\n    <S sid=\"122\" ssid=\"47\">Similarly, determiners (the) take a noun (N, buck) as argument to form a (non-bare) noun phrase whose lexical head comes from the noun: NP[nb]i/Ni.</S>\n    <S sid=\"123\" ssid=\"48\">Thus, the lexical head of the noun phrase the buck is buck, not the.</S>\n    <S sid=\"124\" ssid=\"49\">We also use this coindexation mechanism for lexical categories that project nonlocal dependencies.</S>\n    <S sid=\"125\" ssid=\"50\">For instance, the category of the auxiliary, (S[dcl]\\NP)/(S[ng]\\NP), mediates a dependency between the subject (He) and the main verb (passing).</S>\n    <S sid=\"126\" ssid=\"51\">Like all lexical categories of auxiliaries, modals and subject-raising verbs, the head of the subject NP is coindexed with the head of subject inside the VP argument: (S[dcl]\\NPi)/(S[ng]\\NPi).</S>\n    <S sid=\"127\" ssid=\"52\">The set of categories that project such dependencies is not acquired automatically, but is given (as a list of category templates) to the algorithm which creates the actual dependency structures.</S>\n    <S sid=\"128\" ssid=\"53\">A complete list of the lexical entries in sections 02–21 of the Treebank which use this coindexation mechanism to project nonlocal dependencies is given in the CCGbank manual (Hockenmaier and Steedman 2005).</S>\n    <S sid=\"129\" ssid=\"54\">We believe that in practice this mechanism is largely correct, even though it is based on the (fundamentally flawed) assumption that all lexical categories that have the same syntactic type project the same dependencies.</S>\n    <S sid=\"130\" ssid=\"55\">It may be possible to use the indices on the PRO-null elements (*-1) in the Treebank to identify and resolve ambiguous cases; we leave this to future research.10 Function application and composition typically result in the instantiation of the lexical head of an argument of some functor category, and therefore create new dependencies, whereas coordination creates a new category whose lexical head lists are concatenations of the head lists of the conjuncts.</S>\n    <S sid=\"131\" ssid=\"56\">When the (S[ng]\\NP1)/NP2 passing is combined with the NP the buck, the lexical head of the NP2 is instantiated with buck.</S>\n    <S sid=\"132\" ssid=\"57\">Similarly, when the adverb just (S\\NP1)/(S\\NP)2 is applied to passing the buck, a dependency between just and passing is created: However, because (S\\NP1)/(S\\NP)2 is a modifier category, the head of the resulting S[ng]\\NP is passing, not just (and no dependency is established between just and its NP1).</S>\n    <S sid=\"133\" ssid=\"58\">In the next step, this S[ng]\\NP is combined with the auxiliary (S[dcl]\\NP1)/(S[ng]\\NP)2.</S>\n    <S sid=\"134\" ssid=\"59\">The NP in the (S[ng]\\NP)2 argument of the auxiliary unifies with the (uninstantiated) NP1 argument of passing.</S>\n    <S sid=\"135\" ssid=\"60\">Because the NP in the (S[ng]\\NP)2 is also coindexed with the subject NP1 of the auxiliary, the NP of the resulting S[dcl]\\NP now has two unfilled dependencies to the subject NP1 of is and passing.</S>\n    <S sid=\"136\" ssid=\"61\">When the entire verb phrase is combined with the subject, He fills both slots: Figure 2 shows the resulting CCG derivation and the corresponding list of word– word dependencies for our example sentence.</S>\n    <S sid=\"137\" ssid=\"62\">It is the latter structure that we claim approximates for present purposes the predicate–argument structure or interpretation of the sentence, and provides the gold standard against which parsers can be evaluated.</S>\n    <S sid=\"138\" ssid=\"63\">In order to deal with coordination, both the tree binarization and the category assignment have to be modified.</S>\n    <S sid=\"139\" ssid=\"64\">In CCGbank, coordination is represented by the following binary rule schemata, rather than the ternary rule (7)—compare to Steedman (1989):11 In order to obtain this analysis from Treebank trees, a separate node that spans only the conjuncts and the conjunction or punctuation marks (comma, semicolon) is inserted if necessary.</S>\n    <S sid=\"140\" ssid=\"65\">Identifying the conjuncts often requires a considerable amount of preprocessing.</S>\n    <S sid=\"141\" ssid=\"66\">These trees are then transformed into strictly right-branching binary trees.</S>\n    <S sid=\"142\" ssid=\"67\">The dummy nodes inserted during binarization receive the same category as the conjuncts, but additionally carry a feature [conj]: An additional modification of the grammar is necessary to deal with “unlike coordinate phrases” (UCP), namely, coordinate constructions where the conjuncts do not belong to the same syntactic category: Such constructions are difficult for any formalism.</S>\n    <S sid=\"143\" ssid=\"68\">This phenomenon could be handled elegantly with a feature hierarchy over categories as proposed by Copestake (2002), Villavicencio (2002), and McConville (2007).</S>\n    <S sid=\"144\" ssid=\"69\">Because the induction of such a hierarchy was beyond the scope of our project, we modify our grammar slightly, and allow the algorithm to use instantiations of a special coordination rule schema, such as: This enables us to analyze the previous example as: In CCG, all language-specific information is associated with the lexical categories of words.</S>\n    <S sid=\"145\" ssid=\"70\">There are many syntactic regularities associated with word classes, however, which may potentially generate a large number of lexical entries for each item in that class.</S>\n    <S sid=\"146\" ssid=\"71\">One particularly frequent example of this is clausal adjuncts.</S>\n    <S sid=\"147\" ssid=\"72\">Figure 3 illustrates how the basic algorithm described above leads to a proliferation of adjunct categories.</S>\n    <S sid=\"148\" ssid=\"73\">For example, a past participle such as used would receive a different category in a reduced relative like Figure 3(a) from its standard category (S[pss]\\NP)/(S[to]\\NP).</S>\n    <S sid=\"149\" ssid=\"74\">As a consequence, modifiers of used would also receive different categories depending on what occurrence of used they modify.</S>\n    <S sid=\"150\" ssid=\"75\">This is undesirable, because we are only guaranteed to acquire a complete lexicon if we have seen all participles (and their possible modifiers) in all their possible surface positions.</S>\n    <S sid=\"151\" ssid=\"76\">Similar regularities have been recognized and given a categorial analysis by Carpenter (1992), who advocates lexical rules to account for the use of predicatives as adjuncts.</S>\n    <S sid=\"152\" ssid=\"77\">In a statistical model, the parameters for such lexical rules are difficult to estimate.</S>\n    <S sid=\"153\" ssid=\"78\">We therefore follow the approach of Aone and Wittenburg (1990) and implement these type-changing Type-changing rules reduce the number of lexical category types required for complex adjuncts. operations in the derivational syntax, where these generalizations are captured in a few rules.</S>\n    <S sid=\"154\" ssid=\"79\">If these rules apply recursively to their own output, they can generate an infinite set of category types, leading to a shift in generative power from context-free to recursively enumerable (Carpenter 1991, 1992).</S>\n    <S sid=\"155\" ssid=\"80\">Like Aone and Wittenburg, we therefore consider only a finite number of instantiations of these type-changing rules, namely those which arise when we extend the category assignment procedure in the following way: For any sentential or verb phrase modifier (an adjunct with label S or SBAR with null complementizer, or VP) to which the original algorithm assigns category X|X, apply the following type-changing rule (given in bottom-up notation) in reverse: where S$ is the category that this constituent obtains if it is treated like a head node by the basic algorithm.</S>\n    <S sid=\"156\" ssid=\"81\">S$ has the appropriate verbal features, and can be S\\NP or S/NP.</S>\n    <S sid=\"157\" ssid=\"82\">Some of the most common type-changing rules are the following, for various types of reduced relative modifier: Hockenmaier and Steedman CCGbank In order to obtain the correct predicate–argument structure, the heads of corresponding arguments in the input and output category are unified (as indicated by coindexation).</S>\n    <S sid=\"158\" ssid=\"83\">In written English, certain types of NP-extraposition require a comma before or after the extraposed noun phrase: Factories booked $236.74 billion in orders in September, [NP nearly the same (18) as the $236.79 billion in August] Because any predicative noun phrase could be used in this manner, this construction is also potentially problematic for the coverage of our grammar and lexicon.</S>\n    <S sid=\"159\" ssid=\"84\">However, the fact that a comma is required allows us to use a small number of binary type-changing rules (which do not project any dependencies), such as:</S>\n  </SECTION>\n  <SECTION title=\"5.\" number=\"6\">\n    <S sid=\"160\" ssid=\"1\">The translation algorithm presumes that the trees in the Penn Treebank map directly to the desired CCG derivations.</S>\n    <S sid=\"161\" ssid=\"2\">However, this is not always the case, either because of noise in the Treebank annotation, differences in linguistic analysis, or because CCG, like any other expressive linguistic formalism, requires information that is not present in the Treebank analysis.</S>\n    <S sid=\"162\" ssid=\"3\">Before translation, a number of preprocessing steps are therefore required.</S>\n    <S sid=\"163\" ssid=\"4\">Disregarding the most common preprocessing step (the insertion of a noun level, which is required in virtually all sentences), preprocessing affects almost 43% of all sentences.</S>\n    <S sid=\"164\" ssid=\"5\">Here we summarize the most important preprocessing steps for those constructions that do not involve non-local dependencies.</S>\n    <S sid=\"165\" ssid=\"6\">Preprocessing steps required for constructions involving non-local dependencies (i.e., traces or null elements in the Treebank) are mentioned in Section 6.</S>\n    <S sid=\"166\" ssid=\"7\">Remaining problems are discussed in Section 7.</S>\n    <S sid=\"167\" ssid=\"8\">More detailed and complete descriptions can be found in the CCGbank manual.</S>\n    <S sid=\"168\" ssid=\"9\">Annotation errors and inconsistencies in the Treebank affect the quality of any extracted grammar or lexicon.</S>\n    <S sid=\"169\" ssid=\"10\">This is especially true for formalisms with an extended domain of locality, such as TAG or CCG, where a single elementary tree or lexical category may contain information that is distributed over a number of distinct phrase-structure rules.</S>\n    <S sid=\"170\" ssid=\"11\">Part-of-Speech Tagging Errors.</S>\n    <S sid=\"171\" ssid=\"12\">Ratnaparkhi (1996) estimates a POS tagging error rate of 3% in the Treebank.</S>\n    <S sid=\"172\" ssid=\"13\">The translation algorithm is sensitive to these errors and inconsistencies, because POS tagging errors can lead to incorrect categories or to incorrect features on verbal categories (e.g., when a past participle is wrongly tagged as past tense).</S>\n    <S sid=\"173\" ssid=\"14\">For instance, if a simple past tense form occurs in a verb phrase which itself is the daughter of a verb phrase whose head is an inflected verb, it is highly likely that it should be a past participle instead.</S>\n    <S sid=\"174\" ssid=\"15\">Using the verb form itself and the surrounding context, we have attempted to correct such errors automatically.</S>\n    <S sid=\"175\" ssid=\"16\">In 7% of all sentences, our algorithm modifies at least one POS tag.</S>\n    <S sid=\"176\" ssid=\"17\">Quotation Marks.</S>\n    <S sid=\"177\" ssid=\"18\">Although not strictly coming under the heading of noise, quotation marks cause a number of problems for the translation algorithm.</S>\n    <S sid=\"178\" ssid=\"19\">Although it is tempting to analyze them similarly to parentheticals, quotations often span sentence boundaries, and consequently quotation marks appear to be unbalanced at the sentence level.</S>\n    <S sid=\"179\" ssid=\"20\">We therefore decided to eliminate them during the preprocessing stage.</S>\n    <S sid=\"180\" ssid=\"21\">Unlike a hand-written grammar, the grammar that is implicit in a treebank has to cover all constructions that occur in the corpus.</S>\n    <S sid=\"181\" ssid=\"22\">Expressive formalisms such as CCG provide explicit analyses that contain detailed linguistic information.</S>\n    <S sid=\"182\" ssid=\"23\">For example, CCG derivations assign a lexical head to every constituent and define explicit functor–argument relations between constituents.</S>\n    <S sid=\"183\" ssid=\"24\">In a phrase-structure grammar, analyses can be much coarser, and may omit more fine-grained structures if they are assumed to be implicit in the given analysis.</S>\n    <S sid=\"184\" ssid=\"25\">Furthermore, constructions that are difficult to analyze do not need to be given a detailed analysis.</S>\n    <S sid=\"185\" ssid=\"26\">In both cases, the missing information has to be added before a Treebank tree can be translated into CCG.</S>\n    <S sid=\"186\" ssid=\"27\">If the missing structure is implicit in the Treebank analysis, this step is relatively straightforward, but constructions such as parentheticals, multiword expressions, and fragments require careful reanalysis in order to avoid lexical coverage problems and overgeneration.</S>\n    <S sid=\"187\" ssid=\"28\">Detecting Coordination.</S>\n    <S sid=\"188\" ssid=\"29\">Although the Treebank does not explicitly indicate coordination, it can generally be inferred from the presence of a conjunction.</S>\n    <S sid=\"189\" ssid=\"30\">However, in list-like nominal coordinations, the conjuncts are only separated by commas or semicolons, and may be difficult to distinguish from appositives.</S>\n    <S sid=\"190\" ssid=\"31\">There are also a number of verb-phrase or sentential coordinations in the Treebank where shared arguments or modifiers simply appear at the same level as conjuncts and the conjunction:12 In CCG, the conjuncts and conjunction form a separate constituent.</S>\n    <S sid=\"191\" ssid=\"32\">In 1.8% of all sentences, additional preprocessing is necessary to obtain this structure.</S>\n    <S sid=\"192\" ssid=\"33\">Noun Phrases and Quantifier Phrases.</S>\n    <S sid=\"193\" ssid=\"34\">In the Penn Treebank, non-recursive noun phrases have remarkably little internal structure: (NP (DT the) (NNP Dutch) (VBG publishing) (NN group)) (20) Some, but not all, of the structure that is required to obtain a linguistically adequate analysis can be inferred (semi-)automatically.</S>\n    <S sid=\"194\" ssid=\"35\">The CCGbank grammar distinguishes noun phrases, NP, from nouns, N, and treats determiners (the) as functions from nouns Hockenmaier and Steedman CCGbank to noun phrases (NP[nb]/N).</S>\n    <S sid=\"195\" ssid=\"36\">Therefore, we need to insert an additional noun level, which also includes the adjuncts Dutch and publishing, which receive both the category N/N: However, because nominal compounds in the Treebank have no internal bracketing, we always assume a right-branching analysis, and are therefore not able to obtain the correct dependencies for cases such as (lung cancer) deaths.</S>\n    <S sid=\"196\" ssid=\"37\">QPs (“quantifier phrases”) are another type of constituent where the Treebank annotation lacks internal structure: We use a number of heuristics to identify the internal structure of these constituents— for example, to detect conjuncts and prepositions.</S>\n    <S sid=\"197\" ssid=\"38\">The above example is then re-bracketed: Fragments.</S>\n    <S sid=\"198\" ssid=\"39\">1.24% of the sentences in the Penn Treebank correspond to or contain fragmentary utterances (labeled FRAG), for which no proper analysis could be given: FRAGs are often difficult to analyze, and the annotation is not very consistent.</S>\n    <S sid=\"199\" ssid=\"40\">The CCGbank manual lists heuristics that we used to infer additional structure.</S>\n    <S sid=\"200\" ssid=\"41\">For example, if a node is labeled FRAG, and there is only one daughter (and potentially an end-of-sentence punctuation mark), as in the first example, we treat the tree as if it was labeled with the label of its daughter (NP in this case).</S>\n    <S sid=\"201\" ssid=\"42\">Parentheticals.</S>\n    <S sid=\"202\" ssid=\"43\">Parentheticals are insertions that are often enclosed in parentheses, or preceded by a dash.</S>\n    <S sid=\"203\" ssid=\"44\">Unless the parenthetical element itself is of a type that could be a modifier by itself (e.g., a PP), we assume that the opening parenthesis or first dash takes the parenthetical element as argument and yields a modifier of the appropriate type: (NP (NP the third-highest) (PP-LOC in the developing world))) This results in the following derivation, which ignores the fact that parentheses are usually balanced (Nunberg 1990): – the third-highest in the developing world We use a similar treatment for other constituents that appear after colons and dashes, such as sentence-final appositives, or parentheticals that are not marked as PRN.</S>\n    <S sid=\"204\" ssid=\"45\">Overall, these changes affect 8.7% of all sentences.</S>\n    <S sid=\"205\" ssid=\"46\">Multi-Word Expressions.</S>\n    <S sid=\"206\" ssid=\"47\">Under the assumption that every constituent has a lexical head that corresponds to an individual orthographic word, multi-word expressions require an analysis where one of the items subcategorizes for a specific syntactic type that can only correspond to the other lexical item.</S>\n    <S sid=\"207\" ssid=\"48\">We only attempted an analysis for expressions that are either very frequent or where the multi-word expression has a different subcategorization behavior from the head word of the expression.</S>\n    <S sid=\"208\" ssid=\"49\">This includes some closed-class items (described in the CCGbank manual), including connectives (e.g., as if, as though, because of ), comparatives (so ADJ that, too ADJ to, at least/most/... X), monetary expressions, and dates, affecting 23.8% of all sentences.</S>\n    <S sid=\"209\" ssid=\"50\">Additionally, there are a number of constructions whose Treebank annotation differs from the standard CCG analysis for linguistic reasons.</S>\n    <S sid=\"210\" ssid=\"51\">This includes small clauses, as well as pied-piping, subject extraction from embedded sentences and argument cluster coordination (discussed in Section 6).</S>\n    <S sid=\"211\" ssid=\"52\">Small Clauses.</S>\n    <S sid=\"212\" ssid=\"53\">The Treebank treats constructions such as the following as small clauses: Pollard and Sag (1992) and Steedman (1996) argue against this analysis on the basis of extractions like what does the country want forgiven, which suggest that these cases should rather be treated as involving two complements.</S>\n    <S sid=\"213\" ssid=\"54\">We eliminate the small clause, and transform the trees such that the verb takes both NP children of the small clause as complements, thereby obtaining the lexical category ((S[dcl]\\NP)/NP)/NP for makes.</S>\n    <S sid=\"214\" ssid=\"55\">Because our current grammar treats predicative NPs like ordinary NPs, we are not able to express the relationship between it and supplier, or between pool and hostage.</S>\n    <S sid=\"215\" ssid=\"56\">A correct analysis would assign a functor category S[nom]\\NP (or perhaps NP[prd]\\NP) to predicative NP arguments of verbs like makes, not only in these examples, but also in copular sentences and appositives.</S>\n    <S sid=\"216\" ssid=\"57\">The other case where small clauses are used in the Treebank includes absolute with and though constructions (with the limit in effect).</S>\n    <S sid=\"217\" ssid=\"58\">Here, we also assume that the subordinating conjunction takes the individual constituents in the small clause as complements, and with obtains therefore the category ((S/S)/PP)/NP.</S>\n    <S sid=\"218\" ssid=\"59\">Again, a predicative analysis of the PP might be desirable in order to express the dependencies between limit and in effect.</S>\n    <S sid=\"219\" ssid=\"60\">Eliminating small clauses affects 8.2% of sentences.</S>\n  </SECTION>\n  <SECTION title=\"6.\" number=\"7\">\n    <S sid=\"220\" ssid=\"1\">The treatment of non-local dependencies is one of the most important points of difference between grammar formalisms.</S>\n    <S sid=\"221\" ssid=\"2\">The Treebank uses a large inventory of null element types and traces, including coindexation to represent long-range dependencies.</S>\n    <S sid=\"222\" ssid=\"3\">Hockenmaier and Steedman CCGbank Because standard Treebank parsers use probabilistic versions of context-free grammar, they are generally trained and tested on a version of the Treebank in which these null elements and indices are deleted or ignored, or, in the case of Collin’s (1999) Model 3, only partially captured.</S>\n    <S sid=\"223\" ssid=\"4\">Non-local dependencies are therefore difficult to recover from their output.</S>\n    <S sid=\"224\" ssid=\"5\">In CCG, long-range dependencies are represented without null elements or traces, and coindexation is restricted to arguments of the same lexical functor category.</S>\n    <S sid=\"225\" ssid=\"6\">Although this mechanism is less expressive than the potentially unrestricted coindexation used in the Treebank, it allows parsers to recover non-anaphoric long-range dependencies directly, without the need for further postprocessing or trace insertion.</S>\n    <S sid=\"226\" ssid=\"7\">Passive.</S>\n    <S sid=\"227\" ssid=\"8\">In the Treebank, the surface subject of a passive sentence is coindexed with a ∗ null element in direct object position: Our translation algorithm uses the presence of the ∗ null element to identify passive mode, but ignores it otherwise, assigning the CCG category S[pss]\\NP to noted.13 The dependency between the subject and the participial is mediated through the lexical category of the copula, (S[dcl]\\NPi)/(S[pss]\\NPi) (with the standard semantics apax.px).14 In order to reduce lexical ambiguity and deal with data sparseness, we treat optional by-PPs which contain the “logical” subject (NP-LGS) as adjuncts rather than arguments of the passive participle.15 Here is the resulting CCG derivation, together with its dependency structure: 13 In the case of verbs like pay for, which take a PP argument, the null element appears within the PP.</S>\n    <S sid=\"228\" ssid=\"9\">In order to obtain the correct lexical category of paid, (S[pss]\\NP)/(PP/NP), we treat the null element like an argument of the preposition and percolate it up to the PP level.</S>\n    <S sid=\"229\" ssid=\"10\">14 We assume that the fact that the subject NP argument of passive participials with category S[pss]\\NP identifies the patient, rather than agent, is represented in the semantic interpretation of noted, for example, ax.noted'x one, where one’ is simply a placeholder for a bindable argument, like the relational grammarians’ chˆomeur relation.</S>\n    <S sid=\"230\" ssid=\"11\">15 Extractions such as Who was he paid by require the by-PP to be treated as an argument, and it would in fact be better to use a lexical rule to generate (S[pss]\\NP)/PP[by] from S[pss]\\NP and vice versa.</S>\n    <S sid=\"231\" ssid=\"12\">Infinitival and Participial VPs, Gerunds.</S>\n    <S sid=\"232\" ssid=\"13\">In the Treebank, participial phrases, gerunds, imperatives, and to-VP arguments are annotated as sentences with a ∗ null subject: We treat these like verb phrases (S\\NP) with the appropriate feature ([b], [to], [ng], or [pt]), depending on the part-of-speech tag of the verb.</S>\n    <S sid=\"233\" ssid=\"14\">Control and Raising.</S>\n    <S sid=\"234\" ssid=\"15\">CCGbank does not distinguish between control and raising.</S>\n    <S sid=\"235\" ssid=\"16\">In the Treebank, subject-control and subject-raising verbs (e.g., want and seem) also take an S complement with a null subject that is coindexed with the subject of the main clause: We ignore the coindexation in the Treebank, and treat all control verbs as non-arbitrary control.</S>\n    <S sid=\"236\" ssid=\"17\">As indicated by the index i, we assume that all verbs which subcategorize for a verb phrase complement and take no direct object mediate a dependency between their subject and their complement.</S>\n    <S sid=\"237\" ssid=\"18\">Because the copula and to mediate similar dependencies between their subjects and complements, but do not fill their own subject dependencies, Japanese has the following dependencies: In the Treebank, object-raising verbs (wants half the debt forgiven) take a small clause argument with non-empty subject.</S>\n    <S sid=\"238\" ssid=\"19\">Following our treatment of small clauses (see Section 5.3) we modify this tree so that we obtain the lexical category (((S[dcl]\\NP)/(S[pss]\\NPi))/NPi) for wanted, which mediates the dependency between debt and forgiven.16 Extraposition of Appositives.</S>\n    <S sid=\"239\" ssid=\"20\">Appositive noun phrases can be extraposed out of a sentence or verb phrase, resulting in an anaphoric dependency.</S>\n    <S sid=\"240\" ssid=\"21\">The Penn Treebank analyzes these as adverbial small clauses with a coindexed null subject: We also treat these appositives as sentential modifiers.</S>\n    <S sid=\"241\" ssid=\"22\">However, the corresponding CCG derivation deliberately omits the dependency between dummies and drivers:17 This derivation uses one of the special binary type-changing rules (see Section 4.6) that takes into account that these appositives can only occur adjacent to commas.</S>\n    <S sid=\"242\" ssid=\"23\">The Penn Treebank analyzes wh-questions, relative clauses, topicalization of complements, tough movement, cleft, and parasitic gaps in terms of movement.</S>\n    <S sid=\"243\" ssid=\"24\">These constructions are frequent: The entire Treebank contains 16,056 *T* traces, including 8,877 NP traces, 4,120 S traces, 2,465 ADVP traces, 422 PP traces, and 210 other *T* traces.</S>\n    <S sid=\"244\" ssid=\"25\">Sections 02–21 (39,604 sentences) contain 5,288 full subject relative clauses, as well as 459 full and 873 reduced object relative clauses.</S>\n    <S sid=\"245\" ssid=\"26\">The dependencies involved in these constructions, however, are difficult to obtain from the output of standard parsers such as Collins (1999) or Charniak (2000), and require additional postprocessing that may introduce further noise and errors.</S>\n    <S sid=\"246\" ssid=\"27\">In those cases where the trace corresponds to a “moved” argument, the corresponding long-range dependencies can be recovered directly from the correct CCG derivation.</S>\n    <S sid=\"247\" ssid=\"28\">In the Treebank, the “moved” constituent is coindexed with a trace (*T*), which is inserted at the extraction site: 17 We regard this type of dependency as anaphoric rather than syntactic, on the basis of its immunity to such syntactic restrictions as subject islands.</S>\n    <S sid=\"248\" ssid=\"29\">CCG has a similarly uniform analysis of these constructions, albeit one that does not require syntactic movement.</S>\n    <S sid=\"249\" ssid=\"30\">In the CCG derivation of the example, the relative pronoun has the category (NPi\\NPi)/(S[dcl]/NPi) whereas the verb bought just bears the standard transitive category (S[dcl]\\NP)/NP.</S>\n    <S sid=\"250\" ssid=\"31\">The subject NP and the incomplete VP combine via type-raising and forward composition into an S[dcl]/NP, which the relative pronoun then takes as its argument: The coindexation on the lexical category of the relative pronoun guarantees that the missing object unifies with the modified NP, and we obtain the desired dependencies: This analysis of movement in terms of functors over incomplete constituents allows CCG to use the same category for the verb when its arguments are extracted as when they are in situ.</S>\n    <S sid=\"251\" ssid=\"32\">This includes not only relative clauses and wh-questions, but also piedpiping, tough movement, topicalization, and clefts.</S>\n    <S sid=\"252\" ssid=\"33\">For our translation algorithm, the *T* traces are essential: They indicate the presence of a long-range dependency for a particular argument of the verb, and allow us to use a mechanism similar to GPSG’s slash-feature passing (Gazdar et al. 1985), so that long-range dependencies are represented in the gold-standard dependency structures of the test and training data.</S>\n    <S sid=\"253\" ssid=\"34\">This is crucial to correctly inducing and evaluating grammars and parsers for any expressive formalism, including TAG, GPSG, HPSG, LFG, and MPG.</S>\n    <S sid=\"254\" ssid=\"35\">A detailed description of this mechanism and of our treatment of other constructions that use *T* traces can be found in the CCGbank manual.</S>\n    <S sid=\"255\" ssid=\"36\">This algorithm works also if there is a coordinate structure within the relative clause such that there are two *T* traces (the interest rates they pay *T* on their deposits and charge *T* on their loans), resulting in the following long-range dependencies: that the verb takes the VP and the NP argument in reversed order and change the tree accordingly before translation, resulting in the correct CCG analysis: We obtain the following long-range dependencies: Because our grammar does not use Baldridge’s (2002) modalities or Steedman’s (1996) equivalent rule-based restrictions, which prohibit this category from applying to in situ NPs, this may lead to overgeneralization.</S>\n    <S sid=\"256\" ssid=\"37\">However, such examples are relatively frequent: There are 97 instances of ((S[.</S>\n    <S sid=\"257\" ssid=\"38\">]\\NP)/NP)/(S[dcl]\\NP) in sections 02–21, and to omit this category would reduce coverage and recovery of long-range extractions.</S>\n    <S sid=\"258\" ssid=\"39\">By percolating the *T* trace up to the SQ-level in a similar way to relative clauses and treating Which as syntactic head of the WHNP, we obtain the desired CCG analysis: We coindex the head of the extracted NP with that of the noun (cars): (S[wq]/(S[q]/ NPi))/Ni, and the subject of do with the subject of its complement ((S[q]/(S[b]\\NP1))/NPi) to obtain the following dependencies: In this example, we need to rebracket the Treebank tree so that details of forms a constituent,18 apply a special rule to assign the category (NP\\NP)/NP to the preposition, and combine it via type-raising and composition with details.</S>\n    <S sid=\"259\" ssid=\"40\">This constituent is then treated as an argument of the relative pronoun: With appropriate coindexation ((NP\\NPi)/(S[dcl]\\NPj))\\(NP/NPi)j, we obtain the following non-local dependencies:19 Because adjuncts generally do not extract unboundedly,20 the corresponding traces (which account for 20% of all *T* traces) can be ignored by the translation procedure.</S>\n    <S sid=\"260\" ssid=\"41\">Instead, the dependency between when and dropped is directly established by the fact that dropped is the head of the complement S[dcl]: wh-extraction, which use the same lexical categories as for in situ complements, they also provide an analysis of right node raising constructions without introducing any new lexical categories.</S>\n    <S sid=\"261\" ssid=\"42\">In the Treebank analysis of right node raising, the shared constituent is coindexed with two *RNR* traces in both of its canonical positions: We need to alter the translation algorithm slightly to deal with *RNR* traces in a manner essentially equivalent to the earlier treatment of *T* wh-traces.</S>\n    <S sid=\"262\" ssid=\"43\">Details are in the CCGbank manual.</S>\n    <S sid=\"263\" ssid=\"44\">The CCG derivation for the above example is as follows: The right node raising dependencies are as follows: Our algorithm works also if the shared constituent is an adjunct, or if two conjoined noun phrases share the same head, which is also annotated with *RNR* traces.</S>\n    <S sid=\"264\" ssid=\"45\">Although there are only 209 sentences with *RNR* traces in the entire Treebank, right node raising is actually far more frequent, because *RNR* traces are not used when the conjuncts consist of single verb tokens.</S>\n    <S sid=\"265\" ssid=\"46\">The Treebank contains 349 VPs in which a verb form (/VB/) is immediately followed by a conjunction (CC) and another verb form, and has an NP sister (without any coindexation or function tag).</S>\n    <S sid=\"266\" ssid=\"47\">In CCGbank, sections 02–21 alone contain 444 sentences with verbal or adjectival right node raising.</S>\n    <S sid=\"267\" ssid=\"48\">Right node raising is also marked in the Penn Treebank using *RNR* traces for “parasitic gap” constructions such as the following: These sentences require rules based on the substitution combinator S (Steedman 1996).</S>\n    <S sid=\"268\" ssid=\"49\">Our treatment of right node raising traces deals with the first case correctly, via the backward crossing rule &lt;S×, and allows us to obtain the following correct dependencies: The second type of parasitic gap, (44b), would be handled equally correctly by the forward substitution rule &gt;S, since the PPs are both arguments.</S>\n    <S sid=\"269\" ssid=\"50\">Unfortunately, as we saw in Section 3, the Treebank classifies such PPs as directional adverbials, hence we translate them as adjuncts and lose such examples, of which there are at least three more, all also involving from and to: As in the case of leftward extraction, including such long-range dependencies in the dependency structure is crucial to correct induction and evaluation of all expressive grammar formalisms.</S>\n    <S sid=\"270\" ssid=\"51\">Although no leftward-extracting parasitic gaps appear to occur in the Treebank, our grammar and model predicts examples like the following, and will cover them when encountered: Conflict which the system was held to cause, rather than resolve.</S>\n    <S sid=\"271\" ssid=\"52\">(47) 6.4.1 Argument Cluster Coordination.</S>\n    <S sid=\"272\" ssid=\"53\">If two VPs with the same head are conjoined, the second verb can be omitted.</S>\n    <S sid=\"273\" ssid=\"54\">The Treebank encodes these constructions as a VPcoordination in which the second VP lacks a verb.</S>\n    <S sid=\"274\" ssid=\"55\">The daughters of the second conjunct are coindexed with the corresponding elements in the first conjunct using a = index: In the CCG account of this construction, $5 million right away and additional amounts in the future form constituents (“argument clusters”), which are then coordinated.</S>\n    <S sid=\"275\" ssid=\"56\">These constituents are obtained by type-raising and composing the arguments in each conjunct, yielding a functor which takes a verb with the appropriate category to its left to yield a verb phrase (Dowty 1988; Steedman 1985).</S>\n    <S sid=\"276\" ssid=\"57\">Then the argument clusters are conjoined, and combine with the verb via function application:21 This construction is one in which the CCGbank head-dependency structure (shown subsequently) fails to capture the full set of predicate–argument structure relations that would be implicit in a full logical form: That is, the dependency structure does not express the fact that right away takes scope over $5 million and in future over additional amounts, rather than the other way around.</S>\n    <S sid=\"277\" ssid=\"58\">However, this information is included in the full surface-compositional semantic interpretation that is built by the combinatory rules.</S>\n    <S sid=\"278\" ssid=\"59\">Because the Treebank constituent structure does not correspond to the CCG analysis, we need to transform the tree before we can translate it.</S>\n    <S sid=\"279\" ssid=\"60\">During preprocessing, we create a copy of the entire argument cluster which corresponds to the constituent structure of the CCG analysis.</S>\n    <S sid=\"280\" ssid=\"61\">During normal category assignment, we use the first conjunct in its original form to obtain the correct categories of all constituents.</S>\n    <S sid=\"281\" ssid=\"62\">In a later stage, we use type-raising and composition to combine the constituents within each argument cluster.</S>\n    <S sid=\"282\" ssid=\"63\">For a detailed description of this algorithm and a number of variations on the original Treebank annotation that we did not attempt to deal with, the interested reader is referred to the CCGbank manual.</S>\n    <S sid=\"283\" ssid=\"64\">There are 226 instances of argument-cluster coordination in the entire Penn Treebank.</S>\n    <S sid=\"284\" ssid=\"65\">The algorithm delivers a correct CCG derivation for 146 of these.</S>\n    <S sid=\"285\" ssid=\"66\">Translation failures are due to the fact that the algorithm can at present only deal with this construction if the two conjuncts are isomorphic in structure, which is not always the case.</S>\n    <S sid=\"286\" ssid=\"67\">This is unfortunate, because CCG is particularly suited for this construction.</S>\n    <S sid=\"287\" ssid=\"68\">However, we believe that it would be easier to manually reannotate those sentences that are not at present translated than to try to adapt the algorithm to deal with all of them individually.</S>\n    <S sid=\"288\" ssid=\"69\">(PP-CLR=2 to the general manager))) This construction cannot be handled with the standard combinatory rules of CCG that are assumed for English.</S>\n    <S sid=\"289\" ssid=\"70\">Instead, Steedman (2000) proposes an analysis of gapping that uses a unification-based “decomposition” rule.</S>\n    <S sid=\"290\" ssid=\"71\">Categorial decomposition allows a category type to be split apart into two subparts, and is used to yield an analysis of gapping that is very similar to that of argument cluster coordination:22 22 It is only the syntactic types that are decomposed or recovered in this way: the corresponding semantic entities and in particular the interpretation for the gapped verb group can talk must be available from the left conjunct’s information structure, via anaphora.</S>\n    <S sid=\"291\" ssid=\"72\">That is, decomposition adds very little to the categorial information available from the right conjunct, except to make the syntactic types yield an S. The real work is done in the semantics.</S>\n    <S sid=\"292\" ssid=\"73\">Because the derivation is not a tree anymore, and the decomposed constituents do not correspond to actual constituents in the surface string, this analysis is difficult to represent in a treebank.</S>\n    <S sid=\"293\" ssid=\"74\">The 107 sentences that contain sentential gapping are therefore omitted in the current version of CCGbank, even though special coordination rules that mimic the decomposition analysis are conceivable.</S>\n    <S sid=\"294\" ssid=\"75\">Besides the cases discussed herein, the Treebank contains further kinds of null elements, all of which the algorithm ignores.</S>\n    <S sid=\"295\" ssid=\"76\">The null element *ICH* (“Insert Constituent Here”), which appears 1,240 times, is used for extraposition of modifiers.</S>\n    <S sid=\"296\" ssid=\"77\">Like ellipsis, this is a case of a semantic dependency which we believe to be anaphoric, and therefore not reflected in the syntactic category.</S>\n    <S sid=\"297\" ssid=\"78\">For this reason we treat any constituent that is coindexed with an *ICH* as an adjunct.</S>\n    <S sid=\"298\" ssid=\"79\">The null element *PPA* (“Permanent Predictable Ambiguity,” 26 occurrences) is used for genuine attachment ambiguities.</S>\n    <S sid=\"299\" ssid=\"80\">Since the Treebank manual states that the actual constituent should be attached at the more likely attachment site, we chose to ignore any *PPA* null element.</S>\n    <S sid=\"300\" ssid=\"81\">Our algorithm also ignores the null element *?</S>\n    <S sid=\"301\" ssid=\"82\">*, which occurs 582 times, and indicates “a missing predicate or a piece thereof” (Marcus, Santorini, and Marcinkiewicz 1993).</S>\n    <S sid=\"302\" ssid=\"83\">It is used for VP ellipsis, and can also occur in conjunction with a VP pro-form do (You either believe he can do it or you don’t *?</S>\n    <S sid=\"303\" ssid=\"84\">*), or in comparatives (the total was far higher than expected *?</S>\n    <S sid=\"304\" ssid=\"85\">*).23 We can now define the complete translation algorithm, including the modifications necessary to deal with traces and argument clusters: 23 We believe that both conjuncts in the first example are complete sentences which are related anaphorically.</S>\n    <S sid=\"305\" ssid=\"86\">Therefore, the syntactic category of do is S[dcl]\\NP, not (S[dcl]\\NP)/VP.</S>\n    <S sid=\"306\" ssid=\"87\">In the second example, *?</S>\n    <S sid=\"307\" ssid=\"88\">* indicates a semantic argument of expected that we do not reflect in the syntactic category.</S>\n    <S sid=\"308\" ssid=\"89\">The successive steps have the following more detailed character: preprocessTree: Correct tagging errors, ensure the constituent structure conforms to the CCG analysis.</S>\n    <S sid=\"309\" ssid=\"90\">Eliminate quotes.</S>\n    <S sid=\"310\" ssid=\"91\">Create copies of coordinated argument clusters that correspond to the CCG analysis. determineConstituentTypes: For each node, determine its constituent type (head, complement, adjunct, conjunction, a constituent that is coindexed with a *RNR* trace, spurious null element, or argument cluster). makeBinary: Binarize the tree. percolateTraces: Determine the CCG category of *T* and *RNR* traces in complement position, and percolate them up to the appropriate level in the tree. assignCategories: Assign CCG categories to nodes in the tree, starting at the root node.</S>\n    <S sid=\"311\" ssid=\"92\">Nodes that are coindexed with *RNR* traces receive the category of the corresponding traces.</S>\n    <S sid=\"312\" ssid=\"93\">Argument clusters are ignored in this step. treatArgumentClusters: Assign categories to argument clusters. cutTracesAndUnaryRules: Cut out constituents that are not part of the CCG derivation, such as traces, null elements, and the copy of the first conjunct in argument cluster coordination.</S>\n    <S sid=\"313\" ssid=\"94\">Eliminate resulting unary projections of the form X ⇒ X. verifyDerivation: Discard those trees for which the algorithm does not produce a valid CCG derivation.</S>\n    <S sid=\"314\" ssid=\"95\">In most cases, this is due to argument cluster coordination that is not annotated in a way that our algorithm can deal with. assignDependencies: coindex specific classes of lexical categories to project non-local dependencies, and generate the word–word dependencies that constitute the underlying predicate–argument structure.</S>\n  </SECTION>\n  <SECTION title=\"7.\" number=\"8\">\n    <S sid=\"315\" ssid=\"1\">In a number of cases, missing structure or a necessary distinction between different constructions needed to inform the translation is missing, and cannot be inferred deterministically from the Treebank analysis without further manual re-annotation.</S>\n    <S sid=\"316\" ssid=\"2\">We discuss these residual problems here, because they are likely to present obstacles to the extraction of linguistically adequate grammars in any formalism.</S>\n    <S sid=\"317\" ssid=\"3\">Our translation algorithm requires a distinction between complements and adjuncts.</S>\n    <S sid=\"318\" ssid=\"4\">In many cases, this distinction is easily read off the Treebank annotation, but it is in general an open linguistic problem (McConnell-Ginet 1982).</S>\n    <S sid=\"319\" ssid=\"5\">Because the Treebank annotation does not explicitly distinguish between complements and adjuncts, researchers typically develop their own heuristics—see, for example, Kinyon and Prolo (2002).</S>\n    <S sid=\"320\" ssid=\"6\">For prepositional phrases, we rely on the -CLR (“closely related”) function tag to identify complements, although it is unclear whether the Treebank annotators were able to use this tag consistently.</S>\n    <S sid=\"321\" ssid=\"7\">Not all PP arguments seem to have this function tag, and some PPs that have this tag may have been better considered adjuncts: For TAG, Chen, Bangalore, and Vijay-Shanker (2006) show that different heuristics yield grammars that differ significantly in size, coverage, and linguistic adequacy.</S>\n    <S sid=\"322\" ssid=\"8\">We have not attempted such an investigation.</S>\n    <S sid=\"323\" ssid=\"9\">In a future version of CCGbank, it may be possible to follow Shen and Joshi (2005) in using the semantic roles of the Proposition Bank (Palmer, Gildea, and Kingsbury 2005) to distinguish arguments and adjuncts.</S>\n    <S sid=\"324\" ssid=\"10\">Particle-verb constructions are difficult to identify in the Treebank, because particles can be found as PRT, ADVP-CLR, and ADVP.</S>\n    <S sid=\"325\" ssid=\"11\">Therefore, verbs in the CCGbank grammar do not subcategorize for particles, which are instead treated as adverbial modifiers.</S>\n    <S sid=\"326\" ssid=\"12\">Compound nouns are often inherently ambiguous, and in most cases, the Treebank does not specify their internal structure: In order to obtain the correct analysis, manual re-annotation would be required.</S>\n    <S sid=\"327\" ssid=\"13\">Because this was not deemed feasible within our project, compound nouns are simply translated into strictly right-branching binary trees, which yields the correct analysis in some, but not all, cases.</S>\n    <S sid=\"328\" ssid=\"14\">This eschews the computational problem that a grammar for compound nouns induces all possible binary bracketings, but is linguistically incorrect.</S>\n    <S sid=\"329\" ssid=\"15\">A similar problem arises in compound nouns that involve internal coordination: We include the following (linguistically incorrect) rule in our grammar, which yields a default dependency structure corresponding to N/N coordination: conj N ⇒ N (56) This rule allows us to translate the above tree as follows: N/N cotton N N conj and N N fibers The Treebank markup of NP appositives is indistinguishable from that of NP lists: Therefore, our current grammar does not distinguish between appositives and NP coordination, even though appositives should be analyzed as predicative modifiers.</S>\n    <S sid=\"330\" ssid=\"16\">This leads to a reduction of ambiguity in the grammar, but is semantically incorrect: Our current grammar does not implement number agreement (which is, however, represented in the POS tags).</S>\n    <S sid=\"331\" ssid=\"17\">One problem that prevented us from including number agreement is the above-mentioned inability to distinguish NP lists and appositives.</S>\n    <S sid=\"332\" ssid=\"18\">In the Penn Treebank, all relative clauses are attached at the noun phrase level.</S>\n    <S sid=\"333\" ssid=\"19\">This is semantically undesirable, because a correct interpretation of restrictive relative clauses can only be obtained if they modify the noun, whereas non-restrictive relative clauses are noun phrase modifiers.</S>\n    <S sid=\"334\" ssid=\"20\">Because this distinction requires manual inspection on a caseby-case basis, we were unable to modify the Treebank analysis.</S>\n    <S sid=\"335\" ssid=\"21\">Thus, all CCGbank relative pronouns have categories of the form (NPi\\NPi)/(S/NPi), rather than (Ni\\Ni)/(S/NPi).</S>\n    <S sid=\"336\" ssid=\"22\">This will make life difficult for those trying to provide a Montague-style semantics for relative modifiers.</S>\n    <S sid=\"337\" ssid=\"23\">Like most other problems that we were not able to overcome, this limitation of the Treebank ultimately reflects the sheer difficulty of providing a consistent and reliable annotation for certain linguistic phenomena, such as modifier scope.</S>\n    <S sid=\"338\" ssid=\"24\">7.7.1 Heavy NP Shift.</S>\n    <S sid=\"339\" ssid=\"25\">In English, noun phrase arguments can be shifted to the end of the sentence if they become too “heavy.” This construction was studied extensively by Ross (1967).</S>\n    <S sid=\"340\" ssid=\"26\">The CCG analysis (Steedman 1996) uses backward crossed composition to provide an analysis where brings has its canonical lexical category (VP/PP)/NP: Because the Penn Treebank does not indicate heavy NP shift, the corresponding CCGbank derivation does not conform to the desired analysis, and requires additional lexical categories which may lead to incorrect overgeneralizations:24 This will also be a problem in using the Penn Treebank or CCGbank for any theory of grammar that treats heavy NP shift as extraction or movement.</S>\n    <S sid=\"341\" ssid=\"27\">8.</S>\n    <S sid=\"342\" ssid=\"28\">Coverage, Size, and Evaluation Here we first examine briefly the coverage of the translation algorithm on the entire Penn Treebank.</S>\n    <S sid=\"343\" ssid=\"29\">Then we examine the CCG grammar and lexicon that are obtained from CCGbank.</S>\n    <S sid=\"344\" ssid=\"30\">Although the grammar of CCG is usually thought of as consisting only of the combinatory rule schemata such as (3) and (5), we are interested here in the instantiation of these rules, in which the variables X and Y are bound to values such as S and NP, because statistical parsers such as Hockenmaier and Steedman’s (2002) or Clark and Curran’s (2004) are trained on counts of such instantiations.</S>\n    <S sid=\"345\" ssid=\"31\">We report our results on sections 02–21, the standard training set for Penn Treebank parsers, and use section 00 to evaluate coverage of the training set on unseen data.</S>\n    <S sid=\"346\" ssid=\"32\">Sections 02–21 contains 39,604 sentences (929,552 words/tokens), whereas section 00 consists of 1,913 sentences (45,422 words/tokens).</S>\n    <S sid=\"347\" ssid=\"33\">CCGbank contains 48,934 (99.44%) of the 49,208 sentences in the entire Penn Treebank.</S>\n    <S sid=\"348\" ssid=\"34\">The missing 274 sentences could not be automatically translated to CCG.</S>\n    <S sid=\"349\" ssid=\"35\">This includes 107 instances of sentential gapping, a construction our algorithm does not cover (see Section 6.4.2), and 66 instances of non-sentential gapping, or argument-cluster coordination (see Section 6.4.1).</S>\n    <S sid=\"350\" ssid=\"36\">The remaining translation failures include trees that consist of sequences of NPs that are not separated by commas, some fragments, and a small number of constructions involving long-range dependencies, such as wh-extraction, parasitic gaps, or argument cluster coordinations where the translation did not yield a valid CCG derivation because a complement had been erroneously identified as an adjunct.</S>\n    <S sid=\"351\" ssid=\"37\">24 Backward crossed composition is also used by Steedman (1996, 2000) and Baldridge (2002) to account for constraints on preposition stranding in English.</S>\n    <S sid=\"352\" ssid=\"38\">Because this rule in its unrestricted form leads to overgeneralization, Baldridge restricts crossing rules via the x modality.</S>\n    <S sid=\"353\" ssid=\"39\">The current version of CCGbank does not implement modalities, but because the grammar that is implicit in CCGbank only consists of particular seen rule instantiations, it may not be affected by such overgeneration problems.</S>\n    <S sid=\"354\" ssid=\"40\">A CCG lexicon specifies the lexical categories of words, and therefore contains the entire language-specific grammar.</S>\n    <S sid=\"355\" ssid=\"41\">Here, we examine the size and coverage of the lexicon that consists of the word–category pairs that occur in CCGbank.</S>\n    <S sid=\"356\" ssid=\"42\">This lexicon could be used by any CCG parser, although morphological generalization (which is beyond the scope of the present paper) and ways to treat unknown words are likely to be necessary to obtain a more complete lexicon.</S>\n    <S sid=\"357\" ssid=\"43\">Number of Entries.</S>\n    <S sid=\"358\" ssid=\"44\">The lexicon extracted from sections 02–21 has 74,669 entries for 44,210 word types (or 929,552 word tokens).</S>\n    <S sid=\"359\" ssid=\"45\">Many words have only a small number of categories, but because a number of frequent closed-class items have a large number of categories (see Table 1), the expected number of lexical categories per token is 19.2.</S>\n    <S sid=\"360\" ssid=\"46\">Number and Growth of Lexical Category Types.</S>\n    <S sid=\"361\" ssid=\"47\">How likely is it that we have observed the complete inventory of category types in the English language?</S>\n    <S sid=\"362\" ssid=\"48\">There are 1,286 lexical category types in sections 02–21.</S>\n    <S sid=\"363\" ssid=\"49\">Figure 4 examines the growth of the number of lexical category types as a function of the amount of data translated into CCG.</S>\n    <S sid=\"364\" ssid=\"50\">The log–log plot The growth of lexical category types and rule instantiations (sections 02–21).</S>\n    <S sid=\"365\" ssid=\"51\">A log–log plot of the rank order and frequency of the lexical category types (left) and instantiations of combinatory rules (right) in CCGbank. of the rank order and frequency of the lexical categories in Figure 5 indicates that the underlying distribution is roughly Zipfian, with a small number of very frequent categories and a long tail of rare categories.</S>\n    <S sid=\"366\" ssid=\"52\">We note 439 categories that occur only once, and only 556 categories occur five times or more.</S>\n    <S sid=\"367\" ssid=\"53\">Inspection suggests that although some of the category types that occur only once are due to noise or annotation errors, most are correct and are in fact required for certain constructions.</S>\n    <S sid=\"368\" ssid=\"54\">Typical examples of rare but correct and necessary categories are relative pronouns in pied-piping constructions, or verbs which take expletive subjects.</S>\n    <S sid=\"369\" ssid=\"55\">Lexical Coverage on Unseen Data.</S>\n    <S sid=\"370\" ssid=\"56\">The lexicon extracted from sections 02–21 contains the necessary categories (as determined by our translation algorithm) for 94.0% of all tokens in section 00 (42,707 out of 45,422).</S>\n    <S sid=\"371\" ssid=\"57\">The missing entries that would be required for the remaining 6% of tokens fall into two classes: 1,728, or 3.8%, correspond to completely unknown words that do not appear at all in section 02–21, whereas the other 2.2% of tokens do appear in the training set, but not with the categories required in section 00.</S>\n    <S sid=\"372\" ssid=\"58\">All statistical parsers have to be able to accept unknown words in their input, regardless of the underlying grammar formalism.</S>\n    <S sid=\"373\" ssid=\"59\">Typically, frequency information for rare words in the training data is used to estimate parameters for unknown words (and when these rare or unknown words are encountered during parsing, additional information may be obtained from a POS-tagger (Collins 1997)).</S>\n    <S sid=\"374\" ssid=\"60\">However, in a lexicalized formalism such as CCG, there is the additional problem of missing lexical entries for known words.</S>\n    <S sid=\"375\" ssid=\"61\">Because lexical categories play such an essential role in CCG, even a small fraction of missing lexical entries can have a significant effect on coverage, since the parser will not be able to obtain the correct analysis for any sentence that contains such a token.</S>\n    <S sid=\"376\" ssid=\"62\">Hockenmaier and Steedman (2002) show that this lexical coverage problem does in practice have a significant impact on overall parsing accuracy.</S>\n    <S sid=\"377\" ssid=\"63\">However, because many of the known words with missing entries do not appear very often in the training data, Hockenmaier (2003a) demonstrates that this problem can be partially alleviated if the frequency threshold below which rare words are treated as unseen is set to a much higher value than for standard Treebank parsers.</S>\n    <S sid=\"378\" ssid=\"64\">An alternative approach, advocated by Clark and Curran (2004), is to use a supertagger which predicts lexical CCG categories in combination with a discriminative parsing model.</S>\n    <S sid=\"379\" ssid=\"65\">Size and Growth of Instantiated Syntactic Rule Set.</S>\n    <S sid=\"380\" ssid=\"66\">Statistical CCG parsers such as Hockenmaier and Steedman (2002) or Clark and Curran (2004) are trained on counts of specific instantiations of combinatory rule schemata by category-types.</S>\n    <S sid=\"381\" ssid=\"67\">It is therefore instructive to consider the frequency distribution of these category-instantiated rules.</S>\n    <S sid=\"382\" ssid=\"68\">The grammar for sections 02-21 has 3,262 instantiations of general syntactic combinatory rules like those in (3) with specific categories.</S>\n    <S sid=\"383\" ssid=\"69\">Of these, 1146 appear only once, and 2,027 appear less than five times.</S>\n    <S sid=\"384\" ssid=\"70\">Although there is some noise, many of the CCG rules that appear only once are linguistically correct and should be used by the parser.</S>\n    <S sid=\"385\" ssid=\"71\">They include certain instantiations of type-raising, coordination, or punctuation rules, or rules involved in argument cluster coordinations, pied-piping constructions, or questions, all of which are rare in the Wall Street Journal.</S>\n    <S sid=\"386\" ssid=\"72\">As can be seen from Figure 5, the distribution of rule frequencies is again roughly Zipfian, with the 10 most frequent rules accounting for 59.2% of all rule instantiations (159 rules account for 95%; 591 rules for 99%).</S>\n    <S sid=\"387\" ssid=\"73\">The growth of rule instantiations is shown in Figure 4.</S>\n    <S sid=\"388\" ssid=\"74\">If function tags are ignored, the grammar for the corresponding sections of the original Treebank contains 12,409 phrase-structure rules, out of which 6,765 occur only once (Collins 1999).</S>\n    <S sid=\"389\" ssid=\"75\">These rules also follow a Zipfian distribution (Gaizauskas 1995).</S>\n    <S sid=\"390\" ssid=\"76\">The fact that both category types and rule instances are also Zipfian for CCGbank, despite its binarized rules, shows that the phenomenon is not just due to the Treebank annotation with its very flat rules.</S>\n    <S sid=\"391\" ssid=\"77\">Syntactic Rule Coverage on Unseen Data.</S>\n    <S sid=\"392\" ssid=\"78\">Syntactic rule coverage for unseen data is almost perfect: 51,932 of the 51,984 individual rule instantiations in section 00 (corresponding to 844 different rule types) have been observed in section 02–21.</S>\n    <S sid=\"393\" ssid=\"79\">Out of the 52 missing rule instantiation tokens (corresponding to 38 rule types, because one rule appears 13 times in one sentence), six involve coordination, and three punctuation.</S>\n    <S sid=\"394\" ssid=\"80\">One missing rule is an instance of substitution (caused by a parasitic gap).</S>\n    <S sid=\"395\" ssid=\"81\">Two missing rules are instances of type-raised argument types combining with a verb of a rare type.</S>\n  </SECTION>\n  <SECTION title=\"9.\" number=\"9\">\n    <S sid=\"396\" ssid=\"1\">This paper has presented an algorithm which translates Penn Treebank phrase-structure trees into CCG derivations augmented with word–word dependencies that approximate the underlying predicate–argument structure.</S>\n    <S sid=\"397\" ssid=\"2\">In order to eliminate some of the noise in the original annotation and to obtain linguistically adequate derivations that conform to the “correct” analyses proposed in the literature, considerable preprocessing was necessary.</S>\n    <S sid=\"398\" ssid=\"3\">Even though certain mismatches between the syntactic annotations in the Penn Treebank and the underlying semantics remain, and will affect any similar attempt to obtain expressive grammars from the Treebank, we believe that CCGbank, the resulting corpus, will be of use to the computational linguistics community in the following ways.</S>\n    <S sid=\"399\" ssid=\"4\">CCGbank has already enabled the creation of several robust and accurate wide-coverage CCG parsers, including Hockenmaier and Steedman (2002), Clark, Hockenmaier, and Steedman (2002), Hockenmaier (2003b), and Clark and Curran (2004, 2007).</S>\n    <S sid=\"400\" ssid=\"5\">Although the construction of full logical forms was beyond the scope of this project, CCGbank can also be seen as a resource which may enable the automatic construction of full semantic interpretations by wide-coverage parsers.</S>\n    <S sid=\"401\" ssid=\"6\">Unlike most Penn Treebank parsers, such as Collins (1999) or Charniak (2000), these CCGbank parsers return not only syntactic derivations, but also local and long-range dependencies, including those that arise under relativization and coordination.</S>\n    <S sid=\"402\" ssid=\"7\">Although these dependencies are only an approximation of the full semantic interpretation that can in principle be obtained from a CCG, they may prove useful for tasks such as summarization and question answering (Clark, Steedman, and Curran 2004).</S>\n    <S sid=\"403\" ssid=\"8\">Furthermore, Bos et al. (2004) and Bos (2005) have demonstrated that the output of CCGbank parsers can be successfully translated into Kamp and Reyle’s (1993) Discourse Representation Theory structures, to support question answering and the textual entailment task (Bos and Markert 2005).</S>\n    <S sid=\"404\" ssid=\"9\">We hope that these results can be ported to other corpora and other similarly expressive grammar formalisms.</S>\n    <S sid=\"405\" ssid=\"10\">We also hope that our experiences will be useful in designing guidelines for future treebanks.</S>\n    <S sid=\"406\" ssid=\"11\">Although implementational details will differ across formalisms, similar problems and questions to those that arose in our work will be encountered in any attempt to extract expressive grammars from annotated corpora.</S>\n    <S sid=\"407\" ssid=\"12\">Because CCGbank preserves most of the linguistic information in the Treebank in a somewhat less noisy form, we hope that others will find it directly helpful for inducing grammars and statistical parsing models for other linguistically expressive formalisms.</S>\n    <S sid=\"408\" ssid=\"13\">There are essentially three ways in which this might work.</S>\n    <S sid=\"409\" ssid=\"14\">For lexicalized grammars, it may in some cases be possible to translate the subcategorization frames in the CCG lexicon directly into the target theory.</S>\n    <S sid=\"410\" ssid=\"15\">For type-logical grammars (Moortgat 1988; Morrill 1994; Moot 2003), this is little more than a matter of transducing the syntactic types for the lexicon into the appropriate notation.</S>\n    <S sid=\"411\" ssid=\"16\">For formalisms like LTAG, the relation is more complex, but the work of Joshi and Kulick (1996), who “unfold” CCG categories into TAG elementary trees via partial proof trees, and Shen and Joshi (2005), who define LTAG “spines” that resemble categories, suggest that this is possible.</S>\n    <S sid=\"412\" ssid=\"17\">Transduction into HPSG signs is less obvious, but also seems possible in principle.</S>\n    <S sid=\"413\" ssid=\"18\">A second possibility is to transduce CCGbank itself into a form appropriate to the target formalism.</S>\n    <S sid=\"414\" ssid=\"19\">There seems to be a similar ordering over alternative formalisms from straightforward to less straightforward for this approach.</S>\n    <S sid=\"415\" ssid=\"20\">We would also expect that dependency grammars Mel’ˇcuk and Pertsov 1987; Hudson 1984) and parsers (McDonald, Crammer, and Pereira 2005) could be trained and tested with little extra work on the dependencies in CCGbank.</S>\n    <S sid=\"416\" ssid=\"21\">Finally, we believe that existing methods for translating the Penn Treebank from scratch into other grammar formalisms will benefit from including preprocessing similar to that described here.</S>\n    <S sid=\"417\" ssid=\"22\">As some indication of the relative ease with which these techniques transfer, we offer the observation that the 900K-word German Tiger dependency corpus has recently been translated into CCG using very similar techniques by Hockenmaier (2006), and C¸ akıcı (2005) has derived a Turkish lexicon from the a similarly preprocessed version of the METU-Sabanc¸ı Turkish dependency treebank (Oflazer et al. 2003).</S>\n    <S sid=\"418\" ssid=\"23\">A fundamental assumption behind attempts at the automatic translation of syntactically annotated corpora into different grammatical formalisms such as CCG, TAG, HPSG, or LFG is that the analyses that are captured in the original annotation can be mapped directly (or, at least, without too much additional work) into the desired analyses in the target formalism.</S>\n    <S sid=\"419\" ssid=\"24\">This can only hold if all constructions that are treated in a similar manner in the original corpus are also treated in a similar manner in the target formalism.</S>\n    <S sid=\"420\" ssid=\"25\">For the Penn Treebank, our research and the work of others (Xia 1999; Chen and Vijay-Shanker 2004; Chiang 2000; Cahill et al. 2002) have shown that such a correspondence exists in most cases.</S>\n    <S sid=\"421\" ssid=\"26\">Although the output of most current Treebank parsers is linguistically impoverished, the Treebank annotation itself is not.</S>\n    <S sid=\"422\" ssid=\"27\">It is precisely the linguistic richness and detail of the original annotation—in particular, the additional information present in the null elements and function tags that are ignored by most other parsers—that has made the creation of CCGbank possible.</S>\n    <S sid=\"423\" ssid=\"28\">The translation process would have been easier if some of the annotation had been more explicit and precise (as in the case of VP coordination, where preprocessing was required to identify the conjuncts, or in NP coordination, where we were not able to distinguish NP lists from appositives) and consistent (most importantly in identifying adjuncts and arguments).</S>\n    <S sid=\"424\" ssid=\"29\">An important conclusion that follows for the builders of future treebanks is that the tradition established by the Penn Treebank of including all linguistically relevant dependencies should be continued, with if anything even closer adherence to semantically informed linguistic insights into predicate–argument structural relations.</S>\n    <S sid=\"425\" ssid=\"30\">Our results also indicate that corpora of at least the order of magnitude of the Penn Treebank are necessary to obtain grammars and parsers that are sufficiently expressive, robust, and wide in coverage to recover these relations completely.</S>\n  </SECTION>\n  <SECTION title=\"Acknowledgments\" number=\"10\">\n    <S sid=\"426\" ssid=\"1\">We would like to thank our colleagues in Edinburgh and Philadelphia—in particular Jason Baldridge, Johan Bos, Stephen Clark, James Curran, Michael White, Mitch Marcus, Ann Bies, Martha Palmer, and Aravind Joshi—for numerous conversations and feedback on the corpus.</S>\n    <S sid=\"427\" ssid=\"2\">We would also like to thank the Linguistic Data Consortium for their help in publishing CCGbank, and the Computational Linguistics reviewers for their extensive comments on earlier versions of this paper.</S>\n    <S sid=\"428\" ssid=\"3\">We gratefully acknowledge the financial support provided by EPSRC grant GR/M96889.</S>\n    <S sid=\"429\" ssid=\"4\">JH also acknowledges support by an EPSRC studentship and the Edinburgh Language Technology Group, and by NSF ITR grant 0205456 at the University of Pennsylvania.</S>\n    <S sid=\"430\" ssid=\"5\">MJS acknowledges support from the Scottish Enterprise Edinburgh–Stanford Link (NSF IIS-041628 (R39058)) and EU IST grant PACOPLUS (FP6-2004-IST-4-27657).</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "f9ba3f5acb1245e2b5e04115219e063b",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "CCGbank: A Corpus of CCG Derivations and Dependency Structures Extracted from the Penn Treebank"
                }
            }
        },
        {
            "index": 22,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shiftreduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively.load_json_files",
                    "filename": "A_Classifier-Based_Parser_With_Linear_Run-Time_Complexity",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/W05-1513/Documents_xml/W05-1513.xml",
                    "title": "A Classifier-Based Parser With Linear Run-Time Complexity"
                },
                "llm": {
                    "descriptions": [
                        "Abstract:\n\nThis paper presents a classifier-based shift-reduce parser that operates with linear run-time complexity, producing both dependency and constituent structures simultaneously. The parser employs a deterministic approach, guided by a classifier that makes parsing decisions based on the local context, without the need for an explicit grammar. The simplicity and modularity of the parser allow for easy implementation and adaptation with different classifiers and feature sets. Experiments conducted on the Wall Street Journal corpus of the Penn Treebank demonstrate the parser's efficiency and competitive accuracy. The SVM-based version of the parser (SVMpar) achieved over 87% labeled constituent precision and recall with correctly tagged text, and about 86% with automatically assigned POS tags, while maintaining considerable speed advantages over state-of-the-art statistical parsers. The findings highlight the significant impact of classifier choice on parser performance, and future work aims to explore the effects of individual features, additional classification features, different classifiers, and the potential use of a beam strategy to maintain linear run-time while pursuing multiple parses.",
                        "Abstract:\n\nThis paper introduces a novel shift-reduce parser that employs a classifier to simultaneously produce both dependency and constituent structures in linear time. The parser operates deterministically, using a shift-reduce parsing algorithm complemented by a linear-time classifier that makes parsing decisions without the need for an explicit grammar. The parser's efficiency is achieved through binarization of trees with more than two branches and by limiting the number of unary reductions. Experiments conducted with Support Vector Machines (SVM) and Memory-Based Learning (MBL) classifiers on the Wall Street Journal corpus demonstrate that the SVM-based parser (SVMpar) achieves high accuracy with labeled constituent precision and recall above 87% when using correct POS tags, and 86% with automatically assigned tags, which makes it a viable choice for applications requiring fast parsing. The paper emphasizes the importance of classifier choice in parsing performance and suggests that future work could explore different classifiers, additional features, and strategies for pursuing multiple parses while maintaining linear run-time.",
                        "**Abstract:**\n\nThis paper introduces a simple and fast shift-reduce parser that employs classifiers to construct both dependency and constituent structures simultaneously, with a particular focus on linear run-time complexity. The parser operates deterministically, using a bottom-up shift-reduce parsing algorithm and a classifier that makes parsing decisions based on local context without backtracking or relying on an explicit grammar. The parser's performance is evaluated using two types of classifiers: support vector machines (SVMs) and memory-based learning (MBL). Experiments conducted on the Wall Street Journal corpus of the Penn Treebank demonstrate that the SVM-based parser (SVMpar) achieves a labeled constituent precision and recall of over 87% with correct part-of-speech tags, approaching the accuracy of more complex statistical parsers while maintaining considerable speed advantages. The paper also highlights the importance of the choice of classifier, as SVMpar significantly outperforms the MBL-based parser (MBLpar) in both accuracy and speed. The presented parser's simplicity, modularity, and efficiency make it a compelling option for applications requiring rapid parsing of large datasets. Future work will explore the impact of individual features, additional classification features, different classifiers, and the potential use of tree kernels or tree boosting classifiers to enhance performance.",
                        "Abstract:\n\nThis paper introduces a simple and efficient shift-reduce parser that leverages a classifier to generate both dependency and constituent structures in linear time. Sharing similarities with other deterministic parsers, this approach is distinct in its ability to produce constituent structures without the complexity of state-of-the-art statistical parsers. The parser operates through a single pass over the input string and employs a transformation/detransformation process to handle trees with arbitrary branching. Parsing decisions are made by a classifier trained on a corpus of sentences with known parse trees, and the parser's performance is highly dependent on the choice of classifier. Experiments using two different classifiers, TinySVM and TiMBL, demonstrate that the support vector machine-based parser (SVMpar) outperforms the memory-based learner (MBLpar) in terms of both accuracy and speed. SVMpar achieves over 87% precision and recall with gold standard part-of-speech tags and maintains high accuracy with automatically assigned tags. Although not as accurate as the most advanced statistical parsers, this classifier-based parser offers a competitive, faster alternative for applications where rapid parsing is required. Future work will explore the effects of individual features, the inclusion of additional classification features, and alternative classifiers, with the aim of improving accuracy while maintaining linear runtime complexity."
                    ],
                    "filename": "a_classifierbased_parser_with_li-write_xml_paper_abstract",
                    "generation_prompt_nickname": "write_xml_paper_abstract",
                    "generation_prompt_text": "Read the following academic paper (in XML format) and create an abstract for it.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">A Classifier-Based Parser With Linear Run-Time Complexity</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"6\" ssid=\"1\">Two classifier-based deterministic dependency parsers for English have been proposed recently (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003).</S>\n    <S sid=\"7\" ssid=\"2\">Although they use different parsing algorithms, and differ on whether or not dependencies are labeled, they share the idea of greedily pursuing a single path, following parsing decisions made by a classifier.</S>\n    <S sid=\"8\" ssid=\"3\">Despite their greedy nature, these parsers achieve high accuracy in determining dependencies.</S>\n    <S sid=\"9\" ssid=\"4\">Although state-of-the-art statistical parsers (Collins, 1997; Charniak, 2000) are more accurate, the simplicity and efficiency of deterministic parsers make them attractive in a number of situations requiring fast, light-weight parsing, or parsing of large amounts of data.</S>\n    <S sid=\"10\" ssid=\"5\">However, dependency analyses lack important information contained in constituent structures.</S>\n    <S sid=\"11\" ssid=\"6\">For example, the tree-path feature has been shown to be valuable in semantic role labeling (Gildea and Palmer, 2002).</S>\n    <S sid=\"12\" ssid=\"7\">We present a parser that shares much of the simplicity and efficiency of the deterministic dependency parsers, but produces both dependency and constituent structures simultaneously.</S>\n    <S sid=\"13\" ssid=\"8\">Like the parser of Nivre and Scholz (2004), it uses the basic shift-reduce stack-based parsing algorithm, and runs in linear time.</S>\n    <S sid=\"14\" ssid=\"9\">While it may seem that the larger search space of constituent trees (compared to the space of dependency trees) would make it unlikely that accurate parse trees could be built deterministically, we show that the precision and recall of constituents produced by our parser are close to those produced by statistical parsers with higher run-time complexity.</S>\n    <S sid=\"15\" ssid=\"10\">One desirable characteristic of our parser is its simplicity.</S>\n    <S sid=\"16\" ssid=\"11\">Compared to other successful approaches to corpus-based constituent parsing, ours is remarkably simple to understand and implement.</S>\n    <S sid=\"17\" ssid=\"12\">An additional feature of our approach is its modularity with regard to the algorithm and the classifier that determines the parser’s actions.</S>\n    <S sid=\"18\" ssid=\"13\">This makes it very simple for different classifiers and different sets of features to be used with the same parser with very minimal work.</S>\n    <S sid=\"19\" ssid=\"14\">Finally, its linear runtime complexity allows our parser to be considerably faster than lexicalized PCFG-based parsers.</S>\n    <S sid=\"20\" ssid=\"15\">On the other hand, a major drawback of the classifier-based parsing framework is that, depending on node (NP) with four children.</S>\n    <S sid=\"21\" ssid=\"16\">In the transformed tree, internal structure (marked by nodes with asterisks) was added to the subtree rooted by the node with more than two children.</S>\n    <S sid=\"22\" ssid=\"17\">The word “dog” is the head of the original NP, and it is kept as the head of the transformed NP, as well as the head of each NP* node. the classifier used, its training time can be much longer than that of other approaches.</S>\n    <S sid=\"23\" ssid=\"18\">Like other deterministic parsers (and unlike many statistical parsers), our parser considers the problem of syntactic analysis separately from partof-speech (POS) tagging.</S>\n    <S sid=\"24\" ssid=\"19\">Because the parser greedily builds trees bottom-up in one pass, considering only one path at any point in the analysis, the task of assigning POS tags to words is done before other syntactic analysis.</S>\n    <S sid=\"25\" ssid=\"20\">In this work we focus only on the processing that occurs once POS tagging is completed.</S>\n    <S sid=\"26\" ssid=\"21\">In the sections that follow, we assume that the input to the parser is a sentence with corresponding POS tags for each word.</S>\n  </SECTION>\n  <SECTION title=\"2 Parser Description\" number=\"2\">\n    <S sid=\"27\" ssid=\"1\">Our parser employs a basic bottom-up shift-reduce parsing algorithm, requiring only a single pass over the input string.</S>\n    <S sid=\"28\" ssid=\"2\">The algorithm considers only trees with unary and binary branching.</S>\n    <S sid=\"29\" ssid=\"3\">In order to use trees with arbitrary branching for training, or generating them with the parser, we employ an instance of the transformation/detransformation process described in (Johnson, 1998).</S>\n    <S sid=\"30\" ssid=\"4\">In our case, the transformation step involves simply converting each production with n children (where n &gt; 2) into n – 1 binary productions.</S>\n    <S sid=\"31\" ssid=\"5\">Trees must be lexicalized1, so that the newly created internal structure of constituents with previous branching of more than two contains only subtrees with the same lexical head as the original constituent.</S>\n    <S sid=\"32\" ssid=\"6\">Additional nonterminal symbols introduced in this process are clearly marked.</S>\n    <S sid=\"33\" ssid=\"7\">The transformed (or “binarized”) trees may then be used for training.</S>\n    <S sid=\"34\" ssid=\"8\">Detransformation is applied to trees produced by the parser.</S>\n    <S sid=\"35\" ssid=\"9\">This involves the removal of non-terminals introduced in the transformation process, producing trees with arbitrary branching.</S>\n    <S sid=\"36\" ssid=\"10\">An example of transformation/detransformation is shown in figure 1.</S>\n    <S sid=\"37\" ssid=\"11\">The parsing algorithm involves two main data structures: a stack S, and a queue W. Items in S may be terminal nodes (POS-tagged words), or (lexicalized) subtrees of the final parse tree for the input string.</S>\n    <S sid=\"38\" ssid=\"12\">Items in W are terminals (words tagged with parts-of-speech) corresponding to the input string.</S>\n    <S sid=\"39\" ssid=\"13\">When parsing begins, S is empty and W is initialized by inserting every word from the input string in order, so that the first word is in front of the queue.</S>\n    <S sid=\"40\" ssid=\"14\">Only two general actions are allowed: shift and reduce.</S>\n    <S sid=\"41\" ssid=\"15\">A shift action consists only of removing (shifting) the first item (POS-tagged word) from W (at which point the next word becomes the new first item), and placing it on top of S. Reduce actions are subdivided into unary and binary cases.</S>\n    <S sid=\"42\" ssid=\"16\">In a unary reduction, the item on top of S is popped, and a new item is pushed onto S. The new item consists of a tree formed by a non-terminal node with the popped item as its single child.</S>\n    <S sid=\"43\" ssid=\"17\">The lexical head of the new item is the same as the lexical head of the popped item.</S>\n    <S sid=\"44\" ssid=\"18\">In a binary reduction, two items are popped from S in sequence, and a new item is pushed onto S. The new item consists of a tree formed by a non-terminal node with two children: the first item popped from S is the right child, and the second item is the left child.</S>\n    <S sid=\"45\" ssid=\"19\">The lexical head of the new item is either the lexical head of its left child, or the lexical head of its right child.</S>\n    <S sid=\"46\" ssid=\"20\">If S is empty, only a shift action is allowed.</S>\n    <S sid=\"47\" ssid=\"21\">If W is empty, only a reduce action is allowed.</S>\n    <S sid=\"48\" ssid=\"22\">If both S and W are non-empty, either shift or reduce actions are possible.</S>\n    <S sid=\"49\" ssid=\"23\">Parsing terminates when W is empty and S contains only one item, and the single item in S is the parse tree for the input string.</S>\n    <S sid=\"50\" ssid=\"24\">Because the parse tree is lexicalized, we also have a dependency structure for the sentence.</S>\n    <S sid=\"51\" ssid=\"25\">In fact, the binary reduce actions are very similar to the reduce actions in the dependency parser of Nivre and Scholz (2004), but they are executed in a different order, so constituents can be built.</S>\n    <S sid=\"52\" ssid=\"26\">If W is empty, and more than one item remain in S, and no further reduce actions take place, the input string is rejected.</S>\n    <S sid=\"53\" ssid=\"27\">A parser based on the algorithm described in the previous section faces two types of decisions to be made throughout the parsing process.</S>\n    <S sid=\"54\" ssid=\"28\">The first type concerns whether to shift or reduce when both actions are possible, or whether to reduce or reject the input when only reduce actions are possible.</S>\n    <S sid=\"55\" ssid=\"29\">The second type concerns what syntactic structures are created.</S>\n    <S sid=\"56\" ssid=\"30\">Specifically, what new non-terminal is introduced in unary or binary reduce actions, or which of the left or right children are chosen as the source of the lexical head of the new subtree produced by binary reduce actions.</S>\n    <S sid=\"57\" ssid=\"31\">Traditionally, these decisions are made with the use of a grammar, and the grammar may allow more than one valid action at any single point in the parsing process.</S>\n    <S sid=\"58\" ssid=\"32\">When multiple choices are available, a grammar-driven parser may make a decision based on heuristics or statistical models, or pursue every possible action following a search strategy.</S>\n    <S sid=\"59\" ssid=\"33\">In our case, both types of decisions are made by a classifier that chooses a unique action at every point, based on the local context of the parsing action, with no explicit grammar.</S>\n    <S sid=\"60\" ssid=\"34\">This type of classifierbased parsing where only one path is pursued with no backtracking can be viewed as greedy or deterministic.</S>\n    <S sid=\"61\" ssid=\"35\">In order to determine what actions the parser should take given a particular parser configuration, a classifier is given a set of features derived from that configuration.</S>\n    <S sid=\"62\" ssid=\"36\">This includes, crucially, the two topmost items in the stack S, and the item in front of the queue W. Additionally, a set of context features is derived from a (fixed) limited number of items below the two topmost items of S, and following the item in front of W. The specific features are shown in figure 2.</S>\n    <S sid=\"63\" ssid=\"37\">The classifier’s target classes are parser actions that specify both types of decisions mentioned above.</S>\n    <S sid=\"64\" ssid=\"38\">These classes are: Let: S(n) denote the nth item from the top of the stack S, and W(n) denote the nth item from the front of the queue W. Features: rectly related to the lexicalized constituent trees that are built during parsing, while the features described in items 8 – 13 are more directly related to the dependency structures that are built simultaneously to the constituent structures.</S>\n    <S sid=\"65\" ssid=\"39\">Additionally, the head of the new subtree is the same as the head of the left child of the root node; When the algorithm described in section 2.1 is combined with a trained classifier that determines its parsing actions as described in section 2.2, we have a complete classifier-based parser.</S>\n    <S sid=\"66\" ssid=\"40\">Training the parser is accomplished by training its classifier.</S>\n    <S sid=\"67\" ssid=\"41\">To that end, we need training instances that consist of sets of features paired with their classes corresponding to the correct parsing actions.</S>\n    <S sid=\"68\" ssid=\"42\">These instances can be obtained by running the algorithm on a corpus of sentences for which the correct parse trees are known.</S>\n    <S sid=\"69\" ssid=\"43\">Instead of using the classifier to determine the parser’s actions, we simply determine the correct action by consulting the correct parse trees.</S>\n    <S sid=\"70\" ssid=\"44\">We then record the features and corresponding actions for parsing all sentences in the corpus into their correct trees.</S>\n    <S sid=\"71\" ssid=\"45\">This set of features and corresponding actions is then used to train a classifier, resulting in a complete parser.</S>\n    <S sid=\"72\" ssid=\"46\">When parsing a sentence with n words, the parser takes n shift actions (exactly one for each word in the sentence).</S>\n    <S sid=\"73\" ssid=\"47\">Because the maximum branching factor of trees built by the parser is two, the total number of binary reduce actions is n – 1, if a complete parse is found.</S>\n    <S sid=\"74\" ssid=\"48\">If the input string is rejected, the number of binary reduce actions is less than n – 1.</S>\n    <S sid=\"75\" ssid=\"49\">Therefore, the number of shift and binary reduce actions is linear with the number of words in the input string.</S>\n    <S sid=\"76\" ssid=\"50\">However, the parser as described so far has no limit on the number of unary reduce actions it may take.</S>\n    <S sid=\"77\" ssid=\"51\">Although in practice a parser properly trained on trees reflecting natural language syntax would rarely make more than 2n unary reductions, pathological cases exist where an infinite number of unary reductions would be taken, and the algorithm would not terminate.</S>\n    <S sid=\"78\" ssid=\"52\">Such cases may include the observation in the training data of sequences of unary productions that cycle through (repeated) non-terminals, such as A-&gt;B-&gt;A-&gt;B.</S>\n    <S sid=\"79\" ssid=\"53\">During parsing, it is possible that such a cycle may be repeated infinitely.</S>\n    <S sid=\"80\" ssid=\"54\">This problem can be easily prevented by limiting the number of consecutive unary reductions that may be made to a finite number.</S>\n    <S sid=\"81\" ssid=\"55\">This may be the number of non-terminal types seen in the training data, or the length of the longest chain of unary productions seen in the training data.</S>\n    <S sid=\"82\" ssid=\"56\">In our experiments (described in section 3), we limited the number of consecutive unary reductions to three, although the parser never took more than two unary reduction actions consecutively in any sentence.</S>\n    <S sid=\"83\" ssid=\"57\">When we limit the number of consecutive unary reductions to a finite number m, the parser makes at most (2n – 1)m unary reductions when parsing a sentence of length n. Placing this limit not only guarantees that the algorithm terminates, but also guarantees that the number of actions taken by the parser is O(n), where n is the length of the input string.</S>\n    <S sid=\"84\" ssid=\"58\">Thus, the parser runs in linear time, assuming that classifying a parser action is done in constant time.</S>\n  </SECTION>\n  <SECTION title=\"3 Similarities to Previous Work\" number=\"3\">\n    <S sid=\"85\" ssid=\"1\">As mentioned before, our parser shares similarities with the dependency parsers of Yamada and Matsumoto (2003) and Nivre and Scholz (2004) in that it uses a classifier to guide the parsing process in deterministic fashion.</S>\n    <S sid=\"86\" ssid=\"2\">While Yamada and Matsumoto use a quadratic run-time algorithm with multiple passes over the input string, Nivre and Scholz use a simplified version of the algorithm described here, which handles only (labeled or unlabeled) dependency structures.</S>\n    <S sid=\"87\" ssid=\"3\">Additionally, our parser is in some ways similar to the maximum-entropy parser of Ratnaparkhi (1997).</S>\n    <S sid=\"88\" ssid=\"4\">Ratnaparkhi’s parser uses maximumentropy models to determine the actions of a shiftreduce-like parser, but it is capable of pursuing several paths and returning the top-K highest scoring parses for a sentence.</S>\n    <S sid=\"89\" ssid=\"5\">Its observed time is linear, but parsing is somewhat slow, with sentences of length 20 or more taking more than one second to parse, and sentences of length 40 or more taking more than three seconds.</S>\n    <S sid=\"90\" ssid=\"6\">Our parser only pursues one path per sentence, but it is very fast and of comparable accuracy (see section 4).</S>\n    <S sid=\"91\" ssid=\"7\">In addition, Ratnaparkhi’s parser uses a more involved algorithm that allows it to work with arbitrary branching trees without the need of the binarization transform employed here.</S>\n    <S sid=\"92\" ssid=\"8\">It breaks the usual reduce actions into smaller pieces (CHECK and BUILD), and uses two separate passes (not including the POS tagging pass) for determining chunks and higher syntactic structures separately.</S>\n    <S sid=\"93\" ssid=\"9\">Finally, there have been other deterministic shift-reduce parsers introduced recently, but their levels of accuracy have been well below the stateof-the-art.</S>\n    <S sid=\"94\" ssid=\"10\">The parser in Kalt (2004) uses a similar algorithm to the one described here, but the classification task is framed differently.</S>\n    <S sid=\"95\" ssid=\"11\">Using decision trees and fewer features, Kalt’s parser has significantly faster training and parsing times, but its accuracy is much lower than that of our parser.</S>\n    <S sid=\"96\" ssid=\"12\">Kalt’s parser achieves precision and recall of about 77% and 76%, respectively (with automatically tagged text), compared to our parser’s 86% (see section 4).</S>\n    <S sid=\"97\" ssid=\"13\">The parser of Wong and Wu (1999) uses a separate NP-chunking step and, like Ratnaparkhi’s parser, does not require a binary transracy, and time required to parse the test set.</S>\n    <S sid=\"98\" ssid=\"14\">The parsers of Yamada and Matsumoto (Y&amp;M) and Nivre and Scholz (N&amp;S) do not produce constituent structures, only dependencies.</S>\n    <S sid=\"99\" ssid=\"15\">“unk” indicates unknown values.</S>\n    <S sid=\"100\" ssid=\"16\">Results for MBLpar and SVMpar using correct POS tags (if automatically produced POS tags are used, accuracy figures drop about 1.5% over all metrics). form.</S>\n    <S sid=\"101\" ssid=\"17\">It achieves about 81% precision and 82% recall with gold-standard tags (78% and 79% with automatically tagged text).</S>\n    <S sid=\"102\" ssid=\"18\">Wong and Wu’s parser is further differentiated from the other parsers mentioned here in that it does not use lexical items, working only from part-of-speech tags.</S>\n  </SECTION>\n  <SECTION title=\"4 Experiments\" number=\"4\">\n    <S sid=\"103\" ssid=\"1\">We conducted experiments with the parser described in section 2 using two different classifiers: TinySVM (a support vector machine implementation by Taku Kudo)2, and the memory-based learner TiMBL (Daelemans et al., 2004).</S>\n    <S sid=\"104\" ssid=\"2\">We trained and tested the parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al., 1993) using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing.</S>\n    <S sid=\"105\" ssid=\"3\">Every experiment reported here was performed on a Pentium IV 1.8GHz with 1GB of RAM.</S>\n    <S sid=\"106\" ssid=\"4\">Each tree in the training set had empty-node and function tag information removed, and the trees were lexicalized using similar head-table rules as those mentioned in (Collins, 1996).</S>\n    <S sid=\"107\" ssid=\"5\">The trees were then converted into trees containing only unary and binary branching, using the binarization transform described in section 2.</S>\n    <S sid=\"108\" ssid=\"6\">Classifier training instances of features paired with classes (parser actions) were extracted from the trees in the training set, as described in section 2.3.</S>\n    <S sid=\"109\" ssid=\"7\">The total number of training instances was about 1.5 million.</S>\n    <S sid=\"110\" ssid=\"8\">The classifier in the SVM-based parser (denoted by SVMpar) uses the polynomial kernel with degree 2, following the work of Yamada and Matsumoto (2003) on SVM-based deterministic dependency parsing, and a one-against-all scheme for multi-class classification.</S>\n    <S sid=\"111\" ssid=\"9\">Because of the large number of training instances, we used Yamada and Matsumoto’s idea of splitting the training instances into several parts according to POS tags, and training classifiers on each part.</S>\n    <S sid=\"112\" ssid=\"10\">This greatly reduced the time required to train the SVMs, but even with the splitting of the training set, total training time was about 62 hours.</S>\n    <S sid=\"113\" ssid=\"11\">Training set splitting comes with the cost of reduction in accuracy of the parser, but training a single SVM would likely take more than one week.</S>\n    <S sid=\"114\" ssid=\"12\">Yamada and Matsumoto experienced a reduction of slightly more than 1% in dependency accuracy due to training set splitting, and we expect that a similar loss is incurred here.</S>\n    <S sid=\"115\" ssid=\"13\">When given perfectly tagged text (gold tags extracted from the Penn Treebank), SVMpar has labeled constituent precision and recall of 87.54% and 87.61%, respectively, and dependency accuracy of 90.3% over all sentences in the test set.</S>\n    <S sid=\"116\" ssid=\"14\">The total time required to parse the entire test set was 11 minutes.</S>\n    <S sid=\"117\" ssid=\"15\">Out of more than 2,400 sentences, only 26 were rejected by the parser (about 1.1%).</S>\n    <S sid=\"118\" ssid=\"16\">For these sentences, partial analyses were created by combining the items in the stack in flat structures, and these were included in the evaluation.</S>\n    <S sid=\"119\" ssid=\"17\">Predictably, the labeled constituent precision and recall obtained with automatically POS-tagged sentences were lower, at 86.01% and 86.15%.</S>\n    <S sid=\"120\" ssid=\"18\">The part-of-speech tagger used in our experiments was SVMTool (Giménez and Márquez, 2004), and its accuracy on the test set is 97%.</S>\n    <S sid=\"121\" ssid=\"19\">The MBL-based parser (denoted by MBLpar) uses the IB1 algorithm, with five nearest neighbors, and the modified value difference metric (MVDM), following the work of Nivre and Scholz (2004) on MBL-based deterministic dependency parsing.</S>\n    <S sid=\"122\" ssid=\"20\">MBLpar was trained with all training instances in under 15 minutes, but its accuracy on the test set was much lower than that of SVMpar, with constituent precision and recall of 80.0% and 80.2%, and dependency accuracy of 86.3% (24 sentences were rejected).</S>\n    <S sid=\"123\" ssid=\"21\">It was also much slower than SVMpar in parsing the test set, taking 127 minutes.</S>\n    <S sid=\"124\" ssid=\"22\">In addition, the total memory required for running MBLpar (including the classifier) was close to 1 gigabyte (including the trained classifier), while SVMpar required only about 200 megabytes (including all the classifiers).</S>\n    <S sid=\"125\" ssid=\"23\">Table 1 shows a summary of the results of our experiments with SVMpar and MBLpar, and also results obtained with the Charniak (2000) parser, the Bikel (2003) implementation of the Collins (1997) parser, and the Ratnaparkhi (1997) parser.</S>\n    <S sid=\"126\" ssid=\"24\">We also include the dependency accuracy from Yamada and Matsumoto’s (2003) SVM-based dependency parser, and Nivre and Scholz’s (2004) MBL-based dependency parser.</S>\n    <S sid=\"127\" ssid=\"25\">These results show that the choice of classifier is extremely important in this task.</S>\n    <S sid=\"128\" ssid=\"26\">SVMpar and MBLpar use the same algorithm and features, and differ only on the classifiers used to make parsing decisions.</S>\n    <S sid=\"129\" ssid=\"27\">While in many natural language processing tasks different classifiers perform at similar levels of accuracy, we have observed a dramatic difference between using support vector machines and a memory-based learner.</S>\n    <S sid=\"130\" ssid=\"28\">Although the reasons for such a large disparity in results is currently the subject of further investigation, we speculate that a relatively small difference in initial classifier accuracy results in larger differences in parser performance, due to the deterministic nature of the parser (certain errors may lead to further errors).</S>\n    <S sid=\"131\" ssid=\"29\">We also believe classifier choice to be one major source of the difference in accuracy between Nivre and Scholz’s parser and Yamada and Matsumoto’s parser.</S>\n    <S sid=\"132\" ssid=\"30\">While the accuracy of SVMpar is below that of lexicalized PCFG-based statistical parsers, it is surprisingly good for a greedy parser that runs in linear time.</S>\n    <S sid=\"133\" ssid=\"31\">Additionally, it is considerably faster than lexicalized PCFG-based parsers, and offers a good alternative for when fast parsing is needed.</S>\n    <S sid=\"134\" ssid=\"32\">MBLpar, on the other hand, performed poorly in terms of accuracy and speed.</S>\n  </SECTION>\n  <SECTION title=\"5 Conclusion and Future Work\" number=\"5\">\n    <S sid=\"135\" ssid=\"1\">We have presented a simple shift-reduce parser that uses a classifier to determine its parsing actions and runs in linear time.</S>\n    <S sid=\"136\" ssid=\"2\">Using SVMs for classification, the parser has labeled constituent precision and recall higher than 87% when using the correct part-of-speech tags, and slightly higher than 86% when using automatically assigned partof-speech tags.</S>\n    <S sid=\"137\" ssid=\"3\">Although its accuracy is not as high as those of state-of-the-art statistical parsers, our classifier-based parser is considerably faster than several well-known parsers that employ search or dynamic programming approaches.</S>\n    <S sid=\"138\" ssid=\"4\">At the same time, it is significantly more accurate than previously proposed deterministic parsers for constituent structures.</S>\n    <S sid=\"139\" ssid=\"5\">We have also shown that much of the success of a classifier-based parser depends on what classifier is used.</S>\n    <S sid=\"140\" ssid=\"6\">While this may seem obvious, the differences observed here are much greater than what would be expected from looking, for example, at results from chunking/shallow parsing (Zhang et al., 2001; Kudo and Matsumoto, 2001; Veenstra and van den Bosch, 2000).</S>\n    <S sid=\"141\" ssid=\"7\">Future work includes the investigation of the effects of individual features, the use of additional classification features, and the use of different classifiers.</S>\n    <S sid=\"142\" ssid=\"8\">In particular, the use of tree features seems appealing.</S>\n    <S sid=\"143\" ssid=\"9\">This may be accomplished with SVMs using a tree kernel, or the tree boosting classifier BACT described in (Kudo and Matsumoto, 2004).</S>\n    <S sid=\"144\" ssid=\"10\">Additionally, we plan to investigate the use of the beam strategy of Ratnaparkhi (1997) to pursue multiple parses while keeping the run-time linear.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "6987a6be901a6db4e5fa71a6f01316b3",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "A Classifier-Based Parser With Linear Run-Time Complexity"
                }
            }
        },
        {
            "index": 23,
            "choice": "llm",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "author: Online conversational text, typified by microblogs, and text is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There Abstract We consider the problem of part-of-speech tagging for informal, online conversational text. We systematically evaluate the use of large-scale unsupervised word clustering and new lexical features to improve tagging accuracy. With these features, our system achieves state-of-the-art tagging results on both Twitter and IRC POS tagging tasks; Twitter tagging is improved from 90% to 93% accuracy (more than 3% absolute). Qualitative analysis of these word clusters yields insights about NLP and linguistic phenomena in this genre. Additionally, we contribute the first POS annotation guidelines for such text and release a new dataset of English language tweets annotated using these guidelines. Tagging software, annotation guidelines, and large-scale word clusters are available at:",
                    "filename": "Improved_Part-of-Speech_Tagging_for_Online_Conversational_Text_with_Word_Clusters",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/N13-1039/Documents_xml/N13-1039.xml",
                    "title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters"
                },
                "llm": {
                    "descriptions": [
                        "This paper presents a state-of-the-art part-of-speech (POS) tagger tailored for online conversational text, specifically focusing on Twitter and Internet Relay Chat (IRC). Recognizing the unique linguistic characteristics and novel constructions found in social media text, the authors build upon previous work to incorporate large-scale distributional features into a new Twitter POS tagger. A maximum entropy Markov model (MEMM) serves as the tagging model, with improvements like efficient greedy decoding that processes data at high speeds, crucial for the vast quantities of text generated on social media platforms.\n\nTo address the challenge of novel and nonstandard words prevalent in online discourse, the authors utilize unsupervised word clusters, induced from a sizeable dataset of unlabeled tweets, to enhance tagging performance. These clusters capture linguistic phenomena specific to online conversational English, such as expressive lengthening, contractions, and emoticons. The paper also introduces a new annotated dataset of tweets with POS tags and provides comprehensive annotation guidelines for manual POS tagging of tweets, with both resources made available to the research community.\n\nExperiments demonstrate that the new tagger significantly outperforms previous models, achieving high accuracy rates on the newly annotated datasets. The authors highlight the critical contribution of unsupervised word clusters to the model's performance, particularly with words not found in traditional dictionaries. The paper concludes with the release of all developed resources to facilitate further research and application in NLP tasks involving online conversational text.",
                        "Abstract:\n\nThis paper presents a new part-of-speech (POS) tagger specifically designed for online conversational text, such as Twitter and Internet Relay Chat (IRC), which outperforms existing systems. The tagger is based on a first-order maximum entropy Markov model (MEMM) that incorporates large-scale distributional features, including word clusters induced from a vast amount of unlabeled social media text. A novel dataset of tweets annotated with POS tags was created, and existing datasets were enhanced with improved annotations, all of which are released to the research community. The tagger employs unsupervised word clusters generated through Brown clustering on over 56 million unique tweets, which effectively capture novel words and expressions typical in social media. The model efficiently processes millions of messages with a greedy decoding algorithm, achieving state-of-the-art tagging accuracy while running significantly faster than previous systems. The utility of hierarchical word clusters as features is demonstrated, substantially benefiting the tagging of non-traditional and out-of-vocabulary words. Experiments confirm the effectiveness of the tagger, with notable accuracy improvements over previous approaches. The newly developed resources and the tagger are made available to facilitate future research in analyzing the linguistics of online conversational text.",
                        "This paper presents a novel part-of-speech (POS) tagging system specifically tailored for online conversational text, such as Twitter and Internet Relay Chat (IRC). The system utilizes large-scale distributional features, including unsupervised word clusters derived from a vast collection of unlabeled conversational text. These clusters are instrumental in capturing the lexical variety and dynamic nature of online communication, significantly enhancing the tagger's performance on non-standard words and expressions.\n\nThe paper reports state-of-the-art tagging accuracy for both Twitter and IRC text, outperforming previous systems. A new dataset of tweets annotated with POS tags was developed, and the annotation guidelines were refined and shared with the research community. The authors also release a new Twitter POS tagger, which includes features like elastic net regularization, efficient decoding methods, and integration of generalized lexical class information.\n\nThe proposed tagger demonstrates high efficiency, capable of processing 800 tweets per second, and shows that word clusters provide a substantial contribution to tagging accuracy. The tagger is released to the public, alongside the new annotated dataset, annotation guidelines, and the induced word clusters, fostering further research in the field of natural language processing for social media text.",
                        "Abstract:\n\nThis paper presents a part-of-speech (POS) tagging system specifically tailored for the unique characteristics of online conversational text, such as Twitter and Internet Relay Chat (IRC). To address the challenges posed by novel words and constructions prevalent in social media, the authors introduce new large-scale distributional features derived from word clusters. These clusters are obtained through unsupervised learning on a vast corpus of unlabeled tweets, leveraging hierarchical Brown clustering to capture lexical similarities and variations.\n\nA first-order maximum entropy Markov model (MEMM) forms the basis of the tagging model, which benefits from features such as transition probabilities, word clusters, and a tag dictionary. The system demonstrates state-of-the-art POS tagging performance on both Twitter and IRC text, significantly outperforming previous systems. The authors also introduce a new annotated dataset of tweets and improved annotation guidelines to bolster the manual POS tagging process for tweets.\n\nThe paper details the experimental setup, which confirms the efficacy of the word clusters as a robust source of lexical knowledge, crucial for the improved performance of the tagger. Additionally, the tagger exhibits remarkable speed, processing up to 800 tweets per second, which is critical for analyzing the high volume of data typical in social media applications.\n\nThe authors have made their new datasets, annotation guidelines, and tagging tools publicly available, alongside the induced word clusters, to facilitate further research and development in the field of natural language processing for social media text."
                    ],
                    "filename": "improved_partofspeech_tagging_fo-write_xml_paper_abstract_control_word_count",
                    "generation_prompt_nickname": "write_xml_paper_abstract_control_word_count",
                    "generation_prompt_text": "Read the following academic paper provided in XML format and create an abstract for it.\n\nPlease limit the response to 201 words or less.\n\n---\n\n**Description:**\n\n<PAPER>\n  <S sid=\"0\">Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters</S>\n  <SECTION title=\"1 Introduction\" number=\"1\">\n    <S sid=\"13\" ssid=\"1\">asked for your last name so he can add you on Facebook.</S>\n    <S sid=\"14\" ssid=\"2\">The tagset is defined in Appendix A.</S>\n    <S sid=\"15\" ssid=\"3\">Refer to Fig.</S>\n    <S sid=\"16\" ssid=\"4\">2 for word clusters corresponding to some of these words. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire.</S>\n    <S sid=\"17\" ssid=\"5\">Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010).</S>\n    <S sid=\"18\" ssid=\"6\">To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features.</S>\n    <S sid=\"19\" ssid=\"7\">This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text.</S>\n    <S sid=\"20\" ssid=\"8\">We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets.</S>\n    <S sid=\"21\" ssid=\"9\">We release all of these resources to the research community:</S>\n  </SECTION>\n  <SECTION title=\"2 MEMM Tagger\" number=\"2\">\n    <S sid=\"22\" ssid=\"1\">Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1, and is parameterized by a multiclass logistic regression: We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A).</S>\n    <S sid=\"23\" ssid=\"2\">Our feature sets will be discussed below in detail.</S>\n    <S sid=\"24\" ssid=\"3\">Decoding.</S>\n    <S sid=\"25\" ssid=\"4\">For experiments reported in this paper, we use the O(|x|K2) Viterbi algorithm for prediction; K is the number of tags.</S>\n    <S sid=\"26\" ssid=\"5\">This exactly maximizes p(y  |x), but the MEMM also naturally allows a faster O(|x|K) left-to-right greedy decoding: fort = 1... |x|: yt arg maxk p(yt = k  |�yt−1, x, t; 3) which we find is 3 times faster and yields similar accuracy as Viterbi (an insignificant accuracy decrease of less than 0.1% absolute on the DAILY547 test set discussed below).</S>\n    <S sid=\"27\" ssid=\"6\">Speed is paramount for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software.</S>\n    <S sid=\"28\" ssid=\"7\">2Although when compared to CRFs, MEMMs theoretically suffer from the “label bias” problem (Lafferty et al., 2001), our system substantially outperforms the CRF-based taggers of previous work; and when comparing to Gimpel et al. system with similar feature sets, we observed little difference in accuracy.</S>\n    <S sid=\"29\" ssid=\"8\">This is consistent with conventional wisdom that the quality of lexical features is much more important than the parametric form of the sequence model, at least in our setting: part-ofspeech tagging with a small labeled training set.</S>\n    <S sid=\"30\" ssid=\"9\">This greedy tagger runs at 800 tweets/sec.</S>\n    <S sid=\"31\" ssid=\"10\">(10,000 tokens/sec.) on a single CPU core, about 40 times faster than Gimpel et al.’s system.</S>\n    <S sid=\"32\" ssid=\"11\">The tokenizer by itself (§4) runs at 3,500 tweets/sec.3 Training and regularization.</S>\n    <S sid=\"33\" ssid=\"12\">During training, the MEMM log-likelihood for a tagged tweet (x, y) is the sum over the observed token tags yt, each conditional on the tweet being tagged and the observed previous tag (with a start symbol before the first token in x), We optimize the parameters 3 with OWL-QN, an L1-capable variant of L-BFGS (Andrew and Gao, 2007; Liu and Nocedal, 1989) to minimize the regularized objective where N is the number of tokens in the corpus and the sum ranges over all tagged tweets (x, y) in the training data.</S>\n    <S sid=\"34\" ssid=\"13\">We use elastic net regularization (Zou and Hastie, 2005), which is a linear combination of L1 and L2 penalties; here j indexes over all features: Using even a very small L1 penalty eliminates many irrelevant or noisy features.</S>\n  </SECTION>\n  <SECTION title=\"3 Unsupervised Word Clusters\" number=\"3\">\n    <S sid=\"35\" ssid=\"1\">Our POS tagger can make use of any number of possibly overlapping features.</S>\n    <S sid=\"36\" ssid=\"2\">While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web.</S>\n    <S sid=\"37\" ssid=\"3\">Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia).</S>\n    <S sid=\"38\" ssid=\"4\">We use a similar approach here to improve tagging performance for online conversational text.</S>\n    <S sid=\"39\" ssid=\"5\">We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre.</S>\n    <S sid=\"40\" ssid=\"6\">We obtained hierarchical word clusters via Brown clustering (Brown et al., 1992) on a large set of unlabeled tweets.4 The algorithm partitions words into a base set of 1,000 clusters, and induces a hierarchy among those 1,000 clusters with a series of greedy agglomerative merges that heuristically optimize the likelihood of a hidden Markov model with a one-class-per-lexical-type constraint.</S>\n    <S sid=\"41\" ssid=\"7\">Not only does Brown clustering produce effective features for discriminative models, but its variants are better unsupervised POS taggers than some models developed nearly 20 years later; see comparisons in Blunsom and Cohn (2011).</S>\n    <S sid=\"42\" ssid=\"8\">The algorithm is attractive for our purposes since it scales to large amounts of data.</S>\n    <S sid=\"43\" ssid=\"9\">When training on tweets drawn from a single day, we observed time-specific biases (e.g., numerical dates appearing in the same cluster as the word tonight), so we assembled our unlabeled data from a random sample of 100,000 tweets per day from September 10, 2008 to August 14, 2012, and filtered out non-English tweets (about 60% of the sample) using langid.py (Lui and Baldwin, 2012).5 Each tweet was processed with our tokenizer and lowercased.</S>\n    <S sid=\"44\" ssid=\"10\">We normalized all atmentions to (@MENTION) and URLs/email addresses to their domains (e.g. http://bit.ly/ dP8rR8 ==&gt;- (URL-bit.ly)).</S>\n    <S sid=\"45\" ssid=\"11\">In an effort to reduce spam, we removed duplicated tweet texts (this also removes retweets) before word clustering.</S>\n    <S sid=\"46\" ssid=\"12\">This normalization and cleaning resulted in 56 million unique tweets (847 million tokens).</S>\n    <S sid=\"47\" ssid=\"13\">We set the clustering software’s count threshold to only cluster words appearing 40 or more times, yielding 216,856 word types, which took 42 hours to cluster on a single CPU.</S>\n    <S sid=\"48\" ssid=\"14\">Fig.</S>\n    <S sid=\"49\" ssid=\"15\">2 shows example clusters.</S>\n    <S sid=\"50\" ssid=\"16\">Some of the challenging words in the example tweet (Fig.</S>\n    <S sid=\"51\" ssid=\"17\">1) are highlighted.</S>\n    <S sid=\"52\" ssid=\"18\">The term lololol (an extension of lol for “laughing out loud”) is grouped with a large number of laughter acronyms (A1: “laughing my (fucking) ass off,” “cracking the fuck up”).</S>\n    <S sid=\"53\" ssid=\"19\">Since expressions of laughter are so prevalent on Twitter, the algorithm creates another laughter cluster (A1’s sibling A2), that tends to have onomatopoeic, non-acronym variants (e.g., haha).</S>\n    <S sid=\"54\" ssid=\"20\">The acronym ikr (“I know, right?”) is grouped with expressive variations of “yes” and “no” (A4).</S>\n    <S sid=\"55\" ssid=\"21\">Note that A1–A4 are grouped in a fairly specific subtree; and indeed, in this message ikr and lololol are both tagged as interjections. smh (“shaking my head,” indicating disapproval) seems related, though is always tagged in the annotated data as a miscellaneous abbreviation (G); the difference between acronyms that are interjections versus other acronyms may be complicated.</S>\n    <S sid=\"56\" ssid=\"22\">Here, smh is in a related but distinct subtree from the above expressions (A5); its usage in this example is slightly different from its more common usage, which it shares with the other words in its cluster: message-ending expressions of commentary or emotional reaction, sometimes as a metacomment on the author’s message; e.g., Maybe you could get a guy to date you if you actually respected yourself #smh or There is really NO reason why other girls should send my boyfriend a goodmorning text #justsaying.</S>\n    <S sid=\"57\" ssid=\"23\">We observe many variants of categories traditionally considered closed-class, including pronouns (B: u = “you”) and prepositions (C: fir = “for”).</S>\n    <S sid=\"58\" ssid=\"24\">There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012).</S>\n    <S sid=\"59\" ssid=\"25\">This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011).</S>\n    <S sid=\"60\" ssid=\"26\">We have built an HTML viewer to browse these and numerous other interesting examples.7 We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English).</S>\n    <S sid=\"61\" ssid=\"27\">To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” tion symbols, and emoji to mean symbols rendered in software as small pictures, in line with the text.</S>\n    <S sid=\"62\" ssid=\"28\">Since our tokenizer is careful to preserve emoticons and other symbols (see §4), they are clustered just like other words.</S>\n    <S sid=\"63\" ssid=\"29\">Similar emoticons are clustered together (G1–G4), including separate clusters of happy  :) =) ^_^ ]], sad/disappointed  :/ :( -_- &lt;/3 ]], love  ❑xoxo ❑.❑ ]] and winking  ;) (^_-) ]] emoticons.</S>\n    <S sid=\"64\" ssid=\"30\">The clusters are not perfectly aligned with our POS annotation guidelines; for example, the “sad” emoticon cluster included emotion-bearing terms that our guidelines define as non-emoticons, such as #ugh, #tear, and #fml (“fuck my life”), though these seem potentially useful for sentiment analysis.</S>\n    <S sid=\"65\" ssid=\"31\">One difficult task is classifying different types of symbols in tweets: our annotation guidelines differentiate between emoticons, punctuation, and garbage (apparently non-meaningful symbols or tokenization errors).</S>\n    <S sid=\"66\" ssid=\"32\">Several Unicode character ranges are reserved for emoji-style symbols (including the three Unicode hearts in G4); however, depending on the user’s software, characters in these ranges might be rendered differently or not at all.</S>\n    <S sid=\"67\" ssid=\"33\">We have found instances where the clustering algorithm groups proprietary iOS emoji symbols along with normal emoticons; for example, the character U+E056, which is interpreted on iOS as a smiling face, is in the same G2 cluster as smiley face emoticons.</S>\n    <S sid=\"68\" ssid=\"34\">The symbol U+E12F, which represents a picture of a bag of money, is grouped with the words cash and money.</S>\n    <S sid=\"69\" ssid=\"35\">Since Brown clusters are hierarchical in a binary tree, each word is associated with a tree path represented as a bitstring with length &lt; 16; we use prefixes of the bitstring as features (for all prefix lengths E 12, 4, 6,... ,16}).</S>\n    <S sid=\"70\" ssid=\"36\">This allows sharing of statistical strength between similar clusters.</S>\n    <S sid=\"71\" ssid=\"37\">Using prefix features of hierarchical clusters in this way was similarly found to be effective for named-entity recognition (Turian et al., 2010) and Twitter POS tagging (Ritter et al., 2011).</S>\n    <S sid=\"72\" ssid=\"38\">When checking to see if a word is associated with a cluster, the tagger first normalizes the word using the same techniques as described in §3.1, then creates a priority list of fuzzy match transformations of the word by removing repeated punctuation and repeated characters.</S>\n    <S sid=\"73\" ssid=\"39\">If the normalized word is not in a cluster, the tagger considers the fuzzy matches.</S>\n    <S sid=\"74\" ssid=\"40\">Although only about 3% of the tokens in the development set (§6) did not appear in a clustering, this method resulted in a relative error decrease of 18% among such word tokens.</S>\n    <S sid=\"75\" ssid=\"41\">Besides unsupervised word clusters, there are two other sets of features that contain generalized lexical class information.</S>\n    <S sid=\"76\" ssid=\"42\">We use the tag dictionary feature from Gimpel et al., which adds a feature for a word’s most frequent part-of-speech tag.8 This can be viewed as a feature-based domain adaptation method, since it gives lexical type-level information for standard English words, which the model learns to map between PTB tags to the desired output tags.</S>\n    <S sid=\"77\" ssid=\"43\">Second, since the lack of consistent capitalization conventions on Twitter makes it especially difficult to recognize names—Gimpel et al. and Foster et al.</S>\n    <S sid=\"78\" ssid=\"44\">(2011) found relatively low accuracy on proper nouns—we added a token-level name list feature, which fires on (non-function) words from names from several sources: Freebase lists of celebrities and video games (Google, 2012), the Moby Words list of US Locations,9 and lists of male, female, family, and proper names from Mark Kantrowitz’s name corpus.10</S>\n  </SECTION>\n  <SECTION title=\"4 Tokenization and Emoticon Detection\" number=\"4\">\n    <S sid=\"79\" ssid=\"1\">Word segmentation on Twitter is challenging due to the lack of orthographic conventions; in particular, punctuation, emoticons, URLs, and other symbols may have no whitespace separation from textual 8Frequencies came from the Wall Street Journal and Brown corpus sections of the Penn Treebank.</S>\n    <S sid=\"80\" ssid=\"2\">If a word has multiple PTB tags, each tag is a feature with value for the frequency rank; e.g. for three different tags in the PTB, this feature gives a value of 1 for the most frequent tag, 2/3 for the second, etc.</S>\n    <S sid=\"81\" ssid=\"3\">Coarse versions of the PTB tags are used (Petrov et al., 2011).</S>\n    <S sid=\"82\" ssid=\"4\">While 88% of words in the dictionary have only one tag, using rank information seemed to give a small but consistent gain over only using the most common tag, or using binary features conjoined with rank as in Gimpel et al. words (e.g. no:-d,yes should parse as four tokens), and internally may contain alphanumeric symbols that could be mistaken for words: a naive split(/[^azA-Z0-9]+/) tokenizer thinks the words “p” and “d” are among the top 100 most common words on Twitter, due to misanalysis of :p and :d. Traditional Penn Treebank–style tokenizers are hardly better, often breaking a string of punctuation characters into a single token per character.</S>\n    <S sid=\"83\" ssid=\"5\">We rewrote twokenize (O’Connor et al., 2010), a rule-based tokenizer, emoticon, and URL detector, for use in the tagger.</S>\n    <S sid=\"84\" ssid=\"6\">Emoticons are especially challenging, since they are open-class and productive.</S>\n    <S sid=\"85\" ssid=\"7\">We revise O’Connor et al.’s regular expression grammar that describes possible emoticons, adding a grammar of horizontal emoticons (e.g.</S>\n    <S sid=\"86\" ssid=\"8\">-_-), known as “Eastern-style,”11 though we observe high usage in English-speaking Twitter (Fig.</S>\n    <S sid=\"87\" ssid=\"9\">2, G2–G3).</S>\n    <S sid=\"88\" ssid=\"10\">We also add a number of other improvements to the patterns.</S>\n    <S sid=\"89\" ssid=\"11\">Because this system was used as preprocessing for the word clustering experiment in §3, we were able to infer the emoticon clusters in Fig.</S>\n    <S sid=\"90\" ssid=\"12\">2.</S>\n    <S sid=\"91\" ssid=\"13\">Furthermore, whether a token matches the emoticon pattern is also used as a feature in the tagger (§2).</S>\n    <S sid=\"92\" ssid=\"14\">URL recognition is also difficult, since the http:// is often dropped, resulting in protocol-less URLs like about.me.</S>\n    <S sid=\"93\" ssid=\"15\">We add recognition patterns for these by using a list of top-level and country domains.</S>\n  </SECTION>\n  <SECTION title=\"5 Annotated Dataset\" number=\"5\">\n    <S sid=\"94\" ssid=\"1\">Gimpel et al. (2011) provided a dataset of POStagged tweets consisting almost entirely of tweets sampled from one particular day (October 27, 2010).</S>\n    <S sid=\"95\" ssid=\"2\">We were concerned about overfitting to timespecific phenomena; for example, a substantial fraction of the messages are about a basketball game happening that day.</S>\n    <S sid=\"96\" ssid=\"3\">We created a new test set of 547 tweets for evaluation.</S>\n    <S sid=\"97\" ssid=\"4\">The test set consists of one random English tweet from every day between January 1, 2011 and June 30, 2012.</S>\n    <S sid=\"98\" ssid=\"5\">In order for a tweet to be considered English, it had to contain at least one English word other than a URL, emoticon, or at-mention.</S>\n    <S sid=\"99\" ssid=\"6\">We noticed biases in the outputs of langid.py, so we instead selected these messages completely manually (going through a random sample of one day’s messages until an English message was found).</S>\n    <S sid=\"100\" ssid=\"7\">Gimpel et al. provided a tagset for Twitter (shown in Appendix A), which we used unmodified.</S>\n    <S sid=\"101\" ssid=\"8\">The original annotation guidelines were not published, but in this work we recorded the rules governing tagging decisions and made further revisions while annotating the new data.12 Some of our guidelines reiterate or modify rules made by Penn Treebank annotators, while others treat specific phenomena found on Twitter (refer to the next section).</S>\n    <S sid=\"102\" ssid=\"9\">Our tweets were annotated by two annotators who attempted to match the choices made in Gimpel et al.’s dataset.</S>\n    <S sid=\"103\" ssid=\"10\">The annotators also consulted the POS annotations in the Penn Treebank (Marcus et al., 1993) as an additional reference.</S>\n    <S sid=\"104\" ssid=\"11\">Differences were reconciled by a third annotator in discussion with all annotators.13 During this process, an inconsistency was found in Gimpel et al.’s data, which we corrected (concerning the tagging of this/that, a change to 100 labels, 0.4%).</S>\n    <S sid=\"105\" ssid=\"12\">The new version of Gimpel et al.’s data (called OCT27), as well as the newer messages (called DAILY547), are both included in our data release.</S>\n    <S sid=\"106\" ssid=\"13\">Ritter et al. (2011) annotated tweets using an augmented version of the PTB tagset and presumably followed the PTB annotation guidelines.</S>\n    <S sid=\"107\" ssid=\"14\">We wrote new guidelines because the PTB conventions are inappropriate for Twitter in several ways, as shown in the design of Gimpel et al.’s tagset.</S>\n    <S sid=\"108\" ssid=\"15\">Importantly, “compound” tags (e.g., nominal+verbal and nominal+possessive) are used because tokenization is difficult or seemingly impossible for the nonstandard word forms that are commonplace in conversational text.</S>\n    <S sid=\"109\" ssid=\"16\">For example, the PTB tokenization splits contractions containing apostrophes: I’m ==&gt;.</S>\n    <S sid=\"110\" ssid=\"17\">I/PRP ’m/VBP.</S>\n    <S sid=\"111\" ssid=\"18\">But conversational text often contains variants that resist a single PTB tag (like im), or even challenge traditional English grammatical categories (like imma or umma, which both mean “I am going to”).</S>\n    <S sid=\"112\" ssid=\"19\">One strategy would be to analyze these forms into a PTB-style tokenization, as discussed in Forsyth (2007), who proposes to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult.</S>\n    <S sid=\"113\" ssid=\"20\">We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc.</S>\n    <S sid=\"114\" ssid=\"21\">These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig.</S>\n    <S sid=\"115\" ssid=\"22\">2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near.</S>\n    <S sid=\"116\" ssid=\"23\">We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task.</S>\n    <S sid=\"117\" ssid=\"24\">In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011).</S>\n    <S sid=\"118\" ssid=\"25\">For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal?</S>\n    <S sid=\"119\" ssid=\"26\">Instead, we would like to directly analyze the syntax of online conversational text on its own terms.</S>\n    <S sid=\"120\" ssid=\"27\">Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS patterns that seem quite compatible with our approach.</S>\n    <S sid=\"121\" ssid=\"28\">More complex downstream processing like parsing is an interesting challenge, since contraction parsing on traditional text is probably a benefit to current parsers.</S>\n    <S sid=\"122\" ssid=\"29\">We believe that any PTB-trained tool requires substantial retraining and adaptation for Twitter due to the huge genre and stylistic differences (Foster et al., 2011); thus tokenization conventions are a relatively minor concern.</S>\n    <S sid=\"123\" ssid=\"30\">Our simple-toannotate conventions make it easier to produce new training data.</S>\n  </SECTION>\n  <SECTION title=\"6 Experiments\" number=\"6\">\n    <S sid=\"124\" ssid=\"1\">We are primarily concerned with performance on our annotated datasets described in §5 (OCT27, DAILY547), though for comparison to previous work we also test on other corpora (RITTERTW in §6.2, NPSCHAT in §6.3).</S>\n    <S sid=\"125\" ssid=\"2\">The annotated datasets are listed in Table 1.</S>\n    <S sid=\"126\" ssid=\"3\">We use OCT27 to refer to the entire dataset described in Gimpel et al. ; it is split into training, development, and test portions (OCT27TRAIN, OCT27DEV, OCT27TEST).</S>\n    <S sid=\"127\" ssid=\"4\">We use DAILY547 as an additional test set.</S>\n    <S sid=\"128\" ssid=\"5\">Neither OCT27TEST nor DAILY547 were extensively evaluated against until final ablation testing when writing this paper.</S>\n    <S sid=\"129\" ssid=\"6\">The total number of features is 3.7 million, all of which are used under pure L2 regularization; but only 60,000 are selected by elastic net regularization with (A1, A2) = (0.25, 2), which achieves nearly the same (but no better) accuracy as pure L2,16 and we use it for all experiments.</S>\n    <S sid=\"130\" ssid=\"7\">We observed that it was 16We conducted a grid search for the regularizer values on part of DAILY547, and many regularizer values give the best or nearly the best results.</S>\n    <S sid=\"131\" ssid=\"8\">We suspect a different setup would have yielded similar results. possible to get radically smaller models with only a slight degradation in performance: (4, 0.06) has 0.5% worse accuracy but uses only 1,632 features, a small enough number to browse through manually.</S>\n    <S sid=\"132\" ssid=\"9\">First, we evaluate on the new test set, training on all of OCT27.</S>\n    <S sid=\"133\" ssid=\"10\">Due to DAILY547’s statistical representativeness, we believe this gives the best view of the tagger’s accuracy on English Twitter text.</S>\n    <S sid=\"134\" ssid=\"11\">The full tagger attains 93.2% accuracy (final row of Table 2).</S>\n    <S sid=\"135\" ssid=\"12\">To facilitate comparisons with previous work, we ran a series of experiments training only on OCT27’s training and development sets, then report test results on both OCT27TEST and all of DAILY547, shown in Table 2.</S>\n    <S sid=\"136\" ssid=\"13\">Our tagger achieves substantially higher accuracy than Gimpel et al. (2011).17 Feature ablation.</S>\n    <S sid=\"137\" ssid=\"14\">A number of ablation tests indicate the word clusters are a very strong source of lexical knowledge.</S>\n    <S sid=\"138\" ssid=\"15\">When dropping the tag dictionaries and name lists, the word clusters maintain most of the accuracy (row 2).</S>\n    <S sid=\"139\" ssid=\"16\">If we drop the clusters and rely only on tag dictionaries and namelists, accuracy decreases significantly (row 3).</S>\n    <S sid=\"140\" ssid=\"17\">In fact, we can remove all observation features except for word clusters—no word features, orthographic fea17These numbers differ slightly from those reported by Gimpel et al., due to the corrections we made to the OCT27 data, noted in Section 5.1.</S>\n    <S sid=\"141\" ssid=\"18\">We retrained and evaluated their tagger (version 0.2) on our corrected dataset. tures, affix n-grams, capitalization, emoticon patterns, etc.—and the accuracy is in fact still better than the previous work (row 4).18 We also wanted to know whether to keep the tag dictionary and name list features, but the splits reported in Fig.</S>\n    <S sid=\"142\" ssid=\"19\">2 did not show statistically significant differences; so to better discriminate between ablations, we created a lopsided train/test split of all data with a much larger test portion (26,974 tokens), having greater statistical power (tighter confidence intervals of f 0.3%).19 The full system got 90.8% while the no–tag dictionary, no-namelists ablation had 90.0%, a statistically significant difference.</S>\n    <S sid=\"143\" ssid=\"20\">Therefore we retain these features.</S>\n    <S sid=\"144\" ssid=\"21\">Compared to the tagger in Gimpel et al., most of our feature changes are in the new lexical features described in §3.5.20 We do not reuse the other lexical features from the previous work, including a phonetic normalizer (Metaphone), a name list consisting of words that are frequently capitalized, and distributional features trained on a much smaller unlabeled corpus; they are all worse than our new lexical features described here.</S>\n    <S sid=\"145\" ssid=\"22\">(We did include, however, a variant of the tag dictionary feature that uses phonetic normalization for lookup; it seemed to yield a small improvement.)</S>\n    <S sid=\"146\" ssid=\"23\">18Furthermore, when evaluating the clusters as unsupervised (hard) POS tags, we obtain a many-to-one accuracy of 89.2% on DAILY547.</S>\n    <S sid=\"147\" ssid=\"24\">Before computing this, we lowercased the text to match the clusters and removed tokens tagged as URLs and at-mentions.</S>\n    <S sid=\"148\" ssid=\"25\">19Reported confidence intervals in this paper are 95% binomial normal approximation intervals for the proportion of correctly tagged tokens: f1.96.,/p(1 − p)/ntokm &lt; 1/\\/n.</S>\n    <S sid=\"149\" ssid=\"26\">20Details on the exact feature set are available in a technical report (Owoputi et al., 2012), also available on the website.</S>\n    <S sid=\"150\" ssid=\"27\">Non-traditional words.</S>\n    <S sid=\"151\" ssid=\"28\">The word clusters are especially helpful with words that do not appear in traditional dictionaries.</S>\n    <S sid=\"152\" ssid=\"29\">We constructed a dictionary by lowercasing the union of the ispell ‘American’, ‘British’, and ‘English’ dictionaries, plus the standard Unix words file from Webster’s Second International dictionary, totalling 260,985 word types.</S>\n    <S sid=\"153\" ssid=\"30\">After excluding tokens defined by the gold standard as punctuation, URLs, at-mentions, or emoticons,21 22% of DAILY547’s tokens do not appear in this dictionary.</S>\n    <S sid=\"154\" ssid=\"31\">Without clusters, they are very difficult to classify (only 79.2% accuracy), but adding clusters generates a 5.7 point improvement—much larger than the effect on in-dictionary tokens (Table 3).</S>\n    <S sid=\"155\" ssid=\"32\">Varying the amount of unlabeled data.</S>\n    <S sid=\"156\" ssid=\"33\">A tagger that only uses word clusters achieves an accuracy of 88.6% on the OCT27 development set.22 We created several clusterings with different numbers of unlabeled tweets, keeping the number of clusters constant at 800.</S>\n    <S sid=\"157\" ssid=\"34\">As shown in Fig.</S>\n    <S sid=\"158\" ssid=\"35\">3, there was initially a logarithmic relationship between number of tweets and accuracy, but accuracy (and lexical coverage) levels out after 750,000 tweets.</S>\n    <S sid=\"159\" ssid=\"36\">We use the largest clustering (56 million tweets and 1,000 clusters) as the default for the released tagger. tagset plus several Twitter-specific tags, referred to in Table 1 as RITTERTW.</S>\n    <S sid=\"160\" ssid=\"37\">Linguistic concerns notwithstanding (§5.2), for a controlled comparison, we train and test our system on this data with the same 4-fold cross-validation setup they used, attaining 90.0% (f0.5%) accuracy.</S>\n    <S sid=\"161\" ssid=\"38\">Ritter et al.’s CRFbased tagger had 85.3% accuracy, and their best tagger, trained on a concatenation of PTB, IRC, and Twitter, achieved 88.3% (Table 4).</S>\n    <S sid=\"162\" ssid=\"39\">First, we compare to a tagger in the same setup as experiments on this data in Forsyth (2007), training on 90% of the data and testing on 10%; we average results across 10-fold cross-validation.25 The full tagger model achieved 93.4% (f0.3%) accuracy, significantly improving over the best result they report, 90.8% accuracy with a tagger trained on a mix of several POS-annotated corpora.</S>\n    <S sid=\"163\" ssid=\"40\">We also perform the ablation experiments on this corpus, with a slightly different experimental setup: we first filter out system messages then split data 24Release 1.0: http://faculty.nps.edu/ cmartell/NPSChat.htm 25Forsyth actually used 30 different 90/10 random splits; we prefer cross-validation because the same test data is never repeated, thus allowing straightforward confidence estimation of accuracy from the number of tokens (via binomial sample variance, footnote 19).</S>\n    <S sid=\"164\" ssid=\"41\">In all cases, the models are trained on the same amount of data (90%). into 5,067 training and 2,868 test messages.</S>\n    <S sid=\"165\" ssid=\"42\">Results show a similar pattern as the Twitter data (see final column of Table 2).</S>\n    <S sid=\"166\" ssid=\"43\">Thus the Twitter word clusters are also useful for language in the medium of text chat rooms; we suspect these clusters will be applicable for deeper syntactic and semantic analysis in other online conversational text mediums, such as text messages and instant messages.</S>\n  </SECTION>\n  <SECTION title=\"7 Conclusion\" number=\"7\">\n    <S sid=\"167\" ssid=\"1\">We have constructed a state-of-the-art part-ofspeech tagger for the online conversational text genres of Twitter and IRC, and have publicly released our new evaluation data, annotation guidelines, open-source tagger, and word clusters at http://www.ark.cs.cmu.edu/TweetNLP.</S>\n  </SECTION>\n</PAPER>",
                    "generation_prompt_uid": "80daef7807e8a24ff04898fd1209e936",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters"
                }
            }
        },
        {
            "index": 24,
            "choice": "human",
            "isFirst": false,
            "description": {
                "human": {
                    "abstract": "Recognizing analogies, synonyms, anto nyms, and associations appear to be fourdistinct tasks, requiring distinct NLP al gorithms. In the past, the four tasks have been treated independently, using a widevariety of algorithms. These four seman tic classes, however, are a tiny sample of the full range of semantic phenomena, andwe cannot afford to create ad hoc algo rithms for each semantic phenomenon; weneed to seek a unified approach. We propose to subsume a broad range of phenom ena under analogies. To limit the scope of this paper, we restrict our attention to the subsumption of synonyms, antonyms, and associations. We introduce a supervised corpus-based machine learning algorithm for classifying analogous word pairs, and we show that it can solve multiple-choice SAT analogy questions, TOEFL synonymquestions, ESL synonym-antonym questions, and similar-associated-both ques tions from cognitive psychology.",
                    "filename": "A_Uniform_Approach_to_Analogies_Synonyms_Antonyms_and_Associations",
                    "item_type": "paper",
                    "origin": "Human",
                    "source_xml": "data/raw/scisummnet_release1.1__20190413/top1000_complete/C08-1114/Documents_xml/C08-1114.xml",
                    "title": "A Uniform Approach to Analogies Synonyms Antonyms and Associations"
                },
                "llm": {
                    "descriptions": [
                        "**Abstract:**\n\nThis paper introduces a novel approach to processing four distinct semantic classes in natural language processing (NLP): analogies, synonyms, antonyms, and associations. Traditional algorithms typically handle these tasks separately, but the authors propose a uniform method, subsuming the latter three categories under the broader concept of analogies. They present an algorithm, PairClass, that utilizes a supervised machine learning framework with support vector machines (SVM) and feature vectors based on pattern frequencies within a large corpus. The algorithm is tested on multiple-choice analogy questions from the SAT, synonym questions from the TOEFL, ESL practice questions, and a set of word pairs used in cognitive psychology. The results show that the PairClass algorithm performs competitively with specialized systems, demonstrating the feasibility of a generalized approach to semantic tasks without the need for task-specific tuning. This research moves towards a unified view of semantics in NLP, aligning with the belief that understanding is rooted in analogical reasoning.",
                        "Abstract:\n\nThis paper presents a uniform approach to handling analogies, synonyms, antonyms, and associations in natural language processing (NLP) by treating them all as instances of analogical reasoning. The authors propose a method where semantic relationships within word pairs are classified using a standard supervised machine learning framework. This approach is novel in its application of a common algorithmic structure to address these four distinct types of semantic tasks, which have traditionally been tackled separately. The paper describes an algorithm, PairClass, which utilizes support vector machines (SVM) and feature vectors derived from pattern frequencies in a large corpus to classify word pairs. The authors' experiments demonstrate that their approach achieves competitive performance across multiple tasks, including SAT analogies, TOEFL synonyms, distinguishing synonyms from antonyms in ESL practice questions, and classifying word pairs as similar, associated, or both, based on cognitive psychology experiments. Despite the need for a large corpus and the associated computational costs, the algorithm shows promise for broader application in NLP tasks involving semantic relations. The paper suggests future improvements and applications, highlighting the potential of analogical algorithms in unifying the field of semantics.",
                        "Abstract:\n\nThis paper presents a novel and uniform approach to processing linguistic phenomena such as analogies, synonyms, antonyms, and associations, which traditionally have been treated as distinct semantic classes with separate Natural Language Processing (NLP) algorithms. The authors suggest that these classes can be subsumed by analogies and propose a standard supervised machine learning algorithm that tackles all four tasks without requiring task-specific tuning. The approach uses a large corpus to generate feature vectors based on the frequencies of automatically defined patterns and employs a Support Vector Machine (SVM) to classify these vectors. The algorithm, referred to as PairClass, was evaluated on multiple-choice questions from the SAT college entrance test, the TOEFL test, ESL practice questions, and a set of word pairs labeled for cognitive psychology experiments. The results indicate that PairClass performs competitively with specialized systems, despite being a general-purpose algorithm. The paper also discusses the implications of applying supervised learning to problems that appear to require unsupervised learning, by utilizing a sui generis approach. The main limitation of PairClass is its dependency on a large corpus, but future improvements in hardware and algorithmic efficiency could mitigate this issue. The paper concludes with the potential of analogy-based semantics to unify the field and the possibility of extending PairClass to other semantic relations and NLP tasks.",
                        "Abstract:\n\nThis academic paper introduces PairClass, a novel algorithm designed to uniformly approach the classification of semantic relations in natural language processing, encompassing analogies, synonyms, antonyms, and associations. Traditional methods have treated these semantic classes as distinct, necessitating separate algorithms for each. However, the authors argue that these can be viewed under the umbrella of analogies, simplifying the approach to semantic relation classification. The paper presents PairClass as a corpus-based method, using supervised machine learning to classify word pairs based on frequencies of patterns in a large corpus, without reliance on lexical resources like WordNet.\n\nThe algorithm was evaluated through experiments on multiple-choice analogy questions from the SAT, synonym questions from the TOEFL, ESL questions distinguishing synonyms and antonyms, and a set of word pairs labeled as similar, associated, or both from cognitive psychology experiments. Using a support vector machine with feature selection, PairClass demonstrates competitive performance across these tasks, showing that a single algorithm can effectively handle a range of semantic phenomena.\n\nThe authors highlight the advantages of using supervised learning for semantic tasks, allowing for the exploitation of a vast body of research in the field. They also discuss the potential application of PairClass to other semantic relations and tasks, such as word sense disambiguation and information retrieval. The main limitation of the approach is the need for a large corpus, but this is expected to become less of a concern as computational resources improve.\n\nIn conclusion, the paper posits that most lexical knowledge is relational and that lower-level analogies, such as those addressed by PairClass, can contribute to the understanding of higher-level analogies in the future. The authors suggest that analogy provides a unifying framework for semantics research, with the potential to streamline and simplify the processing of semantic relations in natural language."
                    ],
                    "filename": "a_uniform_approach_to_analogies_-write_xml_paper_abstract",
                    "generation_prompt_nickname": "write_xml_paper_abstract",
                    "generation_prompt_text": "Read the following academic paper (in XML format) and create an abstract for it.\n\n---\n\n**Description:**\n\n<PAPER>\n\t<S sid=\"0\">A Uniform Approach to Analogies Synonyms Antonyms and Associations</S><SECTION title=\"Introduction\" number=\"1\">\n\t\t\t<S sid=\"7\" ssid=\"7\">A pair of words (petrify:stone) is analogous to another pair (vaporize:gas) when the semantic re lations between the words in the first pair are highly similar to the relations in the second pair.</S>\n\t\t\t<S sid=\"8\" ssid=\"8\">Two words (levied and imposed) are synonymousin a context (levied a tax) when they can be interchanged (imposed a tax), they are are antony mous when they have opposite meanings (black c ? 2008, National Research Council of Canada (NRC).Licensed to the Coling 2008 Organizing Committee for pub lication in Coling 2008 and for re-publishing in any form or medium.</S>\n\t\t\t<S sid=\"9\" ssid=\"9\">and white), and they are associated when they tend to co-occur (doctor and hospital).On the surface, it appears that these are four distinct semantic classes, requiring distinct NLP al gorithms, but we propose a uniform approach to all four.</S>\n\t\t\t<S sid=\"10\" ssid=\"10\">We subsume synonyms, antonyms, and associations under analogies.</S>\n\t\t\t<S sid=\"11\" ssid=\"11\">In essence, we say that X and Y are antonyms when the pair X:Y is analogous to the pair black:white, X and Y are synonyms when they are analogous to the pair levied:imposed, and X and Y are associated when they are analogous to the pair doctor:hospital.</S>\n\t\t\t<S sid=\"12\" ssid=\"12\">There is past work on recognizing analogies(Reitman, 1965), synonyms (Landauer and Dumais, 1997), antonyms (Lin et al, 2003), and asso ciations (Lesk, 1969), but each of these four tasks has been examined separately, in isolation from the others.</S>\n\t\t\t<S sid=\"13\" ssid=\"13\">As far as we know, the algorithm proposed here is the first attempt to deal with all four tasks using a uniform approach.</S>\n\t\t\t<S sid=\"14\" ssid=\"14\">We believe that it isimportant to seek NLP algorithms that can han dle a broad range of semantic phenomena, becausedeveloping a specialized algorithm for each phe nomenon is a very inefficient research strategy.It might seem that a lexicon, such as Word Net (Fellbaum, 1998), contains all the information we need to handle these four tasks.</S>\n\t\t\t<S sid=\"15\" ssid=\"15\">However, weprefer to take a corpus-based approach to seman tics.</S>\n\t\t\t<S sid=\"16\" ssid=\"16\">Veale (2004) used WordNet to answer 374 multiple-choice SAT analogy questions, achievingan accuracy of 43%, but the best corpus-based ap proach attains an accuracy of 56% (Turney, 2006).</S>\n\t\t\t<S sid=\"17\" ssid=\"17\">Another reason to prefer a corpus-based approachto a lexicon-based approach is that the former re quires less human labour, and thus it is easier to extend to other languages.In Section 2, we describe our algorithm for rec ognizing analogies.</S>\n\t\t\t<S sid=\"18\" ssid=\"18\">We use a standard supervised 905 machine learning approach, with feature vectorsbased on the frequencies of patterns in a large cor pus.</S>\n\t\t\t<S sid=\"19\" ssid=\"19\">We use a support vector machine (SVM) to learn how to classify the feature vectors (Platt, 1998; Witten and Frank, 1999).</S>\n\t\t\t<S sid=\"20\" ssid=\"20\">Section 3 presents four sets of experiments.</S>\n\t\t\t<S sid=\"21\" ssid=\"21\">We apply our algorithm for recognizing analogies to multiple-choice analogy questions from the SAT college entrance test, multiple-choice synonym questions from the TOEFL (test of English as aforeign language), ESL (English as a second language) practice questions for distinguishing syn onyms and antonyms, and a set of word pairs thatare labeled similar, associated, and both, devel oped for experiments in cognitive psychology.We discuss the results of the experiments in Section 4.</S>\n\t\t\t<S sid=\"22\" ssid=\"22\">The accuracy of the algorithm is competitive with other systems, but the strength of the al gorithm is that it is able to handle all four tasks, with no tuning of the learning parameters to the particular task.</S>\n\t\t\t<S sid=\"23\" ssid=\"23\">It performs well, although it iscompeting against specialized algorithms, devel oped for single tasks.Related work is examined in Section 5 and limitations and future work are considered in Sec tion 6.</S>\n\t\t\t<S sid=\"24\" ssid=\"24\">We conclude in Section 7.</S>\n\t</SECTION>\n\t<SECTION title=\"Classifying Analogous Word Pairs. \" number=\"2\">\n\t\t\t<S sid=\"25\" ssid=\"1\">An analogy, A:B::C:D, asserts that A is to B as C is to D; for example, traffic:street::water:riverbed asserts that traffic is to street as water is to riverbed; that is, the semantic relations between traffic and street are highly similar to the semantic relations between water and riverbed.</S>\n\t\t\t<S sid=\"26\" ssid=\"2\">We may view the task of recognizing word analogies as a problem of classifying word pairs (see Table 1).</S>\n\t\t\t<S sid=\"27\" ssid=\"3\">Word pair Class label carpenter:wood artisan:material mason:stone artisan:material potter:clay artisan:material glassblower:glass artisan:material traffic:street entity:carrier water:riverbed entity:carrier packets:network entity:carrier gossip:grapevine entity:carrierTable 1: Examples of how the task of recogniz ing word analogies may be viewed as a problem of classifying word pairs.</S>\n\t\t\t<S sid=\"28\" ssid=\"4\">We approach this as a standard classificationproblem for supervised machine learning.</S>\n\t\t\t<S sid=\"29\" ssid=\"5\">The al gorithm takes as input a training set of word pairs with class labels and a testing set of word pairs without labels.</S>\n\t\t\t<S sid=\"30\" ssid=\"6\">Each word pair is represented as a vector in a feature space and a supervised learning algorithm is used to classify the feature vectors.</S>\n\t\t\t<S sid=\"31\" ssid=\"7\">The elements in the feature vectors are based on the frequencies of automatically defined patterns in a large corpus.</S>\n\t\t\t<S sid=\"32\" ssid=\"8\">The output of the algorithm is anassignment of labels to the word pairs in the test ing set.</S>\n\t\t\t<S sid=\"33\" ssid=\"9\">For some of the experiments, we selecta unique label for each word pair; for other ex periments, we assign probabilities to each possible label for each word pair.</S>\n\t\t\t<S sid=\"34\" ssid=\"10\">For a given word pair, such as mason:stone, the first step is to generate morphological variations,such as masons:stones.</S>\n\t\t\t<S sid=\"35\" ssid=\"11\">In the following experi ments, we use morpha (morphological analyzer)and morphg (morphological generator) for mor phological processing (Minnen et al, 2001).1 The second step is to search in a large corpus for all phrases of the following form: ?[0 to 1 words] X [0 to 3 words] Y [0 to 1 words]?</S>\n\t\t\t<S sid=\"36\" ssid=\"12\">In this template, X:Y consists of morphologicalvariations of the given word pair, in either order; for example, mason:stone, stone:mason, masons:stones, and so on.</S>\n\t\t\t<S sid=\"37\" ssid=\"13\">A typical phrase for ma son:stone would be ?the mason cut the stone with?.</S>\n\t\t\t<S sid=\"38\" ssid=\"14\">We then normalize all of the phrases that are found, by using morpha to remove suffixes.</S>\n\t\t\t<S sid=\"39\" ssid=\"15\">The template we use here is similar to Turney (2006), but we have added extra context wordsbefore the X and after the Y . Our morpholog ical processing also differs from Turney (2006).In the following experiments, we search in a cor pus of 5 ? 1010 words (about 280 GB of plain text), consisting of web pages gathered by a web crawler.2 To retrieve phrases from the corpus, weuse Wumpus (Bu?ttcher and Clarke, 2005), an effi cient search engine for passage retrieval from large corpora.3 The next step is to generate patterns from allof the phrases that were found for all of the in put word pairs (from both the training and testingsets).</S>\n\t\t\t<S sid=\"40\" ssid=\"16\">To generate patterns from a phrase, we re place the given word pairs with variables, X and Y , and we replace the remaining words with a wild card symbol (an asterisk) or leave them as they are.</S>\n\t\t\t<S sid=\"41\" ssid=\"17\">1http://www.informatics.susx.ac.uk/research/groups/nlp/ carroll/morph.html.</S>\n\t\t\t<S sid=\"42\" ssid=\"18\">2The corpus was collected by Charles Clarke, University of Waterloo.</S>\n\t\t\t<S sid=\"43\" ssid=\"19\">We can provide copies on request.</S>\n\t\t\t<S sid=\"44\" ssid=\"20\">3http://www.wumpus-search.org/.</S>\n\t\t\t<S sid=\"45\" ssid=\"21\">906 For example, the phrase ?the mason cut the stone with?</S>\n\t\t\t<S sid=\"46\" ssid=\"22\">yields the patterns ?the X cut * Y with?, ?* X * the Y *?, and so on.</S>\n\t\t\t<S sid=\"47\" ssid=\"23\">If a phrase contains n words, then it yields 2(n?2) patterns.Each pattern corresponds to a feature in the feature vectors that we will generate.</S>\n\t\t\t<S sid=\"48\" ssid=\"24\">Since a typical input set of word pairs yields millions of pat terns, we need to use feature selection, to reduce the number of patterns to a manageable quantity.</S>\n\t\t\t<S sid=\"49\" ssid=\"25\">For each pattern, we count the number of input word pairs that generated the pattern.</S>\n\t\t\t<S sid=\"50\" ssid=\"26\">For example, ?* X cut * Y *?</S>\n\t\t\t<S sid=\"51\" ssid=\"27\">is generated by both mason:stone and carpenter:wood.</S>\n\t\t\t<S sid=\"52\" ssid=\"28\">We then sort the patterns in descending order of the number of word pairs that generated them.</S>\n\t\t\t<S sid=\"53\" ssid=\"29\">If there are N input word pairs (and thus N feature vectors, including both the training and testing sets), then we select the topkN patterns and drop the remainder.</S>\n\t\t\t<S sid=\"54\" ssid=\"30\">In the fol lowing experiments, k is set to 20.</S>\n\t\t\t<S sid=\"55\" ssid=\"31\">The algorithm is not sensitive to the precise value of k.The reasoning behind the feature selection al gorithm is that shared patterns make more useful features than rare patterns.</S>\n\t\t\t<S sid=\"56\" ssid=\"32\">The number of features (kN ) depends on the number of word pairs (N ), because, if we have more feature vectors, then we need more features to distinguish them.</S>\n\t\t\t<S sid=\"57\" ssid=\"33\">Turney (2006) also selects patterns based on the numberof pairs that generate them, but the number of se lected patterns is a constant (8000), independent of the number of input word pairs.</S>\n\t\t\t<S sid=\"58\" ssid=\"34\">The next step is to generate feature vectors, one vector for each input word pair.</S>\n\t\t\t<S sid=\"59\" ssid=\"35\">Each of the N feature vectors has kN elements, one element for each selected pattern.</S>\n\t\t\t<S sid=\"60\" ssid=\"36\">The value of an element in a vector is given by the logarithm of the frequency in the corpus of the corresponding pattern for the given word pair.</S>\n\t\t\t<S sid=\"61\" ssid=\"37\">For example, suppose the given pair is mason:stone and the pattern is ?* X cut * Y *?.</S>\n\t\t\t<S sid=\"62\" ssid=\"38\">We look at the normalized phrases that we collected for mason:stone and we count how many match this pattern.</S>\n\t\t\t<S sid=\"63\" ssid=\"39\">If f phrases match thepattern, then the value of this element in the fea ture vector is log(f +1) (we add 1 because log(0)is undefined).</S>\n\t\t\t<S sid=\"64\" ssid=\"40\">Each feature vector is then normal ized to unit length.</S>\n\t\t\t<S sid=\"65\" ssid=\"41\">The normalization ensures that features in vectors for high-frequency word pairs (traffic:street) are comparable to features in vectors for low-frequency word pairs (water:riverbed).Now that we have a feature vector for each in put word pair, we can apply a standard supervised learning algorithm.</S>\n\t\t\t<S sid=\"66\" ssid=\"42\">In the following experiments, we use a sequential minimal optimization (SMO)support vector machine (SVM) with a radial basis function (RBF) kernel (Platt, 1998), as implemented in Weka (Waikato Environment for Knowl edge Analysis) (Witten and Frank, 1999).4 The algorithm generates probability estimates for each class by fitting logistic regression models to the outputs of the SVM.</S>\n\t\t\t<S sid=\"67\" ssid=\"43\">We disable the normalizationoption in Weka, since the vectors are already nor malized to unit length.</S>\n\t\t\t<S sid=\"68\" ssid=\"44\">We chose the SMO RBF algorithm because it is fast, robust, and it easily handles large numbers of features.For convenience, we will refer to the above algo rithm as PairClass.</S>\n\t\t\t<S sid=\"69\" ssid=\"45\">In the following experiments, PairClass is applied to each of the four problemswith no adjustments or tuning to the specific prob lems.</S>\n\t\t\t<S sid=\"70\" ssid=\"46\">Some work is required to fit each probleminto the general framework of PairClass (supervised classification of word pairs) but the core al gorithm is the same in each case.</S>\n\t</SECTION>\n\t<SECTION title=\"Experiments. \" number=\"3\">\n\t\t\t<S sid=\"71\" ssid=\"1\">This section presents four sets of experiments, with analogies, synonyms, antonyms, and associations.</S>\n\t\t\t<S sid=\"72\" ssid=\"2\">We explain how each task is treated as a problem of classifying analogous word pairs, we give the experimental results, and we discuss past work on each of the four tasks.</S>\n\t\t\t<S sid=\"73\" ssid=\"3\">3.1 SAT Analogies.</S>\n\t\t\t<S sid=\"74\" ssid=\"4\">In this section, we apply PairClass to the taskof recognizing analogies.</S>\n\t\t\t<S sid=\"75\" ssid=\"5\">To evaluate the performance, we use a set of 374 multiple-choice ques tions from the SAT college entrance exam.</S>\n\t\t\t<S sid=\"76\" ssid=\"6\">Table 2 shows a typical question.</S>\n\t\t\t<S sid=\"77\" ssid=\"7\">The target pair is called the stem.</S>\n\t\t\t<S sid=\"78\" ssid=\"8\">The task is to select the choice pair that is most analogous to the stem pair.</S>\n\t\t\t<S sid=\"79\" ssid=\"9\">Stem: mason:stone Choices: (a) teacher:chalk (b) carpenter:wood (c) soldier:gun (d) photograph:camera (e) book:word Solution: (b) carpenter:wood Table 2: An example of a question from the 374 SAT analogy questions.</S>\n\t\t\t<S sid=\"80\" ssid=\"10\">The problem of recognizing word analogies wasfirst attempted with a system called Argus (Reit 4http://www.cs.waikato.ac.nz/ml/weka/.</S>\n\t\t\t<S sid=\"81\" ssid=\"11\">907man, 1965), using a small hand-built semantic network with a spreading activation algorithm.</S>\n\t\t\t<S sid=\"82\" ssid=\"12\">Turney et al (2003) used a combination of 13 independent modules.</S>\n\t\t\t<S sid=\"83\" ssid=\"13\">Veale (2004) used a spread ing activation algorithm with WordNet (in effect, treating WordNet as a semantic network).</S>\n\t\t\t<S sid=\"84\" ssid=\"14\">Turney (2006) used a corpus-based algorithm.We may view Table 2 as a binary classification problem, in which mason:stone and carpen ter:wood are positive examples and the remaining word pairs are negative examples.</S>\n\t\t\t<S sid=\"85\" ssid=\"15\">The difficulty is that the labels of the choice pairs must be hidden from the learning algorithm.</S>\n\t\t\t<S sid=\"86\" ssid=\"16\">That is, the training set consists of one positive example (the stem pair)and the testing set consists of five unlabeled exam ples (the five choice pairs).</S>\n\t\t\t<S sid=\"87\" ssid=\"17\">To make this task more tractable, we randomly choose a stem pair from one of the 373 other SAT analogy questions, andwe assume that this new stem pair is a negative ex ample, as shown in Table 3.</S>\n\t\t\t<S sid=\"88\" ssid=\"18\">Word pair Train or test Class label mason:stone train positive tutor:pupil train negative teacher:chalk test hidden carpenter:wood test hidden soldier:gun test hidden photograph:camera test hidden book:word test hidden Table 3: How to fit a SAT analogy question into the framework of supervised pair classification.</S>\n\t\t\t<S sid=\"89\" ssid=\"19\">To answer the SAT question, we use PairClass to estimate the probability that each testing example is positive, and we guess the testing example with the highest probability.</S>\n\t\t\t<S sid=\"90\" ssid=\"20\">Learning from a trainingset with only one positive example and one nega tive example is difficult, since the learned model can be highly unstable.</S>\n\t\t\t<S sid=\"91\" ssid=\"21\">To increase the stability, we repeat the learning process 10 times, using adifferent randomly chosen negative training exam ple each time.</S>\n\t\t\t<S sid=\"92\" ssid=\"22\">For each testing word pair, the 10 probability estimates are averaged together.</S>\n\t\t\t<S sid=\"93\" ssid=\"23\">This is a form of bagging (Breiman, 1996).</S>\n\t\t\t<S sid=\"94\" ssid=\"24\">PairClass attains an accuracy of 52.1%.</S>\n\t\t\t<S sid=\"95\" ssid=\"25\">Forcomparison, the ACL Wiki lists 12 previously published results with the 374 SAT analogy ques tions.5 Only 2 of the 12 algorithms have higher accuracy.</S>\n\t\t\t<S sid=\"96\" ssid=\"26\">The best previous result is an accuracy of 56.1% (Turney, 2006).</S>\n\t\t\t<S sid=\"97\" ssid=\"27\">Random guessing would 5For more information, see SAT Analogy Questions (State of the art) at http://aclweb.org/aclwiki/.</S>\n\t\t\t<S sid=\"98\" ssid=\"28\">yield an accuracy of 20%.</S>\n\t\t\t<S sid=\"99\" ssid=\"29\">The average senior high school student achieves 57% correct (Turney, 2006).</S>\n\t\t\t<S sid=\"100\" ssid=\"30\">3.2 TOEFL Synonyms.</S>\n\t\t\t<S sid=\"101\" ssid=\"31\">Now we apply PairClass to the task of recogniz ing synonyms, using a set of 80 multiple-choicesynonym questions from the TOEFL (test of En glish as a foreign language).</S>\n\t\t\t<S sid=\"102\" ssid=\"32\">A sample question is shown in Table 4.</S>\n\t\t\t<S sid=\"103\" ssid=\"33\">The task is to select the choice word that is most similar in meaning to the stem word.</S>\n\t\t\t<S sid=\"104\" ssid=\"34\">Stem: levied Choices: (a) imposed (b) believed (c) requested (d) correlated Solution: (a) imposed Table 4: An example of a question from the 80 TOEFL questions.</S>\n\t\t\t<S sid=\"105\" ssid=\"35\">Synonymy can be viewed as a high degree of semantic similarity.</S>\n\t\t\t<S sid=\"106\" ssid=\"36\">The most common way tomeasure semantic similarity is to measure the dis tance between words in WordNet (Resnik, 1995; Jiang and Conrath, 1997; Hirst and St-Onge, 1998).</S>\n\t\t\t<S sid=\"107\" ssid=\"37\">Corpus-based measures of word similarityare also common (Lesk, 1969; Landauer and Du mais, 1997; Turney, 2001).We may view Table 4 as a binary classifica tion problem, in which the pair levied:imposed is a positive example of the class synonymous and the other possible pairings are negative examples, as shown in Table 5.</S>\n\t\t\t<S sid=\"108\" ssid=\"38\">Word pair Class label levied:imposed positive levied:believed negative levied:requested negative levied:correlated negative Table 5: How to fit a TOEFL question into the framework of supervised pair classification.</S>\n\t\t\t<S sid=\"109\" ssid=\"39\">The 80 TOEFL questions yield 320 (80 ? 4) word pairs, 80 labeled positive and 240 labelednegative.</S>\n\t\t\t<S sid=\"110\" ssid=\"40\">We apply PairClass to the word pairs us ing ten-fold cross-validation.</S>\n\t\t\t<S sid=\"111\" ssid=\"41\">In each random fold, 90% of the pairs are used for training and 10% are used for testing.</S>\n\t\t\t<S sid=\"112\" ssid=\"42\">For each fold, the model that is learned from the training set is used to assign probabilities to the pairs in the testing set.</S>\n\t\t\t<S sid=\"113\" ssid=\"43\">With 908ten separate folds, the ten non-overlapping test ing sets cover the whole dataset.</S>\n\t\t\t<S sid=\"114\" ssid=\"44\">Our guess foreach TOEFL question is the choice with the high est probability of being positive, when paired with the corresponding stem.</S>\n\t\t\t<S sid=\"115\" ssid=\"45\">PairClass attains an accuracy of 76.2%.</S>\n\t\t\t<S sid=\"116\" ssid=\"46\">Forcomparison, the ACL Wiki lists 15 previously published results with the 80 TOEFL synonym questions.6 Of the 15 algorithms, 8 have higher accu racy and 7 have lower.</S>\n\t\t\t<S sid=\"117\" ssid=\"47\">The best previous resultis an accuracy of 97.5% (Turney et al, 2003), ob tained using a hybrid of four different algorithms.</S>\n\t\t\t<S sid=\"118\" ssid=\"48\">Random guessing would yield an accuracy of 25%.</S>\n\t\t\t<S sid=\"119\" ssid=\"49\">The average foreign applicant to a US university achieves 64.5% correct (Landauer and Dumais, 1997).</S>\n\t\t\t<S sid=\"120\" ssid=\"50\">3.3 Synonyms and Antonyms.</S>\n\t\t\t<S sid=\"121\" ssid=\"51\">The task of classifying word pairs as either syn onyms or antonyms readily fits into the framework of supervised classification of word pairs.</S>\n\t\t\t<S sid=\"122\" ssid=\"52\">Table 6shows some examples from a set of 136 ESL (En glish as a second language) practice questions that we collected from various ESL websites.</S>\n\t\t\t<S sid=\"123\" ssid=\"53\">Word pair Class label galling:irksome synonyms yield:bend synonyms naive:callow synonyms advise:suggest synonyms dissimilarity:resemblance antonyms commend:denounce antonyms expose:camouflage antonyms unveil:veil antonyms Table 6: Examples of synonyms and antonyms from 136 ESL practice questions.</S>\n\t\t\t<S sid=\"124\" ssid=\"54\">Lin et al (2003) distinguish synonyms from antonyms using two patterns, ?from X to Y ? and ?either X or Y ?.</S>\n\t\t\t<S sid=\"125\" ssid=\"55\">When X and Y are antonyms, they occasionally appear in a large corpus in oneof these two patterns, but it is very rare for syn onyms to appear in these patterns.</S>\n\t\t\t<S sid=\"126\" ssid=\"56\">Our approach is similar to Lin et al (2003), but we do not rely on hand-coded patterns; instead, PairClass patterns are generated automatically.Using ten-fold cross-validation, PairClass at tains an accuracy of 75.0%.</S>\n\t\t\t<S sid=\"127\" ssid=\"57\">Always guessing the majority class would result in an accuracy of 65.4%.</S>\n\t\t\t<S sid=\"128\" ssid=\"58\">The average human score is unknown and 6For more information, see TOEFL Synonym Questions (State of the art) at http://aclweb.org/aclwiki/.</S>\n\t\t\t<S sid=\"129\" ssid=\"59\">there are no previous results for comparison.</S>\n\t\t\t<S sid=\"130\" ssid=\"60\">3.4 Similar, Associated, and Both.</S>\n\t\t\t<S sid=\"131\" ssid=\"61\">A common criticism of corpus-based measures ofword similarity (as opposed to lexicon-based mea sures) is that they are merely detecting associations(co-occurrences), rather than actual semantic similarity (Lund et al, 1995).</S>\n\t\t\t<S sid=\"132\" ssid=\"62\">To address this criti cism, Lund et al (1995) evaluated their algorithm for measuring word similarity with word pairs that were labeled similar, associated, or both.</S>\n\t\t\t<S sid=\"133\" ssid=\"63\">Theselabeled pairs were originally created for cogni tive psychology experiments with human subjects(Chiarello et al, 1990).</S>\n\t\t\t<S sid=\"134\" ssid=\"64\">Table 7 shows some ex amples from this collection of 144 word pairs (48 pairs in each of the three classes).</S>\n\t\t\t<S sid=\"135\" ssid=\"65\">Word pair Class label table:bed similar music:art similar hair:fur similar house:cabin similar cradle:baby associated mug:beer associated camel:hump associated cheese:mouse associated ale:beer both uncle:aunt both pepper:salt both frown:smile both Table 7: Examples of word pairs labeled similar, associated, or both.</S>\n\t\t\t<S sid=\"136\" ssid=\"66\">Lund et al (1995) did not measure the accuracy of their algorithm on this three-class classification problem.</S>\n\t\t\t<S sid=\"137\" ssid=\"67\">Instead, following standard practice incognitive psychology, they showed that their al gorithm?s similarity scores for the 144 word pairs were correlated with the response times of human subjects in priming tests.</S>\n\t\t\t<S sid=\"138\" ssid=\"68\">In a typical priming test, a human subject reads a priming word (cradle) and is then asked to complete a partial word (complete bab as baby).</S>\n\t\t\t<S sid=\"139\" ssid=\"69\">The time required to perform thetask is taken to indicate the strength of the cogni tive link between the two words (cradle and baby).Using ten-fold cross-validation, PairClass at tains an accuracy of 77.1% on the 144 word pairs.</S>\n\t\t\t<S sid=\"140\" ssid=\"70\">Since the three classes are of equal size, guessing the majority class and random guessing both yield an accuracy of 33.3%.</S>\n\t\t\t<S sid=\"141\" ssid=\"71\">The average human score is unknown and there are no previous results for comparison.</S>\n\t\t\t<S sid=\"142\" ssid=\"72\">909</S>\n\t</SECTION>\n\t<SECTION title=\"Discussion. \" number=\"4\">\n\t\t\t<S sid=\"143\" ssid=\"1\">The four experiments are summarized in Tables 8 and 9.</S>\n\t\t\t<S sid=\"144\" ssid=\"2\">For the first two experiments, where there are previous results, PairClass is not the best, butit performs competitively.</S>\n\t\t\t<S sid=\"145\" ssid=\"3\">For the second two ex periments, PairClass performs significantly abovethe baselines.</S>\n\t\t\t<S sid=\"146\" ssid=\"4\">However, the strength of this ap proach is not its performance on any one task, but the range of tasks it can handle.As far as we know, this is the first time a standard supervised learning algorithm has been ap plied to any of these four problems.</S>\n\t\t\t<S sid=\"147\" ssid=\"5\">The advantageof being able to cast these problems in the frame work of standard supervised learning problems isthat we can now exploit the huge literature on su pervised learning.</S>\n\t\t\t<S sid=\"148\" ssid=\"6\">Past work on these problems has required implicitly coding our knowledge ofthe nature of the task into the structure of the algo rithm.</S>\n\t\t\t<S sid=\"149\" ssid=\"7\">For example, the structure of the algorithmfor latent semantic analysis (LSA) implicitly con tains a theory of synonymy (Landauer and Dumais, 1997).</S>\n\t\t\t<S sid=\"150\" ssid=\"8\">The problem with this approach is that it can be very difficult to work out how to modify the algorithm if it does not behave the way we want.On the other hand, with a supervised learning algo rithm, we can put our knowledge into the labeling of the feature vectors, instead of putting it directly into the algorithm.</S>\n\t\t\t<S sid=\"151\" ssid=\"9\">This makes it easier to guide the system to the desired behaviour.With our approach to the SAT analogy ques tions, we are blurring the line between supervised and unsupervised learning, since the training set for a given SAT question consists of a single realpositive example (and a single ?virtual?</S>\n\t\t\t<S sid=\"152\" ssid=\"10\">or ?simulated?</S>\n\t\t\t<S sid=\"153\" ssid=\"11\">negative example).</S>\n\t\t\t<S sid=\"154\" ssid=\"12\">In effect, a single example (mason:stone) becomes a sui generis; it con stitutes a class of its own.</S>\n\t\t\t<S sid=\"155\" ssid=\"13\">It may be possible to apply the machinery of supervised learning toother problems that apparently call for unsupervised learning (for example, clustering or measur ing similarity), by using this sui generis device.</S>\n\t</SECTION>\n\t<SECTION title=\"Related Work. \" number=\"5\">\n\t\t\t<S sid=\"156\" ssid=\"1\">One of the first papers using supervised ma chine learning to classify word pairs was Rosarioand Hearst?s (2001) paper on classifying nounmodifier pairs in the medical domain.</S>\n\t\t\t<S sid=\"157\" ssid=\"2\">For ex ample, the noun-modifier expression brain biopsy was classified as Procedure.</S>\n\t\t\t<S sid=\"158\" ssid=\"3\">Rosario and Hearst(2001) constructed feature vectors for each nounmodifier pair using MeSH (Medical Subject Headings) and UMLS (Unified Medical Language System) as lexical resources.</S>\n\t\t\t<S sid=\"159\" ssid=\"4\">They then trained a neu ral network to distinguish 13 classes of semantic relations, such as Cause, Location, Measure, andInstrument.</S>\n\t\t\t<S sid=\"160\" ssid=\"5\">Nastase and Szpakowicz (2003) explored a similar approach to classifying general domain noun-modifier pairs, using WordNet and Roget?s Thesaurus as lexical resources.</S>\n\t\t\t<S sid=\"161\" ssid=\"6\">Turney and Littman (2005) used corpus-based features for classifying noun-modifier pairs.</S>\n\t\t\t<S sid=\"162\" ssid=\"7\">Their features were based on 128 hand-coded patterns.</S>\n\t\t\t<S sid=\"163\" ssid=\"8\">They used a nearest-neighbour learning algorithm to classify general-domain noun-modifier pairsinto 30 different classes of semantic relations.</S>\n\t\t\t<S sid=\"164\" ssid=\"9\">Tur ney (2006) later addressed the same problem using 8000 automatically generated patterns.One of the tasks in SemEval 2007 was the clas sification of semantic relations between nominals (Girju et al, 2007).</S>\n\t\t\t<S sid=\"165\" ssid=\"10\">The problem is to classifysemantic relations between nouns and noun com pounds in the context of a sentence.</S>\n\t\t\t<S sid=\"166\" ssid=\"11\">The task attracted 14 teams who created 15 systems, all of which used supervised machine learning with features that were lexicon-based, corpus-based, or both.PairClass is most similar to the algorithm of Tur ney (2006), but it differs in the following ways:?</S>\n\t\t\t<S sid=\"167\" ssid=\"12\">PairClass does not use a lexicon to find syn onyms for the input word pairs.</S>\n\t\t\t<S sid=\"168\" ssid=\"13\">One of our goals in this paper is to show that a pure corpus-based algorithm can handle synonymswithout a lexicon.</S>\n\t\t\t<S sid=\"169\" ssid=\"14\">This considerably simpli fies the algorithm.</S>\n\t\t\t<S sid=\"170\" ssid=\"15\">PairClass uses a support vector machine (SVM) instead of a nearest neighbour (NN) learning algorithm.</S>\n\t\t\t<S sid=\"171\" ssid=\"16\">PairClass does not use the singular value decomposition (SVD) to smooth the feature vectors.</S>\n\t\t\t<S sid=\"172\" ssid=\"17\">It has been our experience that SVD is not necessary with SVMs.</S>\n\t\t\t<S sid=\"173\" ssid=\"18\">PairClass generates probability estimates,whereas Turney (2006) uses a cosine mea sure of similarity.</S>\n\t\t\t<S sid=\"174\" ssid=\"19\">Probability estimates canbe readily used in further downstream pro cessing, but cosines are less useful.?</S>\n\t\t\t<S sid=\"175\" ssid=\"20\">The automatically generated patterns in PairClass are slightly more general than the pat terns of Turney (2006).</S>\n\t\t\t<S sid=\"176\" ssid=\"21\">The morphological processing in PairClass (Minnen et al, 2001) is more sophisticated than in Turney (2006).</S>\n\t\t\t<S sid=\"177\" ssid=\"22\">910 Experiment Number of vectors Number of features Number of classes SAT Analogies 2,244 (374 ? 6) 44,880 (2, 244 ? 20) 374 TOEFL Synonyms 320 (80 ? 4) 6,400 (320 ? 20) 2 Synonyms and Antonyms 136 2,720 (136 ? 20) 2 Similar, Associated, and Both 144 2,880 (144 ? 20) 3 Table 8: Summary of the four tasks.</S>\n\t\t\t<S sid=\"178\" ssid=\"23\">See Section 3 for explanations.</S>\n\t\t\t<S sid=\"179\" ssid=\"24\">Experiment Accuracy Best previous Human Baseline Rank SAT Analogies 52.1% 56.1% 57.0% 20.0% 2 higher out of 12 TOEFL Synonyms 76.2% 97.5% 64.5% 25.0% 8 higher out of 15 Synonyms and Antonyms 75.0% none unknown 65.4% none Similar, Associated, and Both 77.1% none unknown 33.3% none Table 9: Summary of experimental results.</S>\n\t\t\t<S sid=\"180\" ssid=\"25\">See Section 3 for explanations.</S>\n\t\t\t<S sid=\"181\" ssid=\"26\">However, we believe that the main contribution of this paper is not PairClass itself, but the extension of supervised word pair classification beyond theclassification of noun-modifier pairs and semantic relations between nominals, to analogies, syn onyms, antonyms, and associations.</S>\n\t\t\t<S sid=\"182\" ssid=\"27\">As far as we know, this has not been done before.</S>\n\t</SECTION>\n\t<SECTION title=\"Limitations and Future Work. \" number=\"6\">\n\t\t\t<S sid=\"183\" ssid=\"1\">The main limitation of PairClass is the need for a large corpus.</S>\n\t\t\t<S sid=\"184\" ssid=\"2\">Phrases that contain a pair of wordstend to be more rare than phrases that contain either of the members of the pair, thus a large cor pus is needed to ensure that sufficient numbers of phrases are found for each input word pair.</S>\n\t\t\t<S sid=\"185\" ssid=\"3\">The size of the corpus has a cost in terms of disk spaceand processing time.</S>\n\t\t\t<S sid=\"186\" ssid=\"4\">In the future, as hardware im proves, this will become less of an issue, but there may be ways to improve the algorithm, so that a smaller corpus is sufficient.Another area for future work is to apply Pair Class to more tasks.</S>\n\t\t\t<S sid=\"187\" ssid=\"5\">WordNet includes more thana dozen semantic relations (e.g., synonyms, hy ponyms, hypernyms, meronyms, holonyms, and antonyms).</S>\n\t\t\t<S sid=\"188\" ssid=\"6\">PairClass should be applicable to allof these relations.</S>\n\t\t\t<S sid=\"189\" ssid=\"7\">Other potential applications in clude any task that involves semantic relations, such as word sense disambiguation, informationretrieval, information extraction, and metaphor in terpretation.</S>\n\t</SECTION>\n\t<SECTION title=\"Conclusion. \" number=\"7\">\n\t\t\t<S sid=\"190\" ssid=\"1\">In this paper, we have described a uniform approach to analogies, synonyms, antonyms, and as sociations, in which all of these phenomena are subsumed by analogies.</S>\n\t\t\t<S sid=\"191\" ssid=\"2\">We view the problem ofrecognizing analogies as the classification of se mantic relations between words.</S>\n\t\t\t<S sid=\"192\" ssid=\"3\">We believe that most of our lexical knowledge is relational, not attributional.</S>\n\t\t\t<S sid=\"193\" ssid=\"4\">That is, meaning is largely about relations among words, rather thanproperties of individual words, considered in isolation.</S>\n\t\t\t<S sid=\"194\" ssid=\"5\">For example, consider the knowledge en coded in WordNet: much of the knowledge in WordNet is embedded in the graph structure that connects words.</S>\n\t\t\t<S sid=\"195\" ssid=\"6\">Analogies of the form A:B::C:D are calledproportional analogies.</S>\n\t\t\t<S sid=\"196\" ssid=\"7\">These types of lowerlevel analogies may be contrasted with higher level analogies, such as the analogy between the solar system and Rutherford?s model of the atom (Falkenhainer et al, 1989), which are sometimes called conceptual analogies.</S>\n\t\t\t<S sid=\"197\" ssid=\"8\">We believe that the difference between these two types is largely a matter of complexity.</S>\n\t\t\t<S sid=\"198\" ssid=\"9\">A higher-level analogy is composed of many lower-level analogies.</S>\n\t\t\t<S sid=\"199\" ssid=\"10\">Progresswith algorithms for processing lower-level analo gies will eventually contribute to algorithms for higher-level analogies.The idea of subsuming a broad range of semantic phenomena under analogies has been sug gested by many researchers.</S>\n\t\t\t<S sid=\"200\" ssid=\"11\">Minsky (1986) wrote, ?How do we ever understand anything?</S>\n\t\t\t<S sid=\"201\" ssid=\"12\">Almost always, I think, by using one or another kind of analogy.?</S>\n\t\t\t<S sid=\"202\" ssid=\"13\">Hofstadter (2007) claimed, ?all meaningcomes from analogies.?</S>\n\t\t\t<S sid=\"203\" ssid=\"14\">In NLP, analogical algo rithms have been applied to machine translation(Lepage and Denoual, 2005), morphology (Lep age, 1998), and semantic relations (Turney and Littman, 2005).</S>\n\t\t\t<S sid=\"204\" ssid=\"15\">Analogy provides a frameworkthat has the potential to unify the field of seman tics.</S>\n\t\t\t<S sid=\"205\" ssid=\"16\">This paper is a small step towards that goal.</S>\n\t\t\t<S sid=\"206\" ssid=\"17\">AcknowledgementsThanks to Joel Martin and the anonymous review ers of Coling 2008 for their helpful comments.</S>\n\t\t\t<S sid=\"207\" ssid=\"18\">911</S>\n\t</SECTION>\n</PAPER>",
                    "generation_prompt_uid": "9a43fc6ac51cb8767c6c52601fe80fa3",
                    "item_type": "paper",
                    "llm_engine": "gpt-4-1106-preview",
                    "origin": "LLM",
                    "title": "A Uniform Approach to Analogies Synonyms Antonyms and Associations"
                }
            }
        }
    ]
}